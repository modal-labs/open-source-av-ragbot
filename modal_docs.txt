

================================================================================
SOURCE URL: https://modal.com/
================================================================================

at scale
=================================

Add one line of code to run any function in the cloud. Get instant
autoscaling for ML inference, data jobs, and more.

[Get Started](/signup)

Book a Demo

[Try the playground](/playground)

[![

](https://modal-cdn.com/tmp60ydwzz8_ea31268d.webp)](https://modal-cdn.com/landscape-vids/Modal_Hero_3MB.mp4)

[![Lovable](/_app/immutable/assets/Lovable.CoQIjurW.svg)](https://lovable.dev)

[![AI2](data:image/svg+xml,%3csvg%20width='76'%20height='24'%20viewBox='0%200%2076%2024'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cg%20clip-path='url(%23clip0_64_152)'%3e%3cpath%20d='M9.34029%209.7503H4.67015V5.30796H8.42905C8.93023%205.30796%209.34029%204.8979%209.34029%204.39672V0.637817H13.7826V5.30796C13.7826%207.76833%2011.7893%209.7503%209.34029%209.7503ZM4.67015%2010.2059H0V14.6483H3.7589C4.26009%2014.6483%204.67015%2015.0583%204.67015%2015.5595V19.3184H9.11248V14.6483C9.11248%2012.1879%207.11913%2010.2059%204.67015%2010.2059ZM19.6032%209.97811C19.102%209.97811%2018.692%209.56805%2018.692%209.06686V5.30796H14.2496V9.97811C14.2496%2012.4385%2016.243%2014.4204%2018.692%2014.4204H23.3621V9.97811H19.6032ZM9.56811%2019.3184V23.9886H14.0104V20.2297C14.0104%2019.7285%2014.4205%2019.3184%2014.9217%2019.3184H18.6806V14.8761H14.0104C11.5501%2014.8761%209.56811%2016.8694%209.56811%2019.3184Z'%20fill='%23DDFFDC'%20fill-opacity='1'/%3e%3cpath%20d='M53.4788%203.79307H49.0479V0.626483H53.4788V3.79307ZM50.0275%206.4243H47.3621V9.75036H49.6858C50.073%209.75036%2050.392%2010.0693%2050.392%2010.4566V24H53.9003V10.206C53.9003%207.757%2052.317%206.4243%2050.0389%206.4243H50.0275ZM39.3089%200.626483L47.9886%2023.9886H44.298L41.9971%2017.7807H31.1419L28.841%2023.9886H25.1846L33.8642%200.626483H39.3089ZM40.915%2014.8761L36.5752%203.18937L32.2354%2014.8761H40.915ZM62.1812%2021.084L69.3118%2015.457C73.4579%2012.1879%2075.2804%209.90982%2075.2804%206.77741C75.2804%203.64499%2073.2073%200%2066.9539%200C60.7005%200%2056.9074%205.08021%2056.9074%209.75036H60.7005C60.7005%205.53583%2062.5685%202.98434%2066.9539%202.98434C71.3393%202.98434%2071.4874%204.86379%2071.4874%206.81158C71.4874%208.75937%2070.9634%209.76175%2068.9245%2011.4134L56.9757%2021.0726V23.9886H75.7019V21.0726H62.1699L62.1812%2021.084Z'%20fill='%23DDFFDC'%20fill-opacity='1'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_64_152'%3e%3crect%20width='75.7019'%20height='24'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://allenai.org)

[![Harvey](data:image/svg+xml,%3csvg%20width='100'%20height='30'%20viewBox='0%200%20100%2030'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cg%20clip-path='url(%23clip0_32_72)'%3e%3cpath%20d='M16.8833%2021.1123L14.1079%2022.9956V23.1278H22.8965V22.9956L20.1211%2021.1123V2.01542L22.8965%200.132159V0H14.1079V0.132159L16.8833%202.01542V10.4075H6.01322V2.01542L8.78855%200.132159V0H0V0.132159L2.77533%202.01542V21.1123L0%2022.9956V23.1278H8.78855V22.9956L6.01322%2021.1123V11.8612H16.8833V21.1123ZM29.141%2023.4582C30.8921%2023.4582%2032.6432%2022.4339%2033.8656%2021.0132V23.1278H39.4163V22.9956L36.9053%2021.0463V11.5969C36.9053%208.39207%2034.6586%206.80617%2031.0573%206.80617C29.174%206.80617%2026.5969%207.43392%2024.9449%208.09471V12.1916H25.0771C27.0595%209.48238%2028.9427%208.12775%2030.826%208.12775C32.7093%208.12775%2033.8656%209.44934%2033.8656%2011.9273V13.5793L29.7687%2014.5374C26.0683%2015.3634%2024.4163%2016.8172%2024.4163%2019.0969C24.4163%2021.6079%2026.4648%2023.4582%2029.141%2023.4582ZM30.1982%2021.3436C28.7115%2021.3436%2027.6211%2020.2533%2027.6211%2018.7665C27.6211%2017.2797%2028.6123%2016.0903%2030.5617%2015.6608L33.8656%2014.9009V19.8568C32.478%2020.848%2031.2555%2021.3436%2030.1982%2021.3436ZM52.4339%2010.2423L59.0419%2023.5903H59.6366L66.1123%209.28414L68.0947%207.3348V7.20264H62.0485V7.30176L64.5595%209.11894L60.0991%2018.9317L55.2093%209.11894L57.6542%207.30176V7.20264H52.5C49.3612%207.20264%2047.2797%208.16079%2045.6278%2010.0441V6.97137L40.0771%208.25991V8.42511L42.5881%209.87885V21.0463L40.0771%2022.9956V23.1278H48.7996V22.9956L45.6278%2021.0463V11.5969C46.7841%2010.3084%2048.1718%209.71366%2049.6256%209.71366C50.5507%209.71366%2051.4097%209.84581%2052.4339%2010.2423ZM75.2643%2023.4582C78.304%2023.4582%2079.9559%2021.5749%2081.674%2019.7247L80.848%2018.9648C79.163%2020.5507%2077.8745%2021.2445%2076.1894%2021.2445C72.9515%2021.2445%2070.5396%2018.2709%2070.5396%2014.1079H81.5749V13.6123C81.5749%209.71366%2079.2621%206.80617%2075.4626%206.80617C71.2335%206.80617%2067.7643%2010.6057%2067.7643%2015.2974C67.7643%2020.022%2071.0683%2023.4582%2075.2643%2023.4582ZM78.2709%2012.6872H70.6388C71.0352%209.81278%2072.8524%208.16079%2074.9339%208.16079C77.0154%208.16079%2078.304%209.64758%2078.304%2012.0264C78.304%2012.3238%2078.304%2012.5551%2078.2709%2012.6872ZM83.9207%209.28414L90.7599%2022.9295L85.804%2030H89.5374L97.8634%209.28414L99.8458%207.3348V7.20264H93.7996V7.30176L96.3436%209.11894L92.2137%2019.13L87.1916%209.11894L89.6366%207.30176V7.20264H81.8062V7.3348L83.9207%209.28414Z'%20fill='%23DDFFDC'%20fill-opacity='1'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_32_72'%3e%3crect%20width='99.8458'%20height='30'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://harvey.ai)

[![You.com](/_app/immutable/assets/YouDotCom.V2KoO3lf.svg)](https://you.com)

[![Cognition](/_app/immutable/assets/Cognition.DodUjy-h.svg)](https://cognition.ai)

[![OpenPipe](/_app/immutable/assets/OpenPipeGreen.ClRCnlxx.svg)](https://openpipe.ai)

[![Codegen](/_app/immutable/assets/Codegen.ibfW0-BL.svg)](https://codegen.com)

Sub-second
container starts
---------------------------

[![

](https://modal-cdn.com/tmpqemq5_7y_40c13ce7.webp)](https://modal-cdn.com/landscape-vids/Modal_Sequence_Part1_No_UI.mp4)

[

](https://modal-cdn.com/landscape-vids/Modal_Sequence_Part1_Only_UI-hevc-safari.mp4)

We built a Rust-based container stack from scratch so you can iterate
as quickly in the cloud as you can locally.

[View Docs](/docs/guide)

Zero
config files
-----------------

[![

](https://modal-cdn.com/tmp1__y6h68_b7d80106.webp)](https://modal-cdn.com/landscape-vids/Modal_Sequence_Part2_No_UI.mp4)

[

](https://modal-cdn.com/landscape-vids/Modal_Sequence_Part2_Only_UI-hevc-safari.mp4)

Easily define hardware and container requirements next to your Python
functions.

[View Docs](/docs/guide/gpu)

Scale to
hundreds of GPUs
in seconds
------------------------------------

[![

](https://modal-cdn.com/tmpccmwguat_bcf14b78.webp)](https://modal-cdn.com/landscape-vids/Modal_Sequence_Part3_No_UI.mp4)

[

](https://modal-cdn.com/landscape-vids/Modal_Sequence_Part3_Only_UI-hevc-safari.mp4)

Never worry about hitting rate limits again. We autoscale containers
for your functions instantly.

[View Docs](/docs/guide/scale)

Use Cases
---------

### Generative AI Inference that scales with you

[View Examples](/docs/examples/trtllm_latency)

---

Fast cold boots

Load gigabytes of weights in seconds with our optimized container file system.

---

Bring your own code

Deploy anything from custom models to popular frameworks.

---

Seamless autoscaling

Handle bursty and unpredictable load by scaling to thousands of GPUs and back down to zero.

Generate

---

Fast cold boots

---

Bring your own code

---

Seamless autoscaling

[View Examples](/docs/examples/trtllm_latency)

### Fine-tuning and training without managing infrastructure

[View Examples](/docs/examples/llm-finetuning)

---

Start training immediately

Provision Nvidia A100 and H100 GPUs in seconds. Your drivers and custom packages are already there.

---

Never wait in line

Run as many experiments as you need to, in parallel. Stop paying for idle GPUs when you’re done.

---

Cloud storage

Mount weights and data in distributed volumes, then access them wherever they’re needed.

![Fine-tuning graphic](https://modal-cdn.com/tmp_d4wxk9j_76c56b72.webp)

---

Start training immediately

---

Never wait in line

---

Cloud storage

[View Examples](/docs/examples/llm-finetuning)

### Batch processing optimized for high-volume workloads

[View Examples](/docs/examples/s3_bucket_mount)

---

Supercomputing scale

Serverless, but for high-performance compute. Run things on massive amounts of CPU and memory.

---

Serverless pricing

Pay only for resources consumed, by the second, as you spin up containers.

---

Powerful compute primitives

Simple fan-out parallelism that scales to thousands of containers, with a single line of Python.

![Batch processing graphic](https://modal-cdn.com/tmpwql3zd3v_a00ed2e5.webp)

---

Supercomputing scale

---

Serverless pricing

---

Powerful compute primitives

[View Examples](/docs/examples/s3_bucket_mount)

Build anything with Modal

[Language Models](/use-cases/language-models)
[Image, Video, 3D](/use-cases/image-video-3d)
[Audio Processing](/use-cases/audio)
[Fine-Tuning](/use-cases/fine-tuning)
[Batch Processing](/use-cases/job-queues)
[Sandboxed Code](/use-cases/sandboxes)
[Computational Bio](/use-cases/comp-bio)

[Language Models](/use-cases/language-models)
[Image, Video, 3D](/use-cases/image-video-3d)
[Audio Processing](/use-cases/audio)
[Fine-Tuning](/use-cases/fine-tuning)
[Batch Processing](/use-cases/job-queues)
[Sandboxed Code](/use-cases/sandboxes)
[Computational Bio](/use-cases/comp-bio)

Features
--------

Flexible Environments

Bring your own image or build one in Python, scale resources as needed, and leverage state-of-the-art GPUs like H100s & A100s for high-performance computing.

Seamless Integrations

Export function logs to Datadog or any OpenTelemetry-compatible provider, and easily mount cloud storage from major providers (S3, R2 etc.).

Data Storage

Manage data effortlessly with storage solutions (network volumes, key-value stores and queues). Provision storage types and interact with them using familiar Python syntax.

Job Scheduling

Take control of your workloads with powerful scheduling. Set up cron jobs, retries, and timeouts, or use batching to optimize resource usage.

Web Endpoints

Deploy and manage web services with ease. Create custom domains, set up streaming and websockets, and serve functions as secure HTTPS endpoints.

Built-In Debugging

Troubleshoot efficiently with built-in debugging tools. Use the modal shell for interactive debugging and set breakpoints to pinpoint issues quickly.

---

Flexible Environments

---

Seamless Integrations

---

Data Storage

---

Job Scheduling

---

Web Endpoints

---

Built-In Debugging

[![

](https://modal-cdn.com/tmpm9makfxp_73c169ef.webp)](https://modal-cdn.com/landscape-vids/Modal_Bars-hevc-safari.mp4)

Only pay when your

code is
running

Scale up to hundreds of nodes and down to zero within seconds. Pay for
actual compute, by the CPU cycle. With $30 of compute on us, every
month.

### Compute costs

Per hour

Per second

Per hour

Per second

---

GPU Tasks

Nvidia B200

$0.001736
/ sec

Nvidia H200

$0.001261
/ sec

Nvidia H100

$0.001097
/ sec

Nvidia A100, 80 GB

$0.000694
/ sec

Nvidia A100, 40 GB

$0.000583
/ sec

Nvidia L40S

$0.000542
/ sec

Nvidia A10G

$0.000306
/ sec

Nvidia L4

$0.000222
/ sec

Nvidia T4

$0.000164
/ sec

---

CPU

Physical core

(2 vCPU
equivalent
)

$0.0000131
/ core / sec

\*minimum of 0.125 cores per container

---

Memory

$0.00000222
/ GiB / sec

For teams

of all scales

Starter

For small teams and independent developers looking to level up.

Team

For startups and larger organizations looking to scale quickly.

Enterprise

For organizations prioritizing security, support, and reliability.

[View Pricing](/pricing)

Security
and governance
-----------------------

---

Built on top of gVisor

---

SOC 2 and HIPAA

---

Region support

---

SSO sign in for enterprise

[Learn More](/docs/guide/security)

[![

](https://modal-cdn.com/tmpikav2uo5_78a38e65.webp)](https://modal-cdn.com/landscape-vids/Modal_Security.mp4)

---

Built on top of gVisor

---

SOC 2 and HIPAA

---

Region support

---

SSO sign in for enterprise

[Learn More](/docs/guide/security)

Built with Modal
----------------

[View all](/docs/examples)

[### Deploy an OpenAI-compatible LLM service

Run large language models with a drop-in replacement for the OpenAI API](/docs/examples/vllm_inference)

[### Custom pet art from Flux with Hugging Face and Gradio

Fine-tune an image generation model on pictures of your pet](/docs/examples/dreambooth_app)

[### Run llama.cpp

Run DeepSeek-R1 and Phi-4 on llama.cpp](/docs/examples/llama_cpp)

[### Sandbox a LangGraph agent's code

Run an LLM coding agent that runs its own language models](/docs/examples/agent)

[### Transcribe speech in batches with Whisper

Turn audio bytes into text at scale](/docs/examples/batched_whisper)

[### Voice chat with LLMs

Build an interactive voice chat app](/docs/examples/llm-voice-chat)

[### Edit images with Flux Kontext

Transform images with SotA diffusion models](/docs/examples/image_to_image)

[### Fold proteins with Boltz-2

Predict molecular structures and binding affinities from sequences with SotA open source models](/docs/examples/boltz_predict)

[### Serverless WebRTC

Stream YOLO detections on webcam footage in real time](/docs/examples/webrtc_yolo)

[### Serve diffusion models

Serve Flux on Modal with optimizations for blazingly fast inference](/docs/examples/flux)

[### Serverless TensorRT-LLM (LLaMA 3 8B)

Run interactive language model applications](/docs/examples/trtllm_latency)

[### Transcribe speech with Kyutai STT

Stream transcripts at the speed of speech](/docs/examples/streaming_kyutai_stt)

[### Star in custom music videos

Fine-tune a Wan2.1 video model on your face and run it in parallel](/docs/examples/music-video-gen)

[### Create music

Turn prompts into music with MusicGen](/docs/examples/musicgen)

[### RAG Chat with PDFs

Use ColBERT-style, multimodal embeddings with a Vision-Language Model to answer questions about documents](/docs/examples/chat_with_pdf_vision)

[### Bring images to life

Prompt a generative video model to animate an image](/docs/examples/image_to_video)

[### Fast podcast transcriptions

Build an end-to-end podcast transcription app that leverages dozens of containers for super-fast processing](/docs/examples/whisper-transcriber)

[### Build a protein folding dashboard

Serve a web UI for a protein model with ESM3, Molstar, and Gradio](/docs/examples/esm3)

[### Deploy a Hacker News Slackbot

Periodically post new Hacker News posts to Slack](/docs/examples/hackernews_alerts)

[### Fold proteins with Chai-1

Predict molecular structures from sequences with SotA open source models](/docs/examples/chai1)

[### Retrieval-Augmented Generation (RAG) for Q&A

Build a question-answering web endpoint that can cite its sources](/docs/examples/potus_speech_qanda)

[### Document OCR job queue

Use Modal as an infinitely scalable job queue that can service async tasks from a web app](/docs/examples/doc_ocr_jobs)

[### Parallel processing of Parquet files on S3

Analyze data from the Taxi and Limousine Commission of NYC in parallel](/docs/examples/s3_bucket_mount)

> “Modal Sandboxes enable us to execute generated code securely and flexibly. We expedited the development of our code interpreter feature integrated into Le Chat.”

Wendy Shang
, AI Scientist

> “Modal makes it easy to write code that runs on 100s of GPUs in parallel, transcribing podcasts in a fraction of the time.”

Mike Cohen
, Head of Data

> “Tasks that would have taken days to complete take minutes instead. We’ve saved thousands of dollars deploying LLMs on Modal.”

Rahul Sengottuvelu
, Head of Applied AI

> “The beauty of Modal is that all you need to know is that you can scale your function calls in the cloud with a few lines of Python.”

Georg Kucsko
, Co-founder and CTO

Join
Modal's developer

community

[Modal Community Slack](/slack)

[![Twitter profile @garrrikkotua](https://pbs.twimg.com/profile_images/1757781535267180544/TR1Coi0v.jpg)](https://twitter.com/garrrikkotua/status/1786042460143247506)
[Igor Kotua

Engineer, The Linux Foundation](https://twitter.com/garrrikkotua/status/1786042460143247506)

If you building AI stuff with Python and haven't tried
[@modal\_labs](https://x.com/modal_labs)

you are missing out big time

[![Twitter profile @danrothenberg](https://modal-cdn.com/cdnbot/tmpndzqy7fc_1fe4563b.webp)](https://twitter.com/danrothenberg/status/1835055915516805301)
[Daniel Rothenberg

Co-founder, Brightband](https://twitter.com/danrothenberg/status/1835055915516805301)

[@modal\_labs](https://x.com/modal_labs)

continues to be magical... 10 minutes of effort and the `joblib`-based parallelism I use to test on my local machine can trivially scale out on the cloud. Makes life so easy!

[![Twitter profile @erinselene](https://pbs.twimg.com/profile_images/1595233713536704513/j2d9PYiK.jpg)](https://twitter.com/erinselene/status/1601060264102678528)
[Erin Boyle

ML Engineer, Tesla](https://twitter.com/erinselene/status/1601060264102678528)

This tool is awesome. So empowering to have your infra needs met with just a couple decorators. Good people, too!

[![Twitter profile @jai_chopra](https://pbs.twimg.com/profile_images/1664774588772020225/TlVJPiQu.jpg)](https://twitter.com/jai_chopra/status/1661033887819268096)
[Jai Chopra

Product, LanceDB](https://twitter.com/jai_chopra/status/1661033887819268096)

Recently built an app on Lambda and just started to use
[@modal\_labs](https://x.com/modal_labs)

, the difference is insane! Modal is amazing, virtually no cold start time, onboarding experience is great 🚀

[![Twitter profile @dieegosf](https://pbs.twimg.com/profile_images/1623322737836949504/fYVRjQXS.jpg)](https://twitter.com/dieegosf/status/1811018060200874157)
[Diego Fernandes

Co-founder & CTO, RocketSeat](https://twitter.com/dieegosf/status/1811018060200874157)

Probably one of the best piece of software I'm using this year:
[modal.com](https://modal.com/)

[![Twitter profile @AAAzzam](https://pbs.twimg.com/profile_images/1656519795976687619/abuB5K8p.jpg)](https://twitter.com/AAAzzam/status/1793118336525447302)
[Adam Azzam

Product, Prefect](https://twitter.com/AAAzzam/status/1793118336525447302)

feels weird at this point to use anything else than
[@modal\_labs](https://x.com/modal_labs)

for this — absolutely the GOAT of dynamic sandboxes

[![Twitter profile @remilouf](https://pbs.twimg.com/profile_images/1570519314142318595/PJGWEkfu.jpg)](https://twitter.com/remilouf/status/1845742524997963800)
[Rémi 📎

Co-founder & CEO, .txt](https://twitter.com/remilouf/status/1845742524997963800)

Nothing beats
[@modal\_labs](https://x.com/modal_labs)

when it comes to deploying a quick POC

[![Twitter profile @holdenmatt](https://pbs.twimg.com/profile_images/1675120765761654784/jJz2F_r9.jpg)](https://twitter.com/holdenmatt/status/1797695485479915795)
[Matt Holden

Founder](https://twitter.com/holdenmatt/status/1797695485479915795)

Late to the party, but finally playing with
[@modal\_labs](https://x.com/modal_labs)

to run some backend jobs.
DX is sooo nice (compared to Docker, Cloud Run, Lambda, etc). Just decorate a Python function and deploy. And it's fast! Love it.

[![Twitter profile @garrrikkotua](https://pbs.twimg.com/profile_images/1757781535267180544/TR1Coi0v.jpg)](https://twitter.com/garrrikkotua/status/1786042460143247506)
[Igor Kotua

Engineer, The Linux Foundation](https://twitter.com/garrrikkotua/status/1786042460143247506)

If you building AI stuff with Python and haven't tried
[@modal\_labs](https://x.com/modal_labs)

you are missing out big time

[![Twitter profile @danrothenberg](https://modal-cdn.com/cdnbot/tmpndzqy7fc_1fe4563b.webp)](https://twitter.com/danrothenberg/status/1835055915516805301)
[Daniel Rothenberg

Co-founder, Brightband](https://twitter.com/danrothenberg/status/1835055915516805301)

[@modal\_labs](https://x.com/modal_labs)

continues to be magical... 10 minutes of effort and the `joblib`-based parallelism I use to test on my local machine can trivially scale out on the cloud. Makes life so easy!

[![Twitter profile @erinselene](https://pbs.twimg.com/profile_images/1595233713536704513/j2d9PYiK.jpg)](https://twitter.com/erinselene/status/1601060264102678528)
[Erin Boyle

ML Engineer, Tesla](https://twitter.com/erinselene/status/1601060264102678528)

This tool is awesome. So empowering to have your infra needs met with just a couple decorators. Good people, too!

[![Twitter profile @jai_chopra](https://pbs.twimg.com/profile_images/1664774588772020225/TlVJPiQu.jpg)](https://twitter.com/jai_chopra/status/1661033887819268096)
[Jai Chopra

Product, LanceDB](https://twitter.com/jai_chopra/status/1661033887819268096)

Recently built an app on Lambda and just started to use
[@modal\_labs](https://x.com/modal_labs)

, the difference is insane! Modal is amazing, virtually no cold start time, onboarding experience is great 🚀

[![Twitter profile @dieegosf](https://pbs.twimg.com/profile_images/1623322737836949504/fYVRjQXS.jpg)](https://twitter.com/dieegosf/status/1811018060200874157)
[Diego Fernandes

Co-founder & CTO, RocketSeat](https://twitter.com/dieegosf/status/1811018060200874157)

Probably one of the best piece of software I'm using this year:
[modal.com](https://modal.com/)

[![Twitter profile @AAAzzam](https://pbs.twimg.com/profile_images/1656519795976687619/abuB5K8p.jpg)](https://twitter.com/AAAzzam/status/1793118336525447302)
[Adam Azzam

Product, Prefect](https://twitter.com/AAAzzam/status/1793118336525447302)

feels weird at this point to use anything else than
[@modal\_labs](https://x.com/modal_labs)

for this — absolutely the GOAT of dynamic sandboxes

[![Twitter profile @remilouf](https://pbs.twimg.com/profile_images/1570519314142318595/PJGWEkfu.jpg)](https://twitter.com/remilouf/status/1845742524997963800)
[Rémi 📎

Co-founder & CEO, .txt](https://twitter.com/remilouf/status/1845742524997963800)

Nothing beats
[@modal\_labs](https://x.com/modal_labs)

when it comes to deploying a quick POC

[![Twitter profile @holdenmatt](https://pbs.twimg.com/profile_images/1675120765761654784/jJz2F_r9.jpg)](https://twitter.com/holdenmatt/status/1797695485479915795)
[Matt Holden

Founder](https://twitter.com/holdenmatt/status/1797695485479915795)

Late to the party, but finally playing with
[@modal\_labs](https://x.com/modal_labs)

to run some backend jobs.
DX is sooo nice (compared to Docker, Cloud Run, Lambda, etc). Just decorate a Python function and deploy. And it's fast! Love it.

[![Twitter profile @calebfahlgren](https://pbs.twimg.com/profile_images/1716604301563289600/YycgFNAn.jpg)](https://twitter.com/calebfahlgren/status/1825733420976124199)
[Caleb

ML Engineer, Hugging Face](https://twitter.com/calebfahlgren/status/1825733420976124199)

Bullish on
[@modal\_labs](https://x.com/modal_labs)

- Great Docs + Examples
- Healthy Free Plan (30$ free compute / month)
- Never have to worry about infra / just Python

[![Twitter profile @mattzcarey](https://modal-cdn.com/cdnbot/tmpbisrydal_4d61419e.webp)](https://twitter.com/mattzcarey/status/1806003178691006905)
[@mattzcarey.com on blsky

AI Engineer, StackOne](https://twitter.com/mattzcarey/status/1806003178691006905)

[@modal\_labs](https://x.com/modal_labs)

has got a bunch of stuff just worked out
this should be how you deploy python apps. wow

[![Twitter profile @_amankishore](https://modal-cdn.com/aman_kishore.jpg)](https://twitter.com/_amankishore/status/1669845359634575360)
[Aman Kishore

Research Engineer, Harvey](https://twitter.com/_amankishore/status/1669845359634575360)

If you are still using AWS Lambda instead of
[@modal\_labs](https://x.com/modal_labs)

you're not moving fast enough

[![Twitter profile @isidoremiller](https://pbs.twimg.com/profile_images/1679859339073404930/_PB_4LM0.jpg)](https://twitter.com/isidoremiller/status/1645953205480878080)
[Izzy Miller

DevRel, Hex](https://twitter.com/isidoremiller/status/1645953205480878080)

special shout out to
[@modal\_labs](https://x.com/modal_labs)

and
[@\_hex\_tech](https://x.com/_hex_tech)

for providing the crucial infrastructure to run this! Modal is the coolest tool I’ve tried in a really long time— cannnot say enough good things.

[![Twitter profile @marktenenholtz](https://pbs.twimg.com/profile_images/1468741945560289283/YZ3cOr_H.jpg)](https://twitter.com/marktenenholtz/status/1784348202545614937)
[Mark Tenenholtz

Head of AI, PredeloHQ](https://twitter.com/marktenenholtz/status/1784348202545614937)

I use
[@modal\_labs](https://x.com/modal_labs)

because it brings me joy. There isn't much more to it.

[![Twitter profile @schrockn](https://pbs.twimg.com/profile_images/1452768656711032833/Vq8wBRJc.jpg)](https://twitter.com/schrockn/status/1787504282700255676)
[Nick Schrock

Founder, Dagster Labs](https://twitter.com/schrockn/status/1787504282700255676)

I have tried
[@modal\_labs](https://x.com/modal_labs)

and am now officially Modal-pilled. Great work
[@bernhardsson](https://x.com/bernhardsson)

and team. Every hyperscalar should be trying this out and immediately pivoting their compute teams' roadmaps to match this DX.

[![Twitter profile @moinnadeem](https://pbs.twimg.com/profile_images/1252656439149105156/dU40XVKb.jpg)](https://twitter.com/moinnadeem/status/1814729047181832484)
[Moin Nadeem

Co-founder, Phonic](https://twitter.com/moinnadeem/status/1814729047181832484)

I've realized
[@modal\_labs](https://x.com/modal_labs)

is actually a great fit for ML training pipelines.
If you're running model-based evals, why not just call a serverless Modal function and have it evaluate your model on a separate worker GPU? This makes evaluation during training really easy.

[![Twitter profile @calebfahlgren](https://pbs.twimg.com/profile_images/1716604301563289600/YycgFNAn.jpg)](https://twitter.com/calebfahlgren/status/1825733420976124199)
[Caleb

ML Engineer, Hugging Face](https://twitter.com/calebfahlgren/status/1825733420976124199)

Bullish on
[@modal\_labs](https://x.com/modal_labs)

- Great Docs + Examples
- Healthy Free Plan (30$ free compute / month)
- Never have to worry about infra / just Python

[![Twitter profile @mattzcarey](https://modal-cdn.com/cdnbot/tmpbisrydal_4d61419e.webp)](https://twitter.com/mattzcarey/status/1806003178691006905)
[@mattzcarey.com on blsky

AI Engineer, StackOne](https://twitter.com/mattzcarey/status/1806003178691006905)

[@modal\_labs](https://x.com/modal_labs)

has got a bunch of stuff just worked out
this should be how you deploy python apps. wow

[![Twitter profile @_amankishore](https://modal-cdn.com/aman_kishore.jpg)](https://twitter.com/_amankishore/status/1669845359634575360)
[Aman Kishore

Research Engineer, Harvey](https://twitter.com/_amankishore/status/1669845359634575360)

If you are still using AWS Lambda instead of
[@modal\_labs](https://x.com/modal_labs)

you're not moving fast enough

[![Twitter profile @isidoremiller](https://pbs.twimg.com/profile_images/1679859339073404930/_PB_4LM0.jpg)](https://twitter.com/isidoremiller/status/1645953205480878080)
[Izzy Miller

DevRel, Hex](https://twitter.com/isidoremiller/status/1645953205480878080)

special shout out to
[@modal\_labs](https://x.com/modal_labs)

and
[@\_hex\_tech](https://x.com/_hex_tech)

for providing the crucial infrastructure to run this! Modal is the coolest tool I’ve tried in a really long time— cannnot say enough good things.

[![Twitter profile @marktenenholtz](https://pbs.twimg.com/profile_images/1468741945560289283/YZ3cOr_H.jpg)](https://twitter.com/marktenenholtz/status/1784348202545614937)
[Mark Tenenholtz

Head of AI, PredeloHQ](https://twitter.com/marktenenholtz/status/1784348202545614937)

I use
[@modal\_labs](https://x.com/modal_labs)

because it brings me joy. There isn't much more to it.

[![Twitter profile @schrockn](https://pbs.twimg.com/profile_images/1452768656711032833/Vq8wBRJc.jpg)](https://twitter.com/schrockn/status/1787504282700255676)
[Nick Schrock

Founder, Dagster Labs](https://twitter.com/schrockn/status/1787504282700255676)

I have tried
[@modal\_labs](https://x.com/modal_labs)

and am now officially Modal-pilled. Great work
[@bernhardsson](https://x.com/bernhardsson)

and team. Every hyperscalar should be trying this out and immediately pivoting their compute teams' roadmaps to match this DX.

[![Twitter profile @moinnadeem](https://pbs.twimg.com/profile_images/1252656439149105156/dU40XVKb.jpg)](https://twitter.com/moinnadeem/status/1814729047181832484)
[Moin Nadeem

Co-founder, Phonic](https://twitter.com/moinnadeem/status/1814729047181832484)

I've realized
[@modal\_labs](https://x.com/modal_labs)

is actually a great fit for ML training pipelines.
If you're running model-based evals, why not just call a serverless Modal function and have it evaluate your model on a separate worker GPU? This makes evaluation during training really easy.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/docs/guide
================================================================================

Introduction
============

Modal is a cloud function platform that lets you:

* Run any code remotely within seconds.
* Define
  [container environments](/docs/guide/images)

  in code (or use one of our pre-built backends).
* [Scale out horizontally](/docs/guide/scale)

  to thousands of containers.
* Attach
  [GPUs](/docs/guide/gpu)

  with a single line of code.
* Serve your functions as
  [web endpoints](/docs/guide/webhooks)

  .
* Deploy and monitor
  [persistent scheduled jobs](/docs/guide/cron)

  .
* Store data in distributed versions of
  [dictionaries](/docs/guide/dicts)

  and
  [queues](/docs/guide/queues)

  .

You get
[full serverless execution and pricing](/pricing)

, because we host everything and charge per second of usage. Notably, there’s zero configuration in Modal - everything is code. Take a breath of fresh air and feel how good it tastes with no YAML in it.

Getting started
---------------

The nicest thing about all of this is that
**you don’t have to set up any
infrastructure.**
Just:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the
   `modal`
   Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )

…and you can start running jobs right away. Check out some of our simple getting started examples:

* [Hello, world!](/docs/examples/hello_world)
* [A simple web scraper](/docs/examples/web-scraper)

You can also learn Modal interactively without installing anything through our
[code playground](/playground)

.

How does it work?
-----------------

Modal takes your code, puts it in a container, and executes it in the cloud.

Where does it run? Modal runs it in its own cloud environment. The benefit is
that we solve all the hard infrastructure problems for you, so you don’t have to
do anything. You don’t need to mess with Kubernetes, Docker or even an AWS
account.

Modal is currently Python-only, but we may support other languages in the
future.

[Introduction](#introduction)

[Getting started](#getting-started)

[How does it work?](#how-does-it-work)

See it in action

[Hello, world!](/docs/examples/hello_world)

[A simple web scraper](/docs/examples/web-scraper)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/apps
================================================================================

Apps, Functions, and entrypoints
================================

An
`App`
is the object that represents an application running on Modal.
All functions and classes are associated with an
[`App`](/docs/reference/modal.App#modalapp)

.

When you
[`run`](/docs/reference/cli/run)

or
[`deploy`](/docs/reference/cli/deploy)

an
`App`
, it creates an ephemeral or a
deployed
`App`
, respectively.

You can view a list of all currently running Apps on the
[`apps`](/apps)

page.

Ephemeral Apps
--------------

An ephemeral App is created when you use the
[`modal run`](/docs/reference/cli/run)

CLI command, or the
[`app.run`](/docs/reference/modal.App#run)

method. This creates a temporary
App that only exists for the duration of your script.

Ephemeral Apps are stopped automatically when the calling program exits, or when
the server detects that the client is no longer connected.
You can use
[`--detach`](/docs/reference/cli/run)

in order to keep an ephemeral App running even
after the client exits.

By using
`app.run`
you can run your Modal apps from within your Python scripts:

```
def main():
    ...
    with app.run():
        some_modal_function.remote()
```

By default, running your app in this way won’t propagate Modal logs and progress bar messages. To enable output, use the
[`modal.enable_output`](/docs/reference/modal.enable_output)

context manager:

```
def main():
    ...
    with modal.enable_output():
        with app.run():
            some_modal_function.remote()
```

Deployed Apps
-------------

A deployed App is created using the
[`modal deploy`](/docs/reference/cli/deploy)

CLI command. The App is persisted indefinitely until you delete it via the
[web UI](/apps)

. Functions in a deployed App that have an attached
[schedule](/docs/guide/cron)

will be run on a schedule. Otherwise, you can
invoke them manually using
[web endpoints or Python](/docs/guide/trigger-deployed-functions)

.

Deployed Apps are named via the
[`App`](/docs/reference/modal.App#modalapp)

constructor. Re-deploying an existing
`App`
(based on the name) will update it
in place.

Entrypoints for ephemeral Apps
------------------------------

The code that runs first when you
`modal run`
an App is called the “entrypoint”.

You can register a local entrypoint using the
[`@app.local_entrypoint()`](/docs/reference/modal.App#local_entrypoint)

decorator. You can also use a regular Modal function as an entrypoint, in which
case only the code in global scope is executed locally.

### Argument parsing

If your entrypoint function takes arguments with primitive types,
`modal run`
automatically parses them as CLI options. For example, the following function
can be called with
`modal run script.py --foo 1 --bar "hello"`
:

```
# script.py

@app.local_entrypoint()
def main(foo: int, bar: str):
    some_modal_function.remote(foo, bar)
```

If you wish to use your own argument parsing library, such as
`argparse`
, you can instead accept a variable-length argument list for your entrypoint or your function. In this case, Modal skips CLI parsing and forwards CLI arguments as a tuple of strings. For example, the following function can be invoked with
`modal run my_file.py --foo=42 --bar="baz"`
:

```
import argparse

@app.function()
def train(*arglist):
    parser = argparse.ArgumentParser()
    parser.add_argument("--foo", type=int)
    parser.add_argument("--bar", type=str)
    args = parser.parse_args(args = arglist)
```

### Manually specifying an entrypoint

If there is only one
`local_entrypoint`
registered,
[`modal run script.py`](/docs/reference/cli/run)

will automatically use it. If
you have no entrypoint specified, and just one decorated Modal function, that
will be used as a remote entrypoint instead. Otherwise, you can direct
`modal run`
to use a specific entrypoint.

For example, if you have a function decorated with
[`@app.function()`](/docs/reference/modal.App#function)

in your file:

```
# script.py

@app.function()
def f():
    print("Hello world!")

@app.function()
def g():
    print("Goodbye world!")

@app.local_entrypoint()
def main():
    f.remote()
```

Running
[`modal run script.py`](/docs/reference/cli/run)

will execute the
`main`
function locally, which would call the
`f`
function remotely. However you can
instead run
`modal run script.py::app.f`
or
`modal run script.py::app.g`
to
execute
`f`
or
`g`
directly.

Apps were once Stubs
--------------------

The
`modal.App`
class in the client was previously called
`modal.Stub`
. The
old name was kept as an alias for some time, but from Modal 1.0.0 onwards,
using
`modal.Stub`
will result in an error.

[Apps, Functions, and entrypoints](#apps-functions-and-entrypoints)

[Ephemeral Apps](#ephemeral-apps)

[Deployed Apps](#deployed-apps)

[Entrypoints for ephemeral Apps](#entrypoints-for-ephemeral-apps)

[Argument parsing](#argument-parsing)

[Manually specifying an entrypoint](#manually-specifying-an-entrypoint)

[Apps were once Stubs](#apps-were-once-stubs)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/async
================================================================================

Asynchronous API usage
======================

All of the functions in Modal are available in both standard (blocking) and
asynchronous variants. The async interface can be accessed by appending
`.aio`
to any function in the Modal API.

For example, instead of
`my_modal_function.remote("hello")`
in a blocking
context, you can use
`await my_modal_function.remote.aio("hello")`
to get an
asynchronous coroutine response, for use with Python’s
`asyncio`
library.

```
import asyncio
import modal

app = modal.App()

@app.function()
async def myfunc():
    ...

@app.local_entrypoint()
async def main():
    # execute 100 remote calls to myfunc in parallel
    await asyncio.gather(*[myfunc.remote.aio() for i in range(100)])
```

This is an advanced feature. If you are comfortable with asynchronous
programming, you can use this to create arbitrary parallel execution patterns,
with the added benefit that any Modal functions will be executed remotely.

Async functions
---------------

Regardless if you use an async runtime (like
`asyncio`
) in your usage of
*Modal
itself*
, you are free to define your
`app.function`
-decorated function bodies
as either async or blocking. Both kinds of definitions will work for remote
Modal function calls from both any context.

An async function can call a blocking function, and vice versa.

```
@app.function()
def blocking_function():
    return 42

@app.function()
async def async_function():
    x = await blocking_function.remote.aio()
    return x * 10

@app.local_entrypoint()
def blocking_main():
    print(async_function.remote())  # => 420
```

If a function is configured to support multiple concurrent inputs per container,
the behavior varies slightly between blocking and async contexts:

* In a blocking context, concurrent inputs will run on separate Python threads.
  These are subject to the GIL, but they can still lead to race conditions if
  used with non-threadsafe objects.
* In an async context, concurrent inputs are simply scheduled as coroutines on
  the executor thread. Everything remains single-threaded.

[Asynchronous API usage](#asynchronous-api-usage)

[Async functions](#async-functions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/batch-processing
================================================================================

Batch Processing
================

Modal is optimized for large-scale batch processing, allowing functions to scale to thousands of parallel containers with zero additional configuration. Function calls can be submitted asynchronously for background execution, eliminating the need to wait for jobs to finish or tune resource allocation.

This guide covers Modal’s batch processing capabilities, from basic invocation to integration with existing pipelines.

Background Execution with
`.spawn_map`
--------------------------------------

The fastest way to submit multiple jobs for asynchronous processing is by invoking a function with
`.spawn_map`
. When combined with the
[`--detach`](/docs/reference/cli/run)

flag, your App continues running until all jobs are completed.

Here’s an example of submitting 100,000 videos for parallel embedding. You can disconnect after submission, and the processing will continue to completion in the background:

```
# Kick off asynchronous jobs with `modal run --detach batch_processing.py`
import modal

app = modal.App("batch-processing-example")
volume = modal.Volume.from_name("video-embeddings", create_if_missing=True)

@app.function(volumes={"/data": volume})
def embed_video(video_id: int):
    # Business logic:
    # - Load the video from the volume
    # - Embed the video
    # - Save the embedding to the volume
    ...

@app.local_entrypoint()
def main():
    embed_video.spawn_map(range(100_000))
```

This pattern works best for jobs that store results externally—for example, in a
[Modal Volume](/docs/guide/volumes)

,
[Cloud Bucket Mount](/docs/guide/cloud-bucket-mounts)

, or your own database\*.

*\* For database connections, consider using
[Modal Proxy](/docs/guide/proxy-ips)

to maintain a static IP across thousands of containers.*

Parallel Processing with
`.map`
-------------------------------

Using
`.map`
allows you to offload expensive computations to powerful machines while gathering results. This is particularly useful for pipeline steps with bursty resource demands. Modal handles all infrastructure provisioning and de-provisioning automatically.

Here’s how to implement parallel video similarity queries as a single Modal function call:

```
# Run jobs and collect results with `modal run gather.py`
import modal

app = modal.App("gather-results-example")

@app.function(gpu="L40S")
def compute_video_similarity(query: str, video_id: int) -> tuple[int, int]:
    # Embed video with GPU acceleration & compute similarity with query
    return video_id, score

@app.local_entrypoint()
def main():
    import itertools

    queries = itertools.repeat("Modal for batch processing")
    video_ids = range(100_000)

    for video_id, score in compute_video_similarity.map(queries, video_ids):
        # Process results (e.g., extract top 5 most similar videos)
        pass
```

This example runs
`compute_video_similarity`
on an autoscaling pool of L40S GPUs, returning scores to a local process for further processing.

Integration with Existing Systems
---------------------------------

The recommended way to use Modal Functions within your existing data pipeline is through
[deployed function invocation](/docs/guide/trigger-deployed-functions)

. After deployment, you can call Modal functions from external systems:

```
def external_function(inputs):
    compute_similarity = modal.Function.from_name(
        "gather-results-example",
        "compute_video_similarity"
    )
    for result in compute_similarity.map(inputs):
        # Process results
        pass
```

You can invoke Modal Functions from any Python context, gaining access to built-in observability, resource management, and GPU acceleration.

[Batch Processing](#batch-processing)

[Background Execution with .spawn\_map](#background-execution-with-spawn_map)

[Parallel Processing with .map](#parallel-processing-with-map)

[Integration with Existing Systems](#integration-with-existing-systems)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/cloud-bucket-mounts
================================================================================

Cloud bucket mounts
===================

The
[`modal.CloudBucketMount`](/docs/reference/modal.CloudBucketMount)

is a
mutable volume that allows for both reading and writing files from a cloud
bucket. It supports AWS S3, Cloudflare R2, and Google Cloud Storage buckets.

Cloud bucket mounts are built on top of AWS’
[`mountpoint`](https://github.com/awslabs/mountpoint-s3)

technology and inherits
its limitations. See the
[Limitations and troubleshooting](#limitations-and-troubleshooting)

section for more details.

Mounting Cloudflare R2 buckets
------------------------------

`CloudBucketMount`
enables Cloudflare R2 buckets to be mounted as file system
volumes. Because Cloudflare R2 is
[S3-Compatible](https://developers.cloudflare.com/r2/api/s3/api/)

the setup is
very similar between R2 and S3. See
[modal.CloudBucketMount](/docs/reference/modal.CloudBucketMount#modalcloudbucketmount)

for usage instructions.

When creating the R2 API token for use with the mount, you need to have the
ability to read, write, and list objects in the specific buckets you will mount.
You do
*not*
need admin permissions, and you should
*not*
use “Client IP Address
Filtering”.

Mounting Google Cloud Storage buckets
-------------------------------------

`CloudBucketMount`
enables Google Cloud Storage (GCS) buckets to be mounted as file system
volumes. See
[modal.CloudBucketMount](/docs/reference/modal.CloudBucketMount#modalcloudbucketmount)

for GCS setup instructions.

Mounting S3 buckets
-------------------

`CloudBucketMount`
enables S3 buckets to be mounted as file system volumes. To
interact with a bucket, you must have the appropriate IAM permissions configured
(refer to the section on
[IAM Permissions](#iam-permissions)

).

```
import modal
import subprocess

app = modal.App()

s3_bucket_name = "s3-bucket-name"  # Bucket name not ARN.
s3_access_credentials = modal.Secret.from_dict({
    "AWS_ACCESS_KEY_ID": "...",
    "AWS_SECRET_ACCESS_KEY": "...",
    "AWS_REGION": "..."
})

@app.function(
    volumes={
        "/my-mount": modal.CloudBucketMount(s3_bucket_name, secret=s3_access_credentials)
    }
)
def f():
    subprocess.run(["ls", "/my-mount"])
```

### Specifying S3 bucket region

Amazon S3 buckets are associated with a single AWS Region.
[`Mountpoint`](https://github.com/awslabs/mountpoint-s3)

attempts to automatically detect the region for your S3 bucket at startup time and directs all S3 requests to that region. However, in certain scenarios, like if your container is running on an AWS worker in a certain region, while your bucket is in a different region, this automatic detection may fail.

To avoid this issue, you can specify the region of your S3 bucket by adding an
`AWS_REGION`
key to your Modal secrets, as in the code example above.

### Using AWS temporary security credentials

`CloudBucketMount`
s also support AWS temporary security credentials by passing
the additional environment variable
`AWS_SESSION_TOKEN`
. Temporary credentials
will expire and will not get renewed automatically. You will need to update
the corresponding Modal Secret in order to prevent failures.

You can get temporary credentials with the
[AWS CLI](https://aws.amazon.com/cli/)

with:

```
$ aws configure export-credentials --format env
export AWS_ACCESS_KEY_ID=XXX
export AWS_SECRET_ACCESS_KEY=XXX
export AWS_SESSION_TOKEN=XXX...
```

All these values are required.

### Using OIDC identity tokens

Modal provides
[OIDC integration](/docs/guide/oidc-integration)

and will automatically generate identity tokens to authenticate to AWS.
OIDC eliminates the need for manual token passing through Modal secrets and is based on short-lived tokens, which limits the window of exposure if a token is compromised.
To use this feature, you must
[configure AWS to trust Modal’s OIDC provider](/docs/guide/oidc-integration#step-1-configure-aws-to-trust-modals-oidc-provider)

and
[create an IAM role that can be assumed by Modal Functions](/docs/guide/oidc-integration#step-2-create-an-iam-role-that-can-be-assumed-by-modal-functions)

.

Then, you specify the IAM role that your Modal Function should assume to access the S3 bucket.

```
import modal

app = modal.App()

s3_bucket_name = "s3-bucket-name"
role_arn = "arn:aws:iam::123456789abcd:role/s3mount-role"

@app.function(
    volumes={
        "/my-mount": modal.CloudBucketMount(
            bucket_name=s3_bucket_name,
            oidc_auth_role_arn=role_arn
        )
    }
)
def f():
    subprocess.run(["ls", "/my-mount"])
```

### Mounting a path within a bucket

To mount only the files under a specific subdirectory, you can specify a path prefix using
`key_prefix`
.
Since this prefix specifies a directory, it must end in a
`/`
.
The entire bucket is mounted when no prefix is supplied.

```
import modal
import subprocess

app = modal.App()

s3_bucket_name = "s3-bucket-name"
prefix = 'path/to/dir/'

s3_access_credentials = modal.Secret.from_dict({
    "AWS_ACCESS_KEY_ID": "...",
    "AWS_SECRET_ACCESS_KEY": "...",
})

@app.function(
    volumes={
        "/my-mount": modal.CloudBucketMount(
            bucket_name=s3_bucket_name,
            key_prefix=prefix,
            secret=s3_access_credentials
        )
    }
)
def f():
    subprocess.run(["ls", "/my-mount"])
```

This will only mount the files in the bucket
`s3-bucket-name`
that are prefixed by
`path/to/dir/`
.

### Read-only mode

To mount a bucket in read-only mode, set
`read_only=True`
as an argument.

```
import modal
import subprocess

app = modal.App()

s3_bucket_name = "s3-bucket-name"  # Bucket name not ARN.
s3_access_credentials = modal.Secret.from_dict({
    "AWS_ACCESS_KEY_ID": "...",
    "AWS_SECRET_ACCESS_KEY": "...",
})

@app.function(
    volumes={
        "/my-mount": modal.CloudBucketMount(s3_bucket_name, secret=s3_access_credentials, read_only=True)
    }
)
def f():
    subprocess.run(["ls", "/my-mount"])
```

While S3 mounts support both write and read operations, they are optimized for
reading large files sequentially. Certain file operations, such as renaming
files, are not supported. For a comprehensive list of supported operations,
consult the
[Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md)

.

### IAM permissions

To utilize
`CloudBucketMount`
for reading and writing files from S3 buckets,
your IAM policy must include permissions for
`s3:PutObject`
,
`s3:AbortMultipartUpload`
, and
`s3:DeleteObject`
. These permissions are not
required for mounts configured with
`read_only=True`
.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Sid": "ModalListBucketAccess",
      "Effect": "Allow",
      "Action": ["s3:ListBucket"],
      "Resource": ["arn:aws:s3:::<MY-S3-BUCKET>"]
    },
    {
      "Sid": "ModalBucketAccess",
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:AbortMultipartUpload",
        "s3:DeleteObject"
      ],
      "Resource": ["arn:aws:s3:::<MY-S3-BUCKET>/*"]
    }
  ]
}
```

Limitations and troubleshooting
-------------------------------

Cloud Bucket Mounts have certain limitations that do not apply to
[Volumes](/docs/guide/volumes)

.
These limitations are primarily around the way that files can be opened and edited in Cloud Bucket Mounts. For
a comprehensive list of limitations, see the
[Mountpoint troubleshooting documentation](https://github.com/awslabs/mountpoint-s3/blob/a6179c72bfc237a1fdd06eb4a0863ca537f8d8a7/doc/TROUBLESHOOTING.md)

and the
[Mountpoint semantics documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md)

.

The most common issues that users encounter are:

* Files cannot be opened in append mode.
* Files cannot be written to at arbitrary offsets i.e.
  `seek`
  and write are not supported together.
* To write to a file, you must open it in
  `truncate`
  mode.

These operations typically result in a
`PermissionError: [Errno 1] Operation not permitted`
error.

If you need these features, give
[Volumes](/docs/guide/volumes)

a try! If you need these features in S3
and are willing to pay extra for your bucket, you may be able to use
[S3 Express](https://aws.amazon.com/s3/storage-classes/express-one-zone/)

.
Contact us
[in our Slack](https://modal.com/slack)

if you’re interested in using S3 Express.

### Writing files in append mode

If you’re using a library which must open a file in append mode, it’s best to write to a temporary file
and then move it to your bucket’s mount path. A similar approach can be used to write to a file at an arbitrary offset.

```
import tempfile
import shutil

@app.function(
    volumes={"/bucket": modal.CloudBucketMount("my-bucket", secret=s3_credentials)}
)
def append_to_log():
    # Write to a temporary file that supports append mode
    with tempfile.NamedTemporaryFile(mode='a', delete=False) as temp_file:
        temp_file.write("Log entry 1\n")
        temp_file.write("Log entry 2\n")
        temp_path = temp_file.name

    # Move the completed file to the bucket mount
    shutil.move(temp_path, "/bucket/logfile.txt")
```

### Creating a file without a parent directory

If you try to create a file in a directory that doesn’t exist, you’ll get a
`Operation not permitted`
error.
To fix this, create the parent directory first with
`Path(dst).parent.mkdir(exist_ok=True, parents=True)`
.

### Using `np.savez`

`np.savez`
seeks to random offsets in a file, making it unsafe for Cloud Bucket Mounts. If your file is large,
you can write it to a temporary file and then move it to your bucket’s mount path. If it’s small, however,
you can solve this with an in-memory buffer:

```
import io
import numpy as np
import shutil

data = np.random.rand(1000, 512)

# 1. Build the archive entirely in memory
tmp = io.BytesIO()
np.savez_compressed(tmp, array=data)

# 2. Copy it once, sequentially, to the mount point
dest = "/bucket/data.npz"
with open(dest, "wb") as f:
    shutil.copyfileobj(tmp, f)
```

### Torchtune writing checkpoint files

Old versions of
[Torchtune](https://github.com/pytorch/torchtune)

are incompatible with Cloud Bucket Mounts.
Upgrade to a version greater than or equal to
`0.6.1`
to ensure checkpoints can be written to the bucket.

### Using the TensorBoard `SummaryWriter`

The TensorBoard
`SummaryWriter`
opens log files in append mode. These files are quite small, though,
so we recommend writing to a temporary directory and using the
[Watchdog](https://github.com/gorakhargosh/watchdog)

Python library to copy the files to the bucket mount path as they come in.

This is a case where it may be worth it to use
[Volumes](/docs/guide/volumes)

instead - in particular,
training logs are sometimes not subject to the same compliance requirements that force something like checkpoints
or model weights to be stored in a secure location. We even have an example of
[how to use TensorBoard on Volumes](/docs/examples/torch_profiling#serving-tensorboard-on-modal-to-view-pytorch-profiles-and-traces)

.

[Cloud bucket mounts](#cloud-bucket-mounts)

[Mounting Cloudflare R2 buckets](#mounting-cloudflare-r2-buckets)

[Mounting Google Cloud Storage buckets](#mounting-google-cloud-storage-buckets)

[Mounting S3 buckets](#mounting-s3-buckets)

[Specifying S3 bucket region](#specifying-s3-bucket-region)

[Using AWS temporary security credentials](#using-aws-temporary-security-credentials)

[Using OIDC identity tokens](#using-oidc-identity-tokens)

[Mounting a path within a bucket](#mounting-a-path-within-a-bucket)

[Read-only mode](#read-only-mode)

[IAM permissions](#iam-permissions)

[Limitations and troubleshooting](#limitations-and-troubleshooting)

[Writing files in append mode](#writing-files-in-append-mode)

[Creating a file without a parent directory](#creating-a-file-without-a-parent-directory)

[Using np.savez](#using-npsavez)

[Torchtune writing checkpoint files](#torchtune-writing-checkpoint-files)

[Using the TensorBoard SummaryWriter](#using-the-tensorboard-summarywriter)

See it in action

[Mount S3 buckets in Modal apps](/docs/examples/s3_bucket_mount)

[Create a LoRA Playground with Modal, Gradio, and S3](/docs/examples/cloud_bucket_mount_loras)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/cold-start
================================================================================

Cold start performance
======================

Modal Functions are run in
[containers](/docs/guide/images)

.

If a container is already ready to run your Function, it will be reused.

If not, Modal spins up a new container.
This is known as a
*cold start*
,
and it is often associated with higher latency.

There are two sources of increased latency during cold starts:

1. inputs may
   **spend more time waiting**
   in a queue for a container
   to become ready or “warm”.
2. when an input is handled by the container that just started,
   there may be
   **extra work that only needs to be done on the first invocation**
   (“initialization”).

This guide presents techniques and Modal features for reducing the impact of both queueing
and initialization on observed latencies.

If you are invoking Functions with no warm containers
or if you otherwise see inputs spending too much time in the “pending” state,
you should
[target queueing time for optimization](#reduce-time-spent-queueing-for-warm-containers)

.

If you see some Function invocations taking much longer than others,
and those invocations are the first handled by a new container,
you should
[target initialization for optimization](#reduce-latency-from-initialization)

.

Reduce time spent queueing for warm containers
----------------------------------------------

New containers are booted when there are not enough other warm containers to
to handle the current number of inputs.

For example, the first time you send an input to a Function,
there are zero warm containers and there is one input,
so a single container must be booted up.
The total latency for the input will include
the time it takes to boot a container.

If you send another input right after the first one finishes,
there will be one warm container and one pending input,
and no new container will be booted.

Generalizing, there are two factors that affect the time inputs spend queueing:
the time it takes for a container to boot and become warm (which we solve by booting faster)
and the time until a warm container is available to handle an input (which we solve by having more warm containers).

### Warm up containers faster

The time taken for a container to become warm
and ready for inputs can range from seconds to minutes.

Modal’s custom container stack has been heavily optimized to reduce this time.
Containers boot in about one second.

But before a container is considered warm and ready to handle inputs,
we need to execute any logic in your code’s global scope (such as imports)
or in any
[`modal.enter`
methods](/docs/guide/lifecycle-functions)

.
So if your boots are slow, these are the first places to work on optimization.

For example, you might be downloading a large model from a model server
during the boot process.
You can instead
[download the model ahead of time](/docs/guide/model-weights)

,
so that it only needs to be downloaded once.

For models in the tens of gigabytes,
this can reduce boot times from minutes to seconds.

### Run more warm containers

It is not always possible to speed up boots sufficiently.
For example, seconds of added latency to load a model may not
be acceptable in an interactive setting.

In this case, the only option is to have more warm containers running.
This increases the chance that an input will be handled by a warm container,
for example one that finishes an input while another container is booting.

Modal currently exposes
[three parameters](/docs/guide/scale)

that control how
many containers will be warm:
`scaledown_window`
,
`min_containers`
,
and
`buffer_containers`
.

All of these strategies can increase the resources consumed by your Function
and so introduce a trade-off between cold start latencies and cost.

#### Keep containers warm for longer with `scaledown_window`

Modal containers will remain idle for a short period before shutting down. By
default, the maximum idle time is 60 seconds. You can configure this by setting
the
`scaledown_window`
on the
[`@function`](/docs/reference/modal.App#function)

decorator. The value is measured in seconds, and it can be set anywhere between
two seconds and twenty minutes.

```
import modal

app = modal.App()

@app.function(scaledown_window=300)
def my_idle_greeting():
    return {"hello": "world"}
```

Increasing the
`scaledown_window`
reduces the chance that subsequent requests
will require a cold start, although you will be billed for any resources used
while the container is idle (e.g., GPU reservation or residual memory
occupancy). Note that containers will not necessarily remain alive for the
entire window, as the autoscaler will scale down more agressively when the
Function is substantially over-provisioned.

#### Overprovision resources with `min_containers` and `buffer_containers`

Keeping already warm containers around longer doesn’t help if there are no warm
containers to begin with, as when Functions scale from zero.

To keep some containers warm and running at all times, set the
`min_containers`
value on the
[`@function`](/docs/reference/modal.App#function)

decorator. This
puts a floor on the the number of containers so that the Function doesn’t scale
to zero. Modal will still scale up and spin down more containers as the
demand for your Function fluctuates above the
`min_containers`
value, as usual.

While
`min_containers`
overprovisions containers while the Function is idle,
`buffer_containers`
provisions extra containers while the Function is active.
This “buffer” of extra containers will be idle and ready to handle inputs if
the rate of requests increases. This parameter is particularly useful for
bursty request patterns, where the arrival of one input predicts the arrival of more inputs,
like when a new user or client starts hitting the Function.

```
import modal

app = modal.App(image=modal.Image.debian_slim().pip_install("fastapi"))

@app.function(min_containers=3, buffer_containers=3)
def my_warm_greeting():
    return "Hello, world!"
```

Reduce latency from initialization
----------------------------------

Some work is done the first time that a function is invoked
but can be used on every subsequent invocation.
This is
[*amortized work*](https://www.cs.cornell.edu/courses/cs312/2006sp/lectures/lec18.html)

done at initialization.

For example, you may be using a large pre-trained model
whose weights need to be loaded from disk to memory the first time it is used.

This results in longer latencies for the first invocation of a warm container,
which shows up in the application as occasional slow calls: high tail latency or elevated p9Xs.

### Move initialization work out of the first invocation

Some work done on the first invocation can be moved up and completed ahead of time.

Any work that can be saved to disk, like
[downloading model weights](/docs/guide/model-weights)

,
should be done as early as possible. The results can be included in the
[container’s Image](/docs/guide/images)

or saved to a
[Modal Volume](/docs/guide/volumes)

.

Some work is tricky to serialize, like spinning up a network connection or an inference server.
If you can move this initialization logic out of the function body and into the global scope or a
[container
`enter`
method](https://modal.com/docs/guide/lifecycle-functions#enter)

,
you can move this work into the warm up period.
Containers will not be considered warm until all
`enter`
methods have completed,
so no inputs will be routed to containers that have yet to complete this initialization.

For more on how to use
`enter`
with machine learning model weights, see
[this guide](/docs/guide/model-weights)

.

Note that
`enter`
doesn’t get rid of the latency —
it just moves the latency to the warm up period,
where it can be handled by
[running more warm containers](#run-more-warm-containers)

.

### Share initialization work across cold starts with memory snapshots

Cold starts can also be made faster by using memory snapshots.

Invocations of a Function after the first
are faster in part because the memory is already populated
with values that otherwise need to be computed or read from disk,
like the contents of imported libraries.

Memory snapshotting captures the state of a container’s memory
at user-controlled points after it has been warmed up
and reuses that state in future boots, which can substantially
reduce cold start latency penalties and warm up period duration.

Refer to the
[memory snapshot](/docs/guide/memory-snapshot)

guide for details.

### Optimize initialization code

Sometimes, there is nothing to be done but to speed this work up.

Here, we share specific patterns that show up in optimizing initialization
in Modal Functions.

#### Load multiple large files concurrently

Often Modal applications need to read large files into memory (eg. model
weights) before they can process inputs. Where feasible these large file
reads should happen concurrently and not sequentially. Concurrent IO takes
full advantage of our platform’s high disk and network bandwidth
to reduce latency.

One common example of slow sequential IO is loading multiple independent
Huggingface
`transformers`
models in series.

```
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration
model_a = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor_a = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
model_b = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large")
processor_b = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
```

The above snippet does four
`.from_pretrained`
loads sequentially.
None of the components depend on another being already loaded in memory, so they
can be loaded concurrently instead.

They could instead be loaded concurrently using a function like this:

```
from concurrent.futures import ThreadPoolExecutor, as_completed
from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration

def load_models_concurrently(load_functions_map: dict) -> dict:
    model_id_to_model = {}
    with ThreadPoolExecutor(max_workers=len(load_functions_map)) as executor:
        future_to_model_id = {
            executor.submit(load_fn): model_id
            for model_id, load_fn in load_functions_map.items()
        }
        for future in as_completed(future_to_model_id.keys()):
            model_id_to_model[future_to_model_id[future]] = future.result()
    return model_id_to_model

components = load_models_concurrently({
    "clip_model": lambda: CLIPModel.from_pretrained("openai/clip-vit-base-patch32"),
    "clip_processor": lambda: CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32"),
    "blip_model": lambda: BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large"),
    "blip_processor": lambda: BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")
})
```

If performing concurrent IO on large file reads does
*not*
speed up your cold
starts, it’s possible that some part of your function’s code is holding the
Python
[GIL](https://wiki.python.org/moin/GlobalInterpreterLock)

and reducing
the efficacy of the multi-threaded executor.

[Cold start performance](#cold-start-performance)

[Reduce time spent queueing for warm containers](#reduce-time-spent-queueing-for-warm-containers)

[Warm up containers faster](#warm-up-containers-faster)

[Run more warm containers](#run-more-warm-containers)

[Keep containers warm for longer with scaledown\_window](#keep-containers-warm-for-longer-with-scaledown_window)

[Overprovision resources with min\_containers and buffer\_containers](#overprovision-resources-with-min_containers-and-buffer_containers)

[Reduce latency from initialization](#reduce-latency-from-initialization)

[Move initialization work out of the first invocation](#move-initialization-work-out-of-the-first-invocation)

[Share initialization work across cold starts with memory snapshots](#share-initialization-work-across-cold-starts-with-memory-snapshots)

[Optimize initialization code](#optimize-initialization-code)

[Load multiple large files concurrently](#load-multiple-large-files-concurrently)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/concurrent-inputs
================================================================================

Input concurrency
=================

As traffic to your application increases, Modal will automatically scale up the
number of containers running your Function:

By default, each container will be assigned one input at a time. Autoscaling
across containers allows your Function to process inputs in parallel. This is
ideal when the operations performed by your Function are CPU-bound.

For some workloads, though, it is inefficient for containers to process inputs
one-by-one. Modal supports these workloads with its
*input concurrency*
feature,
which allows individual containers to process multiple inputs at the same time:

When used effectively, input concurrency can reduce latency and lower costs.

Use cases
---------

Input concurrency can be especially effective for workloads that are primarily
I/O-bound, e.g.:

* Querying a database
* Making external API requests
* Making remote calls to other Modal Functions

For such workloads, individual containers may be able to concurrently process
large numbers of inputs with minimal additional latency. This means that your
Modal application will be more efficient overall, as it won’t need to scale
containers up and down as traffic ebbs and flows.

Another use case is to leverage
*continuous batching*
on GPU-accelerated
containers. Frameworks such as
[vLLM](/docs/examples/vllm_inference)

can
achieve the benefits of batching across multiple inputs even when those
inputs do not arrive simultaneously (because new batches are formed for each
forward pass of the model).

Note that for CPU-bound workloads, input concurrency will likely not be as
effective (or will even be counterproductive), and you may want to use
Modal’s
[*dynamic batching*
feature](/docs/guide/dynamic-batching)

instead.

Enabling input concurrency
--------------------------

To enable input concurrency, add the
`@modal.concurrent`
decorator:

```
@app.function()
@modal.concurrent(max_inputs=100)
def my_function(input: str):
    ...
```

When using the class pattern, the decorator should be applied at the level of
the
*class*
, not on individual methods:

```
@app.cls()
@modal.concurrent(max_inputs=100)
class MyCls:

    @modal.method()
    def my_method(self, input: str):
        ...
```

Because all methods on a class will be served by the same containers, a class
with input concurrency enabled will concurrently run distinct methods in
addition to multiple inputs for the same method.

**Note:**
The
`@modal.concurrent`
decorator was added in v0.73.148 of the Modal
Python SDK. Input concurrency could previously be enabled by setting the
`allow_concurrent_inputs`
parameter on the
`@app.function`
decorator.

Setting a concurrency target
----------------------------

When using the
`@modal.concurrent`
decorator, you must always configure the
maximum number of inputs that each container will concurrently process. If
demand exceeds this limit, Modal will automatically scale up more containers.

Additional inputs may need to queue up while these additional containers cold
start. To help avoid degraded latency during scaleup, the
`@modal.concurrent`
decorator has a separate
`target_inputs`
parameter. When set, Modal’s autoscaler
will aim for this target as it provisions resources. If demand increases faster
than new containers can spin up, the active containers will be allowed to burst
above the target up to the
`max_inputs`
limit:

```
@app.function()
@modal.concurrent(max_inputs=120, target_inputs=100)  # Allow a 20% burst
def my_function(input: str):
    ...
```

It may take some experimentation to find the right settings for these parameters
in your particular application. Our suggestion is to set the
`target_inputs`
based on your desired latency and the
`max_inputs`
based on resource constraints
(i.e., to avoid GPU OOM). You may also consider the relative latency cost of
scaling up a new container versus overloading the existing containers.

Concurrency mechanisms
----------------------

Modal uses different concurrency mechanisms to execute your Function depending
on whether it is defined as synchronous or asynchronous. Each mechanism imposes
certain requirements on the Function implementation. Input concurrency is an
advanced feature, and it’s important to make sure that your implementation
complies with these requirements to avoid unexpected behavior.

For synchronous Functions, Modal will execute concurrent inputs on separate
threads.
*This means that the Function implementation must be thread-safe.*

```
# Each container can execute up to 10 inputs in separate threads
@app.function()
@modal.concurrent(max_inputs=10)
def sleep_sync():
    # Function must be thread-safe
    time.sleep(1)
```

For asynchronous Functions, Modal will execute concurrent inputs using
separate
`asyncio`
tasks on a single thread. This does not require thread
safety, but it does mean that the Function needs to participate in
collaborative multitasking (i.e., it should not block the event loop).

```
# Each container can execute up to 10 inputs with separate async tasks
@app.function()
@modal.concurrent(max_inputs=10)
async def sleep_async():
    # Function must not block the event loop
    await asyncio.sleep(1)
```

Gotchas
-------

Input concurrency is a powerful feature, but there are a few caveats that can
be useful to be aware of before adopting it.

### Input cancellations

Synchronous and asynchronous Functions handle input cancellations differently.
Modal will raise a
`modal.exception.InputCancellation`
exception in synchronous
Functions and an
`asyncio.CancelledError`
in asynchronous Functions.

When using input concurrency with a synchronous Function, a single input
cancellation will terminate the entire container. If your workflow depends on
graceful input cancellations, we recommend using an asynchronous
implementation.

### Concurrent logging

The separate threads or tasks that are executing the concurrent inputs will
write any logs to the same stream. This makes it difficult to associate logs
with a specific input, and filtering for a specific function call in Modal’s web
dashboard will show logs for all inputs running at the same time.

To work around this, we recommend including a unique identifier in the messages
you log (either your own identifier or the
`modal.current_input_id()`
) so that
you can use the search functionality to surface logs for a specific input:

```
@app.function()
@modal.concurrent(max_inputs=10)
async def better_concurrent_logging(x: int):
    logger.info(f"{modal.current_input_id()}: Starting work with {x}")
```

[Input concurrency](#input-concurrency)

[Use cases](#use-cases)

[Enabling input concurrency](#enabling-input-concurrency)

[Setting a concurrency target](#setting-a-concurrency-target)

[Concurrency mechanisms](#concurrency-mechanisms)

[Gotchas](#gotchas)

[Input cancellations](#input-cancellations)

[Concurrent logging](#concurrent-logging)

See it in action

[Single GPU serving concurrent requests](/docs/examples/vllm_inference)

[Responsive Stable Diffusion web UI](/docs/examples/stable_diffusion_cli)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/continuous-deployment
================================================================================

Continuous deployment
=====================

It’s a common pattern to auto-deploy your Modal App as part of a CI/CD pipeline.
To get you started, below is a guide to doing continuous deployment of a Modal
App in GitHub.

GitHub Actions
--------------

Here’s a sample GitHub Actions workflow that deploys your App on every push to
the
`main`
branch.

This requires you to create a
[Modal token](/settings/tokens)

and add it as a
[secret for your Github Actions workflow](https://github.com/Azure/actions-workflow-samples/blob/master/assets/create-secrets-for-GitHub-workflows.md)

.

After setting up secrets, create a new workflow file in your repository at
`.github/workflows/ci-cd.yml`
with the following contents:

```
name: CI/CD

on:
  push:
    branches:
      - main

jobs:
  deploy:
    name: Deploy
    runs-on: ubuntu-latest
    env:
      MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}
      MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install Modal
        run: |
          python -m pip install --upgrade pip
          pip install modal

      - name: Deploy job
        run: |
          modal deploy -m my_package.my_file
```

Be sure to replace
`my_package.my_file`
with your actual entrypoint.

If you use multiple Modal
[Environments](/docs/guide/environments)

, you can
additionally specify the target environment in the YAML using
`MODAL_ENVIRONMENT=xyz`
.

[Continuous deployment](#continuous-deployment)

[GitHub Actions](#github-actions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/cron
================================================================================

Scheduling remote cron jobs
===========================

A common requirement is to perform some task at a given time every day or week
automatically. Modal facilitates this through function schedules.

Basic scheduling
----------------

Let’s say we have a Python module
`heavy.py`
with a function,
`perform_heavy_computation()`
.

```
# heavy.py
def perform_heavy_computation():
    ...

if __name__ == "__main__":
    perform_heavy_computation()
```

To schedule this function to run once per day, we create a Modal App and attach
our function to it with the
`@app.function`
decorator and a schedule parameter:

```
# heavy.py
import modal

app = modal.App()

@app.function(schedule=modal.Period(days=1))
def perform_heavy_computation():
    ...
```

To activate the schedule, deploy your app, either through the CLI:

```
modal deploy --name daily_heavy heavy.py
```

Or programmatically:

```
if __name__ == "__main__":
   app.deploy()
```

Now the function will run every day, at the time of the initial deployment,
without any further interaction on your part.

When you make changes to your function, just rerun the deploy command to
overwrite the old deployment.

Note that when you redeploy your function,
`modal.Period`
resets, and the
schedule will run X hours after this most recent deployment.

If you want to run your function at a regular schedule not disturbed by deploys,
`modal.Cron`
(see below) is a better option.

Monitoring your scheduled runs
------------------------------

To see past execution logs for the scheduled function, go to the
[Apps](https://modal.com/apps)

section on the Modal web site.

Schedules currently cannot be paused. Instead the schedule should be removed and
the app redeployed. Schedules can be started manually on the app’s dashboard
page, using the “run now” button.

Schedule types
--------------

There are two kinds of base schedule values -
[`modal.Period`](/docs/reference/modal.Period)

and
[`modal.Cron`](/docs/reference/modal.Cron)

.

[`modal.Period`](/docs/reference/modal.Period)

lets you specify an interval
between function calls, e.g.
`Period(days=1)`
or
`Period(hours=5)`
:

```
# runs once every 5 hours
@app.function(schedule=modal.Period(hours=5))
def perform_heavy_computation():
    ...
```

[`modal.Cron`](/docs/reference/modal.Cron)

gives you finer control using
[cron](https://en.wikipedia.org/wiki/Cron)

syntax:

```
# runs at 8 am (UTC) every Monday
@app.function(schedule=modal.Cron("0 8 * * 1"))
def perform_heavy_computation():
    ...

# runs daily at 6 am (New York time)
@app.function(schedule=modal.Cron("0 6 * * *", timezone="America/New_York"))
def send_morning_report():
    ...
```

For more details, see the API reference for
[Period](/docs/reference/modal.Period)

,
[Cron](/docs/reference/modal.Cron)

and
[Function](/docs/reference/modal.Function)

[Scheduling remote cron jobs](#scheduling-remote-cron-jobs)

[Basic scheduling](#basic-scheduling)

[Monitoring your scheduled runs](#monitoring-your-scheduled-runs)

[Schedule types](#schedule-types)

See it in action

[Hacker News Slackbot](/docs/examples/hackernews_alerts)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/cuda
================================================================================

Using CUDA on Modal
===================

Modal makes it easy to accelerate your workloads with datacenter-grade NVIDIA GPUs.

To take advantage of the hardware, you need to use matching software: the CUDA stack.
This guide explains the components of that stack and how to install them on Modal.
For more on which GPUs are available on Modal and how to choose a GPU for your use case,
see
[this guide](/docs/guide/gpu)

. For a deep dive on both the
[GPU hardware](/gpu-glossary/device-hardware)

and
[software](/gpu-glossary/device-software)

and for even more detail on
[the CUDA stack](/gpu-glossary/host-software/)

,
see our
[GPU Glossary](/gpu-glossary/readme)

.

Here’s the tl;dr:

* The
  [NVIDIA Accelerated Graphics Driver for Linux-x86\_64](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#driver-installation)

  , version 575.57.08,
  and
  [CUDA Driver API](https://docs.nvidia.com/cuda/archive/12.9.0/cuda-driver-api/index.html)

  , version 12.8, are already installed.
  You can call
  `nvidia-smi`
  or run compiled CUDA programs from any Modal Function with access to a GPU.
* That means you can install many popular libraries like
  `torch`
  that bundle their other CUDA dependencies
  [with a simple
  `pip_install`](#install-gpu-accelerated-torch-and-transformers-with-pip_install)

  .
* For bleeding-edge libraries like
  `flash-attn`
  , you may need to install CUDA dependencies manually.
  To make your life easier,
  [use an existing image](#for-more-complex-setups-use-an-officially-supported-cuda-image)

  .

What is CUDA?
-------------

When someone refers to “installing CUDA” or “using CUDA”,
they are referring not to a library, but to a
[stack](/gpu-glossary/host-software/cuda-software-platform)

with multiple layers.
Your application code (and its dependencies) can interact
with the stack at different levels.

![The CUDA stack](/_app/immutable/assets/cuda-stack-diagram.BdEpPviG.png)

This leads to a lot of confusion. To help clear that up, the following sections explain each component in detail.

### Level 0: Kernel-mode driver components

At the lowest level are the
[*kernel-mode driver components*](https://docs.nvidia.com/cuda/cuda-installation-guide-linux/#nvidia-open-gpu-kernel-modules)

.
The Linux kernel is essentially a single program operating the entire machine and all of its hardware.
To add hardware to the machine, this program is extended by loading new modules into it.
These components communicate directly with hardware — in this case the GPU.

Because they are kernel modules, these driver components are tightly integrated with the host operating system
that runs your containerized Modal Functions and are not something you can inspect or change yourself.

### Level 1: User-mode driver API

All action in Linux that doesn’t occur in the kernel occurs in
[user space](https://en.wikipedia.org/wiki/User_space)

.
To talk to the kernel drivers from our user space programs, we need
*user-mode driver components*
.

Most prominently, that includes:

* the
  [CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

  ,
  a
  [shared object](https://en.wikipedia.org/wiki/Shared_library)

  called
  `libcuda.so`
  .
  This object exposes functions like
  [`cuMemAlloc`](https://docs.nvidia.com/cuda/archive/12.8.0/cuda-driver-api/group__CUDA__MEM.html#group__CUDA__MEM_1gb82d2a09844a58dd9e744dc31e8aa467)

  ,
  for allocating GPU memory.
* the
  [NVIDIA management library](https://developer.nvidia.com/management-library-nvml)

  ,
  `libnvidia-ml.so`
  , and its command line interface
  [`nvidia-smi`](https://developer.nvidia.com/system-management-interface)

  .
  You can use these tools to check the status of the system’s GPU(s).

These components are installed on all Modal machines with access to GPUs.
Because they are user-level components, you can use them directly:

```
import modal

app = modal.App()

@app.function(gpu="any")
def check_nvidia_smi():
    import subprocess
    output = subprocess.check_output(["nvidia-smi"], text=True)
    assert "Driver Version:" in output
    assert "CUDA Version:" in output
    print(output)
    return output
```

### Level 2: CUDA Toolkit

Wrapping the CUDA Driver API is the
[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

, the
`libcudart.so`
shared library.
This API includes functions like
[`cudaLaunchKernel`](https://docs.nvidia.com/cuda/archive/12.8.0/cuda-runtime-api/group__CUDART__HIGHLEVEL.html#group__CUDART__HIGHLEVEL_1g7656391f2e52f569214adbfc19689eb3)

and is more commonly used in CUDA programs (see
[this HackerNews comment](https://news.ycombinator.com/item?id=20616385)

for color commentary on why).
This shared library is
*not*
installed by default on Modal.

The CUDA Runtime API is generally installed as part of the larger
[NVIDIA CUDA Toolkit](https://docs.nvidia.com/cuda/index.html)

,
which includes the
[NVIDIA CUDA compiler driver](/gpu-glossary/host-software/nvcc)

(
`nvcc`
) and its toolchain
and a number of
[useful goodies](/gpu-glossary/host-software/cuda-binary-utilities)

for writing and debugging CUDA programs (
`cuobjdump`
,
`cudnn`
, profilers, etc.).

Contemporary GPU-accelerated machine learning workloads like LLM inference frequently make use of many components of the CUDA Toolkit,
such as the run-time compilation library
[`nvrtc`](https://docs.nvidia.com/cuda/archive/12.8.0/nvrtc/index.html)

.

So why aren’t these components installed along with the drivers?
A compiled CUDA program can run without the CUDA Runtime API installed on the system,
by
[statically linking](https://en.wikipedia.org/wiki/Static_library)

the CUDA Runtime API into the program binary,
though this is fairly uncommon for CUDA-accelerated Python programs.
Additionally, older versions of these components are needed for some applications
and some application deployments even use several versions at once.
Both patterns are compatible with the host machine driver provided on Modal.

Install GPU-accelerated
`torch`
and
`transformers`
with
`pip_install`
---------------------------------------------------------------------

The components of the CUDA Toolkit can be installed via
`pip`
,
via PyPI packages like
[`nvidia-cuda-runtime-cu12`](https://pypi.org/project/nvidia-cuda-runtime-cu12/)

and
[`nvidia-cuda-nvrtc-cu12`](https://pypi.org/project/nvidia-cuda-nvrtc-cu12/)

.
These components are listed as dependencies of some popular GPU-accelerated Python libraries, like
`torch`
.

Because Modal already includes the lower parts of the CUDA stack, you can install these libraries
with
[the
`pip_install`
method of
`modal.Image`](/docs/guide/images#add-python-packages-with-pip_install)

, just like any other Python library:

```
image = modal.Image.debian_slim().pip_install("torch")

@app.function(gpu="any", image=image)
def run_torch():
    import torch
    has_cuda = torch.cuda.is_available()
    print(f"It is {has_cuda} that torch can access CUDA")
    return has_cuda
```

Many libraries for running open-weights models, like
`transformers`
and
`vllm`
,
use
`torch`
under the hood and so can be installed in the same way:

```
image = modal.Image.debian_slim().pip_install("transformers[torch]")
image = image.apt_install("ffmpeg")  # for audio processing

@app.function(gpu="any", image=image)
def run_transformers():
    from transformers import pipeline
    transcriber = pipeline(model="openai/whisper-tiny.en", device="cuda")
    result = transcriber("https://modal-cdn.com/mlk.flac")
    print(result["text"])  # I have a dream that one day this nation will rise up live out the true meaning of its creed
```

For more complex setups, use an officially-supported CUDA image
---------------------------------------------------------------

The disadvantage of installing the CUDA stack via
`pip`
is that
many other libraries that depend on its components being installed as normal system packages cannot find them.

For these cases, we recommend you use an image that already has the full CUDA stack installed as system packages
and all environment variables set correctly, like the
[`nvidia/cuda:*-devel-*`
images on Docker Hub](https://hub.docker.com/r/nvidia/cuda)

.

[TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/overview.html)

is an inference engine that accelerates and optimizes performance for the large language models. It requires the full CUDA toolkit for installation.

```
cuda_version = "12.8.1"  # should be no greater than host CUDA version
flavor = "devel"  # includes full CUDA toolkit
operating_sys = "ubuntu24.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"
HF_CACHE_PATH = "/cache"

image = (
    modal.Image.from_registry(f"nvidia/cuda:{tag}", add_python="3.12")
    .entrypoint([])  # remove verbose logging by base image on entry
    .apt_install("libopenmpi-dev")  # required for tensorrt
    .pip_install("tensorrt-llm==0.19.0", "pynvml", extra_index_url="https://pypi.nvidia.com")
    .pip_install("hf-transfer", "huggingface_hub[hf_xet]")
    .env({"HF_HUB_CACHE": HF_CACHE_PATH, "HF_HUB_ENABLE_HF_TRANSFER": "1", "PMIX_MCA_gds": "hash"})
)

app = modal.App("tensorrt-llm", image=image)
hf_cache_volume = modal.Volume.from_name("hf_cache_tensorrt", create_if_missing=True)

@app.function(gpu="A10G", volumes={HF_CACHE_PATH: hf_cache_volume})
def run_tiny_model():
    from tensorrt_llm import LLM, SamplingParams

    sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

    llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")

    output = llm.generate("The capital of France is", sampling_params)
    print(f"Generated text: {output.outputs[0].text}")
    return output.outputs[0].text
```

Make sure to choose a version of CUDA that is no greater than the version provided by the host machine.
Older minor (
`12.*`
) versions are guaranteed to be compatible with the host machine’s driver,
but older major (
`11.*`
,
`10.*`
, etc.) versions may not be.

What next?
----------

For more on accessing and choosing GPUs on Modal, check out
[this guide](/docs/guide/gpu)

.
To dive deep on GPU internals, check out our
[GPU Glossary](/gpu-glossary/readme)

.

To see these installation patterns in action, check out these examples:

* [Fast LLM inference with vLLM](/docs/examples/vllm_inference)
* [Finetune a character LoRA for your pet](/docs/examples/diffusers_lora_finetune)
* [Optimized Flux inference](/docs/examples/flux)

[Using CUDA on Modal](#using-cuda-on-modal)

[What is CUDA?](#what-is-cuda)

[Level 0: Kernel-mode driver components](#level-0-kernel-mode-driver-components)

[Level 1: User-mode driver API](#level-1-user-mode-driver-api)

[Level 2: CUDA Toolkit](#level-2-cuda-toolkit)

[Install GPU-accelerated torch and transformers with pip\_install](#install-gpu-accelerated-torch-and-transformers-with-pip_install)

[For more complex setups, use an officially-supported CUDA image](#for-more-complex-setups-use-an-officially-supported-cuda-image)

[What next?](#what-next)

See it in action

[High-speed inference with vLLM](/docs/examples/vllm_inference)

[Run Stable Diffusion with a CLI, API, and web UI](/docs/examples/stable_diffusion_cli)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/custom-container
================================================================================

Images
======

This guide walks you through how to define the environment your Modal Functions run in.

These environments are called
*containers*
. Containers are like light-weight
virtual machines — container engines use
[operating system tricks](https://earthly.dev/blog/chroot/)

to isolate programs
from each other (“containing” them), making them work as though they were
running on their own hardware with their own filesystem. This makes execution
environments more reproducible, for example by preventing accidental
cross-contamination of environments on the same machine. For added security,
Modal runs containers using the sandboxed
[gVisor container runtime](https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime)

.

Containers are started up from a stored “snapshot” of their filesystem state
called an
*image*
. Producing the image for a container is called
*building*
the
image.

By default, Modal Functions are executed in a
[Debian Linux](https://en.wikipedia.org/wiki/Debian)

container with a basic
Python installation of the same minor version
`v3.x`
as your local Python
interpreter.

To make your Apps and Functions useful, you will probably need some third party system packages
or Python libraries. Modal provides a number of options to customize your container images at
different levels of abstraction and granularity, from high-level convenience
methods like
`pip_install`
through wrappers of core container image build
features like
`RUN`
and
`ENV`
to full on “bring-your-own-Dockerfile”. We’ll
cover each of these in this guide, along with tips and tricks for building
Images effectively when using each tool.

The typical flow for defining an image in Modal is
[method chaining](https://jugad2.blogspot.com/2016/02/examples-of-method-chaining-in-python.html)

starting from a base image, like this:

```
import modal

image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git")
    .pip_install("torch==2.6.0")
    .env({"HALT_AND_CATCH_FIRE": "0"})
    .run_commands("git clone https://github.com/modal-labs/agi && echo 'ready to go!'")
)
```

In addition to being Pythonic and clean, this also matches the onion-like
[layerwise build process](https://docs.docker.com/build/guide/layers/)

of
container images.

Add Python packages with
`pip_install`
--------------------------------------

The simplest and most common container modification is to add some third party
Python package, like
[`pandas`](https://pandas.pydata.org/)

.

You can add Python packages to the environment by passing all the packages you
need to the
[`pip_install`](/docs/reference/modal.Image#pip_install)

method of
an image.

You can include
[typical Python dependency version specifiers](https://peps.python.org/pep-0508/)

,
like
`"torch <= 2.0"`
, in the arguments. But we recommend pinning dependencies
tightly, like
`"torch == 1.9.1"`
, to improve the reproducibility and robustness
of your builds.

```
import modal

datascience_image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install("pandas==2.2.0", "numpy")
)

@app.function(image=datascience_image)
def my_function():
    import pandas as pd
    import numpy as np

    df = pd.DataFrame()
    ...
```

Note that because you can define a different environment for each and every
Modal Function if you so choose, you don’t need to worry about virtual
environment management. Containers make for much better separation of concerns!

If you want to run a specific version of Python remotely rather than just
matching the one you’re running locally, provide the
`python_version`
as a
string when constructing the base image, like we did above.

Add local files with
`add_local_dir`
and
`add_local_file`
---------------------------------------------------------

If you want to forward files from your local system, you can do that using the
`image.add_local_dir`
and
`image.add_local_file`
image builder methods.

```
image = modal.Image.debian_slim().add_local_dir("/user/erikbern/.aws", remote_path="/root/.aws")
```

By default, these files are added to your container as it starts up rather than introducing
a new image layer. This means that the redeployment after making changes is really quick, but
also means you can’t run additional build steps after. You can specify a
`copy=True`
argument
to the
`add_local_`
methods to instead force the files to be included in a built image.

### Adding local Python modules

There is a convenience method for the special case of adding local Python modules to
the container:
[`Image.add_local_python_source`](/docs/reference/modal.Image#add_local_python_source)

The difference from
`add_local_dir`
is that
`add_local_python_source`
takes module names as arguments
instead of a file system path and looks up the local package’s or module’s location via Python’s importing
mechanism. The files are then added to directories that make them importable in containers in the
same way as they are locally.

This is mostly intended for pure Python auxiliary modules that are part of your project and that your code imports,
whereas third party packages should be installed via
[`Image.pip_install()`](/docs/reference/modal.Image#pip_install)

or similar.

```
import modal

app = modal.App()

image_with_module = modal.Image.debian_slim().add_local_python_source("my_local_module")

@app.function(image=image_with_module)
def f():
    import my_local_module  # this will now work in containers
    my_local_module.do_stuff()
```

### What if I have different Python packages locally and remotely?

You might want to use packages inside your Modal code that you don’t have on
your local computer. In the example above, we build a container that uses
`pandas`
. But if we don’t have
`pandas`
locally, on the computer launching the
Modal job, we can’t put
`import pandas`
at the top of the script, since it would
cause an
`ImportError`
.

The easiest solution to this is to put
`import pandas`
in the function body
instead, as you can see above. This means that
`pandas`
is only imported when
running inside the remote Modal container, which has
`pandas`
installed.

Be careful about what you return from Modal Functions that have different
packages installed than the ones you have locally! Modal Functions return Python
objects, like
`pandas.DataFrame`
s, and if your local machine doesn’t have
`pandas`
installed, it won’t be able to handle a
`pandas`
object (the error
message you see will mention
[serialization](https://hazelcast.com/glossary/serialization/)

/
[deserialization](https://hazelcast.com/glossary/deserialization/)

).

If you have a lot of functions and a lot of Python packages, you might want to
keep the imports in the global scope so that every function can use the same
imports. In that case, you can use the
[`imports()`](/docs/reference/modal.Image#imports)

context manager:

```
import modal

pandas_image = modal.Image.debian_slim().pip_install("pandas", "numpy")

with pandas_image.imports():
    import pandas as pd
    import numpy as np

@app.function(image=pandas_image)
def my_function():
    df = pd.DataFrame()
```

Because these imports happen before a new container processes its first input,
you can combine this decorator with
[memory snapshots](/docs/guide/memory-snapshot)

to improve
[cold start performance](/docs/guide/cold-start#share-initialization-work-across-cold-starts-with-memory-snapshots)

for Functions that frequently scale from zero.

Run shell commands with
`.run_commands`
---------------------------------------

You can also supply shell commands that should be executed when building the
container image.

You might use this to preload custom assets, like model parameters, so that they
don’t need to be retrieved when Functions start up:

```
import modal

image_with_model = (
    modal.Image.debian_slim().apt_install("curl").run_commands(
        "curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface.xml",
    )
)

@app.function(image=image_with_model)
def find_cats():
    content = open("/haarcascade_frontalcatface.xml").read()
    ...
```

You can also use this command to install Python packages. For example,
you can use it to install packages with
[`uv`](https://github.com/astral-sh/uv)

,
which can be substantially faster than
`pip`
:

```
import modal

image = (
    modal.Image.debian_slim()
    .pip_install("uv")
    .run_commands("uv pip install --system --compile-bytecode torch")
)
```

Note that it is important to pass
`--compile-bytecode`
when using
`uv`
on Modal.
Unlike
`pip`
,
`uv`
does not produce
[Python bytecode](https://realpython.com/ref/glossary/bytecode/)

(the contents of the
`.pyc`
files in those
`__pycache__`
folders you may have noticed in your Python projects)
by default when packages are installed. On a serverless platform like Modal, skipping that work at installation time
means it instead has to be done every time a container starts.

Run a Python function during your build with
`.run_function`
------------------------------------------------------------

Instead of using shell commands, you can also run a Python function as an image
build step using the
[`Image.run_function`](/docs/reference/modal.Image#run_function)

method. For
example, you can use this to download model parameters from Hugging Face into
your Image:

```
import os
import modal

def download_models() -> None:
    import diffusers

    model_name = "segmind/small-sd"
    pipe = diffusers.StableDiffusionPipeline.from_pretrained(
        model_name, use_auth_token=os.environ["HF_TOKEN"]
    )
    pipe.save_pretrained("/model")

image = (
    modal.Image.debian_slim()
        .pip_install("diffusers[torch]", "transformers", "ftfy", "accelerate")
        .run_function(download_models, secrets=[modal.Secret.from_name("huggingface-secret")])
)
```

Any kwargs accepted by
[`@app.function`](/docs/reference/modal.App#function)

(
[`Volume`
s](/docs/guide/volumes)

, and specifications of
resources like
[GPUs](/docs/guide/gpu)

) can be supplied here.

Essentially, this is equivalent to running a Modal Function and snapshotting the
resulting filesystem as an image.

Whenever you change other features of your image, like the base image or the
version of a Python package, the image will automatically be rebuilt the next
time it is used. This is a bit more complicated when changing the contents of
functions. See the
[reference documentation](/docs/reference/modal.Image#run_function)

for details.

Attach GPUs during setup
------------------------

If a step in the setup of your container image should be run on an instance with
a GPU (e.g., so that a package can query the GPU to set compilation flags), pass a
desired GPU type when defining that step:

```
import modal

image = (
    modal.Image.debian_slim()
    .pip_install("bitsandbytes", gpu="H100")
)
```

Use
`mamba`
instead of
`pip`
with
`micromamba_install`
------------------------------------------------------

`pip`
installs Python packages, but some Python workloads require the
coordinated installation of system packages as well. The
`mamba`
package manager
can install both. Modal provides a pre-built
[Micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html)

base image that makes it easy to work with
`micromamba`
:

```
import modal

app = modal.App("bayes-pgm")

numpyro_pymc_image = (
    modal.Image.micromamba()
    .micromamba_install("pymc==5.10.4", "numpyro==0.13.2", channels=["conda-forge"])
)

@app.function(image=numpyro_pymc_image)
def sample():
    import pymc as pm
    import numpyro as np

    print(f"Running on PyMC v{pm.__version__} with JAX/numpyro v{np.__version__} backend")
    ...
```

Use an existing container image with
`.from_registry`
-----------------------------------------------------

You don’t always need to start from scratch! Public registries like
[Docker Hub](https://hub.docker.com/)

have many pre-built container images for
common software packages.

You can use any public image in your function using
[`Image.from_registry`](/docs/reference/modal.Image#from_registry)

, so long as:

* Python 3.9 or later is installed on the
  `$PATH`
  as
  `python`
* `pip`
  is installed correctly
* The image is built for the
  [`linux/amd64`
  platform](https://unix.stackexchange.com/questions/53415/why-are-64-bit-distros-often-called-amd64)
* The image has a
  [valid
  `ENTRYPOINT`](#entrypoint)

```
import modal

sklearn_image = modal.Image.from_registry("huanjason/scikit-learn")

@app.function(image=sklearn_image)
def fit_knn():
    from sklearn.neighbors import KNeighborsClassifier
    ...
```

If an existing image does not have either
`python`
or
`pip`
set up properly, you
can still use it. Just provide a version number as the
`add_python`
argument to
install a reproducible
[standalone build](https://github.com/indygreg/python-build-standalone)

of Python:

```
import modal

image1 = modal.Image.from_registry("ubuntu:22.04", add_python="3.11")
image2 = modal.Image.from_registry("gisops/valhalla:latest", add_python="3.11")
```

The
`from_registry`
method can load images from all public registries, such as
[Nvidia’s
`nvcr.io`](https://catalog.ngc.nvidia.com/containers)

,
[AWS ECR](https://aws.amazon.com/ecr/)

, and
[GitHub’s
`ghcr.io`](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry)

.

We also support access to
[private AWS ECR and GCP Artifact Registry images](/docs/guide/private-registries)

.

Bring your own image definition with
`.from_dockerfile`
-------------------------------------------------------

Sometimes, you might be already have a container image defined in a Dockerfile.

You can define an Image with a Dockerfile using
[`Image.from_dockerfile`](/docs/reference/modal.Image#from_dockerfile)

.
It takes a path to an existing Dockerfile.

For instance, we might write a Dockerfile that adds scikit-learn to the official Python image:

```
FROM python:3.9
RUN pip install sklearn
```

and then define a Modal Image with it:

```
import modal

dockerfile_image = modal.Image.from_dockerfile("Dockerfile")

@app.function(image=dockerfile_image)
def fit():
    import sklearn
    ...
```

Note that you can still do method chaining to extend this image!

### Dockerfile command compatibility

Since Modal doesn’t use Docker to build containers, we have our own
implementation of the
[Dockerfile specification](https://docs.docker.com/engine/reference/builder/)

.
Most Dockerfiles should work out of the box, but there are some differences to
be aware of.

First, a few minor Dockerfile commands and flags have not been implemented yet.
These include
`ONBUILD`
,
`STOPSIGNAL`
, and
`VOLUME`
.
Please reach out to us if your use case requires any of these.

Next, there are some command-specific things that may be useful when porting a
Dockerfile to Modal.

#### `ENTRYPOINT`

While the
[`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#entrypoint)

command is supported, there is an additional constraint to the entrypoint script
provided: when used with a Modal Function, it must also
`exec`
the arguments passed to it at some point.
This is so the Modal Function runtime’s Python entrypoint can run after your own. Most entrypoint
scripts in Docker containers are wrappers over other scripts, so this is likely
already the case.

If you wish to write your own entrypoint script, you can use the following as a
template:

```
#!/usr/bin/env bash

# Your custom startup commands here.

exec "$@" # Runs the command passed to the entrypoint script.
```

If the above file is saved as
`/usr/bin/my_entrypoint.sh`
in your container,
then you can register it as an entrypoint with
`ENTRYPOINT ["/usr/bin/my_entrypoint.sh"]`
in your Dockerfile, or with
[`entrypoint`](/docs/reference/modal.Image#entrypoint)

as an
Image build step.

```
import modal

image = (
    modal.Image.debian_slim()
    .pip_install("foo")
    .entrypoint(["/usr/bin/my_entrypoint.sh"])
)
```

#### `ENV`

We currently don’t support default values in
[interpolations](https://docs.docker.com/compose/compose-file/12-interpolation/)

,
such as
`${VAR:-default}`

Image caching and rebuilds
--------------------------

Modal uses the definition of an Image to determine whether it needs to be
rebuilt. If the definition hasn’t changed since the last time you ran or
deployed your App, the previous version will be pulled from the cache.

Images are cached per layer (i.e., per
`Image`
method call), and breaking
the cache on a single layer will cause cascading rebuilds for all subsequent
layers. You can shorten iteration cycles by defining frequently-changing
layers last so that the cached version of all other layers can be used.

In some cases, you may want to force an Image to rebuild, even if the
definition hasn’t changed. You can do this by adding the
`force_build=True`
argument to any of the Image building methods.

```
import modal

image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install("slack-sdk", force_build=True)
    .run_commands("echo hi")
)
```

As in other cases where a layer’s definition changes, both the
`pip_install`
and
`run_commands`
layers will rebuild, but the
`apt_install`
will not. Remember to
remove
`force_build=True`
after you’ve rebuilt the Image, or it will
rebuild every time you run your code.

Alternatively, you can set the
`MODAL_FORCE_BUILD`
environment variable (e.g.
`MODAL_FORCE_BUILD=1 modal run ...`
) to rebuild all images attached to your App.
But note that when you rebuild a base layer, the cache will be invalidated for
*all*
Images that depend on it, and they will rebuild the next time you run or deploy
any App that uses that base. If you’re debugging an issue with your Image, a better
option might be using
`MODAL_IGNORE_CACHE=1`
. This will rebuild the Image from the
top without breaking the Image cache or affecting subsequent builds.

Image builder updates
---------------------

Because changes to base images will cause cascading rebuilds, Modal is
conservative about updating the base definitions that we provide. But many
things are baked into these definitions, like the specific versions of the Image
OS, the included Python, and the Modal client dependencies.

We provide a separate mechanism for keeping base images up-to-date without
causing unpredictable rebuilds: the “Image Builder Version”. This is a workspace
level-configuration that will be used for every Image built in your workspace.
We release a new Image Builder Version every few months but allow you to update
your workspace’s configuration when convenient. After updating, your next
deployment will take longer, because your Images will rebuild. You may also
encounter problems, especially if your Image definition does not pin the version
of the third-party libraries that it installs (as your new Image will get the
latest version of these libraries, which may contain breaking changes).

You can set the Image Builder Version for your workspace by going to your
[workspace settings](/settings/image-config)

. This page also documents the
important updates in each version.

[Images](#images)

[Add Python packages with pip\_install](#add-python-packages-with-pip_install)

[Add local files with add\_local\_dir and add\_local\_file](#add-local-files-with-add_local_dir-and-add_local_file)

[Adding local Python modules](#adding-local-python-modules)

[What if I have different Python packages locally and remotely?](#what-if-i-have-different-python-packages-locally-and-remotely)

[Run shell commands with .run\_commands](#run-shell-commands-with-run_commands)

[Run a Python function during your build with .run\_function](#run-a-python-function-during-your-build-with-run_function)

[Attach GPUs during setup](#attach-gpus-during-setup)

[Use mamba instead of pip with micromamba\_install](#use-mamba-instead-of-pip-with-micromamba_install)

[Use an existing container image with .from\_registry](#use-an-existing-container-image-with-from_registry)

[Bring your own image definition with .from\_dockerfile](#bring-your-own-image-definition-with-from_dockerfile)

[Dockerfile command compatibility](#dockerfile-command-compatibility)

[ENTRYPOINT](#entrypoint)

[ENV](#env)

[Image caching and rebuilds](#image-caching-and-rebuilds)

[Image builder updates](#image-builder-updates)

See it in action

[Registry image for Algolia indexing](/docs/examples/algolia_indexer)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/datadog-integration
================================================================================

Connecting Modal to your Datadog account
========================================

You can use the
[Modal + Datadog Integration](https://docs.datadoghq.com/integrations/modal/)

to export Modal function logs to Datadog. You’ll find the Modal Datadog
Integration available for install in the Datadog marketplace.

What this integration does
--------------------------

This integration allows you to:

1. Export Modal audit logs in Datadog
2. Export Modal function logs to Datadog
3. Export container metrics to Datadog

Installing the integration
--------------------------

1. Open the
   [Modal Tile](https://app.datadoghq.com/integrations?integrationId=modal)

   (or the EU tile
   [here](https://app.datadoghq.eu/integrations?integrationId=modal)

   )
   in the Datadog integrations page
2. Click “Install Integration”
3. Click Connect Accounts to begin authorization of this integration.
   You will be redirected to log into Modal, and once logged in, you’ll
   be redirected to the Datadog authorization page.
4. Click “Authorize” to complete the integration setup

Metrics
-------

The Modal Datadog Integration will forward the following metrics to Datadog:

* `modal.cpu.utilization`
* `modal.memory.utilization`
* `modal.gpu.memory.utilization`
* `modal.gpu.compute.utilization`
* `modal.input_events.elapsed_time_us`
* `modal.input_events.successes`
* `modal.input_events.total_inputs`

`modal.input_events.successes`
and
`modal.input_events.total_inputs`
can be used to measure the success rate of a certain function or app.

These metrics come free of charge and are tagged with
`container_id`
,
`environment_name`
, and
`workspace_name`
. The input event metrics are, in addition, tagged with
`app_name`
,
`app_id`
,
`function_name`
,
`function_id`
, and
`workspace_id`
.

Structured logging
------------------

Logs from Modal are sent to Datadog in plaintext without any structured
parsing. This means that if you have custom log formats, you’ll need to
set up a
[log processing pipeline](https://docs.datadoghq.com/logs/log_configuration/pipelines/?tab=source)

in Datadog to parse them.

Modal passes log messages in the
`.message`
field of the log record. To
parse logs, you should operate over this field. Note that the Modal Integration
does set up some basic pipelines. In order for your pipelines to work, ensure
that your pipelines come before Modal’s pipelines in your log settings.

Cost Savings
------------

The Modal Datadog Integration will forward all logs to Datadog which could be
costly for verbose apps. We recommend using either
[Log Pipelines](https://docs.datadoghq.com/logs/log_configuration/pipelines/?tab=source)

or
[Index Exclusion Filters](https://docs.datadoghq.com/logs/indexes/?tab=ui#exclusion-filters)

to filter logs before they are sent to Datadog.

The Modal Integration tags all logs with the
`environment`
attribute. The
simplest way to filter logs is to create a pipeline that filters on this
attribute and to isolate verbose apps in a separate environment.

Uninstalling the integration
----------------------------

Once the integration is uninstalled, all logs will stop being sent to
Datadog, and authorization will be revoked.

1. Navigate to the
   [Modal metrics settings page](http://modal.com/settings/metrics)

   and select “Delete Datadog Integration”.
2. On the Configure tab in the Modal integration tile in Datadog,
   click Uninstall Integration.
3. Confirm that you want to uninstall the integration.
4. Ensure that all API keys associated with this integration have been
   disabled by searching for the integration name on the
   [API Keys](https://app.datadoghq.com/organization-settings/api-keys?filter=Modal)

   page.

[Connecting Modal to your Datadog account](#connecting-modal-to-your-datadog-account)

[What this integration does](#what-this-integration-does)

[Installing the integration](#installing-the-integration)

[Metrics](#metrics)

[Structured logging](#structured-logging)

[Cost Savings](#cost-savings)

[Uninstalling the integration](#uninstalling-the-integration)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/dataset-ingestion
================================================================================

Large dataset ingestion
=======================

This guide provides best practices for downloading, transforming, and storing large datasets within
Modal. A dataset is considered large if it contains hundreds of thousands of files and/or is over
100 GiB in size.

These guidelines ensure that large datasets can be ingested fully and reliably.

Configure your Function for heavy disk usage
--------------------------------------------

Large datasets should be downloaded and transformed using a
`modal.Function`
and stored
into a
`modal.CloudBucketMount`
. We recommend backing the latter with a Cloudflare R2 bucket,
because Cloudflare does not charge network egress fees and has lower GiB/month storage costs than AWS S3.

This
`modal.Function`
should specify a large
`timeout`
because large dataset processing can take hours,
and it should request a larger ephemeral disk in cases where the dataset being downloaded and processed
is hundreds of GiBs.

```
@app.function(
    volumes={
        "/mnt": modal.CloudBucketMount(
            "datasets",
            bucket_endpoint_url="https://abc123example.r2.cloudflarestorage.com",
            secret=modal.Secret.from_name("cloudflare-r2-datasets"),
        )
    },
    ephemeral_disk=1000 * 1000,  # 1 TiB
    timeout=60 * 60 * 12,  # 12 hours

)
def download_and_transform() -> None:
    ...
```

### Use compressed archives on Modal Volumes

`modal.Volume`
s are designed for storing tens of thousands of individual files,
but not for hundreds of thousands or millions of files.
However they can be still be used for storing large datasets if files are first combined and compressed
in a dataset transformation step before saving them into the Volume.

See the
[transforming](#transforming)

section below for more details.

Experimentation
---------------

Downloading and transforming large datasets can be fiddly. While iterating on a reliable ingestion program
it is recommended to start a long-running
`modal.Function`
serving a JupyterHub server so that you can maintain
disk state in the face of application errors.

See the
[running Jupyter server within a Modal function](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py)

example as base code.

Downloading
-----------

The raw dataset data should be first downloaded into the container at
`/tmp/`
and not placed
directly into the mounted volume. This serves a couple purposes.

1. Certain download libraries and tools (e.g.
   `wget`
   ) perform filesystem operations not supported properly by
   `CloudBucketMount`
   .
2. The raw dataset data may need to be transformed before use, in which case it is wasteful to store it permanently.

This snippet shows the basic download-and-copy procedure:

```
import pathlib
import shutil
import subprocess

tmp_path = pathlib.Path("/tmp/imagenet/")
vol_path = pathlib.Path("/mnt/imagenet/")
filename = "imagenet-object-localization-challenge.zip"
# 1. Download into /tmp/
subprocess.run(
    f"kaggle competitions download -c imagenet-object-localization-challenge --path {tmp_path}",
    shell=True,
    check=True
)
vol_path.mkdir(exist_ok=True)
# 2. Copy (without transform) into mounted volume.
shutil.copyfile(tmp_path / filename, vol_path / filename)
```

Transforming
------------

When ingesting a large dataset it is sometimes necessary to transform it before storage, so that it is in
an optimal format for loading at runtime. A common kind of necessary transform is gzip decompression. Very large
datasets are often gzipped for storage and network transmission efficiency, but gzip decompression (80 MiB/s)
is hundreds of times slower than reading from a solid state drive (SSD)
and should be done once before storage to avoid decompressing on every read against the dataset.

Transformations should be performed after storing the raw dataset in
`/tmp/`
. Performing transformations almost always increases container disk usage and this is where the
[`ephemeral_disk`
parameter](/docs/reference/modal.App#function)

parameter becomes important. For example, a
100 GiB raw, compressed dataset may decompress to into 500 GiB, occupying 600 GiB of container disk space.

Transformations should also typically be performed against
`/tmp/`
. This is because

1. transforms can be IO intensive and IO latency is lower against local SSD.
2. transforms can create temporary data which is wasteful to store permanently.

Examples
--------

The best practices offered in this guide are demonstrated in the
[`modal-examples`
repository](https://github.com/modal-labs/modal-examples/tree/main/12_datasets)

.

The examples include these popular large datasets:

* [ImageNet](https://www.image-net.org/)

  , the image labeling dataset that kicked off the deep learning revolution
* [COCO](https://cocodataset.org/#download)

  , the Common Objects in COntext dataset of densely-labeled images
* [LAION-400M](https://laion.ai/blog/laion-400-open-dataset/)

  , the Stable Diffusion training dataset
* Data derived from the
  [Big “Fantastic” Database](https://bfd.mmseqs.com/)

  ,
  [Protein Data Bank](https://www.wwpdb.org/)

  , and
  [UniProt Database](https://www.uniprot.org/)

  used in training the
  [RoseTTAFold](https://github.com/RosettaCommons/RoseTTAFold)

  protein structure model

[Large dataset ingestion](#large-dataset-ingestion)

[Configure your Function for heavy disk usage](#configure-your-function-for-heavy-disk-usage)

[Use compressed archives on Modal Volumes](#use-compressed-archives-on-modal-volumes)

[Experimentation](#experimentation)

[Downloading](#downloading)

[Transforming](#transforming)

[Examples](#examples)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/developing-debugging
================================================================================

Developing and debugging
========================

Modal makes it easy to run apps in the cloud, try code changes in the cloud, and
debug remotely executing code as if it were right there on your laptop. To speed
boost your inner dev loop, this guide provides a rundown of tools and techniques
for developing and debugging software in Modal.

Interactivity
-------------

You can launch a Modal App interactively and have it drop you right into the
middle of the action, at an interesting callsite or the site of a runtime
detonation.

### Interactive functions

It is possible to start the interactive Python debugger or start an
`IPython`
REPL right in the middle of your Modal App.

To do so, you first need to run your App in “interactive” mode by using the
`--interactive`
/
`-i`
flag. In interactive mode, you can establish a connection
to the calling terminal by calling
`interact()`
from within your function.

For a simple example, you can accept user input with the built-in Python
`input`
function:

```
@app.function()
def my_fn(hidden):
    modal.interact()

    x = input("Enter a number: ")
    if hidden == x:
        print(f"Your number is {x}, which is the hidden value!")
    else:
        print(f"Your number is {x}, which is not the hidden value")
```

Now when you run your app with the
`--interactive`
flag, you’re able to send
inputs to your app, even though it’s running in a remote container!

```
modal run -i guess_number.py --hidden 5
Enter a number: 5
Your number is 5, which is the hidden value!
```

For a more interesting example, you can
[`pip_install("ipython")`](/docs/reference/modal.Image#pip_install)

and start an
`IPython`
REPL dynamically anywhere in your code:

```
@app.function()
def f():
    model = expensive_function()
    # play around with model
    modal.interact()
    import IPython
    IPython.embed()
```

The built-in Python debugger can be initiated with the language’s
`breakpoint()`
function. For convenience, breakpoints call
`interact`
automatically.

```
@app.function()
def f():
    x = "10point3"
    breakpoint()
    answer = float(x)
```

### Debugging Running Containers

#### Debug Shells

Modal also lets you run interactive commands on your running Containers from the
terminal — much like
`ssh`
-ing into a traditional machine or cloud VM.

To run a command inside a running Container, you first need to get the Container
ID. You can view all running Containers and their Container IDs with
[`modal container list`](/docs/reference/cli/container)

.

After you obtain the Container ID, you can connect to the Container with
`modal shell [container-id]`
. This launches a “Debug Shell” that comes with some preinstalled tools:

* `vim`
* `nano`
* `ps`
* `strace`
* `curl`
* `py-spy`
* and more!

You can use a debug shell to examine or terminate running processes, modify the Container filesystem, run commands, and more. You can also install additional packages using your Container’s package manager (ex.
`apt`
).

Note that debug shells will terminate immediately once your Container has finished running.

#### `modal container exec`

You can also execute a specific command in a running Container with
`modal container exec [container-id] [command...]`
. For example, to see what files are in
`/root`
, you can run
`modal container exec [container-id] ls /root`
.

```
❯ modal container list
                         Active Containers in environment: nathan-dev
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓
┃ Container ID                  ┃ App ID                    ┃ App Name ┃ Start Time           ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩
│ ta-01JK47GVDMWMGPH8MQ0EW30Y25 │ ap-FSuhQ4LpvNAt5b6mKi1CDw │ my-app   │ 2025-02-02 16:02 EST │
└───────────────────────────────┴───────────────────────────┴──────────┴──────────────────────┘

❯ modal container exec ta-01JK47GVDMWMGPH8MQ0EW30Y25 ls /root
__pycache__  test00.py
```

Note that your executed command will terminate immediately once your Container
has finished running.

By default, commands will be run within a
[pseudoterminal (PTY)](https://en.wikipedia.org/wiki/Pseudoterminal)

, but this
can be disabled with the
`--no-pty`
flag.

#### Live container profiling

When a container or input is seemingly stuck or not making progress,
you can use the Modal web dashboard to find out what code that’s executing in the
container in real time. To do so, look for
**Live Profiling**
in the
**Containers**
tab in your
function dashboard.

![Live container profiling](https://modal-public-assets.s3.us-east-1.amazonaws.com/live-profiling-bigger.gif)

### Debugging Container Images

You can also launch an interactive shell in a new Container with the same
environment as your Function. This is handy for debugging issues with your
Image, interactively refining build commands, and exploring the contents of
[`Volume`](/docs/reference/modal.Volume)

s and
[`NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)

s.

The primary interface for accessing this feature is the
[`modal shell`](/docs/reference/cli/shell)

CLI command, which accepts a Function
name in your App (or prompts you to select one, if none is provided), and runs
an interactive command on the same image as the Function, with the same
[`Secret`](/docs/reference/modal.Secret)

s and
[`NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)

s attached as the selected Function.

The default command is
`/bin/bash`
, but you can override this with any other
command of your choice using the
`--cmd`
flag.

Note that
`modal shell [filename].py`
does not attach a shell to a running Container of the
Function, but instead creates a fresh instance of the underlying Image. To attach a shell to a running Container, use
`modal shell [container-id]`
instead.

Live updating
-------------

### Hot reloading with `modal serve`

Modal has the command
`modal serve <filename.py>`
, which creates a loop that
live updates an App when any of the supporting files change.

Live updating works with web endpoints, syncing your changes as you make them,
and it also works well with cron schedules and job queues.

```
import modal

app = modal.App(image=modal.Image.debian_slim().pip_install("fastapi"))

@app.function()
@modal.fastapi_endpoint()
def f():
    return "I update on file edit!"

@app.function(schedule=modal.Period(seconds=5))
def run_me():
    print("I also update on file edit!")
```

If you edit this file, the
`modal serve`
command will detect the change and
update the code, without having to restart the command.

Observability
-------------

Each running Modal App, including all ephemeral Apps, streams logs and resource
metrics back to you for viewing.

On start, an App will log a dashboard link that will take you its App page.

```
$ python3 main.py
✓ Initialized. View app page at https://modal.com/apps/ap-XYZ1234.
...
```

From this page you can access the following:

* logs, both from your application and system-level logs from Modal
* compute resource metrics (CPU, RAM, GPU)
* function call history, including historical success/failure counts

### Debug logs

You can enable Modal’s client debug logs by setting the
`MODAL_LOGLEVEL`
environment variable to
`DEBUG`
.
Running the following will show debug logging from the Modal client running locally.

```
MODAL_LOGLEVEL=DEBUG modal run hello.py
```

To enable debug logs in the Modal client running in the remote container, you can set
`MODAL_LOGLEVEL`
using
a Modal
[`Secret`](/docs/reference/modal.Secret)

.

```
@app.function(secrets=[modal.Secret.from_dict({"MODAL_LOGLEVEL": "DEBUG"})])
def f():
    print("Hello, world!")
```

### Client tracebacks

To see a traceback (a.k.a
[stack trace](https://en.wikipedia.org/wiki/Stack_trace)

) for a client-side exception, you can set the
`MODAL_TRACEBACK`
environment variable to
`1`
.

```
MODAL_TRACEBACK=1 modal run my_app.py
```

We encourage you to report cases where you need to enable this functionality, as it’s indication of an issue in Modal.

[Developing and debugging](#developing-and-debugging)

[Interactivity](#interactivity)

[Interactive functions](#interactive-functions)

[Debugging Running Containers](#debugging-running-containers)

[Debug Shells](#debug-shells)

[modal container exec](#modal-container-exec)

[Live container profiling](#live-container-profiling)

[Debugging Container Images](#debugging-container-images)

[Live updating](#live-updating)

[Hot reloading with modal serve](#hot-reloading-with-modal-serve)

[Observability](#observability)

[Debug logs](#debug-logs)

[Client tracebacks](#client-tracebacks)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/developing-with-llms
================================================================================

Developing Modal code with LLMs
===============================

Excellent developer experience is at the core of Modal. This also means that Modal works well with code generation agents, especially those that can run CLI commands like
`modal run`
in an implement, test and debug loop, like Amp, Claude Code, Cursor’s agent mode, Gemini CLI, etc.

There are of course also many concepts and design patterns that are unique to Modal, so below we gather rules and guidelines that we have found useful when developing Modal code with LLMs. You can paste/import this into your
`AGENTS.md`
,
`CLAUDE.md`
,
`.cursor/rules/modal.mdc`
, etc. or use it as a starting point for your own rules or prompts.

```
# Modal Rules and Guidelines for LLMs

This file provides rules and guidelines for LLMs when implementing Modal code.

## General

- Modal is a serverless cloud platform for running Python code with minimal configuration
- Designed for AI/ML workloads but supports general-purpose cloud compute
- Serverless billing model - you only pay for resources used

## Modal documentation

- Extensive documentation is available at: modal.com/docs (and in markdown format at modal.com/llms-full.txt)
- A large collection of examples is available at: modal.com/docs/examples (and github.com/modal-labs/modal-examples)
- Reference documentation is available at: modal.com/docs/reference

Always refer to documentation and examples for up-to-date functionality and exact syntax.

## Core Modal concepts

### App

- A group of functions, classes and sandboxes that are deployed together.

### Function

- The basic unit of serverless execution on Modal.
- Each Function executes in its own container, and you can configure different Images for different Functions within the same App:

  ```python
  image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install("torch", "transformers")
    .apt_install("ffmpeg")
    .run_commands("mkdir -p /models")
  )

  @app.function(image=image)
  def square(x: int) -> int:
    return x * x
  ```

- You can configure individual hardware requirements (CPU, memory, GPUs, etc.) for each Function.

  ```python
  @app.function(
    gpu="H100",
    memory=4096,
    cpu=2,
  )
  def inference():
    ...
  ```

  Some examples specificly for GPUs:

  ```python
  @app.function(gpu="A10G")  # Single GPU, e.g. T4, A10G, A100, H100, or "any"
  @app.function(gpu="A100:2")  # Multiple GPUs, e.g. 2x A100 GPUs
  @app.function(gpu=["H100", "A100", "any"]) # GPU with fallbacks
  ```

- Functions can be invoked in a number of ways. Some of the most common are:
  - `foo.remote()` - Run the Function in a separate container in the cloud. This is by far the most common.
  - `foo.local()` - Run the Function in the same context as the caller. Note: This does not necessarily mean locally on your machine.
  - `foo.map()` - Parallel map over a set of inputs.
  - `foo.spawn()` - Calls the function with the given arguments, without waiting for the results. Terminating the App will also terminate spawned functions.
- Web endpoint: You can turn any Function into an HTTP web endpoint served by adding a decorator:

  ```python
  @app.function()
  @modal.fastapi_endpoint()
  def fastapi_endpoint():
    return {"status": "ok"}

  @app.function()
  @modal.asgi_app()
  def asgi_app():
    app = FastAPI()
    ...
    return app
  ```

- You can run Functions on a schedule using e.g. `@app.function(schedule=modal.Period(minutes=5))` or `@app.function(schedule=modal.Cron("0 9 * * *"))`.

### Classes (a.k.a. `Cls`)

- For stateful operations with startup/shutdown lifecycle hooks. Example:

  ```python
  @app.cls(gpu="A100")
  class ModelServer:
      @modal.enter()
      def load_model(self):
          # Runs once when container starts
          self.model = load_model()

      @modal.method()
      def predict(self, text: str) -> str:
          return self.model.generate(text)

      @modal.exit()
      def cleanup(self):
          # Runs when container stops
          cleanup()
  ```

### Other important concepts

- Image: Represents a container image that Functions can run in.
- Sandbox: Allows defining containers at runtime and securely running arbitrary code inside them.
- Volume: Provide a high-performance distributed file system for your Modal applications.
- Secret: Enables securely providing credentials and other sensitive information to your Modal Functions.
- Dict: Distributed key/value store, managed by Modal.
- Queue: Distributed, FIFO queue, managed by Modal.

## Differences from standard Python development

- Modal always executes code in the cloud, even while you are developing. You can use Environments for separating development and production deployments.
- Dependencies: It's common and encouraged to have different dependency requirements for different Functions within the same App. Consider defining dependencies in Image definitions (see Image docs) that are attached to Functions, rather than in global `requirements.txt`/`pyproject.toml` files, and putting `import` statements inside the Function `def`. Any code in the global scope needs to be executable in all environments where that App source will be used (locally, and any of the Images the App uses).

## Modal coding style

- Modal Apps, Volumes, and Secrets should be named using kebab-case.
- Always use `import modal`, and qualified names like `modal.App()`, `modal.Image.debian_slim()`.
- Modal evolves quickly, and prints helpful deprecation warnings when you `modal run` an App that uses deprecated features. When writing new code, never use deprecated features.

## Common commands

Running `modal --help` gives you a list of all available commands. All commands also support `--help` for more details.

### Running your Modal app during development

- `modal run path/to/your/app.py` - Run your app on Modal.
- `modal run -m module.path.to.app` - Run your app on Modal, using the Python module path.
- `modal serve modal_server.py` - Run web endpoint(s) associated with a Modal app, and hot-reload code on changes. Will print a URL to the web endpoint(s). Note: you need to use `Ctrl+C` to interrupt `modal serve`.

### Deploying your Modal app

- `modal deploy path/to/your/app.py` - Deploy your app (Functions, web endpoints, etc.) to Modal.
- `modal deploy -m module.path.to.app` - Deploy your app to Modal, using the Python module path.

Logs:

- `modal app logs <app_name>` - Stream logs for a deployed app. Note: you need to use `Ctrl+C` to interrupt the stream.

### Resource management

- There are CLI commands for interacting with resources like `modal app list`, `modal volume list`, and similarly for `secret`, `dict`, `queue`, etc.
- These also support other command than `list` - use e.g. `modal app --help` for more.

## Testing and debugging

- When using `app.deploy()`, you can wrap it in a `with modal.enable_output():` block to get more output.
```

[Developing Modal code with LLMs](#developing-modal-code-with-llms)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/dicts
================================================================================

Dicts
=====

Modal Dicts provide distributed key-value storage to your Modal Apps.

```
import modal

app = modal.App()
kv = modal.Dict.from_name("kv", create_if_missing=True)

@app.local_entrypoint()
def main(key="cloud", value="dictionary", put=True):
    if put:
        kv[key] = value
    print(f"{key}: {kv[key]}")
```

This page is a high-level guide to using Modal Dicts.
For reference documentation on the
`modal.Dict`
object, see
[this page](/docs/reference/modal.Dict)

.
For reference documentation on the
`modal dict`
CLI command, see
[this page](/docs/reference/cli/dict)

.

Modal Dicts are Python dicts in the cloud
-----------------------------------------

Dicts provide distributed key-value storage to your Modal Apps.
Much like a standard Python dictionary, a Dict lets you store and retrieve
values using keys. However, unlike a regular dictionary, a Dict in Modal is
accessible from anywhere, concurrently and in parallel.

```
# create a remote Dict
dictionary = modal.Dict.from_name("my-dict", create_if_missing=True)

dictionary["key"] = "value"  # set a value from anywhere
value = dictionary["key"]    # get a value from anywhere
```

Dicts are persisted, which means that the data in the dictionary is
stored and can be retrieved even after the application is redeployed.

You can access Modal Dicts asynchronously
-----------------------------------------

Modal Dicts live in the cloud, which means reads and writes
against them go over the network. That has some unavoidable latency overhead,
relative to just reading from memory, of a few dozen ms.
Reads from Dicts via
`["key"]`
-style indexing are synchronous,
which means that latency is often directly felt by the application.

But like all Modal objects, you can also interact with Dicts asynchronously
by putting the
`.aio`
suffix on methods — in this case,
`put`
and
`get`
,
which are synonyms for bracket-based indexing.
Just add the
`async`
keyword to your
`local_entrypoint`
s or remote Functions
and
`await`
the method calls.

```
import modal

app = modal.App()
dictionary = modal.Dict.from_name("async-dict", create_if_missing=True)

@app.local_entrypoint()
async def main():
    await dictionary.put.aio("key", "value")  # setting a value asynchronously
    assert await dictionary.get.aio("key")   # getting a value asyncrhonrously
```

See the guide to
[asynchronous functions](/docs/guide/async)

for more
information.

Modal Dicts are not
*exactly*
Python dicts
------------------------------------------

Python dicts can have keys of any hashable type and values of any type.

You can store Python objects of any serializable type within Dicts as keys or values.

Objects are serialized using
[`cloudpickle`](https://github.com/cloudpipe/cloudpickle)

,
so precise support is inherited from that library.
`cloudpickle`
can serialize a surprising variety of objects,
like
`lambda`
functions or even Python modules, but it can’t serialize a few things that don’t
really make sense to serialize, like live system resources (sockets, writable file descriptors).

Note that you will need to have the library defining the type installed in the environment
where you retrieve the object so that it can be deserialized.

```
import modal

app = modal.App()
dictionary = modal.Dict.from_name("funky-dict", create_if_missing=True)

@app.function(image=modal.Image.debian_slim().pip_install("numpy"))
def fill():
    import numpy

    dictionary["numpy"] = numpy
    dictionary["modal"] = modal
    dictionary[dictionary] = dictionary  # don't try this at home!

@app.local_entrypoint()
def main():
    fill.remote()
    print(dictionary["modal"])
    print(dictionary[dictionary]["modal"].Dict)
    # print(dictionary["numpy"])  # DeserializationError, if no numpy locally
```

Unlike with normal Python dictionaries, updates to mutable value types will not
be reflected in other containers unless the updated object is explicitly put
back into the Dict. As a consequence, patterns like chained updates
(
`my_dict["outer_key"]["inner_key"] = value`
) cannot be used the same way as
they would with a local dictionary.

Currently, the per-object size limit is 100 MiB and the maximum number of entries
per update is 10,000. It’s recommended to use Dicts for smaller objects (under 5 MiB).
Each object in the Dict will expire after 7 days of inactivity (no reads or writes).

Dicts also provide a locking primitive. See
[this blog post](/blog/cache-dict-launch)

for details.

[Dicts](#dicts)

[Modal Dicts are Python dicts in the cloud](#modal-dicts-are-python-dicts-in-the-cloud)

[You can access Modal Dicts asynchronously](#you-can-access-modal-dicts-asynchronously)

[Modal Dicts are not exactly Python dicts](#modal-dicts-are-not-exactly-python-dicts)

See it in action

[Use Dicts and Queues to coordinate a web scraper](/docs/examples/dicts_and_queues)

[Store 100k checkboxes in a Dict](/docs/examples/fasthtml_checkboxes)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/dicts-and-queues
================================================================================

Use Modal Dicts and Queues together
===================================

Modal Dicts and Queues store and communicate objects in distributed applications on Modal.

To illustrate how Dicts and Queues can interact together in a simple distributed
system, consider the following example program that crawls the web, starting
from some initial page and traversing links to many sites in breadth-first order.

The Modal Queue acts as a job queue, accepting new pages to crawl as they are discovered
by the crawlers and doling them out to be crawled via
[`.spawn`](https://modal.com/docs/reference/modal.Function#spawn)

.

The Dict is used to coordinate termination once the maximum number of URLs to crawl is reached.

Starting from Wikipedia, this spawns several dozen containers (auto-scaled on
demand) and crawls about 100,000 URLs per minute.

```
import queue
import sys
from datetime import datetime

import modal

app = modal.App(
    image=modal.Image.debian_slim().pip_install(
        "requests~=2.32.4", "beautifulsoup4~=4.13.4"
    )
)

def extract_links(url: str) -> list[str]:
    """Extract links from a given URL."""
    import urllib.parse

    import requests
    from bs4 import BeautifulSoup

    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    links = []
    for link in soup.find_all("a"):
        links.append(urllib.parse.urljoin(url, link.get("href")))
    return links

@app.function()
def crawl_pages(q: modal.Queue, d: modal.Dict, urls: set[str]) -> None:
    for url in urls:
        if "stop" in d:
            return
        try:
            s = datetime.now()
            links = extract_links(url)
            print(f"Crawled: {url} in {datetime.now() - s}, with {len(links)} links")
            q.put_many(links)
        except Exception as exc:
            print(
                f"Failed to crawl: {url} with error {exc}, skipping...", file=sys.stderr
            )

@app.function()
def scrape(url: str, max_urls: int = 50_000):
    start_time = datetime.now()

    # Create ephemeral dicts and queues
    with modal.Dict.ephemeral() as d, modal.Queue.ephemeral() as q:
        # The dict is used to signal the scraping to stop
        # The queue contains the URLs that have been crawled

        # Initialize queue with a starting URL
        q.put(url)

        # Crawl until the queue is empty, or reaching some number of URLs
        visited = set()
        max_urls = min(max_urls, 50_000)
        while True:
            try:
                next_urls = q.get_many(2000, timeout=5)
            except queue.Empty:
                break
            new_urls = set(next_urls) - visited
            visited |= new_urls
            if len(visited) < max_urls:
                crawl_pages.spawn(q, d, new_urls)
            else:
                d["stop"] = True

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"Crawled {len(visited)} URLs in {elapsed:.2f} seconds")

@app.local_entrypoint()
def main(starting_url=None, max_urls: int = 10_000):
    starting_url = starting_url or "https://www.wikipedia.org/"
    scrape.remote(starting_url, max_urls=max_urls)
```

[Use Modal Dicts and Queues together](#use-modal-dicts-and-queues-together)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 09_job_queues/dicts_and_queues.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/dynamic-batching
================================================================================

Dynamic batching (beta)
=======================

Modal’s
`@batched`
feature allows you to accumulate requests
and process them in dynamically-sized batches, rather than one-by-one.

Batching increases throughput at a potential cost to latency.
Batched requests can share resources and reuse work, reducing the time and cost per request.
Batching is particularly useful for GPU-accelerated machine learning workloads,
as GPUs are designed to maximize throughput and are frequently bottlenecked on shareable resources,
like weights stored in memory.

Static batching can lead to unbounded latency, as the function waits for a fixed number of requests to arrive.
Modal’s dynamic batching waits for the lesser of a fixed time
*or*
a fixed number of requests before executing,
maximizing the throughput benefit of batching while minimizing the latency penalty.

Enable dynamic batching with
`@batched`
---------------------------------------

To enable dynamic batching, apply the
[`@modal.batched`
decorator](/docs/reference/modal.batched)

to the target
Python function. Then, wrap it in
`@app.function()`
and run it on Modal,
and the inputs will be accumulated and processed in batches.

Here’s what that looks like:

```
import modal

app = modal.App()

@app.function()
@modal.batched(max_batch_size=2, wait_ms=1000)
async def batch_add(xs: list[int], ys: list[int]) -> list[int]:
    return [x + y for x, y in zip(xs, ys)]
```

When you invoke a function decorated with
`@batched`
, you invoke it asynchronously on individual inputs.
Outputs are returned where they were invoked.

For instance, the code below invokes the decorated
`batch_add`
function above three times, but
`batch_add`
only executes twice:

```
@app.local_entrypoint()
async def main():
    inputs = [(1, 300), (2, 200), (3, 100)]
    async for result in batch_add.starmap.aio(inputs):
        print(f"Sum: {result}")
        # Sum: 301
        # Sum: 202
        # Sum: 103
```

The first time it is executed with
`xs`
batched to
`[1, 2]`
and
`ys`
batched to
`[300, 200]`
. After about a one second delay, it is executed with
`xs`
batched to
`[3]`
and
`ys`
batched to
`[100]`
.
The result is an iterator that yields
`301`
,
`202`
, and
`101`
.

Use
`@batched`
with functions that take and return lists
--------------------------------------------------------

For a Python function to be compatible with
`@modal.batched`
, it must adhere to
the following rules:

* **The inputs to the function must be lists.**
  In the example above, we pass
  `xs`
  and
  `ys`
  , which are both lists of
  `int`
  s.
* **The function must return a list**
  . In the example above, the function returns
  a list of sums.
* **The lengths of all the input lists and the output list must be the same.**
  In the example above, if
  `L == len(xs) == len(ys)`
  , then
  `L == len(batch_add(xs, ys))`
  .

Modal
`Cls`
methods are compatible with dynamic batching
--------------------------------------------------------

Methods on Modal
[`Cls`](/docs/guide/lifecycle-functions)

es also support dynamic batching.

```
import modal

app = modal.App()

@app.cls()
class BatchedClass():
    @modal.batched(max_batch_size=2, wait_ms=1000)
    async def batch_add(self, xs: list[int], ys: list[int]) -> list[int]:
        return [x + y for x, y in zip(xs, ys)]
```

One additional rule applies to classes with Batched Methods:

* If a class has a Batched Method, it
  **cannot have other Batched Methods or
  [Methods](/docs/reference/modal.method#modalmethod)**
  .

Configure the wait time and batch size of dynamic batches
---------------------------------------------------------

The
`@batched`
decorator takes in two required configuration parameters:

* `max_batch_size`
  limits the number of inputs combined into a single batch.
* `wait_ms`
  limits the amount of time the Function waits for more inputs after
  the first input is received.

The first invocation of the Batched Function initiates a new batch, and subsequent
calls add requests to this ongoing batch. If
`max_batch_size`
is reached,
the batch immediately executes. If the
`max_batch_size`
is not met but
`wait_ms`
has passed since the first request was added to the batch, the unfilled batch is
executed.

### Selecting a batch configuration

To optimize the batching configurations for your application, consider the following heuristics:

* Set
  `max_batch_size`
  to the largest value your function can handle, so you
  can amortize and parallelize as much work as possible.
* Set
  `wait_ms`
  to the difference between your targeted latency and the execution time. Most applications
  have a targeted latency, and this allows the latency of any request to stay
  within that limit.

Serve web endpoints with dynamic batching
-----------------------------------------

Here’s a simple example of serving a Function that batches requests dynamically
with a
[`@modal.fastapi_endpoint`](/docs/guide/webhooks)

. Run
[`modal serve`](/docs/reference/cli/serve)

, submit requests to the endpoint,
and the Function will batch your requests on the fly.

```
import modal

app = modal.App(image=modal.Image.debian_slim().pip_install("fastapi"))

@app.function()
@modal.batched(max_batch_size=2, wait_ms=1000)
async def batch_add(xs: list[int], ys: list[int]) -> list[int]:
    return [x + y for x, y in zip(xs, ys)]

@app.function()
@modal.fastapi_endpoint(method="POST", docs=True)
async def add(body: dict[str, int]) -> dict[str, int]:
    result = await batch_add.remote.aio(body["x"], body["y"])
    return {"result": result}
```

Now, you can submit requests to the web endpoint and process them in batches. For instance, the three requests
in the following example, which might be requests from concurrent clients in a real deployment,
will be batched into two executions:

```
import asyncio
import aiohttp

async def send_post_request(session, url, data):
    async with session.post(url, json=data) as response:
        return await response.json()

async def main():
    # Enter the URL of your web endpoint here
    url = "https://workspace--app-name-endpoint-name.modal.run"

    async with aiohttp.ClientSession() as session:
        # Submit three requests asynchronously
        tasks = [
            send_post_request(session, url, {"x": 1, "y": 300}),
            send_post_request(session, url, {"x": 2, "y": 200}),
            send_post_request(session, url, {"x": 3, "y": 100}),
        ]
        results = await asyncio.gather(*tasks)
        for result in results:
            print(f"Sum: {result['result']}")

asyncio.run(main())
```

[Dynamic batching (beta)](#dynamic-batching-beta)

[Enable dynamic batching with @batched](#enable-dynamic-batching-with-batched)

[Use @batched with functions that take and return lists](#use-batched-with-functions-that-take-and-return-lists)

[Modal Cls methods are compatible with dynamic batching](#modal-cls-methods-are-compatible-with-dynamic-batching)

[Configure the wait time and batch size of dynamic batches](#configure-the-wait-time-and-batch-size-of-dynamic-batches)

[Selecting a batch configuration](#selecting-a-batch-configuration)

[Serve web endpoints with dynamic batching](#serve-web-endpoints-with-dynamic-batching)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/environment_variables
================================================================================

Environment variables
=====================

The Modal runtime sets several environment variables during initialization. The
keys for these environment variables are reserved and cannot be overridden by
your Function or Sandbox configuration.

These variables provide information about the containers’s runtime
environment.

Container runtime environment variables
---------------------------------------

The following variables are present in every Modal container:

* **`MODAL_CLOUD_PROVIDER`**
  — Modal executes containers across a number of cloud
  providers (
  [AWS](https://aws.amazon.com/)

  ,
  [GCP](https://cloud.google.com/)

  ,
  [OCI](https://www.oracle.com/cloud/)

  ). This variable specifies which cloud
  provider the Modal container is running within.
* **`MODAL_IMAGE_ID`**
  — The ID of the
  [`modal.Image`](/docs/reference/modal.Image)

  used by the Modal container.
* **`MODAL_REGION`**
  — This will correspond to a geographic area identifier from
  the cloud provider associated with the Modal container (see above). For AWS, the
  identifier is a “region”. For GCP it is a “zone”, and for OCI it is an
  “availability domain”. Example values are
  `us-east-1`
  (AWS),
  `us-central1`
  (GCP),
  `us-ashburn-1`
  (OCI).
* **`MODAL_TASK_ID`**
  — The ID of the container running the Modal Function or Sandbox.

Function runtime environment variables
--------------------------------------

The following variables are present in containers running Modal Functions:

* **`MODAL_ENVIRONMENT`**
  — The name of the
  [Modal Environment](/docs/guide/environments)

  the container is running within.
* **`MODAL_IS_REMOTE`**
  - Set to ‘1’ to indicate that Modal Function code is running in
  a remote container.
* **`MODAL_IDENTITY_TOKEN`**
  — An
  [OIDC token](/docs/guide/oidc-integration)

  encoding the identity of the Modal Function.

Sandbox environment variables
-----------------------------

The following variables are present within
[`modal.Sandbox`](/docs/reference/modal.Sandbox)

instances.

* **`MODAL_SANDBOX_ID`**
  — The ID of the Sandbox.

Container image environment variables
-------------------------------------

The container image layers used by a
`modal.Image`
may set
environment variables. These variables will be present within your container’s runtime
environment. For example, the
[`debian_slim`](/docs/reference/modal.Image#debian_slim)

image sets the
`GPG_KEY`
variable.

To override image variables or set new ones, use the
[`.env`](https://modal.com/docs/reference/modal.Image#env)

method provided by
`modal.Image`
.

[Environment variables](#environment-variables)

[Container runtime environment variables](#container-runtime-environment-variables)

[Function runtime environment variables](#function-runtime-environment-variables)

[Sandbox environment variables](#sandbox-environment-variables)

[Container image environment variables](#container-image-environment-variables)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/environments
================================================================================

Environments
============

Environments are sub-divisions of workspaces, allowing you to deploy the same app
(or set of apps) in multiple instances for different purposes without changing
your code. Typical use cases for environments include having one
`dev`
environment and one
`prod`
environment, preventing overwriting production apps
when developing new features, while still being able to deploy changes to a
“live” and potentially complex structure of apps.

Each environment has its own set of
[Secrets](/docs/guide/secrets)

and any
object lookups performed from an app in an environment will by default look for
objects in the same environment.

By default, every workspace has a single Environment called “main”. New
Environments can be created on the CLI:

```
modal environment create dev
```

(You can run
`modal environment --help`
for more info)

Once created, Environments show up as a dropdown menu in the navbar of the
[Modal dashboard](/home)

, letting you set browse all Modal Apps and Secrets
filtered by which Environment they were deployed to.

Most CLI commands also support an
`--env`
flag letting you specify which
Environment you intend to interact with, e.g.:

```
modal run --env=dev app.py
modal volume create --env=dev storage
```

To set a default Environment for your current CLI profile you can use
`modal config set-environment`
, e.g.:

```
modal config set-environment dev
```

Alternatively, you can set the
`MODAL_ENVIRONMENT`
environment variable.

Environment web suffixes
------------------------

Environments have a ‘web suffix’ which is used to make
[web endpoint URLs](/docs/guide/webhook-urls)

unique across your workspace. One
Environment is allowed to have no suffix (
`""`
).

Cross environment lookups
-------------------------

It’s possible to explicitly look up objects in Environments other than the Environment
your app runs within:

```
production_secret = modal.Secret.from_name(
    "my-secret",
    environment_name="main"
)
```

```
modal.Function.from_name(
    "my_app",
    "some_function",
    environment_name="dev"
)
```

However, the
`environment_name`
argument is optional and omitting it will use
the Environment from the object’s associated App or calling context.

[Environments](#environments)

[Environment web suffixes](#environment-web-suffixes)

[Cross environment lookups](#cross-environment-lookups)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/fast-pull-from-registry
================================================================================

Fast pull from registry
=======================

The performance of pulling public and private images from registries into Modal
can be significantly improved by adopting the
[eStargz](https://github.com/containerd/stargz-snapshotter/blob/main/docs/estargz.md)

compression format.

By applying eStargz compression during your image build and push, Modal will be much
more efficient at pulling down your image from the registry.

How to use estargz
------------------

If you have
[Buildkit](https://docs.docker.com/build/buildkit/)

version greater than
`0.10.0`
, adopting
`estargz`
is as simple as
adding some flags to your
`docker buildx build`
command:

* `type=registry`
  flag will instruct BuildKit to push the image after building.
  + If you do not push the image from immediately after build and instead attempt to push it later with docker push, the image will be converted to a standard gzip image.
* `compression=estargz`
  specifies that we are using the
  [eStargz](https://github.com/containerd/stargz-snapshotter/blob/main/docs/estargz.md)

  compression format.
* `oci-mediatypes=true`
  specifies that we are using the OCI media types, which is required for eStargz.
* `force-compression=true`
  will recompress the entire image and convert the base image to eStargz if it is not already.

```
docker buildx build --tag "<registry>/<namespace>/<repo>:<version>" \
--output type=registry,compression=estargz,force-compression=true,oci-mediatypes=true \
.
```

Then reference the container image as normal in your Modal code.

```
app = modal.App(
    "example-estargz-pull",
    image=modal.Image.from_registry(
        "public.ecr.aws/modal/estargz-example-images:text-generation-v1-esgz"
    )
)
```

At build time you should see the eStargz-enabled puller activate:

```
Building image im-TinABCTIf12345ydEwTXYZ

=> Step 0: FROM public.ecr.aws/modal/estargz-example-images:text-generation-v1-esgz
Using estargz to speed up image pull (index loaded in 1.86s)...
Progress: 10% complete... (1.11s elapsed)
Progress: 20% complete... (3.10s elapsed)
Progress: 30% complete... (4.18s elapsed)
Progress: 40% complete... (4.76s elapsed)
Progress: 50% complete... (5.51s elapsed)
Progress: 62% complete... (6.17s elapsed)
Progress: 74% complete... (6.99s elapsed)
Progress: 81% complete... (7.23s elapsed)
Progress: 99% complete... (8.90s elapsed)
Progress: 100% complete... (8.90s elapsed)
Copying image...
Copied image in 5.81s
```

Supported registries
--------------------

Currently, Modal supports fast estargz pulling images with the following registries:

* AWS Elastic Container Registry (ECR)
* Docker Hub (docker.io)
* Google Artifact Registry (gcr.io, pkg.dev)

We are working on adding support for GitHub Container Registry (ghcr.io).

[Fast pull from registry](#fast-pull-from-registry)

[How to use estargz](#how-to-use-estargz)

[Supported registries](#supported-registries)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/geographic-latency
================================================================================

Geographic Latency
==================

Modal’s worker cluster is multi-cloud and multi-region. The vast majority of workers are located
in the continental USA, but we do run workers in Europe and Asia.

Modal’s control plane is hosted in Virginia, USA (
`us-east-1`
).

Any time data needs to travel between the Modal client, our control plane servers, and our workers
latency will be incurred.
[Cloudping.co](https://www.cloudping.co)

provides good estimates on the
significance of the latency between regions. For example, the roundtrip latency between AWS
`us-east-1`
(Virginia, USA) and
`us-west-1`
(California, USA) is around 60ms.

You can observe the location identifier of a container
[via an environment variable](/docs/guide/environment_variables)

.
Logging this environment variable alongside latency information can reveal when geography is impacting your application
performance.

Region selection
----------------

In cases where low-latency communication is required between your container and a network dependency (e.g a database),
it is useful to ensure that Modal schedules your container in only regions geographically proximate to that dependency.
For example, if you have an AWS RDS database in Virginia, USA (
`us-east-1`
), ensuring your Modal containers are also scheduled in Virginia
means that network latency between the container and the database will be less than 5 milliseconds.

For more information, please see
[Region selection](/docs/guide/region-selection)

.

[Geographic Latency](#geographic-latency)

[Region selection](#region-selection)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/global-variables
================================================================================

Global variables
================

There are cases where you might want objects or data available in
**global**
scope. For example:

* You need to use the data in a scheduled function (scheduled functions don’t
  accept arguments)
* You need to construct objects (e.g. Secrets) in global scope to use as
  function annotations
* You don’t want to clutter many function signatures with some common arguments
  they all use, and pass the same arguments through many layers of function
  calls.

For these cases, you can use the
`modal.is_local`
function, which returns
`True`
if the app is running locally (initializing) or
`False`
if the app is executing
in the cloud.

For instance, to create a
[`modal.Secret`](/docs/guide/secrets)

that you can pass
to your function decorators to create environment variables, you can run:

```
import os

if modal.is_local():
    pg_password = modal.Secret.from_dict({"PGPASS": os.environ["MY_LOCAL_PASSWORD"]})
else:
    pg_password = modal.Secret.from_dict({})

@app.function(secrets=[pg_password])
def get_secret_data():
    connection = psycopg2.connect(password=os.environ["PGPASS"])
    ...
```

Warning about regular module globals
------------------------------------

If you try to construct a global in module scope using some local data
*without*
using something like
`modal.is_local`
, it might have unexpected effects since
your Python modules will be not only be loaded on your local machine, but also
on the remote worker.

E.g., this will typically not work:

```
# blob.json doesn't exist on the remote worker, so this will cause an error there
data_blob = open("blob.json", "r").read()

@app.function()
def foo():
    print(data_blob)
```

[Global variables](#global-variables)

[Warning about regular module globals](#warning-about-regular-module-globals)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/gpu
================================================================================

GPU acceleration
================

Modal makes it easy to run any code on GPUs.

Quickstart
----------

Here’s a simple example of a function running on an A100 in Modal:

```
import modal

app = modal.App()
image = modal.Image.debian_slim().pip_install("torch")

@app.function(gpu="A100", image=image)
def run():
    import torch
    print(torch.cuda.is_available())
```

This installs PyTorch on top of a base image, and is able to use GPUs with
PyTorch.

Specifying GPU type
-------------------

You can pick a specific GPU type for your function via the
`gpu`
argument.
Modal supports the following values for this parameter:

* `T4`
* `L4`
* `A10G`
* `A100-40GB`
* `A100-80GB`
* `L40S`
* `H100`
* `H200`
* `B200`

For instance, to use an H100, you can use
`@app.function(gpu="H100")`
.

Refer to our
[pricing page](/pricing)

for the latest pricing on each GPU type.

Specifying GPU count
--------------------

You can specify more than 1 GPUs per container by appending
`:n`
to the GPU
argument. For instance, to run a function with 8\*H100:

```

@app.function(gpu="H100:8")
def run_llama_405b_fp8():
    ...
```

Currently B200, H200, H100, A100, L4, T4 and L40S instances support up to 8 GPUs (up to 640 GB GPU RAM),
and A10G instances support up to 4 GPUs (up to 96 GB GPU RAM). Note that requesting
more than 2 GPUs per container will usually result in larger wait times. These
GPUs are always attached to the same physical machine.

Picking a GPU
-------------

For running, rather than training, neural networks, we recommend starting off
with the
[L40S](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413)

,
which offers an excellent trade-off of cost and performance and 48 GB of GPU
RAM for storing model weights.

For more on how to pick a GPU for use with neural networks like LLaMA or Stable
Diffusion, and for tips on how to make that GPU go brrr, check out
[Tim Dettemers’ blog post](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)

or the
[Full Stack Deep Learning page on Cloud GPUs](https://fullstackdeeplearning.com/cloud-gpus/)

.

GPU fallbacks
-------------

Modal allows specifying a list of possible GPU types, suitable for functions that are
compatible with multiple options. Modal respects the ordering of this list and
will try to allocate the most preferred GPU type before falling back to less
preferred ones.

```
@app.function(gpu=["H100", "A100-40GB:2"])
def run_on_80gb():
    ...
```

See
[this example](/docs/examples/gpu_fallbacks)

for more detail.

H100 GPUs
---------

Modal’s fastest GPUs are the
[H100s](https://www.nvidia.com/en-us/data-center/h100/)

, NVIDIA’s
flagship data center chip for the Hopper/Lovelace
[architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

.

To request an H100, set the
`gpu`
argument to
`"H100"`

```
@app.function(gpu="H100")
def run_text_to_video():
    ...
```

Check out
[this example](/docs/examples/flux)

to see how you can generate images
from the Flux.schnell model in under a second using an H100.

Before you jump for the most powerful (and so most expensive) GPU, make sure you
understand where the bottlenecks are in your computations. For example, running
language models with small batch sizes (e.g. one prompt at a time) results in a
[bottleneck on memory, not arithmetic](https://kipp.ly/transformer-inference-arithmetic/)

.
Since arithmetic throughput has risen faster than memory throughput in recent
hardware generations, speedups for memory-bound GPU jobs are not as extreme and
may not be worth the extra cost.

**H200 GPUs**

Modal may automatically upgrade an H100 request to an
[H200](https://www.nvidia.com/en-us/data-center/h200/)

, NVIDIA’s evolution of the H100 chip
for the Hopper/Lovelace
[architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

.
This automatic upgrade
*does not*
change the cost of the GPU.

H200s are software compatible with H100s, so your code always works for both, but an upgrade
to an H200 brings higher memory bandwidth! NVIDIA H200’s HBM3e memory bandwidth of 4.8TB/s is 1.4x faster than NVIDIA H100 with HBM3.

In cases where an automatic upgrade to H200 would not be desired (e.g., benchmarking) you can pass
`gpu=H100!`
to avoid it.

A100 GPUs
---------

[A100s](https://www.nvidia.com/en-us/data-center/a100/)

are the previous
generation of top-of-the-line data center chip from NVIDIA, based on the Ampere
[architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

.
Modal offers two versions of the A100: one with 40 GB of RAM and another with 80 GB of RAM.

To request an A100 with 40 GB of
[GPU memory](/gpu-glossary/device-hardware/gpu-ram)

, use
`gpu="A100"`
:

```
@app.function(gpu="A100")
def llama_7b():
    ...
```

To request an 80 GB A100, use the string
`A100-80GB`
:

```
@app.function(gpu="A100-80GB")
def llama_70b_fp8():
    ...
```

Multi GPU training
------------------

Modal currently supports multi-GPU training on a single machine, with multi-node training in closed beta (
[contact us](https://modal.com/slack)

for access). Depending on which framework you are using, you may need to use different techniques to train on multiple GPUs.

If the framework re-executes the entrypoint of the Python process (like
[PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/index.html)

) you need to either set the strategy to
`ddp_spawn`
or
`ddp_notebook`
if you wish to invoke the training directly. Another option is to run the training script as a subprocess instead.

```
@app.function(gpu="A100:2")
def run():
    import subprocess
    import sys
    subprocess.run(
        ["python", "train.py"],
        stdout=sys.stdout, stderr=sys.stderr,
        check=True,
    )
```

Examples and more resources.
----------------------------

For more information about GPUs in general, check out our
[GPU Glossary](/gpu-glossary/readme)

.

Or take a look some examples of Modal apps using GPUs:

* [Fine-tune a character LoRA for your pet](/docs/examples/dreambooth_app)
* [Fast LLM inference with vLLM](/docs/examples/vllm_inference)
* [Stable Diffusion with a CLI, API, and web UI](/docs/examples/stable_diffusion_cli)
* [Rendering Blender videos](/docs/examples/blender_video)

[GPU acceleration](#gpu-acceleration)

[Quickstart](#quickstart)

[Specifying GPU type](#specifying-gpu-type)

[Specifying GPU count](#specifying-gpu-count)

[Picking a GPU](#picking-a-gpu)

[GPU fallbacks](#gpu-fallbacks)

[H100 GPUs](#h100-gpus)

[A100 GPUs](#a100-gpus)

[Multi GPU training](#multi-gpu-training)

[Examples and more resources.](#examples-and-more-resources)

See it in action

[High-speed inference with vLLM](/docs/examples/vllm_inference)

[Stable Diffusion 3.5 Large](/docs/examples/stable_diffusion_cli)

[Blender video renderer](/docs/examples/blender_video)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/gpu-health
================================================================================

GPU Health
==========

Modal constantly monitors host GPU health, draining Workers with critical issues
and surfacing warnings for customer triage.

Application level observability of GPU health is facilitated by
[metrics](/docs/guide/gpu-metrics)

and event logging to container log streams.

`[gpu-health]`
logging
----------------------

Containers with attached NVIDIA GPUs are connected to our
`gpu-health`
monitoring system
and receive event logs which originate from either application software behavior, system software behavior, or hardware failure.

These logs are in the following format:
`[gpu-health] [LEVEL] GPU-[UUID]: EVENT_TYPE: MSG`

* `gpu-health`
  : Name indicating the source is Modal’s observability system.
* `LEVEL`
  : Represents the severity level of the log message.
* `GPU_UUID`
  : A unique identifier for the GPU device associated with the event, if any.
* `EVENT_TYPE`
  : The type of event source. Modal monitors for multiple types of errors,
  including Xid, SXid, and uncorrectable ECC. See below for more details.
* `MSG`
  : The message component is either the original message taken from the event source, or a description provided by Modal of the problem.

Level
-----

The severity level may be
`CRITICAL`
or
`WARN`
. Modal automatically responds to
`CRITICAL`
level events by draining the underlying Worker and migrating customer containers.
`WARN`
level logs may be benign or indication of an application or library bug. No automatic action is taken by our system for warnings.

Xid & SXid
----------

The Xid message is an error report from the NVIDIA driver. The SXid, or “Switch Xid” is a report for the NVSwitch component used in GPU-to-GPU communication, and is thus only relevant in multi-GPU containers.

A classic critical Xid error is the ‘fell of the bus’ report, code 79. The
`gpu-health`
event log looks like this:

```
[gpu-health] [CRITICAL] GPU-1234: XID: NVRM: Xid (PCI:0000:c6:00): 79, pid=1101234, name=nvc:[driver], GPU has fallen off the bus.
```

There are over 100 Xid codes and they are of highly varying frequency, severity, and specificity.
See
[NVIDIA’s official documentation](https://docs.nvidia.com/deploy/xid-errors/index.html)

for more information.

[GPU Health](#gpu-health)

[[gpu-health] logging](#gpu-health-logging)

[Level](#level)

[Xid & SXid](#xid--sxid)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/gpu-metrics
================================================================================

GPU Metrics
===========

Modal exposes a number of GPU metrics that help monitor the health and utilization of the GPUs you’re using.

* **GPU utilization %**
  is the percentage of time that the GPU was executing at least one CUDA kernel. This is the same metric reported as utilization by
  [`nvidia-smi`](/gpu-glossary/host-software/nvidia-smi)

  . GPU utilization is helpful for determining the amount of time GPU work is blocked on CPU work, like PyTorch compute graph construction or input processing. However, it is far from indicating what fraction of the GPU’s computing firepower (FLOPS or memory throughput,
  [CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

  ,
  [SMs](/gpu-glossary/device-hardware/streaming-multiprocessor)

  ) is being used. See
  [this blog post](https://arthurchiao.art/blog/understanding-gpu-performance)

  for details.
* **GPU power utilization %**
  is the percentage of the maximum power draw that the device is currently drawing. When aggregating across containers, we also report
  **Total GPU power usage**
  in Watts. Because high-performance GPUs are
  [fundamentally limited by power draw](https://www.thonking.ai/p/strangely-matrix-multiplications)

  , both for computation and memory access, the power usage can be used as a proxy of how much work the GPU is doing. A fully-saturated GPU should draw at or near its entire power budget (which can also be found by running
  `nvidia-smi`
  ).
* **GPU temperature**
  is the temperature measured on the die of the GPU. Like power draw, which is the source of the thermal energy, the ability to efflux heat is a fundamental limit on GPU performance: continuing to draw full power without removing the waste heat would damage the system. At the highest temperatures readily observed in proper GPU deployments (i.e. mid-70s Celsius for an H100), increased error correction from thermal noise can already reduce performance. Generally, power utilization is a better proxy for performance, but we report temperature for completeness.
* **GPU memory used**
  is the amount of memory allocated on the GPU, in bytes.

In general, these metrics are useful signals or correlates of performance, but can’t be used to directly debug performance issues. Instead, we (and
[the manufacturers!](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#assess-parallelize-optimize-deploy)

) recommend tracing and profiling workloads. See
[this example](/docs/examples/torch_profiling)

of profiling PyTorch applications on Modal.

[GPU Metrics](#gpu-metrics)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/images
================================================================================

Images
======

This guide walks you through how to define the environment your Modal Functions run in.

These environments are called
*containers*
. Containers are like light-weight
virtual machines — container engines use
[operating system tricks](https://earthly.dev/blog/chroot/)

to isolate programs
from each other (“containing” them), making them work as though they were
running on their own hardware with their own filesystem. This makes execution
environments more reproducible, for example by preventing accidental
cross-contamination of environments on the same machine. For added security,
Modal runs containers using the sandboxed
[gVisor container runtime](https://cloud.google.com/blog/products/identity-security/open-sourcing-gvisor-a-sandboxed-container-runtime)

.

Containers are started up from a stored “snapshot” of their filesystem state
called an
*image*
. Producing the image for a container is called
*building*
the
image.

By default, Modal Functions are executed in a
[Debian Linux](https://en.wikipedia.org/wiki/Debian)

container with a basic
Python installation of the same minor version
`v3.x`
as your local Python
interpreter.

To make your Apps and Functions useful, you will probably need some third party system packages
or Python libraries. Modal provides a number of options to customize your container images at
different levels of abstraction and granularity, from high-level convenience
methods like
`pip_install`
through wrappers of core container image build
features like
`RUN`
and
`ENV`
to full on “bring-your-own-Dockerfile”. We’ll
cover each of these in this guide, along with tips and tricks for building
Images effectively when using each tool.

The typical flow for defining an image in Modal is
[method chaining](https://jugad2.blogspot.com/2016/02/examples-of-method-chaining-in-python.html)

starting from a base image, like this:

```
import modal

image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git")
    .pip_install("torch==2.6.0")
    .env({"HALT_AND_CATCH_FIRE": "0"})
    .run_commands("git clone https://github.com/modal-labs/agi && echo 'ready to go!'")
)
```

In addition to being Pythonic and clean, this also matches the onion-like
[layerwise build process](https://docs.docker.com/build/guide/layers/)

of
container images.

Add Python packages with
`pip_install`
--------------------------------------

The simplest and most common container modification is to add some third party
Python package, like
[`pandas`](https://pandas.pydata.org/)

.

You can add Python packages to the environment by passing all the packages you
need to the
[`pip_install`](/docs/reference/modal.Image#pip_install)

method of
an image.

You can include
[typical Python dependency version specifiers](https://peps.python.org/pep-0508/)

,
like
`"torch <= 2.0"`
, in the arguments. But we recommend pinning dependencies
tightly, like
`"torch == 1.9.1"`
, to improve the reproducibility and robustness
of your builds.

```
import modal

datascience_image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install("pandas==2.2.0", "numpy")
)

@app.function(image=datascience_image)
def my_function():
    import pandas as pd
    import numpy as np

    df = pd.DataFrame()
    ...
```

Note that because you can define a different environment for each and every
Modal Function if you so choose, you don’t need to worry about virtual
environment management. Containers make for much better separation of concerns!

If you want to run a specific version of Python remotely rather than just
matching the one you’re running locally, provide the
`python_version`
as a
string when constructing the base image, like we did above.

Add local files with
`add_local_dir`
and
`add_local_file`
---------------------------------------------------------

If you want to forward files from your local system, you can do that using the
`image.add_local_dir`
and
`image.add_local_file`
image builder methods.

```
image = modal.Image.debian_slim().add_local_dir("/user/erikbern/.aws", remote_path="/root/.aws")
```

By default, these files are added to your container as it starts up rather than introducing
a new image layer. This means that the redeployment after making changes is really quick, but
also means you can’t run additional build steps after. You can specify a
`copy=True`
argument
to the
`add_local_`
methods to instead force the files to be included in a built image.

### Adding local Python modules

There is a convenience method for the special case of adding local Python modules to
the container:
[`Image.add_local_python_source`](/docs/reference/modal.Image#add_local_python_source)

The difference from
`add_local_dir`
is that
`add_local_python_source`
takes module names as arguments
instead of a file system path and looks up the local package’s or module’s location via Python’s importing
mechanism. The files are then added to directories that make them importable in containers in the
same way as they are locally.

This is mostly intended for pure Python auxiliary modules that are part of your project and that your code imports,
whereas third party packages should be installed via
[`Image.pip_install()`](/docs/reference/modal.Image#pip_install)

or similar.

```
import modal

app = modal.App()

image_with_module = modal.Image.debian_slim().add_local_python_source("my_local_module")

@app.function(image=image_with_module)
def f():
    import my_local_module  # this will now work in containers
    my_local_module.do_stuff()
```

### What if I have different Python packages locally and remotely?

You might want to use packages inside your Modal code that you don’t have on
your local computer. In the example above, we build a container that uses
`pandas`
. But if we don’t have
`pandas`
locally, on the computer launching the
Modal job, we can’t put
`import pandas`
at the top of the script, since it would
cause an
`ImportError`
.

The easiest solution to this is to put
`import pandas`
in the function body
instead, as you can see above. This means that
`pandas`
is only imported when
running inside the remote Modal container, which has
`pandas`
installed.

Be careful about what you return from Modal Functions that have different
packages installed than the ones you have locally! Modal Functions return Python
objects, like
`pandas.DataFrame`
s, and if your local machine doesn’t have
`pandas`
installed, it won’t be able to handle a
`pandas`
object (the error
message you see will mention
[serialization](https://hazelcast.com/glossary/serialization/)

/
[deserialization](https://hazelcast.com/glossary/deserialization/)

).

If you have a lot of functions and a lot of Python packages, you might want to
keep the imports in the global scope so that every function can use the same
imports. In that case, you can use the
[`imports()`](/docs/reference/modal.Image#imports)

context manager:

```
import modal

pandas_image = modal.Image.debian_slim().pip_install("pandas", "numpy")

with pandas_image.imports():
    import pandas as pd
    import numpy as np

@app.function(image=pandas_image)
def my_function():
    df = pd.DataFrame()
```

Because these imports happen before a new container processes its first input,
you can combine this decorator with
[memory snapshots](/docs/guide/memory-snapshot)

to improve
[cold start performance](/docs/guide/cold-start#share-initialization-work-across-cold-starts-with-memory-snapshots)

for Functions that frequently scale from zero.

Run shell commands with
`.run_commands`
---------------------------------------

You can also supply shell commands that should be executed when building the
container image.

You might use this to preload custom assets, like model parameters, so that they
don’t need to be retrieved when Functions start up:

```
import modal

image_with_model = (
    modal.Image.debian_slim().apt_install("curl").run_commands(
        "curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface.xml",
    )
)

@app.function(image=image_with_model)
def find_cats():
    content = open("/haarcascade_frontalcatface.xml").read()
    ...
```

You can also use this command to install Python packages. For example,
you can use it to install packages with
[`uv`](https://github.com/astral-sh/uv)

,
which can be substantially faster than
`pip`
:

```
import modal

image = (
    modal.Image.debian_slim()
    .pip_install("uv")
    .run_commands("uv pip install --system --compile-bytecode torch")
)
```

Note that it is important to pass
`--compile-bytecode`
when using
`uv`
on Modal.
Unlike
`pip`
,
`uv`
does not produce
[Python bytecode](https://realpython.com/ref/glossary/bytecode/)

(the contents of the
`.pyc`
files in those
`__pycache__`
folders you may have noticed in your Python projects)
by default when packages are installed. On a serverless platform like Modal, skipping that work at installation time
means it instead has to be done every time a container starts.

Run a Python function during your build with
`.run_function`
------------------------------------------------------------

Instead of using shell commands, you can also run a Python function as an image
build step using the
[`Image.run_function`](/docs/reference/modal.Image#run_function)

method. For
example, you can use this to download model parameters from Hugging Face into
your Image:

```
import os
import modal

def download_models() -> None:
    import diffusers

    model_name = "segmind/small-sd"
    pipe = diffusers.StableDiffusionPipeline.from_pretrained(
        model_name, use_auth_token=os.environ["HF_TOKEN"]
    )
    pipe.save_pretrained("/model")

image = (
    modal.Image.debian_slim()
        .pip_install("diffusers[torch]", "transformers", "ftfy", "accelerate")
        .run_function(download_models, secrets=[modal.Secret.from_name("huggingface-secret")])
)
```

Any kwargs accepted by
[`@app.function`](/docs/reference/modal.App#function)

(
[`Volume`
s](/docs/guide/volumes)

, and specifications of
resources like
[GPUs](/docs/guide/gpu)

) can be supplied here.

Essentially, this is equivalent to running a Modal Function and snapshotting the
resulting filesystem as an image.

Whenever you change other features of your image, like the base image or the
version of a Python package, the image will automatically be rebuilt the next
time it is used. This is a bit more complicated when changing the contents of
functions. See the
[reference documentation](/docs/reference/modal.Image#run_function)

for details.

Attach GPUs during setup
------------------------

If a step in the setup of your container image should be run on an instance with
a GPU (e.g., so that a package can query the GPU to set compilation flags), pass a
desired GPU type when defining that step:

```
import modal

image = (
    modal.Image.debian_slim()
    .pip_install("bitsandbytes", gpu="H100")
)
```

Use
`mamba`
instead of
`pip`
with
`micromamba_install`
------------------------------------------------------

`pip`
installs Python packages, but some Python workloads require the
coordinated installation of system packages as well. The
`mamba`
package manager
can install both. Modal provides a pre-built
[Micromamba](https://mamba.readthedocs.io/en/latest/user_guide/micromamba.html)

base image that makes it easy to work with
`micromamba`
:

```
import modal

app = modal.App("bayes-pgm")

numpyro_pymc_image = (
    modal.Image.micromamba()
    .micromamba_install("pymc==5.10.4", "numpyro==0.13.2", channels=["conda-forge"])
)

@app.function(image=numpyro_pymc_image)
def sample():
    import pymc as pm
    import numpyro as np

    print(f"Running on PyMC v{pm.__version__} with JAX/numpyro v{np.__version__} backend")
    ...
```

Use an existing container image with
`.from_registry`
-----------------------------------------------------

You don’t always need to start from scratch! Public registries like
[Docker Hub](https://hub.docker.com/)

have many pre-built container images for
common software packages.

You can use any public image in your function using
[`Image.from_registry`](/docs/reference/modal.Image#from_registry)

, so long as:

* Python 3.9 or later is installed on the
  `$PATH`
  as
  `python`
* `pip`
  is installed correctly
* The image is built for the
  [`linux/amd64`
  platform](https://unix.stackexchange.com/questions/53415/why-are-64-bit-distros-often-called-amd64)
* The image has a
  [valid
  `ENTRYPOINT`](#entrypoint)

```
import modal

sklearn_image = modal.Image.from_registry("huanjason/scikit-learn")

@app.function(image=sklearn_image)
def fit_knn():
    from sklearn.neighbors import KNeighborsClassifier
    ...
```

If an existing image does not have either
`python`
or
`pip`
set up properly, you
can still use it. Just provide a version number as the
`add_python`
argument to
install a reproducible
[standalone build](https://github.com/indygreg/python-build-standalone)

of Python:

```
import modal

image1 = modal.Image.from_registry("ubuntu:22.04", add_python="3.11")
image2 = modal.Image.from_registry("gisops/valhalla:latest", add_python="3.11")
```

The
`from_registry`
method can load images from all public registries, such as
[Nvidia’s
`nvcr.io`](https://catalog.ngc.nvidia.com/containers)

,
[AWS ECR](https://aws.amazon.com/ecr/)

, and
[GitHub’s
`ghcr.io`](https://docs.github.com/en/packages/working-with-a-github-packages-registry/working-with-the-container-registry)

.

We also support access to
[private AWS ECR and GCP Artifact Registry images](/docs/guide/private-registries)

.

Bring your own image definition with
`.from_dockerfile`
-------------------------------------------------------

Sometimes, you might be already have a container image defined in a Dockerfile.

You can define an Image with a Dockerfile using
[`Image.from_dockerfile`](/docs/reference/modal.Image#from_dockerfile)

.
It takes a path to an existing Dockerfile.

For instance, we might write a Dockerfile that adds scikit-learn to the official Python image:

```
FROM python:3.9
RUN pip install sklearn
```

and then define a Modal Image with it:

```
import modal

dockerfile_image = modal.Image.from_dockerfile("Dockerfile")

@app.function(image=dockerfile_image)
def fit():
    import sklearn
    ...
```

Note that you can still do method chaining to extend this image!

### Dockerfile command compatibility

Since Modal doesn’t use Docker to build containers, we have our own
implementation of the
[Dockerfile specification](https://docs.docker.com/engine/reference/builder/)

.
Most Dockerfiles should work out of the box, but there are some differences to
be aware of.

First, a few minor Dockerfile commands and flags have not been implemented yet.
These include
`ONBUILD`
,
`STOPSIGNAL`
, and
`VOLUME`
.
Please reach out to us if your use case requires any of these.

Next, there are some command-specific things that may be useful when porting a
Dockerfile to Modal.

#### `ENTRYPOINT`

While the
[`ENTRYPOINT`](https://docs.docker.com/engine/reference/builder/#entrypoint)

command is supported, there is an additional constraint to the entrypoint script
provided: when used with a Modal Function, it must also
`exec`
the arguments passed to it at some point.
This is so the Modal Function runtime’s Python entrypoint can run after your own. Most entrypoint
scripts in Docker containers are wrappers over other scripts, so this is likely
already the case.

If you wish to write your own entrypoint script, you can use the following as a
template:

```
#!/usr/bin/env bash

# Your custom startup commands here.

exec "$@" # Runs the command passed to the entrypoint script.
```

If the above file is saved as
`/usr/bin/my_entrypoint.sh`
in your container,
then you can register it as an entrypoint with
`ENTRYPOINT ["/usr/bin/my_entrypoint.sh"]`
in your Dockerfile, or with
[`entrypoint`](/docs/reference/modal.Image#entrypoint)

as an
Image build step.

```
import modal

image = (
    modal.Image.debian_slim()
    .pip_install("foo")
    .entrypoint(["/usr/bin/my_entrypoint.sh"])
)
```

#### `ENV`

We currently don’t support default values in
[interpolations](https://docs.docker.com/compose/compose-file/12-interpolation/)

,
such as
`${VAR:-default}`

Image caching and rebuilds
--------------------------

Modal uses the definition of an Image to determine whether it needs to be
rebuilt. If the definition hasn’t changed since the last time you ran or
deployed your App, the previous version will be pulled from the cache.

Images are cached per layer (i.e., per
`Image`
method call), and breaking
the cache on a single layer will cause cascading rebuilds for all subsequent
layers. You can shorten iteration cycles by defining frequently-changing
layers last so that the cached version of all other layers can be used.

In some cases, you may want to force an Image to rebuild, even if the
definition hasn’t changed. You can do this by adding the
`force_build=True`
argument to any of the Image building methods.

```
import modal

image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .pip_install("slack-sdk", force_build=True)
    .run_commands("echo hi")
)
```

As in other cases where a layer’s definition changes, both the
`pip_install`
and
`run_commands`
layers will rebuild, but the
`apt_install`
will not. Remember to
remove
`force_build=True`
after you’ve rebuilt the Image, or it will
rebuild every time you run your code.

Alternatively, you can set the
`MODAL_FORCE_BUILD`
environment variable (e.g.
`MODAL_FORCE_BUILD=1 modal run ...`
) to rebuild all images attached to your App.
But note that when you rebuild a base layer, the cache will be invalidated for
*all*
Images that depend on it, and they will rebuild the next time you run or deploy
any App that uses that base. If you’re debugging an issue with your Image, a better
option might be using
`MODAL_IGNORE_CACHE=1`
. This will rebuild the Image from the
top without breaking the Image cache or affecting subsequent builds.

Image builder updates
---------------------

Because changes to base images will cause cascading rebuilds, Modal is
conservative about updating the base definitions that we provide. But many
things are baked into these definitions, like the specific versions of the Image
OS, the included Python, and the Modal client dependencies.

We provide a separate mechanism for keeping base images up-to-date without
causing unpredictable rebuilds: the “Image Builder Version”. This is a workspace
level-configuration that will be used for every Image built in your workspace.
We release a new Image Builder Version every few months but allow you to update
your workspace’s configuration when convenient. After updating, your next
deployment will take longer, because your Images will rebuild. You may also
encounter problems, especially if your Image definition does not pin the version
of the third-party libraries that it installs (as your new Image will get the
latest version of these libraries, which may contain breaking changes).

You can set the Image Builder Version for your workspace by going to your
[workspace settings](/settings/image-config)

. This page also documents the
important updates in each version.

[Images](#images)

[Add Python packages with pip\_install](#add-python-packages-with-pip_install)

[Add local files with add\_local\_dir and add\_local\_file](#add-local-files-with-add_local_dir-and-add_local_file)

[Adding local Python modules](#adding-local-python-modules)

[What if I have different Python packages locally and remotely?](#what-if-i-have-different-python-packages-locally-and-remotely)

[Run shell commands with .run\_commands](#run-shell-commands-with-run_commands)

[Run a Python function during your build with .run\_function](#run-a-python-function-during-your-build-with-run_function)

[Attach GPUs during setup](#attach-gpus-during-setup)

[Use mamba instead of pip with micromamba\_install](#use-mamba-instead-of-pip-with-micromamba_install)

[Use an existing container image with .from\_registry](#use-an-existing-container-image-with-from_registry)

[Bring your own image definition with .from\_dockerfile](#bring-your-own-image-definition-with-from_dockerfile)

[Dockerfile command compatibility](#dockerfile-command-compatibility)

[ENTRYPOINT](#entrypoint)

[ENV](#env)

[Image caching and rebuilds](#image-caching-and-rebuilds)

[Image builder updates](#image-builder-updates)

See it in action

[Registry image for Algolia indexing](/docs/examples/algolia_indexer)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/job-queue
================================================================================

Job processing
==============

Modal can be used as a scalable job queue to handle asynchronous tasks submitted
from a web app or any other Python application. This allows you to offload up to 1 million
long-running or resource-intensive tasks to Modal, while your main application
remains responsive.

Creating jobs with .spawn()
---------------------------

The basic pattern for using Modal as a job queue involves three key steps:

1. Defining and deploying the job processing function using
   `modal deploy`
   .
2. Submitting a job using
   [`modal.Function.spawn()`](/docs/reference/modal.Function#spawn)
3. Polling for the job’s result using
   [`modal.FunctionCall.get()`](/docs/reference/modal.FunctionCall#get)

Here’s a simple example that you can run with
`modal run my_job_queue.py`
:

```
# my_job_queue.py
import modal

app = modal.App("my-job-queue")

@app.function()
def process_job(data):
    # Perform the job processing here
    return {"result": data}

def submit_job(data):
    # Since the `process_job` function is deployed, need to first look it up
    process_job = modal.Function.from_name("my-job-queue", "process_job")
    call = process_job.spawn(data)
    return call.object_id

def get_job_result(call_id):
    function_call = modal.FunctionCall.from_id(call_id)
    try:
        result = function_call.get(timeout=5)
    except modal.exception.OutputExpiredError:
        result = {"result": "expired"}
    except TimeoutError:
        result = {"result": "pending"}
    return result

@app.local_entrypoint()
def main():
    data = "my-data"

    # Submit the job to Modal
    call_id = submit_job(data)
    print(get_job_result(call_id))
```

In this example:

* `process_job`
  is the Modal function that performs the actual job processing.
  To deploy the
  `process_job`
  function on Modal, run
  `modal deploy my_job_queue.py`
  .
* `submit_job`
  submits a new job by first looking up the deployed
  `process_job`
  function, then calling
  `.spawn()`
  with the job data. It returns the unique ID
  of the spawned function call.
* `get_job_result`
  attempts to retrieve the result of a previously submitted job
  using
  [`FunctionCall.from_id()`](/docs/reference/modal.FunctionCall#from_id)

  and
  [`FunctionCall.get()`](/docs/reference/modal.FunctionCall#get)

  .
  [`FunctionCall.get()`](/docs/reference/modal.FunctionCall#get)

  waits indefinitely
  by default. It takes an optional timeout argument that specifies the maximum
  number of seconds to wait, which can be set to 0 to poll for an output
  immediately. Here, if the job hasn’t completed yet, we return a pending
  response.
* The results of a
  `.spawn()`
  are accessible via
  `FunctionCall.get()`
  for up to
  7 days after completion. After this period, we return an expired response.

[Document OCR Web App](/docs/examples/doc_ocr_webapp)

is an example that uses
this pattern.

Integration with web frameworks
-------------------------------

You can easily integrate the job queue pattern with web frameworks like FastAPI.
Here’s an example, assuming that you have already deployed
`process_job`
on
Modal with
`modal deploy`
as above. This example won’t work if you haven’t
deployed your app yet.

```
# my_job_queue_endpoint.py
import fastapi
import modal

image = modal.Image.debian_slim().pip_install("fastapi[standard]")
app = modal.App("fastapi-modal", image=image)
web_app = fastapi.FastAPI()

@app.function()
@modal.asgi_app()
def fastapi_app():
    return web_app

@web_app.post("/submit")
async def submit_job_endpoint(data):
    process_job = modal.Function.from_name("my-job-queue", "process_job")

    call = process_job.spawn(data)
    return {"call_id": call.object_id}

@web_app.get("/result/{call_id}")
async def get_job_result_endpoint(call_id: str):
    function_call = modal.FunctionCall.from_id(call_id)
    try:
        result = function_call.get(timeout=0)
    except modal.exception.OutputExpiredError:
        return fastapi.responses.JSONResponse(content="", status_code=404)
    except TimeoutError:
        return fastapi.responses.JSONResponse(content="", status_code=202)

    return result
```

In this example:

* The
  `/submit`
  endpoint accepts job data, submits a new job using
  `process_job.spawn()`
  , and returns the job’s ID to the client.
* The
  `/result/{call_id}`
  endpoint allows the client to poll for the job’s
  result using the job ID. If the job hasn’t completed yet, it returns a 202
  status code to indicate that the job is still being processed. If the job
  has expired, it returns a 404 status code to indicate that the job is not found.

You can try this app by serving it with
`modal serve`
:

```
modal serve my_job_queue_endpoint.py
```

Then interact with its endpoints with
`curl`
:

```
# Make a POST request to your app endpoint with.
$ curl -X POST $YOUR_APP_ENDPOINT/submit?data=data
{"call_id":"fc-XXX"}

# Use the call_id value from above.
$ curl -X GET $YOUR_APP_ENDPOINT/result/fc-XXX
```

Scaling and reliability
-----------------------

Modal automatically scales the job queue based on the workload, spinning up new
instances as needed to process jobs concurrently. It also provides built-in
reliability features like automatic retries and timeout handling.

You can customize the behavior of the job queue by configuring the
`@app.function()`
decorator with options like
[`retries`](/docs/guide/retries#function-retries)

,
[`timeout`](/docs/guide/timeouts#timeouts)

, and
[`max_containers`](/docs/guide/scale#configuring-autoscaling-behavior)

.

[Job processing](#job-processing)

[Creating jobs with .spawn()](#creating-jobs-with-spawn)

[Integration with web frameworks](#integration-with-web-frameworks)

[Scaling and reliability](#scaling-and-reliability)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/lifecycle-functions
================================================================================

Container lifecycle hooks
=========================

Since Modal will reuse the same container for multiple inputs, sometimes you
might want to run some code exactly once when the container starts or exits.

To accomplish this, you need to use Modal’s class syntax and the
[`@app.cls`](/docs/reference/modal.App#cls)

decorator. Specifically, you’ll
need to:

1. Convert your function to a method by making it a member of a class.
2. Decorate the class with
   `@app.cls(...)`
   with same arguments you previously
   had for
   `@app.function(...)`
   .
3. Instead of the
   `@app.function`
   decorator on the original method, use
   `@method`
   or the appropriate decorator for a
   [web endpoint](#lifecycle-hooks-for-web-endpoints)

   .
4. Add the correct method “hooks” to your class based on your need:
   * `@enter`
     for one-time initialization (remote)
   * `@exit`
     for one-time cleanup (remote)

`@enter`
--------

The container entry handler is called when a new container is started. This is
useful for doing one-time initialization, such as loading model weights or
importing packages that are only present in that image.

To use, make your function a member of a class, and apply the
`@enter()`
decorator to one or more class methods:

```
import modal

app = modal.App()

@app.cls(cpu=8)
class Model:
    @modal.enter()
    def run_this_on_container_startup(self):
        import pickle
        self.model = pickle.load(open("model.pickle"))

    @modal.method()
    def predict(self, x):
        return self.model.predict(x)

@app.local_entrypoint()
def main():
    Model().predict.remote(x=123)
```

When working with an
[asynchronous Modal](/docs/guide/async)

app, you may use an
async method instead:

```
import modal

app = modal.App()

@app.cls(memory=1024)
class Processor:
    @modal.enter()
    async def my_enter_method(self):
        self.cache = await load_cache()

    @modal.method()
    async def run(self, x):
        return await do_some_async_stuff(x, self.cache)

@app.local_entrypoint()
async def main():
    await Processor().run.remote(x=123)
```

Note: The
`@enter()`
decorator replaces the earlier
`__enter__`
syntax, which
has been deprecated.

`@exit`
-------

The container exit handler is called when a container is about to exit. It is
useful for doing one-time cleanup, such as closing a database connection or
saving intermediate results. To use, make your function a member of a class, and
apply the
`@exit()`
decorator:

```
import modal

app = modal.App()

@app.cls()
class ETLPipeline:
    @modal.enter()
    def open_connection(self):
        import psycopg2
        self.connection = psycopg2.connect(os.environ["DATABASE_URI"])

    @modal.method()
    def run(self):
        # Run some queries
        pass

    @modal.exit()
    def close_connection(self):
        self.connection.close()

@app.local_entrypoint()
def main():
    ETLPipeline().run.remote()
```

Exit handlers are also called when a container is
[preempted](/docs/guide/preemption)

.
The exit handler is given a grace period of 30 seconds to finish, and it will be
killed if it takes longer than that to complete.

Lifecycle hooks for web endpoints
---------------------------------

Modal
`@function`
s that are
[web endpoints](/docs/guide/webhooks)

can be
converted to the class syntax as well. Instead of
`@modal.method`
, simply use
whichever of the web endpoint decorators (
`@modal.fastapi_endpoint`
,
`@modal.asgi_app`
or
`@modal.wsgi_app`
) you were using before.

```
from fastapi import Request

import modal

image = modal.Image.debian_slim().pip_install("fastapi")
app = modal.App("web-endpoint-cls", image=image)

@app.cls()
class Model:
    @modal.enter()
    def run_this_on_container_startup(self):
        self.model = pickle.load(open("model.pickle"))

    @modal.fastapi_endpoint()
    def predict(self, request: Request):
        ...
```

[Container lifecycle hooks](#container-lifecycle-hooks)

[@enter](#enter)

[@exit](#exit)

[Lifecycle hooks for web endpoints](#lifecycle-hooks-for-web-endpoints)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/local-data
================================================================================

Passing local data
==================

If you have a function that needs access to some data not present in your Python
files themselves you have a few options for bundling that data with your Modal
app.

Passing function arguments
--------------------------

The simplest and most straight-forward way is to read the data from your local
script and pass the data to the outermost Modal function call:

```
import json

@app.function()
def foo(a):
    print(sum(a["numbers"]))

@app.local_entrypoint()
def main():
    data_structure = json.load(open("blob.json"))
    foo.remote(data_structure)
```

Any data of reasonable size that is serializable through
[cloudpickle](https://github.com/cloudpipe/cloudpickle)

is passable as an
argument to Modal functions.

Refer to the section on
[global variables](/docs/guide/global-variables)

for how
to work with objects in global scope that can only be initialized locally.

Including local files
---------------------

For including local files for your Modal Functions to access, see
[Defining Images](/docs/guide/images)

.

[Passing local data](#passing-local-data)

[Passing function arguments](#passing-function-arguments)

[Including local files](#including-local-files)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/managing-deployments
================================================================================

Managing deployments
====================

Once you’ve finished using
`modal run`
or
`modal serve`
to iterate on your Modal
code, it’s time to deploy. A Modal deployment creates and then persists an
application and its objects, providing the following benefits:

* Repeated application function executions will be grouped under the deployment,
  aiding observability and usage tracking. Programmatically triggering lots of
  ephemeral App runs can clutter your web and CLI interfaces.
* Function calls are much faster because deployed functions are persistent and
  reused, not created on-demand by calls. Learn how to trigger deployed
  functions in
  [Invoking deployed functions](/docs/guide/trigger-deployed-functions)

  .
* [Scheduled functions](/docs/guide/cron)

  will continue scheduling separate from
  any local iteration you do, and will notify you on failure.
* [Web endpoints](/docs/guide/webhooks)

  keep running when you close your laptop,
  and their URL address matches the deployment name.

Creating deployments
--------------------

Deployments are created using the
[`modal deploy`
command](/docs/reference/cli/app#modal-app-list)

.

```
 % modal deploy -m whisper_pod_transcriber.main
✓ Initialized. View app page at https://modal.com/apps/ap-PYc2Tb7JrkskFUI8U5w0KG.
✓ Created objects.
├── 🔨 Created populate_podcast_metadata.
├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber at /root/whisper_pod_transcriber
├── 🔨 Created fastapi_app => https://modal-labs-whisper-pod-transcriber-fastapi-app.modal.run
├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber/whisper_frontend/dist at /assets
├── 🔨 Created search_podcast.
├── 🔨 Created refresh_index.
├── 🔨 Created transcribe_segment.
├── 🔨 Created transcribe_episode..
└── 🔨 Created fetch_episodes.
✓ App deployed! 🎉

View Deployment: https://modal.com/apps/modal-labs/whisper-pod-transcriber
```

Running this command on an existing deployment will redeploy the App,
incrementing its version. For detail on how live deployed apps transition
between versions, see the
[Updating deployments](#updating-deployments)

section.

Deployments can also be created programmatically using Modal’s
[Python API](/docs/reference/modal.App#deploy)

.

Viewing deployments
-------------------

Deployments can be viewed either on the
[apps](/apps)

web page or by using the
[`modal app list`
command](/docs/reference/cli/app#modal-app-list)

.

Updating deployments
--------------------

A deployment can deploy a new App or redeploy a new version of an existing
deployed App. It’s useful to understand how Modal handles the transition between
versions when an App is redeployed. In general, Modal aims to support
zero-downtime deployments by gradually transitioning traffic to the new version.

If the deployment involves building new versions of the Images used by the App,
the build process will need to complete succcessfully. The existing version of
the App will continue to handle requests during this time. Errors during the
build will abort the deployment with no change to the status of the App.

After the build completes, Modal will start to bring up new containers running
the latest version of the App. The existing containers will continue handling
requests (using the previous version of the App) until the new containers have
completed their cold start.

Once the new containers are ready, old containers will stop accepting new
requests. However, the old containers will continue running any requests they
had previously accepted. The old containers will not terminate until they have
finished processing all ongoing requests.

Any warm pool containers will also be cycled during a deployment, as the
previous version’s warm pool are now outdated.

Deployment rollbacks
--------------------

To quickly reset an App back to a previous version, you can perform a deployment
*rollback*
. Rollbacks can be triggered from either the App dashboard or the CLI.
Rollback deployments look like new deployments: they increment the version number
and are attributed to the user who triggered the rollback. But the App’s functions
and metadata will be reset to their previous state independently of your current
App codebase.

Note that deployment rollbacks are supported only on the Team and Enterprise plans.

Stopping deployments
--------------------

Deployed apps can be stopped in the web UI by clicking the red “Stop app” button on
the App’s “Overview” page, or alternatively from the command line using the
[`modal app stop`
command](/docs/reference/cli/app#modal-app-stop)

.

Stopping an App is a destructive action. Apps cannot be restarted from this state;
a new App will need to be deployed from the same source files. Objects associated
with stopped deployments will eventually be garbage collected.

[Managing deployments](#managing-deployments)

[Creating deployments](#creating-deployments)

[Viewing deployments](#viewing-deployments)

[Updating deployments](#updating-deployments)

[Deployment rollbacks](#deployment-rollbacks)

[Stopping deployments](#stopping-deployments)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/memory-snapshot
================================================================================

Memory Snapshot
===============

Memory snapshots can dramatically improve cold start performance for compatible Modal Functions.

During startup, your Python function typically reads many files from the file system, which
is expensive. For example, the
`torch`
package is
[hundreds of MiB](https://pypi.org/project/torch/#files)

and requires over 20,000 file operations to load! With memory snapshots, Modal
will produce restorable saves of your Function’s container right after initialization and use these when available to lower startup latency. Functions with memory snapshots enabled
**typically start 1.5-3x faster**
.

Modal produces snapshots for deployed Functions on demand, creating and maintaining several snapshots to ensure coverage across our diverse worker fleet. Modal will also automatically expire snapshots and create new ones as we make runtime and security updates.

You don’t need to modify CPU Functions to take advantage of snapshotting in most
cases. GPU-enabled Functions typically require refactoring to move GPU
initialization into post-restore lifecycle functions (see below).

Enabling snapshots
------------------

You can enable memory snapshots for your Function with the
`enable_memory_snapshot=True`
parameter:

```
import modal

app = modal.App("example-memory-snapshot")

@app.function(enable_memory_snapshot=True)
def my_func():
    print("hello")
```

Then deploy the App with
`modal deploy`
. Memory snapshots are created only when an App is in a deployed state and aren’t enabled for ephemeral Apps.

Keep the following in mind when using memory snapshots:

* Every time a snapshot is created, Modal logs
  `Creating memory snapshot for Function.`
  .
* Modal creates several snapshots for a given version of your Function (see
  [Snapshot compatibility](#snapshot-compatibility)

  section).
* Redeploying your Function may cause Modal to create new snapshots, as existing snapshots
  might not be compatible with your updated Function.
* Creating memory snapshots adds latency to a Function’s startup time, so expect
  your Function to be slower to start during the first invocations.
* [modal.Volume](/docs/guide/volumes)

  mutations do not cause snapshots to update.
  Deleting files in a Volume used during restore will cause restore failures.

Updating snapshots
------------------

Redeploying your Function with new configuration (e.g. a new GPU type) or new code will cause previous snapshots to become obsolete. Subsequent invocations to the new Function version will automatically create new snapshots from the new configuration and code.

Modal also automatically recreates your snapshots to keep up with platform’s latest runtime and security changes.

Snapshot compatibility
----------------------

Modal will create memory snapshots for every new version of your Function.
Changing your Function or updating its dependencies will trigger a new
snapshotting operation when you run your Function anew.

Additionally, you may observe in application logs your Function being memory
snapshots multiple times during its first few invocations. This happens because
memory snapshots are compatible with the underlying worker type that created them,
and Modal Functions run across a handful of worker types.

CPU-only Functions need around 6 snapshots for coverage, and Functions targeting a specific
GPU (e.g. A100) need 2-3. The cold boot benefits should greatly outweigh the penalty of creating multiple
snapshots.

Using snapshots with lifecycle functions
----------------------------------------

It’s currently not possible to snapshot GPU memory. We avoid exposing GPU
devices to your Function during the snapshotting stage (e.g. when
`@enter(snap=True)`
). NVIDIA drivers are available but no GPU devices are.

To work around this limitation, we suggest refactoring your initialization code
to run across two separate
`@modal.enter`
functions: one that runs before
creating the snapshot (
`snap=True`
), and one that runs after restoring from the
snapshot (
`snap=False`
). Load model weights onto CPU memory in the
`snap=True`
method, and then move the weights onto GPU memory in the
`snap=False`
method.

Here’s an example using the
`sentence-transformers`
package:

```
import modal

image = modal.Image.debian_slim().pip_install("sentence-transformers")
app = modal.App("sentence-transformers", image=image)

with image.imports():
    from sentence_transformers import SentenceTransformer

model_vol = modal.Volume.from_name("sentence-transformers-models", create_if_missing=True)

@app.cls(gpu="a10g", volumes={"/models": model_vol}, enable_memory_snapshot=True)
class Embedder:
    model_id = "BAAI/bge-small-en-v1.5"

    @modal.enter(snap=True)
    def load(self):
        # Create a memory snapshot with the model loaded in CPU memory.
        self.model = SentenceTransformer(f"/models/{self.model_id}", device="cpu")

    @modal.enter(snap=False)
    def setup(self):
        self.model.to("cuda")  # Move the model to a GPU!

    @modal.method()
    def run(self, sentences:list[str]):
        embeddings = self.model.encode(sentences, normalize_embeddings=True)
        print(embeddings)

@app.local_entrypoint()
def main():
    Embedder().run.remote(sentences=["what is the meaning of life?"])

if __name__ == "__main__":
    cls = modal.Cls.from_name("sentence-transformers", "Embedder")
    cls().run.remote(sentences=["what is the meaning of life?"])
```

Snapshotting reduces the time it takes for this App’s Function to startup by about 3x, from ~6 seconds down to just ~2 seconds.

### Caching GPU information

If your program calls functions that check if GPUs are available during snapshotting,
they will get a misleading report.

In the following example, GPUs are not available when
`no_gpus_available_during_snapshots()`
is called, but they are when the app
is restored and
`gpus_available_following_restore()`
is called:

```
import modal

app = modal.App(image=modal.Image.debian_slim().pip_install("torch"))

@app.cls(enable_memory_snapshot=True, gpu="any")
class GPUAvailability:

    @modal.enter(snap=True)
    def no_gpus_available_during_snapshots(self):
        import torch
        print(f"GPUs available: {torch.cuda.is_available()}")  # False

    @modal.enter(snap=False)
    def gpus_available_following_restore(self):
        import torch
        print(f"GPUs available: {torch.cuda.is_available()}")  # True

    @modal.method()
    def demo(self):
        print("Done!")
```

The
`torch.cuda`
module has multiple functions which, if called during
snapshotting, will initialize CUDA as having zero GPU devices. Such functions
include
`torch.cuda.is_available`
and
`torch.cuda.get_device_capability`
.

If you’re using a framework that calls these methods during its import phase,
it may not be compatible with memory snapshots. The problem can manifest as
confusing “cuda not available” or “no CUDA-capable device is detected” errors.

In particular,
`xformers`
is known to call
`torch.cuda.get_device_capability`
on
import, so if it is imported during snapshotting it can unhelpfully initialize
CUDA with zero GPUs. The
[workaround](https://github.com/facebookresearch/xformers/issues/1030)

for this
is to set the
`XFORMERS_ENABLE_TRITON`
environment variable to
`1`
in your
`modal.Image`
.

```
image = modal.Image.debian_slim().pip_install("xformers>=0.28")  # for instance
image = image.env({"XFORMERS_ENABLE_TRITON": "1"})
```

### Randomness and uniqueness

If your application depends on uniqueness of state, you must evaluate your
Function code and verify that it is resilient to snapshotting operations. For
example, if a variable is randomly initialized and snapshotted, that variable
will be identical after every restore, possibly breaking uniqueness expectations
of the proceeding Function code.

[Memory Snapshot](#memory-snapshot)

[Enabling snapshots](#enabling-snapshots)

[Updating snapshots](#updating-snapshots)

[Snapshot compatibility](#snapshot-compatibility)

[Using snapshots with lifecycle functions](#using-snapshots-with-lifecycle-functions)

[Caching GPU information](#caching-gpu-information)

[Randomness and uniqueness](#randomness-and-uniqueness)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/modal-1-0-migration
================================================================================

Modal 1.0 migration guide
=========================

We released version 1.0 of the Modal Python SDK in May 2025.
This release signifies an increased commitment to API stability and implies
some changes to our development workflow.

Preceding the 1.0 release, we introduced a number of deprecations and changes
based on feedback that we received from early users. These changes were intended
to address pain points and reduce confusion about some aspects of the Modal API.
While adapting to them requires some changes to existing code, we believe that
they’ll make it easier to use Modal going forward.

This page highlights the major changes for 1.0 and provides some advice for how
to migrate your code to the new stable APIs. Most deprecations introduced prior
to the release of v1.0 will not be enforced (actually cause breaking changes)
until a subsequent minor (v1.x) release, but we recommend updating your code so
that you can take advantage of new features and avoid any future issues.

Deprecating
`Image.copy_*`
methods
----------------------------------

*Introduced in: v0.72.11*

We recently introduced new
`Image`
methods —
`Image.add_local_dir`
and
`Image.add_local_file`
— to replace the existing
`Image.copy_local_dir`
and
`Image.copy_local_file`
.

The new methods subsume the functionality of the old ones, but their default
behavior is different and more performant. By default, files will be mounted to
the container at runtime rather than copied into a new
`Image`
layer. This can
speed up development substantially when iterating on the contents of the files.

Building a new
`Image`
layer should be necessary only when subsequent build
steps will use the added files. In that case, you can pass
`copy=True`
in
`Image.add_local_file`
or
`Image.add_local_dir`
.

The
`Image.add_local_dir`
method also has an
`ignore=`
parameter, which you can
use to pass file-matching patterns (using dockerignore rules) or predicate
functions to exclude files.

Deprecating
`Mount`
as part of the public API
---------------------------------------------

*Introduced in: v0.72.4*
|
*Enforced in: v1.0.0*

Currently, local files can be mounted to the container filesystem either by
including them in the
`Image`
definition or by passing a
`modal.Mount`
object
directly to the
`App.function`
or
`App.cls`
decorators. As part of the 1.0
release, we are simplifying the container filesystem configuration to be defined
only by the
`Image`
used for each Function. This implies deprecation of the
following:

* The
  `mount=`
  parameter of
  `App.function`
  and
  `App.cls`
* The
  `context_mount=`
  parameter of several
  `modal.Image`
  methods
* The
  `Image.copy_mount`
  method
* The
  `Mount`
  object

Code that uses the
`mount=`
parameter of
`App.function`
and
`App.cls`
should be
migrated to pass those files / directories to the
`Image`
used by that Function
or Cls, i.e. using the
`Image.add_local_file`
,
`Image.add_local_dir`
, or
`Image.add_local_python_source`
methods:

```
# Mounting local files

# Old way (deprecated)
mount = modal.Mount.from_local_dir("data").add_local_file("config.yaml")
@app.function(image=image, mount=mount)
def f():
    ...

# New way
image = image.add_local_dir("data", "/root/data").add_local_file("config.yaml", "/root/config.yaml")
@app.function(image=image)
def f():
    ...

## Mounting local Python source code

# Old way (deprecated)
mount = modal.Mount.from_local_python_packages("my-lib"))
@app.function(image=image, mount=mount)
def f()
    ...

# New way
image = image.add_local_python_source("my-lib")
@app.function(image=image)
def f(...):
    ...

## Using Image.copy_mount

# Old way (deprecated)
mount = modal.Mount.from_local_dir("data").add_local_file("config.yaml")
image.copy_mount(mount)

# New way
image.add_local_dir("data", "root/data").add_local_file("config.yaml", "/root/config.yaml")
```

Code that uses the
`context_mount=`
parameter of
`Image.from_dockerfile`
and
`Image.dockerfile_commands`
methods can delete that parameter; we now
automatically infer the files that need to be included in the context.

Deprecating the
`@modal.build`
decorator
----------------------------------------

*Introduced in: v0.72.17*

As part of consolidating the filesystem configuration API, we are also
deprecating the
`modal.build`
decorator.

For use cases where
`modal.build`
would previously have been the suggested
approach (e.g., downloading model weights or other large assets to the
container filesystem), we now recommend using a
`modal.Volume`
instead. The
main advantage of storing weights in a
`Volume`
instead of an
`Image`
is that
the weights do not need to be re-downloaded every time you change something else
about the
`Image`
definition.

Many frameworks, such as Hugging Face, automatically cache downloaded model
weights. When using these frameworks, you just need to ensure that you mount a
`modal.Volume`
to the expected location of the framework’s cache:

```
cache_vol = modal.Volume.from_name("hf-hub-cache")
@app.cls(
    image=image.env({"HF_HUB_CACHE": "/cache"}),
    volumes={"/cache": cache_vol},
    ...
)
class Model:
    @modal.enter()
    def load_model(self):
        self.model = ModelClass.from_pretrained(...)
```

For frameworks that don’t support automatic caching, you could write a separate
function to download the weights and write them directly to the Volume, then
`modal run`
against this function before you deploy.

In some cases (e.g., if the step runs very quickly), you may wish for the logic
currently decorated with
`@modal.build`
to continue modifying the Image
filesystem. In that case, you can extract the method as a standalone function
and pass it to
`Image.run_function`
:

```
def download_weights():
    ...

image = image.run_function(download_weights)
```

Requiring explicit inclusion of local Python dependencies
---------------------------------------------------------

*Introduced in: 0.73.11*
|
*Enforced in: 1.0.0*

Prior to 1.0, Modal will inspect the modules that are imported when running
your App code and automatically include any “local” modules in the remote
container environment. This behavior is referred to as “automounting”.

While convenient, this approach has a number of edge cases and surprising
behaviors, such as ignoring modules with imports that are deferred using
`Image.imports`
. Additionally, it is difficult to configure the automounting
behavior to, e.g., ignore large data files that are stored within your local
Python project directories.

Going forward, it will be necessary to explicitly include the local dependencies
of your Modal App. The easiest way to do this is with
[`Image.add_local_python_source`](/docs/reference/modal.Image#add_local_python_source)

:

```
import modal
import helpers

image = modal.Image.debian_slim().add_local_python_source("helpers")
```

In the period leading up to the change in default behavior, the Modal client
will issue deprecation warnings when automounted modules are not included
in the Image. Updating the Image definition will remove these warnings.

Note that Modal will continue to automatically include the source module or
package defining the App itself. We’re introducing a new App or Function-level
parameter,
`include_source`
, which can be set to
`False`
in cases where this is
not desired (i.e., because your Image definition already includes the App
source).

Renaming autoscaler parameters
------------------------------

*Introduced in: v0.73.76*

We’re renaming several parameters that configure autoscaling behavior:

* `keep_warm`
  is now
  `min_containers`
* `concurrency_limit`
  is now
  `max_containers`
* `container_idle_timeout`
  is now
  `scaledown_window`

The renaming is intended to address some persistent confusion about
the meaning of these parameters. The migration path is a simple
find-and-replace operation.

Additionally, we’re promoting a fourth parameter,
`buffer_containers`
,
from experimental status (previously
`_experimental_buffer_containers`
).
Like
`min_containers`
,
`buffer_containers`
can help mitigate cold-start
penalties by overprovisioning containers while the Function is active.

Renaming
`modal.web_endpoint`
to
`modal.fastapi_endpoint`
---------------------------------------------------------

*Introduced in: v0.73.89*

We’re renaming the
`modal.web_endpoint`
decorator to
`modal.fastapi_endpoint`
so that the implicit dependency on FastAPI is more clear. This can be a
simple name substitution in your code as the semantics are otherwise identical.

We may reintroduce a lightweight
`modal.web_endpoint`
without external
dependencies in the future.

Replacing
`allow_concurrent_inputs`
with
`@modal.concurrent`
------------------------------------------------------------

*Introduced in: v0.73.148*

The
`allow_concurrent_inputs`
parameter is being replaced with a new decorator,
`@modal.concurrent`
. The decorator can be applied either to a Function or a Cls.
We’re moving the input concurrency feature out of “Beta” status as part of this
change.

The new decorator exposes two distinct parameters:
`max_inputs`
(the limit
on the number of inputs the Function will concurrently accept) and
`target_inputs`
(the level of concurrency targeted by the Modal autoscaler).
The simplest migration path is to replace
`allow_concurrent_inputs=N`
with
`@modal.concurrent(max_inputs=N)`
:

```
# Old way, with a function (deprecated)
@app.function(allow_concurrent_inputs=1000)
def f(...):
    ...

# New way, with a function
@app.function()
@modal.concurrent(max_inputs=1000)
def f(...):
    ...

# Old way, with a class (deprecated)
@app.cls(allow_concurrent_inputs=1000)
class MyCls:
    ...

# New way, with a class
@app.cls()
@modal.concurrent(max_inputs=1000)
class MyCls:
    ...
```

Setting
`target_inputs`
along with
`max_inputs`
may benefit performance by
reducing latency during periods where the container pool is scaling up. See the
[input concurrency guide](/docs/guide/concurrent-inputs)

for more information.

Deprecating the
`.lookup`
method on Modal objects
-------------------------------------------------

*Introduced in: v0.72.56*

Most Modal objects can be instantiated through two distinct methods:
`.from_name`
and
`.lookup`
. The redundancy between these methods is a persistent
source of confusion.

The
`.from_name`
method is lazy: it operates entirely locally and instantiates
only a shell for the object. The local object won’t be associated with its
identity on the Modal server until you interact with it. In contrast, the
`.lookup`
method is eager: it triggers a remote call to the Modal server, and it
returns a fully-hydrated object.

Because Modal objects can now be hydrated on-demand, when they are first
used, there is rarely any need to eagerly hydrate. Therefore, we’re deprecating
`.lookup`
so that there’s only one obvious way to instantiate objects.

In most cases, the migration is a simple find-and-replace of
`.lookup`
→
`.from_name`
.

One exception is when your code needs to access object metadata, such as its ID,
or a web endpoint’s URL. In that case, you can explicitly force hydration of the
object by calling its
`.hydrate()`
method. There may be other subtle consequences,
such as errors being rasied at a different location if no object exists with the
given name.

Removing support for custom Cls constructors
--------------------------------------------

*Introduced in: v0.74.0*

Classes decorated with
`App.cls`
are no longer allowed to have a custom constructor
(
`__init__`
method). Instead, class parameterization should be exposed using
dataclass-style
[`modal.parameter`](/docs/reference/modal.parameter)

annotations:

```
# Old way (deprecated)
@app.cls()
class MyCls:
    def __init__(self, name: str = "Bert"):
        self.name = name

# New way
@app.cls()
class MyCls:
    name: str = modal.parameter(default="Bert")
```

Modal will provide a synthetic constructor for classes that use
`modal.parameter`
.
Arguments to the synthetic constructor must be passed using keywords, so you may
need to update your calling code as well:

```
obj = MyCls(name="Bert")  # name= is now required
```

We’re making this change to address some persistent confusion about when
constructors execute for remote calls and what operations are allowed to run in
them. If your custom constructor performs any setup logic beyond storing the
parameter values, you should move it to a method decorated with
`@modal.enter()`
.

Additionally, we’re reducing the types that we support as class parameters to
a small number of primitives (
`str`
,
`int`
,
`bool`
, and
`bytes`
).

Limiting class parameterization to primitive types will also allow us to provide
better observability over parameterized class instances in the web dashboard,
CLI, and other contexts where it is not possible to represent arbitrary Python
objects.

If you need to parameterize classes across more complex types, you can implement
your own serialization logic, e.g. using strings as the wire format:

```
@app.cls()
class MyCls:
    param_str: str = modal.parameter()

    @modal.enter()
    def deserialize_parameters(self):
        self.param_obj = SomeComplexType.from_str(self.param_str)
```

We recommend adopting interpretable constructor arguments (i.e., prefer
meaningful strings over pickled bytes) so that you will be able to get the most
benefit from future improvements to parameterized class observability.

Simplifying Cls lookup patterns
-------------------------------

*Introduced in: v0.73.26*

Modal previously supported several different patterns for looking up a
`modal.Cls`
and remotely invoking one of its methods:

```
# Documented pattern
MyCls = modal.Cls.from_name("my-app", "MyCls")
obj = MyCls()
obj.some_method.remote(...)

# Alternate pattern: skipping the object instantiation
MyCls = modal.Cls.from_name("my-app", "MyCls")
MyCls.some_method.remote(...)

# Alternate pattern: looking up the method as a Function
f = modal.Function.lookup("my-app", "MyCls.some_method")
f.remote(...)
```

While each pattern could successfully trigger a remote function call, there were
a number of subtle differences in behavior between them.

Going forward, we will only support the first pattern. Making remote calls to a
method on a deployed Cls will require you to (a) look up the object using
`modal.Cls`
and (b) instantiate the object before calling its methods.

Deprecating
`modal.gpu`
objects
-------------------------------

*Introduced in: v0.73.31*

The
`modal.gpu`
objects are being deprecated; going forward, all GPU resource
configuration should be accomplished using strings.

This should be an easy code substitution, e.g.
`gpu=modal.gpu.H100()`
can be
replaced with
`gpu="H100"`
. When using the
`count=`
parameter of the GPU class,
simply append it to the name with a colon (e.g.
`gpu="H100:8"`
). In the case of
the
`modal.gpu.A100(size="80GB")`
variant, the name of the corresponding gpu is
`"A100-80GB"`
.

Note that string arguments are case-insensitive, so
`"H100"`
and
`"h100"`
are
both accepted.

The main rationale for this change is that it will allow us to introduce new
GPU models in the future without requring users to upgrade their SDK.

Requiring explicit invocation for module mode
---------------------------------------------

*Introduced in: 0.73.58*

The Modal CLI allows you to reference the source code for your App as either
a file path (e.g.
`src/my_app.py`
) or as a module name (e.g.
`src.my_app`
).

As in Python, the choice has some implications for how relative imports are
resolved. To make this more salient, Modal will mirror Python going forwared
and require that you explicitly invoke module mode by passing
`-m`
on your
command line (e.g.,
`modal deploy -m src.my_app`
).

[Modal 1.0 migration guide](#modal-10-migration-guide)

[Deprecating Image.copy\_\* methods](#deprecating-imagecopy_-methods)

[Deprecating Mount as part of the public API](#deprecating-mount-as-part-of-the-public-api)

[Deprecating the @modal.build decorator](#deprecating-the-modalbuild-decorator)

[Requiring explicit inclusion of local Python dependencies](#requiring-explicit-inclusion-of-local-python-dependencies)

[Renaming autoscaler parameters](#renaming-autoscaler-parameters)

[Renaming modal.web\_endpoint to modal.fastapi\_endpoint](#renaming-modalweb_endpoint-to-modalfastapi_endpoint)

[Replacing allow\_concurrent\_inputs with @modal.concurrent](#replacing-allow_concurrent_inputs-with-modalconcurrent)

[Deprecating the .lookup method on Modal objects](#deprecating-the-lookup-method-on-modal-objects)

[Removing support for custom Cls constructors](#removing-support-for-custom-cls-constructors)

[Simplifying Cls lookup patterns](#simplifying-cls-lookup-patterns)

[Deprecating modal.gpu objects](#deprecating-modalgpu-objects)

[Requiring explicit invocation for module mode](#requiring-explicit-invocation-for-module-mode)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/modal-user-account-setup
================================================================================

Modal user account setup
========================

To run and deploy applications on Modal you’ll need to sign up and create a user
account.

You can visit the
[signup](/signup)

page to begin the process or execute
[`modal setup`](/docs/reference/cli/setup#modal-setup)

on the command line.

Users can also be provisioned through
[Okta SSO](/docs/guide/okta-sso)

, which is
an enterprise feature that you can request. For the typical user you’ll sign-up
using an existing GitHub account. If you’re interested in authenticating with
other identity providers let us know at
[support@modal.com](mailto:support@modal.com)

.

What GitHub permissions does signing up require?
------------------------------------------------

* `user:email`
  — gives us the emails associated with the GitHub account.
* `read:org`
  (invites only) — needed for Modal workspace invites. Note: this
  only allows us to see what organization memberships you have
  (
  [GitHub docs](https://docs.github.com/en/apps/oauth-apps/building-oauth-apps/scopes-for-oauth-apps)

  ).
  We won’t be able to access any code repositories or other details.

How can I change my email?
--------------------------

You can change your email on the
[settings](/settings)

page.

[Modal user account setup](#modal-user-account-setup)

[What GitHub permissions does signing up require?](#what-github-permissions-does-signing-up-require)

[How can I change my email?](#how-can-i-change-my-email)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/model-weights
================================================================================

Storing model weights on Modal
==============================

Efficiently managing the weights of large models is crucial for optimizing the
build times and startup latency of many ML and AI applications.

Our recommended method for working with model weights is to store them in a Modal
[Volume](/docs/guide/volumes)

,
which acts as a distributed file system, a “shared disk” all of your Modal Functions can access.

Storing weights in a Modal Volume
---------------------------------

To store your model weights in a Volume, you need to either
make the Volume available to a Modal Function that saves the model weights
or upload the model weights into the Volume from a client.

### Saving model weights into a Modal Volume from a Modal Function

If you’re already generating the weights on Modal, you just need to
attach the Volume to your Modal Function, making it available for reading and writing:

```
from pathlib import Path

volume = modal.Volume.from_name("model-weights-vol", create_if_missing=True)
MODEL_DIR = Path("/models")

@app.function(gpu="any", volumes={MODEL_DIR: volume})  # attach the Volume
def train_model(data, config):
    import run_training

    model = run_training(config, data)
    model.save(config, MODEL_DIR)
```

Volumes are attached by including them in a dictionary that maps
a path on the remote machine to a
`modal.Volume`
object.
They look just like a normal file system, so model weights can be saved to them
without adding any special code.

If the model weights are generated outside of Modal and made available
over the Internet, for example by an open-weights model provider
or your own training job on a dedicated cluster,
you can also download them into a Volume from a Modal Function:

```
@app.function(volumes={MODEL_DIR: volume})
def download_model(model_id):
    import model_hub

    model_hub.download(model_id, local_dir=MODEL_DIR / model_id)
```

Add
[Modal Secrets](/docs/guide/secrets)

to access weights that require authentication.

See
[below](#storing-weights-from-the-hugging-face-hub-on-modal)

for
more on downloading from the popular Hugging Face Hub.

### Uploading model weights into a Modal Volume

Instead of pulling weights into a Modal Volume from inside a Modal Function,
you might wish to push weights into Modal from a client,
like your laptop or a dedicated training cluster.

For that, you can use the
`batch_upload`
method of
[`modal.Volume`](/docs/reference/modal.Volume)

s
via the Modal Python client library:

```
volume = modal.Volume.from_name("model-weights-vol", create_if_missing=True)

@app.local_entrypoint()
def main(local_path: str, remote_path: str):
    with volume.batch_upload() as upload:
        upload.put_directory(local_path, remote_path)
```

Alternatively, you can upload model weights using the
[`modal volume`](/docs/reference/cli/volume)

CLI command:

```
modal volume put model-weights-vol path/to/model path/on/volume
```

### Mounting cloud buckets as Modal Volumes

If your model weights are already in cloud storage,
for example in an S3 bucket, you can connect them
to Modal Functions with a
`CloudBucketMount`
.

See
[the guide](/docs/guide/cloud-bucket-mounts)

for details.

Reading model weights from a Modal Volume
-----------------------------------------

You can read weights from a Volume as you would normally read them
from disk, so long as you attach the Volume to your Function.

```
@app.function(gpu="any", volumes={MODEL_DIR: volume})
def inference(prompt, model_id):
    import load_model

    model = load_model(MODEL_DIR / model_id)
    model.run(prompt)
```

Storing weights in the Modal Image
----------------------------------

It is also possible to store weights in your Function’s Modal
[Image](/docs/guide/images)

,
the private file system state that a Function sees when it starts up.
The weights might be downloaded via shell commands with
[`Image.run_commands`](/docs/guide/images)

or downloaded using a Python function with
[`Image.run_function`](/docs/guide/images)

.

We recommend storing model weights in a Modal
[Volume](/docs/guide/volumes)

,
as described
[above](#storing-weights-in-a-modal-volume)

. Performance is similar
for the two methods. Volumes are more flexible.
Images are rebuilt when their definition changes, starting from the changed layer,
which increases reproducibility for some builds but leads to unnecessary extra downloads
in most cases.

Optimizing model weight reads with
`@enter`
-------------------------------------------

In the above code samples, weights are loaded from disk into memory each time
the
`inference`
function is run. This isn’t so bad if inference is much
slower than model loading (e.g. it is run on very large datasets)
or if the model loading logic is smart enough to skip reloading.

To guarantee a particular model’s weights are only loaded once, you can use the
`@enter`
[container lifecycle hook](/docs/guide/lifecycle-functions)

to load the weights only when a new container starts.

```
MODEL_ID = "some-model-id"

@app.cls(gpu="any", volumes={MODEL_DIR: volume})
class Model:
    @modal.enter()
    def setup(self, model_id=MODEL_ID):
        import load_model

        self.model = load_model(MODEL_DIR, model_id)

    @modal.method()
    def inference(self, prompt):
        return self.model.run(prompt)
```

Note that methods decorated with
`@enter`
can’t be passed dynamic arguments.

If you need to load a single but possibly different model on each container start, you can
[parametrize](/docs/guide/parametrized-functions)

your Modal Cls.
Below, we use the
`modal.parameter`
syntax.

```
@app.cls(gpu="any", volumes={MODEL_DIR: volume})
class ParametrizedModel:
    model_id: str = modal.parameter()

    @modal.enter()
    def setup(self):
        import load_model

        self.model = load_model(MODEL_DIR, self.model_id)

    @modal.method()
    def inference(self, prompt):
        return self.model.run(prompt)
```

Storing weights from the Hugging Face Hub on Modal
--------------------------------------------------

The
[Hugging Face Hub](https://huggingface.co/models)

has over 1,000,000 models
with weights available for download.

The snippet below shows some additional tricks for downloading models
from the Hugging Face Hub on Modal.

```
from typing import Optional
from pathlib import Path

import modal

# create a Volume, or retrieve it if it exists
volume = modal.Volume.from_name("model-weights-vol", create_if_missing=True)
MODEL_DIR = Path("/models")

# define dependencies for downloading model
download_image = (
    modal.Image.debian_slim()
    .pip_install("huggingface_hub[hf_transfer]")  # install fast Rust download client
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})  # and enable it
)
app = modal.App()

@app.function(
    volumes={MODEL_DIR.as_posix(): volume},  # "mount" the Volume, sharing it with your function
    image=download_image,  # only download dependencies needed here
)
def download_model(
    repo_id: str = "hf-internal-testing/tiny-random-GPTNeoXForCausalLM",
    revision: Optional[str] = None,  # include a revision to prevent surprises!
):
    from huggingface_hub import snapshot_download

    snapshot_download(repo_id=repo_id, local_dir=MODEL_DIR / repo_id, revision=revision)
    print(f"Model downloaded to {MODEL_DIR / repo_id}")
```

[Storing model weights on Modal](#storing-model-weights-on-modal)

[Storing weights in a Modal Volume](#storing-weights-in-a-modal-volume)

[Saving model weights into a Modal Volume from a Modal Function](#saving-model-weights-into-a-modal-volume-from-a-modal-function)

[Uploading model weights into a Modal Volume](#uploading-model-weights-into-a-modal-volume)

[Mounting cloud buckets as Modal Volumes](#mounting-cloud-buckets-as-modal-volumes)

[Reading model weights from a Modal Volume](#reading-model-weights-from-a-modal-volume)

[Storing weights in the Modal Image](#storing-weights-in-the-modal-image)

[Optimizing model weight reads with @enter](#optimizing-model-weight-reads-with-enter)

[Storing weights from the Hugging Face Hub on Modal](#storing-weights-from-the-hugging-face-hub-on-modal)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/multi-node-training
================================================================================

Multi-node clusters (beta)
==========================

Modal supports running a training job across several coordinated containers. Each container can saturate the available GPU devices on its host (a.k.a node) and communicate with peer containers which do the same. By scaling a training job from a single GPU to 16 GPUs you can achieve nearly 16x improvements in training time.

### Cluster compute capability

Modal H100 clusters provide:

* A 50 Gbps
  [IPv6 private network](https://modal.com/docs/guide/private-networking)

  for orchestration, dataset downloading.
* A 3200 Gbps RDMA scale-out network (
  [RoCE](https://en.wikipedia.org/wiki/RDMA_over_Converged_Ethernet)

  ).
* Up-to 32 H100 SXM devices. (64 coming soon!)
* At least 1TB of RAM and 4TB of local NVMe SSD per node.
* Deep burn-in testing.
* Interopability with all Modal platform functionality (Volumes, Dicts, Tunnels, etc.).

The guide will walk you through how the Modal client library enables multi-node training and integrates with
`torchrun`
.

### @clustered

Unlike standard Modal serverless containers, containers in a multi-node training job must be able to:

1. Perform fast, direct network communication between each other.
2. Be scheduled together, all or nothing, at the same time.

The
`@clustered`
decorator enables this behavior.

```
import modal
import modal.experimental

@app.function(
    gpu="H100:8",
    timeout=60 * 60 * 24,
    retries=modal.Retries(initial_delay=0.0, max_retries=10),
)
@modal.experimental.clustered(size=2)
def train_model():
    cluster_info = modal.experimental.get_cluster_info()

    container_rank = cluster_info.rank
    world_size = len(cluster_info.container_ips)
    main_addr = cluster_info.container_ips[0]
    is_main = "(main)" if container_rank == 0 else ""

    print(f"{container_rank=} {is_main} {world_size=} {main_addr=}")
    ...
```

Applying this decorator under
`@app.function`
modifies the Function so that remote calls to it are serviced by a multi-node container group. The above configuration creates a group of four containers each having 8 H100 GPU devices, for a total of 32 devices.

Scheduling
----------

A
`modal.experimental.clustered`
Function runs on multiple nodes in our cloud, but executes like a normal function call. For example, all nodes are scheduled together (
[gang scheduling](https://en.wikipedia.org/wiki/Gang_scheduling)

) so that your code runs on all of the requested hardware or not at all.

Traditionally this kind of cluster and scheduling management would be handled by SLURM, Kubernetes, or manually. But with Modal it’s all provided serverlessly with just an application of the decorator!

### Rank & input broadcast

![diagram](https://modal-cdn.com/cdnbot/multinodepmgnla70_4b57a155.webp)

You may notice above that a single
`.remote`
Function call created three input executions but returned only one output. This is how input-output is structured for multi-node training jobs on Modal. The Function call’s arguments are replicated to each container, but only the rank zero container’s is returned to the caller.

A container’s rank is a key concept in multi-node training jobs. Rank zero is the ‘leader’ rank and typically coordinates the job. Rank zero is also known as the “main” container. Rank zero’s output will always be the output of a multi-node training run.

Networking
----------

Function containers cannot normally make direct network connections to other Function containers, but this is a requirement for multi-node training communication. So, along with gang scheduling, the
`@clustered`
decorator enables Modal’s workspace-private inter-container networking called
[i6pn](https://www.notion.so/Multi-node-docs-1281e7f16949806f966adedfe8b2cb74?pvs=21)

.

The
[cluster networking guide](/docs/guide/private-networking)

goes into more detail on i6pn, but the upshot is that each container in the cluster is made aware of the network address of all the other containers in the cluster, enabling them to communicate with each other quickly via
[TCP](https://pytorch.org/docs/stable/elastic/rendezvous.html)

.

### RDMA (Infiniband)

H100 clusters are equipped with Infiniband providing up-to 3,200Gbps scale-out bandwidth for inter-node communication.
RDMA scale-out networking is enabled with the
`rdma`
parameter of
`modal.experimental.clustered.`

```
@modal.experimental.clustered(size=2, rdma=True)
def train():
    ...
```

To run a simple Infiniband RDMA performance test see the
[`modal-examples`
repository example](https://github.com/modal-labs/multinode-training-guide/tree/main/benchmark)

.

Cluster Info
------------

`modal.experimental.get_cluster_info()`
exposes the following information about the cluster:

* `rank: int`
  is the container’s order within the cluster, starting from
  `0`
  , the leader.
* `container_ips: list[str]`
  contains the ipv6 addresses of each container in the cluster, sorted by rank.

Fault Tolerance
---------------

For a clustered Function, failures in inputs and containers are handled differently.

If an input fails on any container, this failure
**is not propagated**
to other containers in the cluster. Containers are responsible for detecting and responding to input failures on other containers.

Only rank 0’s output matters: if an input fails on the leader container (rank 0), the input is marked as failed, even if the input succeeds on another container. Similarly, if an input succeeds on the leader container but fails on another container, the input will still be marked as successful.

If a container in the cluster is preempted, Modal will terminate all remaining containers in the cluster, and retry the input.

### Input Synchronization

***Important:***
synchronization is not relevant for single training runs, and applies mostly to inference use-cases.

Modal does not synchronize input execution across containers. Containers are responsible for ensuring that they do not process inputs faster than other containers in their cluster.

In particular, it is important that the leader container (rank 0) only starts processing the next input after all other containers have finished processing the current input.

Examples
--------

To get hands-on with multi-node training you can jump into the
[`multinode-training-guide`
repository](https://github.com/modal-labs/multinode-training-guide)

or
[`modal-examples`
repository](https://github.com/modal-labs/modal-examples/tree/main/12_datasets)

and
`modal run`
something!

* [Simple ‘hello world’ 4 X 1 H100 torch cluster example](https://github.com/modal-labs/modal-examples/blob/main/14_clusters/simple_torch_cluster.py)
* [Infiniband RDMA performance test](https://github.com/modal-labs/multinode-training-guide/tree/main/benchmark)
* [Use 2 x 8 H100s to train a ResNet50 model on the ImageNet dataset](https://github.com/modal-labs/multinode-training-guide/tree/main/resnet50)
* [Speedrun GPT-2 training with modded-nanogpt](https://github.com/modal-labs/multinode-training-guide/tree/main/nanoGPT)

### Torchrun Example

```
import modal
import modal.experimental

image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install("torch~=2.5.1", "numpy~=2.2.1")
    .add_local_dir(
        "training", remote_path="/root/training"
    )
)
app = modal.App("example-simple-torch-cluster", image=image)

n_nodes = 4

@app.function(
    gpu=f"H100:8",
    timeout=60 * 60 * 24,
)
@modal.experimental.clustered(size=n_nodes)
def launch_torchrun():
    # import the 'torchrun' interface directly.
    from torch.distributed.run import parse_args, run

    cluster_info = modal.experimental.get_cluster_info()

    run(
        parse_args(
            [
                f"--nnodes={n_nodes}",
                f"--node_rank={cluster_info.rank}",
                f"--master_addr={cluster_info.container_ips[0]}",
                f"--nproc-per-node=8",
                "--master_port=1234",
                "training/train.py",
            ]
        )
    )
```

[Multi-node clusters (beta)](#multi-node-clusters-beta)

[Cluster compute capability](#cluster-compute-capability)

[@clustered](#clustered)

[Scheduling](#scheduling)

[Rank & input broadcast](#rank--input-broadcast)

[Networking](#networking)

[RDMA (Infiniband)](#rdma-infiniband)

[Cluster Info](#cluster-info)

[Fault Tolerance](#fault-tolerance)

[Input Synchronization](#input-synchronization)

[Examples](#examples)

[Torchrun Example](#torchrun-example)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/notebooks
================================================================================

Jupyter notebooks
=================

> **Note:**
> This document is about running Jupyter on Modal. For our hosted notebooks product with real-time collaboration, see
> [Modal Notebooks](/docs/guide/notebooks-modal)
>
> .

You can use the Modal client library in notebook environments like Jupyter! Just
`import modal`
and use as normal. You will likely need to use
[`app.run`](/docs/guide/apps#ephemeral-apps)

to create an ephemeral app to run your functions:

```
# Cell 1

import modal

app = modal.App()

@app.function()
def my_function(x):
    ...

# Cell 2

with modal.enable_output():
    with app.run():
        my_function.remote(42)
```

Known issues
------------

* **Interactive shell and interactive functions are not supported.**

  These can only be run within a live terminal session, so they are not
  supported in notebooks.
* **Local and remote Python versions must match.**

  When defining Modal Functions in a Jupyter notebook, the Function automatically
  has
  `serialized=True`
  set. This implies that the versions of Python and any third-
  party libraries used in your Modal container must match the version you have locally,
  so that the function can be deserialized remotely without errors.

If you encounter issues not documented above, try restarting the notebook kernel, as it may be
in a broken state, which is common in notebook development.

If the issue persists, contact us
[in our Slack](https://modal.com/slack)

.

We are working on removing these known issues so that writing Modal applications
in a notebook feels just like developing in regular Python modules and scripts.

Jupyter inside Modal
--------------------

You can run Jupyter in Modal using the
`modal launch`
command. For example:

```
$ modal launch jupyter --gpu a10g
```

That will start a Jupyter instance with an A10G GPU attached. You’ll be able to
access the app with via a
[Modal Tunnel URL](https://modal.com/docs/guide/tunnels#tunnels-beta)

. Jupyter
will stop running whenever you stop Modal call in your terminal.

See
`--help`
for additional options.

Further examples
----------------

* [Basic demonstration of running Modal in a notebook](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/basic.ipynb)
* [Running Jupyter server within a Modal function](https://github.com/modal-labs/modal-examples/blob/main/11_notebooks/jupyter_inside_modal.py)

[Jupyter notebooks](#jupyter-notebooks)

[Known issues](#known-issues)

[Jupyter inside Modal](#jupyter-inside-modal)

[Further examples](#further-examples)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/notebooks-modal
================================================================================

Modal Notebooks (beta)
======================

Notebooks allow you to write and execute Python code in Modal’s cloud, within your browser. It’s a hosted Jupyter notebook with:

* Serverless pricing and automatic idle shutdown
* Access to Modal GPUs and compute
* Real-time collaborative editing
* Python Intellisense/LSP support and AI autocomplete

[

](https://modal-cdn.com/Modal-Notebooks-Beta.mp4)

Getting started
---------------

Open
[modal.com/notebooks](/notebooks)

in your browser and create a new notebook. You can also upload an
`.ipynb`
file from your computer.

Once you create a notebook, you can start running cells. Try a simple statement like

```
print("Hello, Modal!")
```

Or, import a library and create a plot:

```
import matplotlib.pyplot as plt
import numpy as np

x = np.linspace(-20, 20, 500)
plt.plot(np.cos(x / 3.7 + 0.3), x * np.sin(x))
```

The default notebook image comes with a number of Python packages pre-installed, so you can get started right away. Popular ones include PyTorch, NumPy, Pandas, JAX, Transformers, and Matplotlib. You can find the full image definition
[here](https://github.com/modal-labs/modal-client/blob/main/modal_global_objects/images/notebook_base_image.py)

. If you need another package, just install it:

```
!uv pip install --system [my-package]
```

All output types work out-of-the-box, including rich HTML, images, and interactive plots.

Kernel resources
----------------

Just like with Modal Functions, notebooks run in serverless containers. This means you pay only for the CPU cores and memory you use.

If you need more resources, you can change kernel settings in the sidebar. This lets you set the number of CPU cores, memory, and GPU type for your notebook. You can also set a timeout for idle shutdown, which defaults to 10 minutes.

Use any GPU type available in Modal, including up to 8 Nvidia A100s or H100s. You can switch the kernel configuration in seconds!

![Compute profile tab in notebook sidebar](https://modal-cdn.com/cdnbot/compute-profilev9rvmmvw_365a1197.webp)

Note that the CPU and memory settings are
*reservations*
, so you can usually burst above the request. For example, if you’ve set the notebook to have 0.5 CPU cores, you’ll be billed for that continuously, but you can use up to any available cores on the machine (e.g., 32 CPUs) and will be billed for only the time you use them.

Custom images, volumes and secrets
----------------------------------

Modal Notebooks supports custom images, volumes, and secrets, just like Modal Functions. You can use these to install additional packages, mount persistent storage, or access secrets.

* To use a custom image, you need to have a
  [deployed Modal Function](/docs/guide/managing-deployments)

  using that image. Then, search for that function in the sidebar.
* To use a Secret, simply create a
  [Modal Secret](/secrets)

  using our wizard and attach it to the notebook, so it can be injected as an environment variable automatically.
* To use a Volume, create a
  [Modal Volume](/docs/guide/volumes)

  and attach it to the notebook. This lets you mount high-performance, persistent storage that can be shared across multiple notebooks or functions. They will appear as folders in the
  `/mnt`
  directory by default.

Access and sharing
------------------

Need a colleague—or the whole internet—to see your work? Just click
**Share**
in the top‑right corner of the notebook editor.

By default, notebooks are visible to you and teammates in your workspace. They can open the notebook and run cells; flip the “Allow edits” toggle if you want them to make changes directly. Workspace managers can also change the notebook’s access settings.

Modal supports sharing by public, unlisted link. If you toggle this, it allows
*anyone with the link*
to open the notebook. Pick
**Can view**
(default) or
**Can view and run**
based on your preference. Viewers don’t need a Modal account, so this is perfect for collaborating with other stakeholders outside your workspace.

No matter how the notebook is shared, anyone can fork and run their own copy of it.

Interactive file viewer
-----------------------

The panel on the left-hand side of the notebook shows a
**live view of the container’s filesystem**
:

| Feature | Details |
| --- | --- |
| **Browse & preview** | Click through folders to inspect any file that your code has created or downloaded. |
| **Upload & download** | Drag-and-drop files from your desktop, or click the **⬆** / **⬇** icons to add new data sets, notebooks, or models—or to save results back to your machine. |
| **One-click refresh** | Changes made by your code (for example, writing a CSV) appear instantly; hit the refresh icon if you want to force an update. |
| **Context-aware paths** | The viewer always reflects *exactly* what your code sees (e.g. `/root` , `/mnt/…` ), so you can double-check that that file you just wrote really landed where you expected. |

**Important:**
the underlying container is
**ephemeral**
. Anything stored outside an attached
[Volume](/docs/guide/volumes)

disappears when the kernel shuts down (after your idle-timeout or when you hit
**Stop kernel**
). Mount a Volume for data you want to keep across sessions.

The viewer itself is only active while the kernel is running—if the notebook is stopped you’ll see an “empty” state until you start it again.

Editor features
---------------

Modal Notebooks bundle the same productivity tooling you’d expect from a modern IDE.

With Pyright, you get autocomplete, signature help, and on-hover documentation for every installed library.

We also implemented AI-powered code completion using Anthropic’s
**Claude 4**
model. This keeps you in the flow for everything from small snippets to multi-line functions. Just press
`Tab`
to accept suggestions or
`Esc`
to dismiss them.

Familiar Jupyter shortcuts (
`A`
,
`B`
,
`X`
,
`Y`
,
`M`
, etc.) all work within the notebook, so you can quickly add new cells, delete existing ones, or change cell types.

Finally, we have real-time collaborative editing, so you can work with your team in the same notebook. You can see other users’ cursors and edits in real-time, and you can see when others are running cells with you. This makes it easy to pair program or review code together.

Cell magic
----------

Modal Notebooks have built-in support for the
`%modal`
cell magic. This lets you run code in any
[deployed Modal Function or Cls](/docs/guide/trigger-deployed-functions)

, right from your notebook.

For example, if you have previously run
`modal deploy`
for an app like:

```
import modal

app = modal.App("my-app")

@app.function()
def my_function(s: str):
    return len(s)
```

Then you could access this function from your notebook:

```
%modal from my-app import my_function

my_function.remote("hello, world!")  # returns 13
```

Run
`%modal`
to see all options. This works for Cls as well, and you can import from different environments or alias them with the
`as`
keyword.

Roadmap
-------

The product is in beta, and we’re planning to make a lot of improvements over the coming months. Some bigger features on mind:

* **Modal cloud integrations**
  + Persistent disk storage
  + Memory snapshots to save your notebook session
  + Create notebooks from the
    `modal`
    CLI
  + Custom image registry
* **Notebook editor**
  + Jupyter Widgets support
  + Interactive outline
  + Reactive cell execution
  + Edit history

Let us know via
[Slack](/slack)

if you have any feedback.

[Modal Notebooks (beta)](#modal-notebooks-beta)

[Getting started](#getting-started)

[Kernel resources](#kernel-resources)

[Custom images, volumes and secrets](#custom-images-volumes-and-secrets)

[Access and sharing](#access-and-sharing)

[Interactive file viewer](#interactive-file-viewer)

[Editor features](#editor-features)

[Cell magic](#cell-magic)

[Roadmap](#roadmap)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/oidc-integration
================================================================================

Using OIDC to authenticate with external services
=================================================

Your Functions in Modal may need to access external resources like S3 buckets.
Traditionally, you would need to store long-lived credentials in Modal Secrets
and reference those Secrets in your function code. With the Modal OIDC
integration, you can instead use automatically-generated identity
tokens to authenticate to external services.

How it works
------------

[OIDC](https://auth0.com/docs/authenticate/protocols/openid-connect-protocol)

is
a standard protocol for authenticating users between systems. In Modal, we use
OIDC to generate short-lived tokens that external services can use to verify
that your function is authenticated.

The OIDC integration has two components: the discovery document and the generated
tokens.

The
[OIDC discovery document](https://swagger.io/docs/specification/v3_0/authentication/openid-connect-discovery/)

describes how our OIDC server is configured. It primarily includes the supported
[claims](https://developer.okta.com/blog/2017/07/25/oidc-primer-part-1)

and the
[keys](https://auth0.com/docs/secure/tokens/json-web-tokens/json-web-key-sets)

we use to sign tokens. Discovery documents are always hosted at
`/.well-known/openid-configuration`
, and
you can view ours at
<https://oidc.modal.com/.well-known/openid-configuration>

.

The generated tokens are
[JWTs](https://jwt.io/)

signed by Modal using the keys described in the
discovery document. These tokens contain the full identity of the Function
in the
`sub`
claim, and they use custom claims to make this information more
easily accessible. See our
[discovery document](https://oidc.modal.com/.well-known/openid-configuration)

for a full list of claims.

Generated tokens are injected into your Function’s containers via the
`MODAL_IDENTITY_TOKEN`
environment variable. Below is an example of what claims might be included in a token:

```
{
  "sub": "modal:workspace_id:ac-12345abcd:environment_name:modal-examples:app_name:oidc-token-test:function_name:jwt_return_func:container_id:ta-12345abcd",
  "aud": "oidc.modal.com",
  "exp": 1732137751,
  "iat": 1731964951,
  "iss": "https://oidc.modal.com",
  "jti": "31f92dca-e847-4bc9-8d15-9f234567a123",
  "workspace_id": "ac-12345abcd",
  "environment_id": "en-12345abcd",
  "environment_name": "modal-examples",
  "app_id": "ap-12345abcd",
  "app_name": "oidc-token-test",
  "function_id": "fu-12345abcd",
  "function_name": "jwt_return_func",
  "container_id": "ta-12345abcd"
}
```

### Key thumbprints

RSA keys have
[thumbprints](https://connect2id.com/products/nimbus-jose-jwt/examples/jwk-thumbprints)

. You
can use these thumbprints to verify that the keys in our discovery document are
genuine. This protects against potential Man in the Middle (MitM) attacks, although
our required use of HTTPS mitigates this risk.

If you’d like to have the extra security of verifying the thumbprints, you can
use the following command to print the thumbprints for the keys in our
discovery document:

```
$ openssl s_client -connect oidc.modal.com:443 < /dev/null 2>/dev/null | openssl x509 -fingerprint -noout | awk -F= '{print $2}' | tr -d ':'
F062F2151EDE30D1620B48B7AC91D66047D769D3
```

Note that these thumbprints may change over time as we rotate keys. We recommend
periodically checking for and updating your scripts with the new thumbprints.

### App name format

By default, Modal Apps can be created with arbitrary names. However, when using
OIDC, the App name has a stricter character set. Specifically, it must be 64
characters or less and can only include alphanumeric characters, dashes, periods,
and underscores. If these constraints are violated, the OIDC token will not be
injected into the container.

Note that these are the same constraints that are applied to
[Deployed Apps](/docs/guide/managing-deployments)

.
This means that if an App is deployable, it will also be compatible with OIDC.

Demo usage with AWS
-------------------

To see how OIDC tokens can be used, we’ll demo a simple Function that lists
objects in an S3 bucket.

### Step 0: Understand your OIDC claims

Before we can configure OIDC policies, we need to know what claims we can match
against. We can run a Function and inspect its claims to find out.

```
app = modal.App("oidc-token-test")

jwt_image = modal.Image.debian_slim().pip_install("pyjwt")

@app.function(image=jwt_image)
def jwt_return_func():
    import jwt

    token = os.environ["MODAL_IDENTITY_TOKEN"]
    claims = jwt.decode(token, options={"verify_signature": False})
    print(json.dumps(claims, indent=2))

@app.local_entrypoint()
def main():
    jwt_return_func.remote()
```

Run the function locally to see its claims:

```
$ modal run oidc-token-test.py
{
  "sub": "modal:workspace_id:ac-12345abcd:environment_name:modal-examples:app_name:oidc-token-test:function_name:jwt_return_func:container_id:ta-12345abcd",
  "aud": "oidc.modal.com",
  "exp": 1732137751,
  "iat": 1731964951,
  "iss": "https://oidc.modal.com",
  "jti": "31f92dca-e847-4bc9-8d15-9f234567a123",
  "workspace_id": "ac-12345abcd",
  "environment_id": "en-12345abcd",
  "environment_name": "modal-examples",
  "app_id": "ap-12345abcd",
  "app_name": "oidc-token-test",
  "function_id": "fu-12345abcd",
  "function_name": "jwt_return_func",
  "container_id": "ta-12345abcd"
}
```

Now we can match off these claims to configure our OIDC policies.

### Step 1: Configure AWS to trust Modal’s OIDC provider

We need to make AWS accept Modal identity tokens. To do this, we need to add
Modal’s OIDC provider as a trusted entity in our AWS account.

```
aws iam create-open-id-connect-provider \
    --url https://oidc.modal.com \
    --client-id-list oidc.modal.com \
    # Optionally replace with the thumbprint from the discovery document.
    # Note that this may change over time as we rotate keys, and this argument
    # can be omitted if you'd prefer to rely on the HTTPS verification instead.
    --thumbprint-list "<thumbprint>"
```

This will trigger AWS to pull down our
[JSON Web Key Set (JWKS)](https://auth0.com/docs/secure/tokens/json-web-tokens/json-web-key-sets)

and use it to verify the signatures of any tokens signed by Modal.

### Step 2: Create an IAM role that can be assumed by Modal Functions

Let’s create a simple IAM policy that allows listing objects in an S3 bucket.
Take the policy below and replace the bucket name with your own.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": ["s3:PutObject", "s3:GetObject", "s3:ListBucket"],
      "Resource": ["arn:aws:s3:::fun-bucket", "arn:aws:s3:::fun-bucket/*"]
    }
  ]
}
```

Now, we can create an IAM role that uses this policy. Visit the IAM console
to create this role. Be sure to replace the account ID and workspace ID placeholders
with your own.

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::123456789abcd:oidc-provider/oidc.modal.com"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringEquals": {
          "oidc.modal.com:aud": "oidc.modal.com"
        },
        "StringLike": {
          "oidc.modal.com:sub": "modal:workspace_id:ac-12345abcd:*"
        }
      }
    }
  ]
}
```

Note how we use
`workspace_id`
to limit the scope of the role. This means that
the IAM role can only be assumed by Functions in your Workspace. You can further
limit this by specifying an Environment, App, or Function name.

Ideally, we would use the custom claims for role limiting. Unfortunately, AWS
does not support
[matching on custom claims](https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_iam-condition-keys.html#condition-keys-wif)

,
so we use the
`sub`
claim instead.

### Step 3: Use the OIDC token in your Function

The AWS SDKs have built-in support for OIDC tokens, so you can use them as
follows:

```
import boto3

app = modal.App("oidc-token-test")

boto3_image = modal.Image.debian_slim().pip_install("boto3")

# Trade a Modal OIDC token for AWS credentials
def get_s3_client(role_arn):
    sts_client = boto3.client("sts")

    # Assume role with Web Identity
    credential_response = sts_client.assume_role_with_web_identity(
        RoleArn=role_arn, RoleSessionName="OIDCSession", WebIdentityToken=os.environ["MODAL_IDENTITY_TOKEN"]
    )

    # Extract credentials
    credentials = credential_response["Credentials"]
    return boto3.client(
        "s3",
        aws_access_key_id=credentials["AccessKeyId"],
        aws_secret_access_key=credentials["SecretAccessKey"],
        aws_session_token=credentials["SessionToken"],
    )

# List the contents of an S3 bucket
@app.function(image=boto3_image)
def list_bucket_contents(bucket_name, role_arn):
    s3_client = get_s3_client(role_arn)
    response = s3_client.list_objects_v2(Bucket=bucket_name)
    for obj in response["Contents"]:
        print(f"- {obj['Key']} (Size: {obj['Size']} bytes)")

@app.local_entrypoint()
def main():
    # Replace with the role ARN and bucket name from step 2
    list_bucket_contents.remote("fun-bucket", "arn:aws:iam::123456789abcd:role/oidc_test_role")
```

Run the function locally to see the contents of the bucket:

```
$ modal run oidc-token-test.py
- test-file.txt (Size: 10 bytes)
```

Next steps
----------

The OIDC integration can be used for much more than just AWS. With this same pattern,
you can configure automatic access to
[Vault](https://developer.hashicorp.com/vault/docs/auth/jwt)

,
[GCP](https://cloud.google.com/identity-platform/docs/web/oidc)

,
[Azure](https://learn.microsoft.com/en-us/entra/identity-platform/v2-protocols-oidc)

, and more.

[Using OIDC to authenticate with external services](#using-oidc-to-authenticate-with-external-services)

[How it works](#how-it-works)

[Key thumbprints](#key-thumbprints)

[App name format](#app-name-format)

[Demo usage with AWS](#demo-usage-with-aws)

[Step 0: Understand your OIDC claims](#step-0-understand-your-oidc-claims)

[Step 1: Configure AWS to trust Modal’s OIDC provider](#step-1-configure-aws-to-trust-modals-oidc-provider)

[Step 2: Create an IAM role that can be assumed by Modal Functions](#step-2-create-an-iam-role-that-can-be-assumed-by-modal-functions)

[Step 3: Use the OIDC token in your Function](#step-3-use-the-oidc-token-in-your-function)

[Next steps](#next-steps)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/okta-sso
================================================================================

Okta SSO
========

Prerequisites
-------------

* A Workspace that’s on an
  [Enterprise](/pricing)

  plan
* Admin access to the Workspace you want to configure with Okta Single-Sign-On (SSO)
* Admin privileges for your Okta Organization

Supported features
------------------

* IdP-initiated SSO
* SP-initiated SSO
* Just-In-Time account provisioning

For more information on the listed features, visit the
[Okta Glossary](https://help.okta.com/okta_help.htm?type=oie&id=ext_glossary)

.

Configuration
-------------

### Read this before you enable “Require SSO”

Enabling “Require SSO” will force all users to sign in via Okta. Ensure that you
have admin access to your Modal Workspace through an Okta account before
enabling.

### Configuration steps

#### Step 1: Add Modal app to Okta Applications

1. Sign in to your Okta admin dashboard
2. Navigate to the Applications tab and click “Browse App Catalog”.
   ![Okta browse application](/_app/immutable/assets/okta-browse-applications.BiqGsdcd.png)
3. Select “Modal” and click “Done”.
4. Select the “Sign On” tab and click “Edit”.
   ![Okta sign on edit](/_app/immutable/assets/okta-sign-on-edit.DHny2cIB.png)
5. Fill out Workspace field to configure for your specific Modal workspace. See
   [Step 2](/docs/guide/okta-sso#step-2-link-your-workspace-to-okta-modal-application)

   if you’re unsure what this is.
   ![Okta add workspace](/_app/immutable/assets/okta-add-workspace-username.DoM8qewy.png)

#### Step 2: Link your Workspace to Okta Modal application

1. Navigate to your application on the Okta Admin page.
2. Copy the Metadata URL from the Okta Admin Console (It’s under the “Sign On”
   tab).
   ![Okta metadata url](/_app/immutable/assets/okta-metadata-url.BLDzMpWn.png)
3. Sign in to
   <https://modal.com>

   and visit your Workspace Management page
   (e.g.
   `https://modal.com/settings/[workspace name]/workspace-management`
   )
4. Paste the Metadata URL in the input and click “Save Changes”

#### Step 3: Assign users / groups and test the integration

1. Navigate back to your Okta application on the Okta Admin dashboard.
2. Click on the “Assignments” tab and add the appropriate people or groups.

![Okta Assign Users](/_app/immutable/assets/okta-assign-people.BhAmcJ0m.png)

3. To test the integration, sign in as one of the users you assigned in the previous step.
4. Click on the Modal application on the Okta Dashboard to initiate Single Sign-On.

#### Notes

The following SAML attributes are used by the integration:

| Name | Value |
| --- | --- |
| email | user.email |
| firstName | user.firstName |
| lastName | user.lastName |

SP-initiated SSO
----------------

The sign-in process is initiated from
<https://modal.com/login/sso>

1. Enter your workspace name in the input
2. Click “continue with SSO” to authenticate with Okta

[Okta SSO](#okta-sso)

[Prerequisites](#prerequisites)

[Supported features](#supported-features)

[Configuration](#configuration)

[Read this before you enable “Require SSO”](#read-this-before-you-enable-require-sso)

[Configuration steps](#configuration-steps)

[Step 1: Add Modal app to Okta Applications](#step-1-add-modal-app-to-okta-applications)

[Step 2: Link your Workspace to Okta Modal application](#step-2-link-your-workspace-to-okta-modal-application)

[Step 3: Assign users / groups and test the integration](#step-3-assign-users--groups-and-test-the-integration)

[Notes](#notes)

[SP-initiated SSO](#sp-initiated-sso)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/otel-integration
================================================================================

Connecting Modal to your OpenTelemetry Provider
===============================================

You can export Modal logs to your
[OpenTelemetry](https://opentelemetry.io/docs/what-is-opentelemetry/)

provider using the Modal OpenTelemetry integration. This integration is compatible with
any observability provider that supports the OpenTelemetry HTTP APIs.

What this integration does
--------------------------

This integration allows you to:

1. Export Modal audit logs to your provider
2. Export Modal function logs to your provider
3. Export container metrics to your provider

Metrics
-------

The Modal OpenTelemetry Integration will forward the following metrics to your provider:

* `modal.cpu.utilization`
* `modal.memory.utilization`
* `modal.gpu.memory.utilization`
* `modal.gpu.compute.utilization`

These metrics are tagged with
`container_id`
,
`environment_name`
, and
`workspace_name`
.

Installing the integration
--------------------------

1. Find out the endpoint URL for your OpenTelemetry provider. This is the URL that
   the Modal integration will send logs to. Note that this should be the base URL
   of the OpenTelemetry provider, and not a specific endpoint. For example, for the
   [US New Relic instance](https://docs.newrelic.com/docs/opentelemetry/best-practices/opentelemetry-otlp/#configure-endpoint-port-protocol)

   ,
   the endpoint URL is
   `https://otlp.nr-data.net`
   , not
   `https://otlp.nr-data.net/v1/logs`
   .
2. Find out the API key or other authentication method required to send logs to your
   OpenTelemetry provider. This is the key that the Modal integration will use to authenticate
   with your provider. Modal can provide any key/value HTTP header pairs. For example, for
   [New Relic](https://docs.newrelic.com/docs/opentelemetry/best-practices/opentelemetry-otlp/#api-key)

   ,
   the header is
   `api-key`
   .
3. Create a new OpenTelemetry Secret in Modal with one key per header. These keys should be
   prefixed with
   `OTEL_HEADER_`
   , followed by the name of the header. The value of this
   key should be the value of the header. For example, for New Relic, an example Secret
   might look like
   `OTEL_HEADER_api-key: YOUR_API_KEY`
   . If you use the OpenTelemetry Secret
   template, this will be pre-filled for you.
4. Navigate to the
   [Modal metrics settings page](http://modal.com/settings/metrics)

   and configure
   the OpenTelemetry push URL from step 1 and the Secret from step 3.
5. Save your changes and use the test button to confirm that logs are being sent to your provider.
   If it’s all working, you should see a
   `Hello from Modal! 🚀`
   log from the
   `modal.test_logs`
   service.

Allowlisting the integration’s IPs
----------------------------------

The integration uses a set of static IP addresses (subject to change) to send data to your OpenTelemetry provider:

```
3.215.65.235
3.219.40.38
13.219.36.96
18.206.3.184
35.169.34.255
44.208.153.216
52.86.93.233
54.198.254.114
54.208.217.246
204.236.196.209
```

Uninstalling the integration
----------------------------

Once the integration is uninstalled, all logs will stop being sent to
your provider.

1. Navigate to the
   [Modal metrics settings page](http://modal.com/settings/metrics)

   and disable the OpenTelemetry integration.

[Connecting Modal to your OpenTelemetry Provider](#connecting-modal-to-your-opentelemetry-provider)

[What this integration does](#what-this-integration-does)

[Metrics](#metrics)

[Installing the integration](#installing-the-integration)

[Allowlisting the integration’s IPs](#allowlisting-the-integrations-ips)

[Uninstalling the integration](#uninstalling-the-integration)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/parameterized-functions
================================================================================

Parametrized functions
======================

A single Modal Function can be parametrized by a set of arguments, so that each unique combination of arguments will behave like an individual
Modal Function with its own auto-scaling and lifecycle logic.

For example, you might want to have a separate pool of containers for each unique user that invokes your Function. In this scenario, you would
parametrize your Function by a user ID.

To parametrize a Modal Function, you need to use Modal’s
[class syntax](/docs/guide/lifecycle-functions)

and the
[`@app.cls`](/docs/reference/modal.App#cls)

decorator. Specifically, you’ll need to:

1. Convert your function to a method by making it a member of a class.
2. Decorate the class with
   `@app.cls(...)`
   with the same arguments you previously
   had for
   `@app.function(...)`
   or your
   [web endpoint decorator](/docs/guide/webhooks)

   .
3. If you previously used the
   `@app.function()`
   decorator on your function, replace it with
   `@modal.method()`
   .
4. Define dataclass-style, type-annotated instance attributes with
   `modal.parameter()`
   and optionally set default values:

```
import modal

app = modal.App()

@app.cls()
class MyClass:

    foo: str = modal.parameter()
    bar: int = modal.parameter(default=10)

    @modal.method()
    def baz(self, qux: str = "default") -> str:
        return f"This code is running in container pool ({self.foo}, {self.bar}), with input qux={qux}"
```

The parameters create a keyword-only constructor for your class, and the methods can be called as follows:

```
@app.local_entrypoint()
def main():
    m1 = MyClass(foo="hedgehog", bar=7)
    m1.baz.remote()

    m2 = MyClass(foo="fox")
    m2.baz.remote(qux="override")
```

Function calls for each unique combination of values for
`foo`
and
`bar`
will run in their own separate container pools.
If you re-constructed a
`MyClass`
with the same arguments in a different context, the calls to
`baz`
would be routed to the same set of containers as before.

Some things to note:

* The total size of the arguments is limited to 16 KiB.
* Modal classes can still annotate types of regular class attributes, which are independent of parametrization, by either omitting
  `= modal.parameter()`
  or using
  `= modal.parameter(init=False)`
  to satisfy type checkers.
* The support types are these primitives:
  `str`
  ,
  `int`
  ,
  `bool`
  , and
  `bytes`
  .
* The legacy
  `__init__`
  constructor method is being removed, see
  [the 1.0 migration for details.](/docs/guide/modal-1-0-migration#removing-support-for-custom-cls-constructors)

Looking up a parametrized function
----------------------------------

If you want to call your parametrized function from a Python script running
anywhere, you can use
`Cls.lookup`
:

```
import modal

MyClass = modal.Cls.from_name("parametrized-function-app", "MyClass")  # returns a class-like object
m = MyClass(foo="snake", bar=12)
m.baz.remote()
```

Parametrized web endpoints
--------------------------

Modal
[web endpoints](/docs/guide/webhooks)

can also be parametrized:

```
app = modal.App("parametrized-endpoint")

@app.cls()
class MyClass():

    foo: str = modal.parameter()
    bar: int = modal.parameter(default=10)

    @modal.fastapi_endpoint()
    def baz(self, qux: str = "default") -> str:
        ...
```

Parameters are specified in the URL as query parameter values.

```
curl "https://parametrized-endpoint.modal.run?foo=hedgehog&bar=7&qux=override"
curl "https://parametrized-endpoint.modal.run?foo=hedgehog&qux=override"
curl "https://parametrized-endpoint.modal.run?foo=hedgehog&bar=7"
curl "https://parametrized-endpoint.modal.run?foo=hedgehog"
```

Using parametrized functions with lifecycle functions
-----------------------------------------------------

Parametrized functions can be used with
[lifecycle functions](/docs/guide/lifecycle-functions)

.

For example, here is how you might parametrize the
[`@enter`](/docs/guide/lifecycle-functions#enter)

lifecycle function to load a specific model:

```
@app.cls()
class Model:

    name: str = modal.parameter()
    size: int = modal.parameter(default=100)

    @modal.enter()
    def load_model(self):
        print(f"Loading model {self.name} with size {self.size}")
        self.model = load_model_util(self.name, self.size)

    @modal.method()
    def generate(self, prompt: str) -> str:
        return self.model.generate(prompt)
```

[Parametrized functions](#parametrized-functions)

[Looking up a parametrized function](#looking-up-a-parametrized-function)

[Parametrized web endpoints](#parametrized-web-endpoints)

[Using parametrized functions with lifecycle functions](#using-parametrized-functions-with-lifecycle-functions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/parametrized-functions
================================================================================

Parametrized functions
======================

A single Modal Function can be parametrized by a set of arguments, so that each unique combination of arguments will behave like an individual
Modal Function with its own auto-scaling and lifecycle logic.

For example, you might want to have a separate pool of containers for each unique user that invokes your Function. In this scenario, you would
parametrize your Function by a user ID.

To parametrize a Modal Function, you need to use Modal’s
[class syntax](/docs/guide/lifecycle-functions)

and the
[`@app.cls`](/docs/reference/modal.App#cls)

decorator. Specifically, you’ll need to:

1. Convert your function to a method by making it a member of a class.
2. Decorate the class with
   `@app.cls(...)`
   with the same arguments you previously
   had for
   `@app.function(...)`
   or your
   [web endpoint decorator](/docs/guide/webhooks)

   .
3. If you previously used the
   `@app.function()`
   decorator on your function, replace it with
   `@modal.method()`
   .
4. Define dataclass-style, type-annotated instance attributes with
   `modal.parameter()`
   and optionally set default values:

```
import modal

app = modal.App()

@app.cls()
class MyClass:

    foo: str = modal.parameter()
    bar: int = modal.parameter(default=10)

    @modal.method()
    def baz(self, qux: str = "default") -> str:
        return f"This code is running in container pool ({self.foo}, {self.bar}), with input qux={qux}"
```

The parameters create a keyword-only constructor for your class, and the methods can be called as follows:

```
@app.local_entrypoint()
def main():
    m1 = MyClass(foo="hedgehog", bar=7)
    m1.baz.remote()

    m2 = MyClass(foo="fox")
    m2.baz.remote(qux="override")
```

Function calls for each unique combination of values for
`foo`
and
`bar`
will run in their own separate container pools.
If you re-constructed a
`MyClass`
with the same arguments in a different context, the calls to
`baz`
would be routed to the same set of containers as before.

Some things to note:

* The total size of the arguments is limited to 16 KiB.
* Modal classes can still annotate types of regular class attributes, which are independent of parametrization, by either omitting
  `= modal.parameter()`
  or using
  `= modal.parameter(init=False)`
  to satisfy type checkers.
* The support types are these primitives:
  `str`
  ,
  `int`
  ,
  `bool`
  , and
  `bytes`
  .
* The legacy
  `__init__`
  constructor method is being removed, see
  [the 1.0 migration for details.](/docs/guide/modal-1-0-migration#removing-support-for-custom-cls-constructors)

Looking up a parametrized function
----------------------------------

If you want to call your parametrized function from a Python script running
anywhere, you can use
`Cls.lookup`
:

```
import modal

MyClass = modal.Cls.from_name("parametrized-function-app", "MyClass")  # returns a class-like object
m = MyClass(foo="snake", bar=12)
m.baz.remote()
```

Parametrized web endpoints
--------------------------

Modal
[web endpoints](/docs/guide/webhooks)

can also be parametrized:

```
app = modal.App("parametrized-endpoint")

@app.cls()
class MyClass():

    foo: str = modal.parameter()
    bar: int = modal.parameter(default=10)

    @modal.fastapi_endpoint()
    def baz(self, qux: str = "default") -> str:
        ...
```

Parameters are specified in the URL as query parameter values.

```
curl "https://parametrized-endpoint.modal.run?foo=hedgehog&bar=7&qux=override"
curl "https://parametrized-endpoint.modal.run?foo=hedgehog&qux=override"
curl "https://parametrized-endpoint.modal.run?foo=hedgehog&bar=7"
curl "https://parametrized-endpoint.modal.run?foo=hedgehog"
```

Using parametrized functions with lifecycle functions
-----------------------------------------------------

Parametrized functions can be used with
[lifecycle functions](/docs/guide/lifecycle-functions)

.

For example, here is how you might parametrize the
[`@enter`](/docs/guide/lifecycle-functions#enter)

lifecycle function to load a specific model:

```
@app.cls()
class Model:

    name: str = modal.parameter()
    size: int = modal.parameter(default=100)

    @modal.enter()
    def load_model(self):
        print(f"Loading model {self.name} with size {self.size}")
        self.model = load_model_util(self.name, self.size)

    @modal.method()
    def generate(self, prompt: str) -> str:
        return self.model.generate(prompt)
```

[Parametrized functions](#parametrized-functions)

[Looking up a parametrized function](#looking-up-a-parametrized-function)

[Parametrized web endpoints](#parametrized-web-endpoints)

[Using parametrized functions with lifecycle functions](#using-parametrized-functions-with-lifecycle-functions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/preemption
================================================================================

Preemption
==========

All Modal Functions are subject to preemption. If a preemption event interrupts
a running Function, Modal will gracefully terminate the Function and restart it
on the same input.

Preemptions are rare, but it is always possible that your Function is
interrupted. Long-running Functions such as model training Functions should take
particular care to tolerate interruptions, as likelihood of interruption increases
with Function run duration.

Preparing for interruptions
---------------------------

Design your applications to be fault and preemption tolerant. Modal will send an
interrupt signal to your container when preemption occurs. This will cause the
Function’s
[exit handler](/docs/guide/lifecycle-functions#exit)

to run, which
can perform any cleanup within its grace period.

Other best practices for handling preemptions include:

* Divide long-running operations into small tasks or use checkpoints so that you
  can save your work frequently.
* Ensure preemptible operations are safely retryable (ie. idempotent).

Running uninterruptible Functions
---------------------------------

We currently don’t have a way for Functions to avoid the possibility of
interruption, but it’s a planned feature. If you require Functions guaranteed to
run without interruption, please reach out!

[Preemption](#preemption)

[Preparing for interruptions](#preparing-for-interruptions)

[Running uninterruptible Functions](#running-uninterruptible-functions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/private-networking
================================================================================

Cluster networking
==================

i6pn (IPv6 private networking) is Modal’s private container-to-container networking solution. It allows users to create clusters of Modal containers which can send network traffic to each other with low latency and high bandwidth (≥ 50Gbps).

Normally,
`modal.Function`
containers can initiate outbound network connections to the internet but they are not directly addressable by other containers. i6pn-enabled containers, on the other hand, can be directly connected to by other i6pn-enabled containers and this is a key enabler of Modal’s preview
`@modal.experimental.clustered`
functionality.

You can enable i6pn on any
`modal.Function`
:

```
@app.function(i6pn=True)
def hello_private_network():
    import socket

    i6pn_addr = socket.getaddrinfo("i6pn.modal.local", None, socket.AF_INET6)[0][4][0]
    print(i6pn_addr) # fdaa:5137:3ebf:a70:1b9d:3a11:71f2:5f0f
```

In this snippet we see that the i6pn-enabled container is able to retrieve its own IPv6 address by
resolving
`i6pn.modal.local`
. For this Function container to discover the addresses of
*other*
containers,
address sharing must be implemented using an auxiliary data structure, such as a shared
`modal.Dict`
or
`modal.Queue`
.

Private networking
------------------

All i6pn network traffic is
*Workspace private*
.

![i6pn-diagram](https://modal-cdn.com/cdnbot/i6pn-1eksk4vuy_c4c4a0df.webp)

In the image above, Workspace A has subnet
`fdaa:1::/48`
, while Workspace B has subnet
`fdaa:2::/48`
.

You’ll notice they share the first 16 bits. This is because the
`fdaa::/16`
prefix contains all of our private network IPv6 addresses, while each workspace is assigned a random 32-bit identifier when it is created. Together, these form the 48-bit subnet.

The upshot of this is that only containers in the same workspace can see each other and send each other network packets. i6pn networking is secure by default.

Region boundaries
-----------------

Modal operates a
[global fleet](/docs/guide/region-selection)

and allows containers to run on multiple cloud providers and in many regions. i6pn networking is however region-scoped functionality, meaning that only i6pn-enabled containers in the same region can perform network communication.

Modal’s i6pn-enabled primitives such as
`@modal.experimental.clustered`
automatically restrict container geographic placement and cloud placement to ensure inter-container connectivity.

Public network access to cluster networking
-------------------------------------------

For cluster networked containers that need to be publicly accessible, you need to expose ports with
[modal.Tunnel](/docs/guide/tunnels)

because i6pn addresses are not publicly exposed.

Consider having a container setup a Tunnel and act as the gateway to the private cluster networking.

[Cluster networking](#cluster-networking)

[Private networking](#private-networking)

[Region boundaries](#region-boundaries)

[Public network access to cluster networking](#public-network-access-to-cluster-networking)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/private-registries
================================================================================

Private registries
==================

Modal provides the
[`Image.from_registry`](/docs/guide/images#use-an-existing-container-image-with-from_registry)

function, which can pull public images available from registries such as Docker
Hub and GitHub Container Registry, as well as private images from registries
such as
[AWS Elastic Container Registry (ECR)](https://aws.amazon.com/ecr/)

,
[GCP Artifact Registry](https://cloud.google.com/artifact-registry)

, and Docker
Hub.

Docker Hub (Private)
--------------------

To pull container images from private Docker Hub repositories,
[create an access token](https://docs.docker.com/security/for-developers/access-tokens/)

with “Read-Only” permissions and use this token value and your Docker Hub
username to create a Modal
[Secret](/docs/guide/secrets)

.

Use this Secret with the
[`modal.Image.from_registry`](/docs/reference/modal.Image#from_registry)

method.

Elastic Container Registry (ECR)
--------------------------------

You can pull images from your AWS ECR account by specifying the full image URI
as follows:

```
import modal

aws_secret = modal.Secret.from_name("my-aws-secret")
image = (
    modal.Image.from_aws_ecr(
        "000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:latest",
        secret=aws_secret,
    )
    .pip_install("torch", "huggingface")
)

app = modal.App(image=image)
```

As shown above, you also need to use a
[Modal Secret](/docs/guide/secrets)

containing the environment variables
`AWS_ACCESS_KEY_ID`
,
`AWS_SECRET_ACCESS_KEY`
, and
`AWS_REGION`
. The AWS IAM user account associated
with those keys must have access to the private registry you want to access.

The user needs to have the following read-only policies:

```
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": ["ecr:GetAuthorizationToken"],
      "Effect": "Allow",
      "Resource": "*"
    },
    {
      "Effect": "Allow",
      "Action": [
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:GetRepositoryPolicy",
        "ecr:DescribeRepositories",
        "ecr:ListImages",
        "ecr:DescribeImages",
        "ecr:BatchGetImage",
        "ecr:GetLifecyclePolicy",
        "ecr:GetLifecyclePolicyPreview",
        "ecr:ListTagsForResource",
        "ecr:DescribeImageScanFindings"
      ],
      "Resource": "<MY-REGISTRY-ARN>"
    }
  ]
}
```

You can use the IAM configuration above as a template for creating an IAM user.
You can then
[generate an access key](https://aws.amazon.com/premiumsupport/knowledge-center/create-access-key/)

and create a Modal Secret using the AWS integration option. Modal will use your
access keys to generate an ephemeral ECR token. That token is only used to pull
image layers at the time a new image is built. We don’t store this token but
will cache the image once it has been pulled.

Images on ECR must be private and follow
[image configuration requirements](/docs/reference/modal.Image#from_aws_ecr)

.

Google Artifact Registry and Google Container Registry
------------------------------------------------------

For further detail on how to pull images from Google’s image registries, see
[`modal.Image.from_gcp_artifact_registry`](/docs/reference/modal.Image#from_gcp_artifact_registry)

.

[Private registries](#private-registries)

[Docker Hub (Private)](#docker-hub-private)

[Elastic Container Registry (ECR)](#elastic-container-registry-ecr)

[Google Artifact Registry and Google Container Registry](#google-artifact-registry-and-google-container-registry)

See it in action

[Registry image for Algolia indexing](/docs/examples/algolia_indexer)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/project-structure
================================================================================

Project structure
=================

Apps spanning multiple files
----------------------------

When your project spans multiple files, more care is required to package the
full structure for running or deploying on Modal.

There are two main considerations: (1) ensuring that all of your Functions get
registered to the App, and (2) ensuring that any local dependencies get included
in the Modal container.

Say that you have a simple project that’s distributed across three files:

```
src/
├── app.py  # Defines the `modal.App` as a variable named `app`
├── llm.py  # Imports `app` and decorates some functions
└── web.py  # Imports `app` and decorates other functions
```

With this structure, if you deploy using
`modal deploy src/app.py`
, Modal won’t
discover the Functions defined in the other two modules, because they never get
imported.

If you instead run
`modal deploy src/llm.py`
, Modal will deploy the App with
just the Functions defined in that module.

One option would be to ensure that one module in the project transitively
imports all of the other modules and to point the
`modal deploy`
CLI at it, but
this approach can lead to an awkard project structure.

### Defining your project as a Python package

A better approach would be to define your project as a Python
*package*
and to
use the Modal CLI’s “module mode” invocation pattern.

In Python, a package is a directory containing an
`__init__.py`
file (and
usually some other Python modules). If you have a
`src/__init__.py`
that
imports all of the member modules, it will ensure that any decorated Functions
contained within them get registered to the App:

```
# Contents of __init__.py
import .app
import .llm
import .web
```

*Important: use
*relative*
imports (
`import .app`
) between member modules.*

Unfortunately, it’s not enough just to set this up and make your deploy command
`modal deploy src/app.py`
. Instead, you need to invoke Modal in
*module mode*
:
`modal deploy -m src.app`
. Note the use of the
`-m`
flag and the module path
(
`src.app`
instead of
`src/app.py`
). Akin to
`python -m ...`
, this incantation
treats the target as a package rather than just a single script.

### App composition

As your project grows in scope, it may become helpful to organize it into
multiple component Apps, rather than having the project defined as one large
monolith. That way, as you iterate during development, you can target a specific
component, which will build faster and avoid any conflicts with concurrent work
on other parts of the project.

Projects set up this way can still be deployed as one unit by using
`App.include`
.
Say our project from above defines separate Apps in
`llm.py`
and
`web.py`
and then
adds a new
`deploy.py`
file:

```
# Contents of deploy.py
import modal

from .llm import llm_app
from .web import web_app

app = modal.App("full-app").include(llm_app).include(web_app)
```

This lets you run
`modal deploy -m src.deploy`
to package everything in one
step.

**Note:**
Since the multi-file app still has a single namespace for all
functions, it’s important to name your Modal functions uniquely across the
project even when splitting it up across files: otherwise you risk some
functions “shadowing” others with the same name.

Including local dependencies
----------------------------

Another factor to consider is whether Modal will package all of the local
dependencies that your App requires.

Even if your Modal App itself can be contained to a single file, any local
modules that file imports (like, say, a
`helpers.py`
) also need to be available
in the Modal container.

By default, Modal will automatically include the module or package where a
Function is defined in all containers that run that Function. So if the project
is set up as a package and the helper modules are part of that package, you
should be all set. If you’re not using a package setup, or if the local
dependencies are external to your project’s package, you’ll need to explicitly
include them in the Image, i.e. with
`modal.Image.add_local_python_source`
.

**Note:**
This behavior changed in Modal 1.0. Previously, Modal would
“automount” any local dependencies that were imported by your App source into a
container. This was changed to be more selective to avoid unnecessary inclusion
of large local packages.

[Project structure](#project-structure)

[Apps spanning multiple files](#apps-spanning-multiple-files)

[Defining your project as a Python package](#defining-your-project-as-a-python-package)

[App composition](#app-composition)

[Including local dependencies](#including-local-dependencies)

See it in action

[QuiLLMan - Voice Chat with LLMs](https://github.com/modal-labs/quillman)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/proxy-ips
================================================================================

Proxies (beta)
==============

You can securely connect with resources in your private network
using a Modal Proxy. Proxies are a secure tunnel between
Apps and exit nodes with static IPs. You can allow-list those static IPs
in your network firewall, making sure that only traffic originating from these
IP addresses is allowed into your network.

Proxies are unique and not shared between workspaces. All traffic
between your Apps and the Proxy server is encrypted using
[Wireguard](https://www.wireguard.com/)

.

Modal Proxies are built on top of
[vprox](https://github.com/modal-labs/vprox)

,
a Modal open-source project used to create highly available proxy servers
using Wireguard.

*Modal Proxies are in beta. Please let us know if you run into issues.*

Creating a Proxy
----------------

Proxies are available for
[Team Plan](/pricing)

or
[Enterprise](/pricing)

users.

You can create Proxies in your workspace
[Settings](/settings)

page.
Team Plan users can create one Proxy and Enterprise users three Proxies. Each Proxy
can have a maximum of five static IP addresses.

Please reach out to
[support@modal.com](mailto:support@modal.com)

if you need greater limits.

Using a Proxy
-------------

After a Proxy is online, add it to a Modal Function with the argument
`proxy=Proxy.from_name("<your-proxy>")`
. For example:

```
import modal
import subprocess

app = modal.App(image=modal.Image.debian_slim().apt_install("curl"))

@app.function(proxy=modal.Proxy.from_name("<your-proxy>"))
def my_ip():
    subprocess.run(["curl", "-s", "ifconfig.me"])

@app.local_entrypoint()
def main():
    my_ip.remote()
```

All network traffic from your Function will now use the Proxy as a tunnel.

The program above will always print the same IP address independent
of where it runs in Modal’s infrastructure. If that same program
were to run without a Proxy, it would print a different IP
address depending on where it runs.

Proxy performance
-----------------

All traffic that goes through a Proxy is encrypted by Wireguard. This adds
latency to your Function’s networking. If are experiencing networking issues
with Proxies related to performance, first add more IP addresses to your
Proxy (see
[Adding more IP addresses a Proxy](#adding-more-ip-addresses-to-a-proxy)

).

Adding more IP addresses to a Proxy
-----------------------------------

Proxies support up to five static IP addresses. Adding IP addresses improves
throughput linearly.

You can add an IP address to your workspace in
[Settings](/settings)

> Proxies.
Select the desired Proxy and add a new IP.

If a Proxy has multiple IPs, Modal will randomly pick one when running your Function.

Proxies and Sandboxes
---------------------

Proxies can also be used with
[Sandboxes](/docs/guide/sandbox)

. For example:

```
import modal

app = modal.App.lookup("sandbox-proxy", create_if_missing=True)
sb = modal.Sandbox.create(
    app=app,
    image=modal.Image.debian_slim().apt_install("curl"),
    proxy=modal.Proxy.from_name("<your-proxy>"))

process = sb.exec("curl", "-s", "https://ifconfig.me")
stdout = process.stdout.read()
print(stdout)

sb.terminate()
```

Similarly to our Function implementation, this Sandbox program will
always print the same IP address.

[Proxies (beta)](#proxies-beta)

[Creating a Proxy](#creating-a-proxy)

[Using a Proxy](#using-a-proxy)

[Proxy performance](#proxy-performance)

[Adding more IP addresses to a Proxy](#adding-more-ip-addresses-to-a-proxy)

[Proxies and Sandboxes](#proxies-and-sandboxes)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/queues
================================================================================

Queues
======

Modal Queues provide distributed FIFO queues to your Modal Apps.

```
import modal

app = modal.App()
queue = modal.Queue.from_name("simple-queue", create_if_missing=True)

def producer(x):
    queue.put(x)  # adding a value

@app.function()
def consumer():
    return queue.get()  # retrieving a value

@app.local_entrypoint()
def main(x="some object"):
    # produce and consume tasks from local or remote code
    producer(x)
    print(consumer.remote())
```

This page is a high-level guide to using Modal Queues.
For reference documentation on the
`modal.Queue`
object, see
[this page](/docs/reference/modal.Queue)

.
For reference documentation on the
`modal queue`
CLI command, see
[this page](/docs/reference/cli/queue)

.

Modal Queues are Python queues in the cloud
-------------------------------------------

Like
[Python
`Queue`
s](https://docs.python.org/3/library/queue.html)

,
Modal Queues are multi-producer, multi-consumer first-in-first-out (FIFO) queues.

Queues are particularly useful when you want to handle tasks or process
data asynchronously, or when you need to pass messages between different
components of your distributed system.

Queues are cleared 24 hours after the last
`put`
operation and are backed by
a replicated in-memory database, so persistence is likely, but not guaranteed.
As such,
`Queue`
s are best used for communication between active functions and
not relied on for persistent storage.

[Please get in touch](mailto:support@modal.com)

if you need durability for Queue objects.

Queues are partitioned by key
-----------------------------

Queues are split into separate FIFO partitions via a string key. By default, one
partition (corresponding to an empty key) is used.

A single
`Queue`
can contain up to 100,000 partitions, each with up to 5,000
items. Each item can be up to 1 MiB. These limits also apply to the default
partition.

Each partition has an independent TTL, by default 24 hours.
Lower TTLs can be specified by the
`partition_ttl`
argument in the
`put`
or
`put_many`
methods.

```
import modal

app = modal.App()
my_queue = modal.Queue.from_name("partitioned-queue", create_if_missing=True)

@app.local_entrypoint()
def main():
    # clear all elements, start from a clean slate
    my_queue.clear()

    my_queue.put("some value")  # first in
    my_queue.put(123)

    assert my_queue.get() == "some value"  # first out
    assert my_queue.get() == 123

    my_queue.put(0)
    my_queue.put(1, partition="foo")
    my_queue.put(2, partition="bar")

    # Default and "foo" partition are ignored by the get operation.
    assert my_queue.get(partition="bar") == 2

    # Set custom 10s expiration time on "foo" partition.
    my_queue.put(3, partition="foo", partition_ttl=10)

    # (beta feature) Iterate through items in place (read immutably)
    my_queue.put(1)
    assert [v for v in my_queue.iterate()] == [0, 1]
```

You can access Modal Queues synchronously or asynchronously, blocking or non-blocking
-------------------------------------------------------------------------------------

Queues are synchronous and blocking by default. Consumers will block and wait
on an empty Queue and producers will block and wait on a full Queue,
both with an
`Optional`
, configurable
`timeout`
. If the
`timeout`
is
`None`
,
they will wait indefinitely. If a
`timeout`
is provided,
`get`
methods will raise
[`queue.Empty`](https://docs.python.org/3/library/queue.html#queue.Empty)

exceptions and
`put`
methods will raise
[`queue.Full`](https://docs.python.org/3/library/queue.html#queue.Full)

exceptions, both from the Python standard library.

The
`get`
and
`put`
methods can be made non-blocking by setting the
`block`
argument to
`False`
.
They raise
`queue`
exceptions without waiting on the
`timeout`
.

Queues are stored in the cloud, so all interactions require communication over the network.
This adds some extra latency to calls, apart from the
`timeout`
, on the order of tens of milliseconds.
To avoid this latency impacting application latency, you can asynchronously interact with Queues
by adding the
`.aio`
function suffix to access methods.

```
@app.local_entrypoint()
async def main(value=None):
    await my_queue.put.aio(value or 200)
    assert await my_queue.get.aio() == value
```

See the guide to
[asynchronous functions](/docs/guide/async)

for more
information.

Modal Queues are not
*exactly*
Python Queues
--------------------------------------------

Python Queues can have values of any type.

Modal Queues can store Python objects of any serializable type.

Objects are serialized using
[`cloudpickle`](https://github.com/cloudpipe/cloudpickle)

,
so precise support is inherited from that library.
`cloudpickle`
can serialize a surprising variety of objects,
like
`lambda`
functions or even Python modules, but it can’t serialize a few things that don’t
really make sense to serialize, like live system resources (sockets, writable file descriptors).

Note that you will need to have the library defining the type installed in the environment
where you retrieve the object so that it can be deserialized.

```
import modal

app = modal.App()
queue = modal.Queue.from_name("funky-queue", create_if_missing=True)
queue.clear()  # start from a clean slate

@app.function(image=modal.Image.debian_slim().pip_install("numpy"))
def fill():
    import numpy

    queue.put(modal)
    queue.put(queue)  # don't try this at home!
    queue.put(numpy)

@app.local_entrypoint()
def main():
    fill.remote()
    print(queue.get().Queue)
    print(queue.get())
    # print(queue.get())  # DeserializationError, if no torch locally
```

[Queues](#queues)

[Modal Queues are Python queues in the cloud](#modal-queues-are-python-queues-in-the-cloud)

[Queues are partitioned by key](#queues-are-partitioned-by-key)

[You can access Modal Queues synchronously or asynchronously, blocking or non-blocking](#you-can-access-modal-queues-synchronously-or-asynchronously-blocking-or-non-blocking)

[Modal Queues are not exactly Python Queues](#modal-queues-are-not-exactly-python-queues)

See it in action

[Use Dicts and Queues to coordinate a web scraper](/docs/examples/dicts_and_queues)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/region-selection
================================================================================

Region selection
================

Modal allows you to specify which cloud region you would like to run a Function in. This may be useful if:

* you are required (for regulatory reasons or by your customers) to process data within certain regions.
* you want to reduce egress fees that result from reading data from a dependency like S3.
* you have a latency-sensitive app where app endpoints need to run near an external DB.

Note that regardless of what region your Function runs in, all Function inputs and outputs go through Modal’s control plane in us-east-1.

Pricing
-------

A multiplier on top of our
[base usage pricing](/pricing)

will be applied to any function that has a cloud region defined.

| **Region** | **Multiplier** |
| --- | --- |
| Any region in US/EU/AP | 1.25x |
| All other regions | 2.5x |

Here’s an example: let’s say you have a function that uses 1 T4, 1 CPU core, and 1GB memory. You’ve specified that the function should run in
`us-east-2`
. The cost to run this function for 1 hour would be
`((T4 hourly cost) + (CPU hourly cost for one core) + (Memory hourly cost for one GB)) * 1.25`
.

If you specify multiple regions and they span the two categories above, we will apply the smaller of the two multipliers.

Specifying a region
-------------------

To run your Modal Function in a specific region, pass a
`region=`
argument to the
`function`
decorator.

```
import os
import modal

app = modal.App("...")

@app.function(region="us-east") # also supports a list of options, for example region=["us-central", "us-east"]
def f():
    print(f"running in {os.environ['MODAL_REGION']}") # us-east-1, us-east-2, us-ashburn-1, etc.
```

You can specify a region in addition to the underlying cloud,
`@app.function(cloud="aws", region="us-east")`
would run your Function only in
`"us-east-1"`
or
`"us-east-2"`
for instance.

Region options
--------------

Modal offers varying levels of granularity for regions. Use broader regions when possible, as this increases the pool of available resources your Function can be assigned to, which improves cold-start time and availability.

### United States (“us”)

Use
`region="us"`
to select any region in the United States.

```
     Broad            Specific             Description
 ==============================================================
  "us-east"           "us-east-1"          AWS Virginia
                      "us-east-2"          AWS Ohio
                      "us-east1"           GCP South Carolina
                      "us-east4"           GCP Virginia
                      "us-east5"           GCP Ohio
                      "us-ashburn-1"       OCI Virginia
 --------------------------------------------------------------
  "us-central"        "us-central1"        GCP Iowa
                      "us-chicago-1"       OCI Chicago
                      "us-phoenix-1"       OCI Phoenix
 --------------------------------------------------------------
  "us-west"           "us-west-1"          AWS California
                      "us-west-2"          AWS Oregon
                      "us-west1"           GCP Oregon
                      "us-west3"           GCP Utah
                      "us-west4"           GCP Nevada
                      "us-sanjose-1"       OCI San Jose
```

### Europe (“eu”)

Use
`region="eu"`
to select any region in Europe.

```
     Broad            Specific             Description
 ==============================================================
  "eu-west"           "eu-central-1"       AWS Frankfurt
                      "eu-west-1"          AWS Ireland
                      "eu-west-3"          AWS Paris
                      "europe-west1"       GCP Belgium
                      "europe-west3"       GCP Frankfurt
                      "europe-west4"       GCP Netherlands
                      "eu-frankfurt-1"     OCI Frankfurt
                      "eu-paris-1"         OCI Paris
 --------------------------------------------------------------
  "eu-north"          "eu-north-1"         AWS Stockholm
```

### Asia–Pacific (“ap”)

Use
`region="ap"`
to select any region in Asia–Pacific.

```
     Broad            Specific             Description
 ==============================================================
  "ap-northeast"      "asia-northeast3"    GCP Seoul
                      "asia-northeast1"    GCP Tokyo
                      "ap-northeast-1"     AWS Tokyo
                      "ap-northeast-3"     AWS Osaka
 --------------------------------------------------------------
  "ap-southeast"      "asia-southeast1"    GCP Singapore
                      "ap-southeast-3"     AWS Jakarta
 --------------------------------------------------------------
  "ap-south"          "ap-south-1"         AWS Mumbai
```

### Other regions

```
     Broad            Specific             Description
 ==============================================================
  "ca"                "ca-central-1"       AWS Montreal
                      "ca-toronto-1"       OCI Toronto
 --------------------------------------------------------------
  "uk"                "uk-london-1"        OCI London
                      "europe-west2"       GCP London
                      "eu-west-2"          AWS London
 --------------------------------------------------------------
  "jp"                "ap-northeast-1"     AWS Tokyo
                      "ap-northeast-3"     AWS Osaka
                      "asia-northeast1"    GCP Tokyo
 --------------------------------------------------------------
  "me"                "me-west1"           GCP Tel Aviv
 --------------------------------------------------------------
  "sa"                "sa-east-1"          AWS São Paulo
```

Region selection and GPU availability
-------------------------------------

Region selection limits the pool of instances we can run your Functions on. As a result, you may observe higher wait times between when your Function is called and when it gets executed. Generally, we have higher availability in US/EU versus other regions. Whenever possible, select the broadest possible regions so you get the best resource availability.

[Region selection](#region-selection)

[Pricing](#pricing)

[Specifying a region](#specifying-a-region)

[Region options](#region-options)

[United States (“us”)](#united-states-us)

[Europe (“eu”)](#europe-eu)

[Asia–Pacific (“ap”)](#asiapacific-ap)

[Other regions](#other-regions)

[Region selection and GPU availability](#region-selection-and-gpu-availability)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/resources
================================================================================

Reserving CPU and memory
========================

Each Modal container has a default reservation of 0.125 CPU cores and 128 MiB of memory.
Containers can exceed this minimum if the worker has available CPU or memory.
You can also guarantee access to more resources by requesting a higher reservation.

CPU cores
---------

If you have code that must run on a larger number of cores, you can
request that using the
`cpu`
argument. This allows you to specify a
floating-point number of CPU cores:

```
import modal

app = modal.App()

@app.function(cpu=8.0)
def my_function():
    # code here will have access to at least 8.0 cores
    ...
```

Memory
------

If you have code that needs more guaranteed memory, you can request it using the
`memory`
argument. This expects an integer number of megabytes:

```
import modal

app = modal.App()

@app.function(memory=32768)
def my_function():
    # code here will have access to at least 32 GiB of RAM
    ...
```

How much can I request?
-----------------------

For both CPU and memory, a maximum is enforced at function creation time to
ensure your application can be scheduled for execution. Requests exceeding the
maximum will be rejected with an
[`InvalidError`](/docs/reference/modal.exception#modalexceptioninvaliderror)

.

As the platform grows, we plan to support larger CPU and memory reservations.

Billing
-------

For CPU and memory, you’ll be charged based on whichever is higher: your reservation or actual usage.

Disk requests are billed by increasing the memory request at a 20:1 ratio. For example, requesting 500 GiB of disk will increase the memory request to 25 GiB, if it is not already set higher.

Resource limits
---------------

### CPU limits

Modal containers have a default soft CPU limit that is set at 16 physical cores above the CPU request.
Given that the default CPU request is 0.125 cores the default soft CPU limit is 16.125 cores.
Above this limit the host will begin to throttle the CPU usage of the container.

You can alternatively set the CPU limit explicitly.

```
cpu_request = 1.0
cpu_limit = 4.0
@app.function(cpu=(cpu_request, cpu_limit))
def f():
    ...
```

### Memory limits

Modal containers can have a hard memory limit which will ‘Out of Memory’ (OOM) kill
containers which attempt to exceed the limit. This functionality is useful when a container
has a serious memory leak. You can set the limit and have the container killed to avoid paying
for the leaked GBs of memory.

```
mem_request = 1024
mem_limit = 2048
@app.function(
    memory=(mem_request, mem_limit),
)
def f():
    ...
```

Specify this limit using the
[`memory`
parameter](/docs/reference/modal.App#function)

on Modal Functions.

### Disk limits

Running Modal containers have access to many GBs of SSD disk, but the amount
of writes is limited by:

1. The size of the underlying worker’s SSD disk capacity
2. A per-container disk quota that is set in the 100s of GBs.

Hitting either limit will cause the container’s disk writes to be rejected, which
typically manifests as an
`OSError`
.

Increased disk sizes can be requested with the
[`ephemeral_disk`
parameter](/docs/reference/modal.App#function)

. The maximum
disk size is 3.0 TiB (3,145,728 MiB). Larger disks are intended to be used for
[dataset processing](/docs/guide/dataset-ingestion)

.

[Reserving CPU and memory](#reserving-cpu-and-memory)

[CPU cores](#cpu-cores)

[Memory](#memory)

[How much can I request?](#how-much-can-i-request)

[Billing](#billing)

[Resource limits](#resource-limits)

[CPU limits](#cpu-limits)

[Memory limits](#memory-limits)

[Disk limits](#disk-limits)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/restricted-access
================================================================================

Running untrusted code in Functions
===================================

Modal provides two primitives for running untrusted code: Restricted Functions and
[Sandboxes](/docs/guide/sandbox)

. While both can be used for running untrusted code, they serve different purposes: Sandboxes provide a container-like interface while Restricted Functions provide an interface similar to a traditional Function.

Restricted Functions are useful for executing:

* Code generated by language models (LLMs)
* User-submitted code in interactive environments
* Third-party plugins or extensions

Using
`restrict_modal_access`
-----------------------------

To restrict a Function’s access to Modal resources, set
`restrict_modal_access=True`
on the Function definition:

```
import modal

app = modal.App()

@app.function(restrict_modal_access=True)
def run_untrusted_code(code_input: str):
    # This function cannot access Modal resources
    return eval(code_input)
```

When
`restrict_modal_access`
is enabled:

* The Function cannot access Modal resources (Queues, Dicts, etc.)
* The Function cannot call other Functions
* The Function cannot access Modal’s internal APIs

Comparison with Sandboxes
-------------------------

While both
`restrict_modal_access`
and
[Sandboxes](/docs/guide/sandbox)

can be used for running untrusted code, they serve different purposes:

| Feature | Restricted Function | Sandbox |
| --- | --- | --- |
| State | Stateless | Stateful |
| Interface | Function-like | Container-like |
| Setup | Simple decorator | Requires explicit creation/termination |
| Use case | Quick, isolated code execution | Interactive development, long-running sessions |

Best Practices
--------------

When running untrusted code, consider these additional security measures:

1. Use
   `max_inputs=1`
   to ensure each container only handles one request. Containers that get reused could cause information leakage between users.

```
@app.function(restrict_modal_access=True, max_inputs=1)
def isolated_function(input_data):
    # Each input gets a fresh container
    return process(input_data)
```

2. Set appropriate timeouts to prevent long-running operations:

```
@app.function(
    restrict_modal_access=True,
    timeout=30,  # 30 second timeout
    max_inputs=1
)
def time_limited_function(input_data):
    return process(input_data)
```

3. Consider using
   `block_network=True`
   to prevent the container from making outbound network requests:

```
@app.function(
    restrict_modal_access=True,
    block_network=True,
    max_inputs=1
)
def network_isolated_function(input_data):
    return process(input_data)
```

Example: Running LLM-generated Code
-----------------------------------

Below is a complete example of running code generated by a language model:

```
import modal

app = modal.App("restricted-access-example")

@app.function(restrict_modal_access=True, max_inputs=1, timeout=30, block_network=True)
def run_llm_code(generated_code: str):
    try:
        # Create a restricted environment
        execution_scope = {}

        # Execute the generated code
        exec(generated_code, execution_scope)

        # Return the result if it exists
        return execution_scope.get("result", None)
    except Exception as e:
        return f"Error executing code: {str(e)}"

@app.local_entrypoint()
def main():
    # Example LLM-generated code
    code = """
def calculate_fibonacci(n):
    if n <= 1:
        return n
    return calculate_fibonacci(n-1) + calculate_fibonacci(n-2)

result = calculate_fibonacci(10)
    """

    result = run_llm_code.remote(code)
    print(f"Result: {result}")
```

This example locks down the container to ensure that the code is safe to execute by:

* Restricting Modal access
* Using a fresh container for each execution
* Setting a timeout
* Blocking network access
* Catching and handling potential errors

Error Handling
--------------

When a restricted Function attempts to access Modal resources, it will raise an
`AuthError`
:

```
@app.function(restrict_modal_access=True)
def restricted_function(q: modal.Queue):
    try:
        # This will fail because the Function is restricted
        return q.get()
    except modal.exception.AuthError as e:
        return f"Access denied: {e}"
```

The error message will indicate that the operation is not permitted due to restricted Modal access.

[Running untrusted code in Functions](#running-untrusted-code-in-functions)

[Using restrict\_modal\_access](#using-restrict_modal_access)

[Comparison with Sandboxes](#comparison-with-sandboxes)

[Best Practices](#best-practices)

[Example: Running LLM-generated Code](#example-running-llm-generated-code)

[Error Handling](#error-handling)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/retries
================================================================================

Failures and retries
====================

When you call a function over a sequence of inputs with
[Function.map()](/docs/guide/scale#parallel-execution-of-inputs)

, sometimes
errors can happen during function execution. Exceptions from within the remote
function are propagated to the caller, so you can handle them with a
`try-except`
statement (refer to
[section on custom types](https://modal.com/docs/guide/troubleshooting#custom-types-defined-in-__main__)

for more on how to catch user-defined exceptions):

```
@app.function()
def f(i):
    raise ValueError()

@app.local_entrypoint()
def main():
    try:
        for _ in f.map([1, 2, 3]):
            pass
    except ValueError:
        print("Exception handled")
```

Function retries
----------------

You can configure Modal to automatically retry function failures if you set the
`retries`
option when declaring your function:

```
@app.function(retries=3)
def my_flaky_function():
    pass
```

When used with
`Function.map()`
, each input is retried up to the max number of
retries specified.

The basic configuration shown provides a fixed 1s delay between retry attempts.
For fine-grained control over retry delays, including exponential backoff
configuration, use
[`modal.Retries`](/docs/reference/modal.Retries)

.

To treat exceptions as successful results and aggregate them in the results list instead, pass in
[`return_exceptions=True`](/docs/guide/scale#exceptions)

.

Container crashes
-----------------

If a
`modal.Function`
container crashes (either on start-up, e.g. while handling imports in global scope, or during execution, e.g. an out-of-memory error), Modal will reschedule the container and any work it was currently assigned.

For
[ephemeral apps](/docs/guide/apps#ephemeral-apps)

, container crashes will be retried until a failure rate is exceeded, after which all pending inputs will be failed and the exception will be propagated to the caller.

For
[deployed apps](/docs/guide/apps#deployed-apps)

, container crashes will be retried indefinitely, so as to not disrupt service. Modal will instead apply a crash-loop backoff and the rate of new container creation for the function will be slowed down. Crash-looping containers are displayed in the app dashboard.

[Failures and retries](#failures-and-retries)

[Function retries](#function-retries)

[Container crashes](#container-crashes)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/s3-gateway-endpoints
================================================================================

S3 Gateway endpoints
====================

When running workloads in AWS, our system automatically uses a corresponding
[S3 Gateway endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-s3.html)

to ensure low costs, optimal performance, and network reliability between Modal and S3.

Workloads running on Modal should not incur egress or ingress fees associated
with S3 operations. No configuration is needed in order for your app to use S3 Gateway endpoints.
S3 Gateway endpoints are automatically used when your app runs on AWS.

Endpoint configuration
----------------------

Only use the region-specific endpoint (
`s3.<region>.amazonaws.com`
) or the
global AWS endpoint (
`s3.amazonaws.com`
). Using an S3 endpoint from one region
in another
**will not use the S3 Gateway Endpoint incurring networking costs**
.

Avoid specifying regional endpoints manually, as this can lead to unexpected cost
or performance degradation.

Inter-region costs
------------------

S3 Gateway endpoints guarantee no costs for network traffic within the same AWS region.
However, if your Modal Function runs in one region but your bucket resides in a
different region you will be billed for inter-region traffic.

You can prevent this by scheduling your Modal App in the same region of your
S3 bucket with
[Region selection](https://modal.com/docs/guide/region-selection#region-selection)

.

[S3 Gateway endpoints](#s3-gateway-endpoints)

[Endpoint configuration](#endpoint-configuration)

[Inter-region costs](#inter-region-costs)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/sandbox
================================================================================

Sandboxes
=========

In addition to the Function interface, Modal has a direct
interface for defining containers
*at runtime*
and securely running arbitrary code
inside them.

This can be useful if, for example, you want to:

* Execute code generated by a language model.
* Create isolated environments for running untrusted code.
* Check out a git repository and run a command against it, like a test suite, or
  `npm lint`
  .
* Run containers with arbitrary dependencies and setup scripts.

Each individual job is called a
**Sandbox**
and can be created using the
[`Sandbox.create`](/docs/reference/modal.Sandbox#create)

constructor:

```
import modal

app = modal.App.lookup("my-app", create_if_missing=True)

sb = modal.Sandbox.create(app=app)

p = sb.exec("python", "-c", "print('hello')", timeout=3)
print(p.stdout.read())

p = sb.exec("bash", "-c", "for i in {1..10}; do date +%T; sleep 0.5; done", timeout=5)
for line in p.stdout:
    # Avoid double newlines by using end="".
    print(line, end="")

sb.terminate()
```

**Note:**
you can run the above example as a script directly with
`python my_script.py`
.
`modal run`
is not needed here since there is no
[entrypoint](/docs/guide/apps#entrypoints-for-ephemeral-apps)

.

Sandboxes require an
[`App`](/docs/guide/apps)

to be passed when spawned from outside
of a Modal container. You may pass in a regular
`App`
object or look one up by name with
[`App.lookup`](/docs/reference/modal.App#lookup)

. The
`create_if_missing`
flag on
`App.lookup`
will create an
`App`
with the given name if it doesn’t exist.

Running a Sandbox with an entrypoint
------------------------------------

In most cases, Sandboxes are treated as a generic container that can run arbitrary
commands. However, in some cases, you may want to run a single command or script
as the entrypoint of the Sandbox. You can do this by passing string arguments to the
Sandbox constructor:

```
sb = modal.Sandbox.create("python", "-m", "http.server", "8080", app=my_app, timeout=10)
for line in sb.stdout:
    print(line, end="")
```

This functionality is most useful for running long-lived services that you want
to keep running in the background. See our
[Jupyter notebook example](/docs/examples/jupyter_sandbox)

for a more concrete example of this.

Referencing Sandboxes from other code
-------------------------------------

If you have a running Sandbox, you can retrieve it using the
[`Sandbox.from_id`](/docs/reference/modal.Sandbox#from_id)

method.

```
sb = modal.Sandbox.create(app=my_app)
sb_id = sb.object_id

# ... later in the program ...

sb2 = modal.Sandbox.from_id(sb_id)

p = sb2.exec("echo", "hello")
print(p.stdout.read())
sb2.terminate()
```

A common use case for this is keeping a pool of Sandboxes available for executing tasks
as they come in. You can keep a list of
`object_id`
s of Sandboxes that are “open” and
reuse them, closing over the
`object_id`
in whatever function is using them.

Parameters
----------

Sandboxes support nearly all configuration options found in regular
`modal.Function`
s.
Refer to
[`Sandbox.create`](/docs/reference/modal.Sandbox#create)

for further documentation
on Sandbox parametrization.

For example, Images and Volumes can be used just as with functions:

```
sb = modal.Sandbox.create(
    image=modal.Image.debian_slim().pip_install("pandas"),
    volumes={"/data": modal.Volume.from_name("my-volume")},
    workdir="/repo",
    app=my_app,
)
```

### Using custom images

Sandboxes support custom images just as Functions do. However, while you’ll typically
invoke a Modal Function with the
`modal run`
cli, you typically spawn a Sandbox
with a simple
`python`
call. As such, you need to manually enable output streaming
to see your image build logs:

```
image = modal.Image.debian_slim().pip_install("pandas", "numpy")

with modal.enable_output():
    sb = modal.Sandbox.create(image=image, app=my_app)
```

### Dynamically defined environments

Note that any valid
`Image`
or
`Mount`
can be used with a Sandbox, even if those
images or mounts have not previously been defined. This also means that Images and
Mounts can be built from requirements at
**runtime**
. For example, you could
use a language model to write some code and define your image, and then spawn a
Sandbox with it. Check out
[devlooper](https://github.com/modal-labs/devlooper)

for a concrete example of this.

### Environment variables

You can set environment variables using inline secrets:

```
secret = modal.Secret.from_dict({"MY_SECRET": "hello"})

sb = modal.Sandbox.create(
    secrets=[secret],
    app=my_app,
)
p = sb.exec("bash", "-c", "echo $MY_SECRET")
print(p.stdout.read())
```

### Verbose logging

You can see Sandbox execution logs using
`verbose=True`
. For example:

```
sb = modal.Sandbox.create(app=my_app, verbose=True)

p = sb.exec("python", "-c", "print('hello')")
print(p.stdout.read())

with sb.open("test.txt", "w") as f:
    f.write("Hello World\n")
```

shows Sandbox logs:

```
Sandbox exec started: python -c print('hello')
Opened file 'test.txt': fd-yErSQzGL9sig6WAjyNgTPR
Wrote to file: fd-yErSQzGL9sig6WAjyNgTPR
Closed file: fd-yErSQzGL9sig6WAjyNgTPR
```

Tagging
-------

Sandboxes can be tagged with arbitrary key-value pairs. These tags can be used
to filter results in
[`Sandbox.list`](/docs/reference/modal.Sandbox#list)

.

```
sandbox_v1_1 = modal.Sandbox.create("sleep", "10", app=my_app)
sandbox_v1_2 = modal.Sandbox.create("sleep", "20", app=my_app)

sandbox_v1_1.set_tags({"major_version": "1", "minor_version": "1"})
sandbox_v1_2.set_tags({"major_version": "1", "minor_version": "2"})

for sandbox in modal.Sandbox.list(app_id=my_app.app_id):  # All sandboxes.
    print(sandbox.object_id)

for sandbox in modal.Sandbox.list(
    app_id=my_app.app_id,
    tags={"major_version": "1"},
):  # Also all sandboxes.
    print(sandbox.object_id)

for sandbox in modal.Sandbox.list(
    app_id=app.app_id,
    tags={"major_version": "1", "minor_version": "2"},
):  # Just the latest sandbox.
    print(sandbox.object_id)
```

[Sandboxes](#sandboxes)

[Running a Sandbox with an entrypoint](#running-a-sandbox-with-an-entrypoint)

[Referencing Sandboxes from other code](#referencing-sandboxes-from-other-code)

[Parameters](#parameters)

[Using custom images](#using-custom-images)

[Dynamically defined environments](#dynamically-defined-environments)

[Environment variables](#environment-variables)

[Verbose logging](#verbose-logging)

[Tagging](#tagging)

See it in action

[Building a coding agent with Sandboxes](/docs/examples/agent)

[Building a code interpreter](/docs/examples/simple_code_interpreter)

[Running a Jupyter notebook](/docs/examples/jupyter_sandbox)

[Safe code execution](/docs/examples/safe_code_execution)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/sandbox-files
================================================================================

Filesystem Access
=================

There are multiple options for uploading files to a Sandbox and accessing them
from outside the Sandbox.

Efficient file syncing
----------------------

To efficiently upload local files to a Sandbox, you can use the
[`add_local_file`](/docs/reference/modal.Image#add_local_file)

and
[`add_local_dir`](/docs/reference/modal.Image#add_local_dir)

methods on the
[`Image`](/docs/reference/modal.Image)

class:

```
sb = modal.Sandbox.create(
    app=my_app,
    image=modal.Image.debian_slim().add_local_dir(
        local_path="/home/user/my_dir",
        remote_path="/app"
    )
)
p = sb.exec("ls", "/app")
print(p.stdout.read())
p.wait()
```

Alternatively, it’s possible to use Modal
[Volume](/docs/reference/modal.Volume)

s or
[CloudBucketMount](/docs/guide/cloud-bucket-mounts)

s. These have the benefit that
files created from inside the Sandbox can easily be accessed outside the
Sandbox.

To efficiently upload files to a Sandbox using a Volume, you can use the
[`batch_upload`](/docs/reference/modal.Volume#batch_upload)

method on the
`Volume`
class - for instance, using an ephemeral Volume that
will be garbage collected when the App finishes:

```
with modal.Volume.ephemeral() as vol:
    import io
    with vol.batch_upload() as batch:
        batch.put_file("local-path.txt", "/remote-path.txt")
        batch.put_directory("/local/directory/", "/remote/directory")
        batch.put_file(io.BytesIO(b"some data"), "/foobar")

    sb = modal.Sandbox.create(
        volumes={"/cache": vol},
        app=my_app,
    )
    p = sb.exec("cat", "/cache/remote-path.txt")
    print(p.stdout.read())
    p.wait()
    sb.terminate()
```

The caller also can access files created in the Volume from the Sandbox, even after the Sandbox is terminated:

```
with modal.Volume.ephemeral() as vol:
    sb = modal.Sandbox.create(
        volumes={"/cache": vol},
        app=my_app,
    )
    p = sb.exec("bash", "-c", "echo foo > /cache/a.txt")
    p.wait()
    sb.terminate()
    for data in vol.read_file("a.txt"):
        print(data)
```

Alternatively, if you want to persist files between Sandbox invocations (useful
if you’re building a stateful code interpreter, for example), you can use create
a persisted
`Volume`
with a dynamically assigned label:

```
session_id = "example-session-id-123abc"
vol = modal.Volume.from_name(f"vol-{session_id}", create_if_missing=True)
sb = modal.Sandbox.create(
    volumes={"/cache": vol},
    app=my_app,
)
p = sb.exec("bash", "-c", "echo foo > /cache/a.txt")
p.wait()
sb.terminate()
for data in vol.read_file("a.txt"):
    print(data)
```

File syncing behavior differs between Volumes and CloudBucketMounts. For
Volumes, files are only synced back to the Volume when the Sandbox terminates.
For CloudBucketMounts, files are synced automatically.

Filesystem API (Alpha)
----------------------

If you’re less concerned with efficiency of uploads and want a convenient way
to pass data in and out of the Sandbox during execution, you can use our
filesystem API to easily read and write files. The API supports reading
files up to 100 MiB and writes up to 1 GiB in size.

This API is currently in Alpha, and we don’t recommend using it for production
workloads.

```
import modal

app = modal.App.lookup("sandbox-fs-demo", create_if_missing=True)

sb = modal.Sandbox.create(app=app)

with sb.open("test.txt", "w") as f:
    f.write("Hello World\n")

f = sb.open("test.txt", "rb")
print(f.read())
f.close()
```

The filesystem API is similar to Python’s built-in
[io.FileIO](https://docs.python.org/3/library/io.html#io.FileIO)

and supports many of the same methods, including
`read`
,
`readline`
,
`readlines`
,
`write`
,
`flush`
,
`seek`
, and
`close`
.

We also provide the special methods
`replace_bytes`
and
`delete_bytes`
, which may be useful for LLM-generated code.

```
from modal.file_io import delete_bytes, replace_bytes

with sb.open("example.txt", "w") as f:
    f.write("The quick brown fox jumps over the lazy dog")

with sb.open("example.txt", "r+") as f:
    # The quick brown fox jumps over the lazy dog
    print(f.read())

    # The slow brown fox jumps over the lazy dog
    replace_bytes(f, b"slow", start=4, end=9)

    # The slow red fox jumps over the lazy dog
    replace_bytes(f, b"red", start=9, end=14)

    # The slow red fox jumps over the dog
    delete_bytes(f, start=32, end=37)

    f.seek(0)
    print(f.read())

sb.terminate()
```

We additionally provide commands
[`mkdir`](/docs/reference/modal.Sandbox#mkdir)

,
[`rm`](/docs/reference/modal.Sandbox#rm)

, and
[`ls`](/docs/reference/modal.Sandbox#ls)

to make interacting with the filesystem more ergonomic.

[Filesystem Access](#filesystem-access)

[Efficient file syncing](#efficient-file-syncing)

[Filesystem API (Alpha)](#filesystem-api-alpha)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/sandbox-memory-snapshots
================================================================================

Sandbox memory snapshots
========================

> 🌱 Sandbox memory snapshots are in
> **early preview, pre-beta.**

Sandbox memory snapshots are copies of a Sandbox’s entire state, both in memory and on the filesystem. These Snapshots can be restored later to create a new Sandbox, which is an exact clone of the original Sandbox.

To snapshot a Sandbox, create it with
`_experimental_enable_snapshot`
set to
`True`
, and use the
`_experimental_snapshot`
method, which returns a
`SandboxSnapshot`
object:

```
image = modal.Image.debian_slim().apt_install("curl", "procps")
app = modal.App.lookup("sandbox-snapshot", create_if_missing=True)

with modal.enable_output():
    sb = modal.Sandbox.create(
        "python3", "-m", "http.server", "8000",
        app=app, image=image, _experimental_enable_snapshot=True
    )

print(f"Performing snapshot of {sb.object_id} ...")
snapshot = sb._experimental_snapshot()
```

Create a new Sandbox from the returned SandboxSnapshot with
`Sandbox._experimental_from_snapshot`
:

```
print(f"Restoring from snapshot {sb.object_id} ...")
sb2 = modal.Sandbox._experimental_from_snapshot(snapshot)

print("Let's see that the http.server is still running...")
p = sb2.exec("ps", "aux")
print(p.stdout.read())

# Talk to snapshotted sandbox http.server
p = sb2.exec("curl", "http://localhost:8000/")
reply = p.stdout.read()
print(reply)  # <!DOCTYPE HTML><html lang...
```

The new Sandbox will be a duplicate of your original Sandbox. All running processes will still be running, in the same state as when they were snapshotted, and any changes made to the filesystem will be visible.

You can retrieve the ID of any Sandbox Snapshot with
`snapshot.object_id`
. To restore from a snapshot by ID, first rehydrate the Snapshot with
`SandboxSnapshot.from_id`
and then restore from it:

```
snapshot_id = snapshot.object_id
# ... save the Sandbox ID (sb-123abc) for later
# sometime in the future...
snapshot = modal.SandboxSnapshot.from_id(snapshot_id)
sandbox = modal.Sandbox._experimental_from_snapshot(snapshot)
```

Note that these methods are
*experimental*
, and we may change them in the future.

### Re-snapshotting

Modal supports creating a new snapshot from a restored Sandbox snapshot. To maintain the snapshot’s expiration window, the new snapshot inherits the expiration of its parent.

Continuing from the example code above, we demonstrate re-snapshotting:

```
# Add a file to the snapshotted sandbox
p = sb2.exec("touch", "/foo")
p.wait()

snapshot2 = sb2._experimental_snapshot()
print(f"Restoring from new snapshot {sb.object_id} ...")
sb3 = modal.Sandbox._experimental_from_snapshot(snapshot2)
# Talk to re-snapshotted sandbox http.server
p = sb3.exec("curl", "http://localhost:8000/")
reply = p.stdout.read()
print(reply)  # Shows the new 'foo' directory in the HTML listing.
```

### Limitations

Currently, Sandbox Snapshots will expire 7 days after creation.

Open TCP connections will be closed automatically when a Snapshot is taken, and will need to be reopened when the Snapshot is restored.

Snapshotting a sandbox will currently cause it to terminate. We intend to remove this limitation soon.

Sandboxes created with
`_experimental_enable_snapshot=True`
or restored from Snapshots cannot run with GPUs.

It is not possible to snapshot a sandbox while a
`Sandbox.exec`
command is still running. Furthermore, any background processes launched by a call to
`Sandbox.exec`
will not be properly restored after a snapshot.

[Sandbox memory snapshots](#sandbox-memory-snapshots)

[Re-snapshotting](#re-snapshotting)

[Limitations](#limitations)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/sandbox-networking
================================================================================

Networking and security
=======================

Sandboxes are built to be secure-by-default, meaning that a default Sandbox has
no ability to accept incoming network connections or access your Modal resources.

Networking
----------

Since Sandboxes may run untrusted code, they have options to restrict their network access.
To block all network access, set
`block_network=True`
on
[`Sandbox.create`](/docs/reference/modal.Sandbox#create)

.

For more fine-grained networking control, a Sandbox’s outbound network access
can be restricted using the
`cidr_allowlist`
parameter. This parameter takes a
list of CIDR ranges that the Sandbox is allowed to access, blocking all other
outbound traffic.

### Forwarding ports

Sandboxes can also expose TCP ports to the internet. This is useful if,
for example, you want to connect to a web server running inside a Sandbox.

Use the
`encrypted_ports`
and
`unencrypted_ports`
parameters of
`Sandbox.create`
to specify which ports to forward. You can then access the public URL of a tunnel
using the
[`Sandbox.tunnels`](/docs/reference/modal.Sandbox#tunnels)

method:

```
import requests
import time

sb = modal.Sandbox.create(
    "python",
    "-m",
    "http.server",
    "12345",
    encrypted_ports=[12345],
    app=my_app,
)

tunnel = sb.tunnels()[12345]

time.sleep(1)  # Wait for server to start.

print(f"Connecting to {tunnel.url}...")
print(requests.get(tunnel.url, timeout=5).text)
```

It is also possible to create an encrypted port that uses
`HTTP/2`
rather than
`HTTP/1.1`
with the
`h2_ports`
option. This will return
a URL that you can make H2 (HTTP/2 + TLS) requests to. If you want to run an
`HTTP/2`
server inside a sandbox, this feature may be useful.
Here is an example:

```
import time

port = 4359
sb = modal.Sandbox.create(
    app=my_app,
    image=my_image,
    h2_ports = [port],
)
p = sb.exec("python", "my_http2_server.py")

tunnel = sb.tunnels()[port]
time.sleep(1)
print(f"Tunnel URL: {tunnel.url}")
```

For more details on how tunnels work, see the
[tunnels guide](/docs/guide/tunnels)

.

Security model
--------------

In a typical Modal Function, the Function code can call other Modal APIs allowing
it to spawn containers, create and destroy Volumes, read from Dicts and Queues, etc.
Sandboxes, by contrast, are isolated from the main Modal workspace. They have no API
access, meaning the blast radius of any malicious code is limited to the Sandbox
environment.

Sandboxes are built on top of
[gVisor](https://gvisor.dev/)

, a container runtime
by Google that provides strong isolation properties. gVisor has custom logic to
prevent Sandboxes from making malicious system calls, giving you stronger isolation
than standard
[runc](https://github.com/opencontainers/runc)

containers.

[Networking and security](#networking-and-security)

[Networking](#networking)

[Forwarding ports](#forwarding-ports)

[Security model](#security-model)

See it in action

[Running a Jupyter notebook](/docs/examples/jupyter_sandbox)

[Safe code execution](/docs/examples/safe_code_execution)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/sandbox-snapshots
================================================================================

Snapshots
=========

Sandboxes support snapshotting, allowing you to save your Sandbox’s state
and restore it later. This is useful for:

* Creating custom environments for your Sandboxes to run in
* Backing up your Sandbox’s state for debugging
* Running large-scale experiments with the same initial state
* Branching your Sandbox’s state to test different code changes independently

Filesystem Snapshots
--------------------

Filesystem Snapshots are copies of the Sandbox’s filesystem at a given point in time.
These Snapshots are
[Images](/docs/reference/modal.Image)

and can be used to create
new Sandboxes.

To create a Filesystem Snapshot, you can use the
[`Sandbox.snapshot_filesystem()`](/docs/reference/modal.Sandbox#snapshot_filesystem)

method:

```
import modal

app = modal.App.lookup("sandbox-fs-snapshot-test", create_if_missing=True)

sb = modal.Sandbox.create(app=app)
p = sb.exec("bash", "-c", "echo 'test' > /test")
p.wait()
assert p.returncode == 0, "failed to write to file"
image = sb.snapshot_filesystem()
sb.terminate()

sb2 = modal.Sandbox.create(image=image, app=app)
p2 = sb2.exec("bash", "-c", "cat /test")
assert p2.stdout.read().strip() == "test"
sb2.terminate()
```

Filesystem Snapshots are optimized for performance: they are calculated as the difference
from your base image, so only modified files are stored. Restoring a Filesystem Snapshot
utilizes the same infrastructure we use to get fast cold starts for your Sandboxes.

Memory Snapshots
----------------

[Sandboxes memory snapshots](/docs/guide/sandbox-memory-snapshots)

are in early preview.
Contact us if this is something you’re interested in!

[Snapshots](#snapshots)

[Filesystem Snapshots](#filesystem-snapshots)

[Memory Snapshots](#memory-snapshots)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/sandbox-spawn
================================================================================

Running commands in Sandboxes
=============================

Once you have created a Sandbox, you can run commands inside it using the
[`Sandbox.exec`](/docs/reference/modal.Sandbox#exec)

method.

```
sb = modal.Sandbox.create(app=my_app)

process = sb.exec("echo", "hello", timeout=3)
print(process.stdout.read())

process = sb.exec("python", "-c", "print(1 + 1)", timeout=3)
print(process.stdout.read())

process = sb.exec(
    "bash",
    "-c",
    "for i in $(seq 1 10); do echo foo $i; sleep 0.1; done",
    timeout=5,
)
for line in process.stdout:
    print(line, end="")

sb.terminate()
```

`Sandbox.exec`
returns a
[`ContainerProcess`](/docs/reference/modal.container_process#modalcontainer_processcontainerprocess)

object, which allows access to the process’s
`stdout`
,
`stderr`
, and
`stdin`
.
The
`timeout`
parameter ensures that the
`exec`
command will run for at most
`timeout`
seconds.

Input
-----

The Sandbox and ContainerProcess
`stdin`
handles are
[`StreamWriter`](/docs/reference/modal.io_streams#modalio_streamsstreamwriter)

objects. This object supports flushing writes with both synchronous and asynchronous APIs:

```
import asyncio

sb = modal.Sandbox.create(app=my_app)

p = sb.exec("bash", "-c", "while read line; do echo $line; done")
p.stdin.write(b"foo bar\n")
p.stdin.write_eof()
p.stdin.drain()
p.wait()
sb.terminate()

async def run_async():
    sb = await modal.Sandbox.create.aio(app=my_app)
    p = await sb.exec.aio("bash", "-c", "while read line; do echo $line; done")
    p.stdin.write(b"foo bar\n")
    p.stdin.write_eof()
    await p.stdin.drain.aio()
    await p.wait.aio()
    await sb.terminate.aio()

asyncio.run(run_async())
```

Output
------

The Sandbox and ContainerProcess
`stdout`
and
`stderr`
handles are
[`StreamReader`](/docs/reference/modal.io_streams#modalio_streamsstreamreader)

objects. These objects support reading from the stream in both synchronous and asynchronous manners.
These handles also respect the timeout given to
`Sandbox.exec`
.

To read from a stream after the underlying process has finished, you can use the
`read`
method, which blocks until the process finishes and returns the entire output stream.

```
sb = modal.Sandbox.create(app=my_app)
p = sb.exec("echo", "hello")
print(p.stdout.read())
sb.terminate()
```

To stream output, take advantage of the fact that
`stdout`
and
`stderr`
are
iterable:

```
import asyncio

sb = modal.Sandbox.create(app=my_app)

p = sb.exec("bash", "-c", "for i in $(seq 1 10); do echo foo $i; sleep 0.1; done")

for line in p.stdout:
    # Lines preserve the trailing newline character, so use end="" to avoid double newlines.
    print(line, end="")
p.wait()
sb.terminate()

async def run_async():
    sb = await modal.Sandbox.create.aio(app=my_app)
    p = await sb.exec.aio("bash", "-c", "for i in $(seq 1 10); do echo foo $i; sleep 0.1; done")
    async for line in p.stdout:
        # Avoid double newlines by using end="".
        print(line, end="")
    await p.wait.aio()
    await sb.terminate.aio()

asyncio.run(run_async())
```

### Stream types

By default, all streams are buffered in memory, waiting to be consumed by the
client. You can control this behavior with the
`stdout`
and
`stderr`
parameters.
These parameters are conceptually similar to the
`stdout`
and
`stderr`
parameters of the
[`subprocess`](https://docs.python.org/3/library/subprocess.html#subprocess.DEVNULL)

module.

```
from modal.stream_type import StreamType

sb = modal.Sandbox.create(app=my_app)

# Default behavior: buffered in memory.
p = sb.exec(
    "bash",
    "-c",
    "echo foo; echo bar >&2",
    stdout=StreamType.PIPE,
    stderr=StreamType.PIPE,
)
print(p.stdout.read())
print(p.stderr.read())

# Print the stream to STDOUT as it comes in.
p = sb.exec(
    "bash",
    "-c",
    "echo foo; echo bar >&2",
    stdout=StreamType.STDOUT,
    stderr=StreamType.STDOUT,
)
p.wait()

# Discard all output.
p = sb.exec(
    "bash",
    "-c",
    "echo foo; echo bar >&2",
    stdout=StreamType.DEVNULL,
    stderr=StreamType.DEVNULL,
)
p.wait()

sb.terminate()
```

[Running commands in Sandboxes](#running-commands-in-sandboxes)

[Input](#input)

[Output](#output)

[Stream types](#stream-types)

See it in action

[Building a coding agent with Sandboxes](/docs/examples/agent)

[Building a code interpreter](/docs/examples/simple_code_interpreter)

[Safe code execution](/docs/examples/safe_code_execution)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/scale
================================================================================

Scaling out
===========

Modal makes it trivially easy to scale compute across thousands of containers.
You won’t have to worry about your App crashing if it goes viral or need to wait
a long time for your batch jobs to complete.

For the the most part, scaling out will happen automatically, and you won’t need
to think about it. But it can be helpful to understand how Modal’s autoscaler
works and how you can control its behavior when you need finer control.

How does autoscaling work on Modal?
-----------------------------------

Every Modal Function corresponds to an autoscaling pool of containers. The size
of the pool is managed by Modal’s autoscaler. The autoscaler will spin up new
containers when there is no capacity available for new inputs, and it will spin
down containers when resources are idling. By default, Modal Functions will
scale to zero when there are no inputs to process.

Autoscaling decisions are made quickly and frequently so that your batch jobs
can ramp up fast and your deployed Apps can respond to any sudden changes in
traffic.

Configuring autoscaling behavior
--------------------------------

Modal exposes a few settings that allow you to configure the autoscaler’s
behavior. These settings can be passed to the
`@app.function`
or
`@app.cls`
decorators:

* `max_containers`
  : The upper limit on containers for the specific Function.
* `min_containers`
  : The minimum number of containers that should be kept warm,
  even when the Function is inactive.
* `buffer_containers`
  : The size of the buffer to maintain while the Function is
  active, so that additional inputs will not need to queue for a new container.
* `scaledown_window`
  : The maximum duration (in seconds) that individual
  containers can remain idle when scaling down.

In general, these settings allow you to trade off cost and latency. Maintaining
a larger warm pool or idle buffer will increase costs but reduce the chance that
inputs will need to wait for a new container to start.

Similarly, a longer scaledown window will let containers idle for longer, which
might help avoid unnecessary churn for Apps that receive regular but infrequent
inputs. Note that containers may not wait for the entire scaledown window before
shutting down if the App is substantially overprovisioned.

Dynamic autoscaler updates
--------------------------

It’s also possible to update the autoscaler settings dynamically (i.e., without redeploying
the App) using the
[`Function.update_autoscaler()`](/docs/reference/modal.Function#update_autoscaler)

method:

```
f = modal.Function.from_name("my-app", "f")
f.update_autoscaler(max_containers=100)
```

The autoscaler settings will revert to the configuration in the function
decorator the next time you deploy the App. Or they can be overridden by
further dynamic updates:

```
f.update_autoscaler(min_containers=2, max_containers=10)
f.update_autoscaler(min_containers=4)  # max_containers=10 will still be in effect
```

A common pattern is to run this method in a
[scheduled function](/docs/guide/cron)

that adjusts the size of the warm pool (or container buffer) based on the time of day:

```
@app.function()
def inference_server():
    ...

@app.function(schedule=modal.Cron("0 6 * * *", timezone="America/New_York"))
def increase_warm_pool():
    inference_server.update_autoscaler(min_containers=4)

@app.function(schedule=modal.Cron("0 22 * * *", timezone="America/New_York"))
def decrease_warm_pool():
    inference_server.update_autoscaler(min_containers=0)
```

When you have a
[`modal.Cls`](/docs/reference/modal.Cls)

,
`update_autoscaler`
is a method on an
*instance*
and will control the autoscaling behavior of
containers serving the Function with that specific set of parameters:

```
MyClass = modal.Cls.from_name("my-app", "MyClass")
obj = MyClass(model_version="3.5")
obj.update_autoscaler(buffer_containers=2)  # type: ignore
```

Note that it’s necessary to disable type checking on this line, because the
object will appear as an instance of the class that you defined rather than the
Modal wrapper type.

Parallel execution of inputs
----------------------------

If your code is running the same function repeatedly with different independent
inputs (e.g., a grid search), the easiest way to increase performance is to run
those function calls in parallel using Modal’s
[`Function.map()`](/docs/reference/modal.Function#map)

method.

Here is an example if we had a function
`evaluate_model`
that takes a single
argument:

```
import modal

app = modal.App()

@app.function()
def evaluate_model(x):
    ...

@app.local_entrypoint()
def main():
    inputs = list(range(100))
    for result in evaluate_model.map(inputs):  # runs many inputs in parallel
        ...
```

In this example,
`evaluate_model`
will be called with each of the 100 inputs
(the numbers 0 - 99 in this case) roughly in parallel and the results are
returned as an iterable with the results ordered in the same way as the inputs.

### Exceptions

By default, if any of the function calls raises an exception, the exception will
be propagated. To treat exceptions as successful results and aggregate them in
the results list, pass in
[`return_exceptions=True`](/docs/reference/modal.Function#map)

.

```
@app.function()
def my_func(a):
    if a == 2:
        raise Exception("ohno")
    return a ** 2

@app.local_entrypoint()
def main():
    print(list(my_func.map(range(3), return_exceptions=True, wrap_returned_exceptions=False)))
    # [0, 1, Exception('ohno'))]
```

Note: prior to version 1.0.5, the returned exceptions inadvertently leaked an internal
wrapper type (
`modal.exceptions.UserCodeException`
). To avoid breaking any user code that
was checking exception types, we’re taking a gradual approach to fixing this bug. Adding
`wrap_returned_exceptions=True`
will opt-in to the future default behavior and return the
underlying exception type without a wrapper.

### Starmap

If your function takes multiple variable arguments, you can either use
[`Function.map()`](/docs/reference/modal.Function#map)

with one input iterator
per argument, or
[`Function.starmap()`](/docs/reference/modal.Function#starmap)

with a single input iterator containing sequences (like tuples) that can be
spread over the arguments. This works similarly to Python’s built in
`map`
and
`itertools.starmap`
.

```
@app.function()
def my_func(a, b):
    return a + b

@app.local_entrypoint()
def main():
    assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]
```

### Gotchas

Note that
`.map()`
is a method on the modal function object itself, so you don’t
explicitly
*call*
the function.

Incorrect usage:

```
results = evaluate_model(inputs).map()
```

Modal’s map is also not the same as using Python’s builtin
`map()`
. While the
following will technically work, it will execute all inputs in sequence rather
than in parallel.

Incorrect usage:

```
results = map(evaluate_model, inputs)
```

Asynchronous usage
------------------

All Modal APIs are available in both blocking and asynchronous variants. If you
are comfortable with asynchronous programming, you can use it to create
arbitrary parallel execution patterns, with the added benefit that any Modal
functions will be executed remotely. See the
[async guide](/docs/guide/async)

or
the examples for more information about asynchronous usage.

GPU acceleration
----------------

Sometimes you can speed up your applications by utilizing GPU acceleration. See
the
[gpu section](/docs/guide/gpu)

for more information.

Scaling Limits
--------------

Modal enforces the following limits for every function:

* 2,000 pending inputs (inputs that haven’t been assigned to a container yet)
* 25,000 total inputs (which include both running and pending inputs)

For inputs created with
`.spawn()`
for async jobs, Modal allows up to 1 million pending inputs instead of 2,000.

If you try to create more inputs and exceed these limits, you’ll receive a
`Resource Exhausted`
error, and you should retry your request later. If you need higher limits, please reach out!

Additionally, each
`.map()`
invocation can process at most 1000 inputs concurrently.

[Scaling out](#scaling-out)

[How does autoscaling work on Modal?](#how-does-autoscaling-work-on-modal)

[Configuring autoscaling behavior](#configuring-autoscaling-behavior)

[Dynamic autoscaler updates](#dynamic-autoscaler-updates)

[Parallel execution of inputs](#parallel-execution-of-inputs)

[Exceptions](#exceptions)

[Starmap](#starmap)

[Gotchas](#gotchas)

[Asynchronous usage](#asynchronous-usage)

[GPU acceleration](#gpu-acceleration)

[Scaling Limits](#scaling-limits)

See it in action

[Auto-scaling LLM inference endpoints](/docs/examples/vllm_inference)

[Job queue for OCR](/docs/examples/doc_ocr_jobs)

[Parallel web scraping](/docs/examples/web-scraper#scaling-out)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/secrets
================================================================================

Secrets
=======

Securely provide credentials and other sensitive information to your Modal Functions with Secrets.

You can create and edit Secrets via
the
[dashboard](/secrets)

,
the command line interface (
[`modal secret`](/docs/reference/cli/secret)

), and
programmatically from Python code (
[`modal.Secret`](/docs/reference/modal.Secret)

).

To inject Secrets into the container running your Function, add the
`secrets=[...]`
argument to your
`app.function`
or
`app.cls`
decoration.

Deploy Secrets from the Modal Dashboard
---------------------------------------

The most common way to create a Modal Secret is to use the
[Secrets panel of the Modal dashboard](/secrets)

,
which also shows any existing Secrets.

When you create a new Secret, you’ll be prompted with a number of templates to help you get started.
These templates demonstrate standard formats for credentials for everything from Postgres and MongoDB
to Weights & Biases and Hugging Face.

Use Secrets in your Modal Apps
------------------------------

You can then use your Secret by constructing it
`from_name`
when defining a Modal App
and then accessing its contents as environment variables.
For example, if you have a Secret called
`secret-keys`
containing the key
`MY_PASSWORD`
:

```
@app.function(secrets=[modal.Secret.from_name("secret-keys")])
def some_function():
    import os

    secret_key = os.environ["MY_PASSWORD"]
    ...
```

Each Secret can contain multiple keys and values but you can also inject
multiple Secrets, allowing you to separate Secrets into smaller reusable units:

```
@app.function(secrets=[
    modal.Secret.from_name("my-secret-name"),
    modal.Secret.from_name("other-secret"),
])
def other_function():
    ...
```

The Secrets are applied in order, so key-values from later
`modal.Secret`
objects in the list will overwrite earlier key-values in the case of a clash.
For example, if both
`modal.Secret`
objects above contained the key
`FOO`
, then
the value from
`"other-secret"`
would always be present in
`os.environ["FOO"]`
.

Create Secrets programmatically
-------------------------------

In addition to defining Secrets on the web dashboard, you can
programmatically create a Secret directly in your script and send it along to
your Function using
`Secret.from_dict(...)`
. This can be useful if you want to
send Secrets from your local development machine to the remote Modal App.

```
import os

if modal.is_local():
    local_secret = modal.Secret.from_dict({"FOO": os.environ["LOCAL_FOO"]})
else:
    local_secret = modal.Secret.from_dict({})

@app.function(secrets=[local_secret])
def some_function():
    import os

    print(os.environ["FOO"])
```

If you have
[`python-dotenv`](https://pypi.org/project/python-dotenv/)

installed,
you can also use
`Secret.from_dotenv()`
to create a Secret from the variables in a
`.env`
file

```
@app.function(secrets=[modal.Secret.from_dotenv()])
def some_other_function():
    print(os.environ["USERNAME"])
```

Interact with Secrets from the command line
-------------------------------------------

You can create, list, and delete your Modal Secrets with the
`modal secret`
command line interface.

View your Secrets and their timestamps with

```
modal secret list
```

Create a new Secret by passing
`{KEY}={VALUE}`
pairs to
`modal secret create`
:

```
modal secret create database-secret PGHOST=uri PGPORT=5432 PGUSER=admin PGPASSWORD=hunter2
```

or using environment variables (assuming below that the
`PGPASSWORD`
environment variable is set
e.g. by your CI system):

```
modal secret create database-secret PGHOST=uri PGPORT=5432 PGUSER=admin PGPASSWORD="$PGPASSWORD"
```

Remove Secrets by passing their name to
`modal secret delete`
:

```
modal secret delete database-secret
```

[Secrets](#secrets)

[Deploy Secrets from the Modal Dashboard](#deploy-secrets-from-the-modal-dashboard)

[Use Secrets in your Modal Apps](#use-secrets-in-your-modal-apps)

[Create Secrets programmatically](#create-secrets-programmatically)

[Interact with Secrets from the command line](#interact-with-secrets-from-the-command-line)

See it in action

[OpenAI Secret for LangChain RAG](/docs/examples/potus_speech_qanda)

[Write to Google Sheets](/docs/examples/db_to_sheet)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/security
================================================================================

Security and privacy at Modal
=============================

The document outlines Modal’s security and privacy commitments.

Application security (AppSec)
-----------------------------

AppSec is the practice of building software that is secure by design, secured
during development, secured with testing and review, and deployed securely.

* We build our software using memory-safe programming languages, including Rust
  (for our worker runtime and storage infrastructure) and Python (for our API
  servers and Modal client).
* Software dependencies are audited by Github’s Dependabot.
* We make decisions that minimize our attack surface. Most interactions with
  Modal are well-described in a gRPC API, and occur through
  [`modal`](https://pypi.org/project/modal)

  , our open-source command-line tool
  and Python client library.
* We have automated synthetic monitoring test applications that continuously
  check for network and application isolation within our runtime.
* We use HTTPS for secure connections. Modal forces HTTPS for all services using
  TLS (SSL), including our public website and the Dashboard to ensure secure
  connections. Modal’s
  [client library](https://pypi.org/project/modal)

  connects
  to Modal’s servers over TLS and verify TLS certificates on each connection.
* All user data is encrypted in transit and at rest.
* All public Modal APIs use
  [TLS 1.3](https://datatracker.ietf.org/doc/html/rfc8446)

  , the latest and
  safest version of the TLS protocol.
* Internal code reviews are performed using a modern, PR-based development
  workflow (Github), and engage external penetration testing firms to assess our
  software security.

Corporate security (CorpSec)
----------------------------

CorpSec is the practice of making sure Modal employees have secure access to
Modal company infrastructure, and also that exposed channels to Modal are
secured. CorpSec controls are the primary concern of standards such as SOC2.

* Access to our services and applications is gated on a SSO Identity Provider
  (IdP).
* We mandate phishing-resistant multi-factor authentication (MFA) in all
  enrolled IdP accounts.
* We regularly audit access to internal systems.
* Employee laptops are protected by full disk encryption using FileVault2, and
  managed by Secureframe MDM.

Network and infrastructure security (InfraSec)
----------------------------------------------

InfraSec is the practice of ensuring a hardened, minimal attack surface for
components we deploy on our network.

* Modal uses logging and metrics observability providers, including Datadog and
  Sentry.io.
* Compute jobs at Modal are containerized and virtualized using
  [gVisor](https://github.com/google/gvisor)

  , the sandboxing technology
  developed at Google and used in their
  *Google Cloud Run*
  and
  *Google
  Kubernetes Engine*
  cloud services.
* We conduct annual business continuity and security incident exercises.

Vulnerability remediation
-------------------------

Security vulnerabilities directly affecting Modal’s systems and services will be
patched or otherwise remediated within a timeframe appropriate for the severity
of the vulnerability, subject to the public availability of a patch or other
remediation mechanisms.

If there is a CVSS severity rating accompanying a vulnerability disclosure, we
rely on that as a starting point, but may upgrade or downgrade the severity
using our best judgement.

### Severity timeframes

* **Critical:**
  24 hours
* **High:**
  1 week
* **Medium:**
  1 month
* **Low:**
  3 months
* **Informational:**
  3 months or longer

Shared responsibility model
---------------------------

Modal prioritizes the integrity, security, and availability of customer data. Under our shared responsibility model, customers also have certain responsibilities regarding data backup, recovery, and availability.

1. **Data backup**
   : Customers are responsible for maintaining backups of their data. Performing daily backups is recommended. Customers must routinely verify the integrity of their backups.
2. **Data recovery**
   : Customers should maintain a comprehensive data recovery plan that includes detailed procedures for data restoration in the event of data loss, corruption, or system failure. Customers must routinely test their recovery process.
3. **Availability**
   : While Modal is committed to high service availability, customers must implement contingency measures to maintain business continuity during service interruptions. Customers are also responsible for the reliability of their own IT infrastructure.
4. **Security measures**
   : Customers must implement appropriate security measures, such as encryption and access controls, to protect their data throughout the backup, storage, and recovery processes. These processes must comply with all relevant laws and regulations.

SOC 2
-----

We have successfully completed a
[System and Organization Controls (SOC) 2 Type 2
audit](/blog/soc2type2)

. Go to our
[Security Portal](https://trust.modal.com)

to request access to the report.

HIPAA
-----

HIPAA, which stands for the Health Insurance Portability and Accountability Act, establishes a set of standards that protect health information, including individuals’ medical records and other individually identifiable health information. HIPAA guidelines apply to both covered entities and business associates—of which Modal is the latter if you are processing PHI on Modal.

Modal’s services can be used in a HIPAA compliant manner. It is important to note that unlike other security standards, there is no officially recognized certification process for HIPAA compliance. Instead, we demonstrate our compliance with regulations such as HIPAA via the practices outlined in this doc, our technical and operational security measures, and through official audits for standards compliance such as SOC 2 certification.

To use Modal services for HIPAA-compliant workloads, a Business Associate Agreement (BAA) should be established with us prior to submission of any PHI. This is available on our Enterprise plan. Contact us at
[security@modal.com](mailto:security@modal.com)

to get started. At the moment,
[Volumes](https://modal.com/docs/guide/volumes)

,
[Images](https://modal.com/docs/guide/images)

(persistent storage),
[memory snapshots](https://modal.com/docs/guide/memory-snapshot)

, and user code are out of scope of the commitments within our BAA, so PHI should not be used in those areas of the product.

PCI
---

*Payment Card Industry Data Security Standard*
(PCI) is a standard that defines
the security and privacy requirements for payment card processing.

Modal uses
[Stripe](https://stripe.com)

to securely process transactions and
trusts their commitment to best-in-class security. We do not store personal
credit card information for any of our customers. Stripe is certified as “PCI
Service Provider Level 1”, which is the highest level of certification in the
payments industry.

Bug bounty program
------------------

Keeping user data secure is a top priority at Modal. We welcome contributions
from the security community to identify vulnerabilities in our product and
disclose them to us in a responsible manner. We offer rewards ranging from $100
to $1000+ depending on the severity of the issue discovered. To participate,
please send a report of the vulnerability to
[security@modal.com](mailto:security@modal.com)

.

Data privacy
------------

Modal will never access or use:

* your source code.
* the inputs (function arguments) or outputs (function return values) to your Modal Functions.
* any data you store in Modal, such as in Images or Volumes.

Inputs (function arguments) and outputs (function return values) are deleted from our system after a max TTL of 7 days.

App logs and metadata are stored on Modal. Modal will not access this data
unless permission is granted by the user to help with troubleshooting.

Questions?
----------

[Email us!](mailto:security@modal.com)

[Security and privacy at Modal](#security-and-privacy-at-modal)

[Application security (AppSec)](#application-security-appsec)

[Corporate security (CorpSec)](#corporate-security-corpsec)

[Network and infrastructure security (InfraSec)](#network-and-infrastructure-security-infrasec)

[Vulnerability remediation](#vulnerability-remediation)

[Severity timeframes](#severity-timeframes)

[Shared responsibility model](#shared-responsibility-model)

[SOC 2](#soc-2)

[HIPAA](#hipaa)

[PCI](#pci)

[Bug bounty program](#bug-bounty-program)

[Data privacy](#data-privacy)

[Questions?](#questions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/service-users
================================================================================

Service Users (beta)
====================

Service users are programmatic accounts that allow automated systems to interact with Modal. They’re ideal for CI/CD pipelines, automated deployments, and other workflows that need to authenticate.

Create a Service User
---------------------

Service users are only available for shared workspaces. You will need workspace owner or manager privileges to create service users.

To create a service user:

1. Go to your workspace
   [tokens settings page](/settings/tokens/service-users)
2. Click
   **New Service User**
3. Enter a name for your service user (must be lowercase alphanumeric, can contain hyphens or underscores)
4. Click
   **Create**

After creation, you’ll see the
`MODAL_TOKEN_ID`
and
`MODAL_TOKEN_SECRET`
.
**This is the only time you can view the token secret**
for security reasons.

Use Service User Tokens
-----------------------

Set the service user credentials as environment variables in your automated environment:

```
export MODAL_TOKEN_ID=your-token-id
export MODAL_TOKEN_SECRET=your-token-secret
```

Once configured, you can use Modal’s CLI and Python SDK as usual:

```
modal deploy your_app.py
```

Delete a Service User
---------------------

To remove a service user:

1. Go to the
   [tokens settings page](/settings/tokens/service-users)
2. Find the service user in the table
3. Click
   **Delete**
   when you hover over the row

Permissions
-----------

Service users have the same permissions as workspace members. They cannot do actions that are only permitted for a workspace owner or manager. To learn more about members, managers, and owners, see this
[workspace](/docs/guide/workspaces#administrating-workspace-members)

section.

[Service Users (beta)](#service-users-beta)

[Create a Service User](#create-a-service-user)

[Use Service User Tokens](#use-service-user-tokens)

[Delete a Service User](#delete-a-service-user)

[Permissions](#permissions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/slack-notifications
================================================================================

Slack notifications (beta)
==========================

You can integrate your Modal Workspace with Slack to receive timely essential notifications.

Prerequisites
-------------

* You are a
  [Workspace Manager](/docs/guide/workspaces#administrating-workspace-members)

  in the Modal Workspace you’re installing the Slack integration in.
* You have permissions to install apps in your Slack workspace.

Supported notifications
-----------------------

* Alerts for failed scheduled function runs.
* Alerts for crash-looping containers in a function.
* Alerts when any of your apps have client versions that are out of date.
* Alerts when you hit your GPU resource limits.

Configuration
-------------

### Step 1: Install the Slack integration

Visit the
*Slack Integration*
section on your
[settings](/settings)

page in your Modal Workspace and click the
**Add to Slack**
button.

### Step 2: Add the Modal app to your Slack channel

Navigate to the Slack channel you want to add the Modal to and click on the channel header. On the integrations tab you can add the Modal app.

![Add Modal app to Slack channel](/_app/immutable/assets/slack-add-modal-app.Cy4hnVNV.jpg)

### Step 3: Use `/modal link` to link the Slack channel to your Modal Workspace

You’ll be prompted to select the Workspace you want to link to the Slack channel. You can always unlink the Slack channel by visiting the
*Slack Integration*
section on your
[settings](/settings)

page in your Modal Workspace.

[Slack notifications (beta)](#slack-notifications-beta)

[Prerequisites](#prerequisites)

[Supported notifications](#supported-notifications)

[Configuration](#configuration)

[Step 1: Install the Slack integration](#step-1-install-the-slack-integration)

[Step 2: Add the Modal app to your Slack channel](#step-2-add-the-modal-app-to-your-slack-channel)

[Step 3: Use /modal link to link the Slack channel to your Modal Workspace](#step-3-use-modal-link-to-link-the-slack-channel-to-your-modal-workspace)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/streaming-endpoints
================================================================================

Streaming endpoints
===================

Modal web endpoints support streaming responses using FastAPI’s
[`StreamingResponse`](https://fastapi.tiangolo.com/advanced/custom-response/#streamingresponse)

class. This class accepts asynchronous generators, synchronous generators, or
any Python object that implements the
[*iterator protocol*](https://docs.python.org/3/library/stdtypes.html#typeiter)

,
and can be used with Modal Functions!

Simple example
--------------

This simple example combines Modal’s
`@modal.fastapi_endpoint`
decorator with a
`StreamingResponse`
object to produce a real-time SSE response.

```
import time

def fake_event_streamer():
    for i in range(10):
        yield f"data: some data {i}\n\n".encode()
        time.sleep(0.5)

@app.function(image=modal.Image.debian_slim().pip_install("fastapi[standard]"))
@modal.fastapi_endpoint()
def stream_me():
    from fastapi.responses import StreamingResponse
    return StreamingResponse(
        fake_event_streamer(), media_type="text/event-stream"
    )
```

If you serve this web endpoint and hit it with
`curl`
, you will see the ten SSE
events progressively appear in your terminal over a ~5 second period.

```
curl --no-buffer https://modal-labs--example-streaming-stream-me.modal.run
```

The MIME type of
`text/event-stream`
is important in this example, as it tells
the downstream web server to return responses immediately, rather than buffering
them in byte chunks (which is more efficient for compression).

You can still return other content types like large files in streams, but they
are not guaranteed to arrive as real-time events.

Streaming responses with
`.remote`
----------------------------------

A Modal Function wrapping a generator function body can have its response passed
directly into a
`StreamingResponse`
. This is particularly useful if you want to
do some GPU processing in one Modal Function that is called by a CPU-based web
endpoint Modal Function.

```
@app.function(gpu="any")
def fake_video_render():
    for i in range(10):
        yield f"data: finished processing some data from GPU {i}\n\n".encode()
        time.sleep(1)

@app.function(image=modal.Image.debian_slim().pip_install("fastapi[standard]"))
@modal.fastapi_endpoint()
def hook():
    from fastapi.responses import StreamingResponse
    return StreamingResponse(
        fake_video_render.remote_gen(), media_type="text/event-stream"
    )
```

Streaming responses with
`.map`
and
`.starmap`
----------------------------------------------

You can also combine Modal Function parallelization with streaming responses,
enabling applications to service a request by farming out to dozens of
containers and iteratively returning result chunks to the client.

```
@app.function()
def map_me(i):
    return f"segment {i}\n"

@app.function(image=modal.Image.debian_slim().pip_install("fastapi[standard]"))
@modal.fastapi_endpoint()
def mapped():
    from fastapi.responses import StreamingResponse
    return StreamingResponse(map_me.map(range(10)), media_type="text/plain")
```

This snippet will spread the ten
`map_me(i)`
executions across containers, and
return each string response part as it completes. By default the results will be
ordered, but if this isn’t necessary pass
`order_outputs=False`
as keyword
argument to the
`.map`
call.

### Asynchronous streaming

The example above uses a synchronous generator, which automatically runs on its
own thread, but in asynchronous applications, a loop over a
`.map`
or
`.starmap`
call can block the event loop. This will stop the
`StreamingResponse`
from
returning response parts iteratively to the client.

To avoid this, you can use the
`.aio()`
method to convert a synchronous
`.map`
into its async version. Also, other blocking calls should be offloaded to a
separate thread with
`asyncio.to_thread()`
. For example:

```
@app.function(gpu="any", image=modal.Image.debian_slim().pip_install("fastapi[standard]"))
@modal.fastapi_endpoint()
async def transcribe_video(request):
    from fastapi.responses import StreamingResponse

    segments = await asyncio.to_thread(split_video, request)
    return StreamingResponse(wrapper(segments), media_type="text/event-stream")

# Notice that this is an async generator.
async def wrapper(segments):
    async for partial_result in transcribe_video.map.aio(segments):
        yield "data: " + partial_result + "\n\n"
```

Further examples
----------------

* Complete code the for the simple examples given above is available
  [in our modal-examples Github repository](https://github.com/modal-labs/modal-examples/blob/main/07_web_endpoints/streaming.py)

  .
* [An end-to-end example of streaming Youtube video transcriptions with OpenAI’s whisper model.](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/openai_whisper/streaming/main.py)

[Streaming endpoints](#streaming-endpoints)

[Simple example](#simple-example)

[Streaming responses with .remote](#streaming-responses-with-remote)

[Streaming responses with .map and .starmap](#streaming-responses-with-map-and-starmap)

[Asynchronous streaming](#asynchronous-streaming)

[Further examples](#further-examples)

See it in action

[LLM Voice Chat](/docs/examples/llm-voice-chat)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/timeouts
================================================================================

Timeouts
========

All Modal
[Function](/docs/reference/modal.Function)

executions have a default
execution timeout of 300 seconds (5 minutes), but users may specify timeout
durations between 10 seconds and 24 hours.

```
import time

@app.function()
def f():
    time.sleep(599)  # Timeout!

@app.function(timeout=600)
def g():
    time.sleep(599)
    print("*Just* made it!")
```

The timeout duration is a measure of a Function’s
*execution*
time. It does not
include scheduling time or any other period besides the time your code is
executing in Modal. This duration is also per execution attempt, meaning
Functions configured with
[`modal.Retries`](/docs/reference/modal.Retries)

will
start new execution timeouts on each retry. For example, an infinite-looping
Function with a 100 second timeout and 3 allowed retries will run for least 400
seconds within Modal.

Handling timeouts
-----------------

After exhausting any specified retries, a timeout in a Function will produce a
`modal.exception.FunctionTimeoutError`
which you may catch in your code.

```
import modal.exception

@app.function(timeout=100)
def f():
    time.sleep(200)  # Timeout!

@app.local_entrypoint()
def main():
    try:
        f.remote()
    except modal.exception.FunctionTimeoutError:
        ... # Handle the timeout.
```

Timeout accuracy
----------------

Functions will run for
*at least*
as long as their timeout allows, but they may
run a handful of seconds longer. If you require accurate and precise timeout
durations on your Function executions, it is recommended that you implement
timeout logic in your user code.

[Timeouts](#timeouts)

[Handling timeouts](#handling-timeouts)

[Timeout accuracy](#timeout-accuracy)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/trigger-deployed-functions
================================================================================

Invoking deployed functions
===========================

Modal lets you take a function created by a
[deployment](/docs/guide/managing-deployments)

and call it from other contexts.

There are two ways of invoking deployed functions. If the invoking client is
running Python, then the same
[Modal client library](https://pypi.org/project/modal/)

used to write Modal code
can be used. HTTPS is used if the invoking client is not running Python and
therefore cannot import the Modal client library.

Invoking with Python
--------------------

Some use cases for Python invocation include:

* An existing Python web server (eg. Django, Flask) wants to invoke Modal
  functions.
* You have split your product or system into multiple Modal applications that
  deploy independently and call each other.

### Function lookup and invocation basics

Let’s say you have a script
`my_shared_app.py`
and this script defines a Modal
app with a function that computes the square of a number:

```
import modal

app = modal.App("my-shared-app")

@app.function()
def square(x: int):
    return x ** 2
```

You can deploy this app to create a persistent deployment:

```
% modal deploy shared_app.py
✓ Initialized.
✓ Created objects.
├── 🔨 Created square.
├── 🔨 Mounted /Users/erikbern/modal/shared_app.py.
✓ App deployed! 🎉

View Deployment: https://modal.com/apps/erikbern/my-shared-app
```

Let’s try to run this function from a different context. For instance, let’s
fire up the Python interactive interpreter:

```
% python
Python 3.9.5 (default, May  4 2021, 03:29:30)
[Clang 12.0.0 (clang-1200.0.32.27)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import modal
>>> f = modal.Function.from_name("my-shared-app", "square")
>>> f.remote(42)
1764
>>>
```

This works exactly the same as a regular modal
`Function`
object. For example,
you can
`.map()`
over functions invoked this way too:

```
>>> f = modal.Function.from_name("my-shared-app", "square")
>>> f.map([1, 2, 3, 4, 5])
[1, 4, 9, 16, 25]
```

#### Authentication

The Modal Python SDK will read the token from
`~/.modal.toml`
which typically is
created using
`modal token new`
.

Another method of providing the credentials is to set the environment variables
`MODAL_TOKEN_ID`
and
`MODAL_TOKEN_SECRET`
. If you want to call a Modal function
from a context such as a web server, you can expose these environment variables
to the process.

#### Lookup of lifecycle functions

[Lifecycle functions](/docs/guide/lifecycle-functions)

are defined on classes,
which you can look up in a different way. Consider this code:

```
import modal

app = modal.App("my-shared-app")

@app.cls()
class MyLifecycleClass:
    @modal.enter()
    def enter(self):
        self.var = "hello world"

    @modal.method()
    def foo(self):
        return self.var
```

Let’s say you deploy this app. You can then call the function by doing this:

```
>>> cls = modal.Cls.from_name("my-shared-app", "MyLifecycleClass")
>>> obj = cls()  # You can pass any constructor arguments here
>>> obj.foo.remote()
'hello world'
```

### Asynchronous invocation

In certain contexts, a Modal client will need to trigger Modal functions without
waiting on the result. This is done by spawning functions and receiving a
[`FunctionCall`](/docs/reference/modal.FunctionCall)

as a
handle to the triggered execution.

The following is an example of a Flask web server (running outside Modal) which
accepts model training jobs to be executed within Modal. Instead of the HTTP
POST request waiting on a training job to complete, which would be infeasible,
the relevant Modal function is spawned and the
[`FunctionCall`](/docs/reference/modal.FunctionCall)

object is stored for later polling of execution status.

```
from uuid import uuid4
from flask import Flask, jsonify, request

app = Flask(__name__)
pending_jobs = {}

...

@app.route("/jobs", methods = ["POST"])
def create_job():
    predict_fn = modal.Function.from_name("example", "train_model")
    job_id = str(uuid4())
    function_call = predict_fn.spawn(
        job_id=job_id,
        params=request.json,
    )
    pending_jobs[job_id] = function_call
    return {
        "job_id": job_id,
        "status": "pending",
    }
```

### Importing a Modal function between Modal apps

You can also import one function defined in an app from another app:

```
import modal

app = modal.App("another-app")

square = modal.Function.from_name("my-shared-app", "square")

@app.function()
def cube(x):
    return x * square.remote(x)

@app.local_entrypoint()
def main():
    assert cube.remote(42) == 74088
```

### Comparison with HTTPS

Compared with HTTPS invocation, Python invocation has the following benefits:

* Avoids the need to create web endpoint functions.
* Avoids handling serialization of request and response data between Modal and
  your client.
* Uses the Modal client library’s built-in authentication.
  + Web endpoints are public to the entire internet, whereas function
    `lookup`
    only exposes your code to you (and your org).
* You can work with shared Modal functions as if they are normal Python
  functions, which might be more convenient.

Invoking with HTTPS
-------------------

Any non-Python application client can interact with deployed Modal applications
via
[web endpoint functions](/docs/guide/webhooks)

.

Anything able to make HTTPS requests can trigger a Modal web endpoint function.
Note that all deployed web endpoint functions have
[a stable HTTPS URL](/docs/guide/webhook-urls)

.

Some use cases for HTTPS invocation include:

* Calling Modal functions from a web browser client running Javascript
* Calling Modal functions from non-Python backend services (Java, Go, Ruby,
  NodeJS, etc)
* Calling Modal functions using UNIX tools (
  `curl`
  ,
  `wget`
  )

However, if the client of your Modal deployment is running Python, it’s better
to use the
[Modal client library](https://pypi.org/project/modal/)

to invoke
your Modal code.

For more detail on setting up functions for invocation over HTTP see the
[web endpoints guide](/docs/guide/webhooks)

.

[Invoking deployed functions](#invoking-deployed-functions)

[Invoking with Python](#invoking-with-python)

[Function lookup and invocation basics](#function-lookup-and-invocation-basics)

[Authentication](#authentication)

[Lookup of lifecycle functions](#lookup-of-lifecycle-functions)

[Asynchronous invocation](#asynchronous-invocation)

[Importing a Modal function between Modal apps](#importing-a-modal-function-between-modal-apps)

[Comparison with HTTPS](#comparison-with-https)

[Invoking with HTTPS](#invoking-with-https)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/troubleshooting
================================================================================

Troubleshooting
===============

“Command not found” errors
--------------------------

If you installed Modal but you’re seeing an error like
`modal: command not found`
when trying to run the CLI, this means that the
installation location of Python package executables (“binaries”) are not present
on your system path. This is a common problem; you need to reconfigure your
system’s environment variables to fix it.

One workaround is to use
`python -m modal.cli`
instead of
`modal`
. However, this
is just a patch. There’s no single solution for the problem because Python
installs dependencies on different locations depending on your environment. See
this
[popular StackOverflow question](https://stackoverflow.com/q/35898734)

for
pointers on how to resolve your system path issue.

Custom types defined in
`__main__`
----------------------------------

Modal currently uses
[cloudpickle](https://github.com/cloudpipe/cloudpickle)

to
transfer objects returned or exceptions raised by functions that are executed in
Modal. This gives a lot of flexibility and support for custom data types.

However, any types that are declared in your Python entrypoint file (The one you
call on the command line) will currently be
*redeclared*
if they are returned
from Modal functions, and will therefore have the same structure and type name
but not maintain class object identity with your local types. This means that
you
*can’t*
catch specific custom exception classes:

```
import modal
app = modal.App()

class MyException(Exception):
    pass

@app.function()
def raise_custom():
    raise MyException()

@app.local_entrypoint()
def main():
    try:
        raise_custom.remote()
    except MyException:  # this will not catch the remote exception
        pass
    except Exception:  # this will catch it instead, as it's still a subclass of Exception
        pass
```

Nor can you do object equality checks on
`dataclasses`
, or
`isinstance`
checks:

```
import modal
import dataclasses

@dataclasses.dataclass
class MyType:
    foo: int

app = modal.App()

@app.function()
def return_custom():
    return MyType(foo=10)

@app.local_entrypoint()
def main():
    data = return_custom.remote()
    assert data == MyType(foo=10)  # false!
    assert data.foo == 10  # true!, the type still has the same fields etc.
    assert isinstance(data, MyType)  # false!
```

If this is a problem for you, you can easily solve it by moving your custom type
definitions to a separate Python file from the one you trigger to run your Modal
code, and import that file instead.

```
# File: my_types.py
import dataclasses

@dataclasses.dataclass
class MyType:
    foo: int
```

```
# File: modal_script.py
import modal
from my_types import MyType

app = modal.App()

@app.function()
def return_custom():
    return MyType(foo=10)

@app.local_entrypoint()
def main():
    data = return_custom.remote()
    assert data == MyType(foo=10)  # true!
    assert isinstance(data, MyType)  # true!
```

Function side effects
---------------------

The same container
*can*
be reused for multiple invocations of the same function
within an app. This means that if your function has side effects like modifying
files on disk, they may or may not be present for subsequent calls to that
function. You should not rely on the side effects to be present, but you might
have to be careful so they don’t cause problems.

For example, if you create a disk-backed database using sqlite3:

```
import modal
import sqlite3

app = modal.App()

@app.function()
def db_op():
    db = sqlite3("db_file.sqlite3")
    db.execute("CREATE TABLE example (col_1 TEXT)")
    ...
```

This function
*can*
(but will not necessarily) fail on the second invocation
with an

`OperationalError: table foo already exists`

To get around this, take care to either clean up your side effects (e.g.
deleting the db file at the end your function call above) or make your functions
take them into consideration (e.g. adding an
`if os.path.exists("db_file.sqlite")`
condition or randomize the filename
above).

Heartbeat timeout
-----------------

The Modal client in
`modal.Function`
containers runs a heartbeat loop that the host uses to healthcheck the container’s main process.
If the container stops heartbeating for a long period (minutes) the container will be terminated due to a
`heartbeat timeout`
, which is displayed in logs.

Container heartbeat timeouts are rare, and typically caused by one of two application-level sources:

* [Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock)

  is held for a long time, stopping the heartbeat thread from making progress.
  [py-spy](https://github.com/benfred/py-spy?tab=readme-ov-file#how-does-gil-detection-work)

  can detect GIL holding. We include
  `py-spy`
  [automatically in
  `modal shell`](/docs/guide/developing-debugging#debug-shells)

  for convenience. A quick fix for GIL holding is to run the code which holds the GIL
  [in a subprocess](https://docs.python.org/3/library/multiprocessing.html#the-process-class)

  .
* Container process initiates shutdown, intentionally stopping the heartbeats, but it does not complete shutdown.

In both cases
[turning on debug logging](/docs/guide/developing-debugging#debug-logs)

will help diagnose the issue.

`413 Content Too Large`
errors
------------------------------

If you receive a
`413 Content Too Large`
error, this might be because you are
hitting our gRPC payload size limits.

The size limit is currently 100MB.

`403`
errors when connecting to GCP services.
---------------------------------------------

GCP will sometimes return 403 errors to Modal when connecting directly to GCP
cloud services like Google Cloud Storage. This is a known issue.

The workaround is to pin the
`cloud`
parameter in the
[`@app.function`](https://modal.com/docs/reference/modal.App#function)

or
[`@app.cls`](https://modal.com/docs/reference/modal.App#cls)

.

For example:

```
@app.function(cloud="gcp")
def f():
    ...
```

```
@app.cls(cloud="gcp")
class MyClass:
    ...
```

Outdated kernel version (4.4.0)
-------------------------------

Our secure runtime
[reports a misleadingly old](https://github.com/google/gvisor/issues/11117)

kernel version, 4.4.0.
Certain software libraries will detect this and report a warning. These warnings can be ignored because the runtime
actually implements Linux kernel features from versions 5.15+.

If the outdated kernel version reporting creates errors in your application please contact us
[in our Slack](https://modal.com/slack)

.

[Troubleshooting](#troubleshooting)

[“Command not found” errors](#command-not-found-errors)

[Custom types defined in \_\_main\_\_](#custom-types-defined-in-__main__)

[Function side effects](#function-side-effects)

[Heartbeat timeout](#heartbeat-timeout)

[413 Content Too Large errors](#413-content-too-large-errors)

[403 errors when connecting to GCP services.](#403-errors-when-connecting-to-gcp-services)

[Outdated kernel version (4.4.0)](#outdated-kernel-version-440)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/tunnels
================================================================================

Tunnels
=======

Modal allows you to expose live TCP ports on a Modal container. This is done by
creating a
*tunnel*
that forwards the port to the public Internet.

```
import modal

app = modal.App()

@app.function()
def start_app():
    # Inside this `with` block, port 8000 on the container can be accessed by
    # the address at `tunnel.url`, which is randomly assigned.
    with modal.forward(8000) as tunnel:
        print(f"tunnel.url        = {tunnel.url}")
        print(f"tunnel.tls_socket = {tunnel.tls_socket}")
        # ... start some web server at port 8000, using any framework
```

Tunnels are direct connections and terminate TLS automatically. Within a few
milliseconds of container startup, this function prints a message such as:

```
tunnel.url        = https://wtqcahqwhd4tu0.r5.modal.host
tunnel.tls_socket = ('wtqcahqwhd4tu0.r5.modal.host', 443)
```

You can also create tunnels on a
[Sandbox](/docs/guide/sandbox-networking#forwarding-ports)

to directly expose the container’s ports.

Build with tunnels
------------------

Tunnels are the fastest way to get a low-latency, direct connection to a running
container. You can use them to run live browser applications with
**interactive
terminals**
,
**Jupyter notebooks**
,
**VS Code servers**
, and more.

As a quick example, here is how you would expose a Jupyter notebook:

```
import os
import secrets
import subprocess

import modal

app = modal.App()
app.image = modal.Image.debian_slim().pip_install("jupyterlab")

@app.function()
def run_jupyter():
    token = secrets.token_urlsafe(13)
    with modal.forward(8888) as tunnel:
        url = tunnel.url + "/?token=" + token
        print(f"Starting Jupyter at {url}")
        subprocess.run(
            [
                "jupyter",
                "lab",
                "--no-browser",
                "--allow-root",
                "--ip=0.0.0.0",
                "--port=8888",
                "--LabApp.allow_origin='*'",
                "--LabApp.allow_remote_access=1",
            ],
            env={**os.environ, "JUPYTER_TOKEN": token, "SHELL": "/bin/bash"},
            stderr=subprocess.DEVNULL,
        )
```

When you run the function, it starts Jupyter and gives you the public URL. It’s
as simple as that.

All Modal features are supported. If you
[need GPUs](https://modal.com/docs/guide/gpu)

, pass
`gpu=`
to the
`@app.function()`
decorator. If you
[need more CPUs, RAM](https://modal.com/docs/guide/resources)

, or to attach
[volumes](https://modal.com/docs/guide/volumes)

, those
also just work.

### Programmable startup

The tunnel API is completely on-demand, so you can start them as the result of a
web request.

For example, you could make something like Jupyter Hub without leaving Modal,
giving your users their own Jupyter notebooks when they visit a URL:

```
import modal

image = modal.Image.debian_slim().pip_install("fastapi[standard]")
app = modal.App(image=image)

@app.function(timeout=900)  # 15 minutes
def run_jupyter(q):
    ...  # as before, but return the URL on app.q

@app.function()
@modal.fastapi_endpoint(method="POST")
def jupyter_hub():
    from fastapi import HTTPException
    from fastapi.responses import RedirectResponse

    ...  # do some validation on the secret or bearer token

    if is_valid:
        with modal.Queue.ephemeral() as q:
            run_jupyter.spawn(q)
            url = q.get()
            return RedirectResponse(url, status_code=303)

    else:
        raise HTTPException(401, "Not authenticated")
```

This gives every user who sends a POST request to the web endpoint their own
Jupyter notebook server, on a fully isolated Modal container.

You could do the same with VS Code and get some basic version of an instant,
serverless IDE!

### Advanced: Unencrypted TCP tunnels

By default, tunnels are only exposed to the Internet at a secure random URL, and
connections have automatic TLS (the “S” in HTTPS). However, sometimes you might
need to expose a protocol like an SSH server that goes directly over TCP. In
this case, we have support for
*unencrypted*
tunnels:

```
with modal.forward(8000, unencrypted=True) as tunnel:
    print(f"tunnel.tcp_socket = {tunnel.tcp_socket}")
```

Might produce an output like:

```
tunnel.tcp_socket = ('r3.modal.host', 23447)
```

You can then connect over TCP, for example with
`nc r3.modal.host 23447`
. Unlike
encrypted TLS sockets, these cannot be given a non-guessable, cryptographically
random URL due to how the TCP protocol works, so they are assigned a random port
number instead.

Pricing
-------

Modal only charges for containers based on
[the resources you use](https://modal.com/pricing)

. There is no additional
charge for having an active tunnel.

For example, if you start a Jupyter notebook on port 8888 and access it via
tunnel, you can use it for an hour for development (with 0.01 CPUs) and then
actually run an intensive job with 16 CPUs for one minute. The amount you would
be billed for in that hour is 0.01 + 16 \* (1/60) =
**0.28 CPUs**
, even though
you had access to 16 CPUs without needing to restart your notebook.

Security
--------

Tunnels are run on Modal’s private global network of Internet relays. On
startup, your container will connect to the nearest tunnel so you get the
minimum latency, very similar in performance to a direct connection with the
machine.

This makes them ideal for live debugging sessions, using web-based terminals
like
[ttyd](https://github.com/tsl0922/ttyd)

.

The generated URLs are cryptographically random, but they are also public on the
Internet, so anyone can access your application if they are given the URL.

We do not currently do any detection of requests above L4, so if you are running
a web server, we will not add special proxy HTTP headers or translate HTTP/2.
You’re just getting the TLS-encrypted TCP stream directly!

[Tunnels](#tunnels)

[Build with tunnels](#build-with-tunnels)

[Programmable startup](#programmable-startup)

[Advanced: Unencrypted TCP tunnels](#advanced-unencrypted-tcp-tunnels)

[Pricing](#pricing)

[Security](#security)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/volumes
================================================================================

Volumes
=======

Modal Volumes provide a high-performance distributed file system for your Modal applications.
They are designed for write-once, read-many I/O workloads, like creating machine learning model
weights and distributing them for inference.

Creating a Volume
-----------------

The easiest way to create a Volume and use it as a part of your App is to use
the
[`modal volume create`](/docs/reference/cli/volume#modal-volume-create)

CLI command. This will create the Volume and output
some sample code:

```
% modal volume create my-volume
Created volume 'my-volume' in environment 'main'.
```

Using a Volume on Modal
-----------------------

To attach an existing Volume to a Modal Function, use
[`Volume.from_name`](/docs/reference/modal.Volume#from_name)

:

```
vol = modal.Volume.from_name("my-volume")

@app.function(volumes={"/data": vol})
def run():
    with open("/data/xyz.txt", "w") as f:
        f.write("hello")
    vol.commit()  # Needed to make sure all changes are persisted before exit
```

You can also browse and manipulate Volumes from an ad hoc Modal Shell:

```
% modal shell --volume my-volume --volume another-volume
```

Volumes will be mounted under
`/mnt`
.

Downloading a file from a Volume
--------------------------------

While there’s no file size limit for individual files in a volume, the frontend only supports downloading files up to 16 MB. For larger files, please use the CLI:

```
% modal volume get my-volume xyz.txt xyz-local.txt
```

### Creating Volumes lazily from code

You can also create Volumes lazily from code using:

```
vol = modal.Volume.from_name("my-volume", create_if_missing=True)
```

This will create the Volume if it doesn’t exist.

Using a Volume from outside of Modal
------------------------------------

Volumes can also be used outside Modal via the
[Python SDK](/docs/reference/modal.Volume#modalvolume)

or our
[CLI](/docs/reference/cli/volume)

.

### Using a Volume from local code

You can interact with Volumes from anywhere you like using the
`modal`
Python client library.

```
vol = modal.Volume.from_name("my-volume")

with vol.batch_upload() as batch:
    batch.put_file("local-path.txt", "/remote-path.txt")
    batch.put_directory("/local/directory/", "/remote/directory")
    batch.put_file(io.BytesIO(b"some data"), "/foobar")
```

For more details, see the
[reference documentation](/docs/reference/modal.Volume)

.

### Using a Volume via the command line

You can also interact with Volumes using the command line interface. You can run
`modal volume`
to get a full list of its subcommands:

```
% modal volume
Usage: modal volume [OPTIONS] COMMAND [ARGS]...

 Read and edit modal.Volume volumes.
 Note: users of modal.NetworkFileSystem should use the modal nfs command instead.

╭─ Options ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ --help          Show this message and exit.                                                                                                                                                            │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─ File operations ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ cp       Copy within a modal.Volume. Copy source file to destination file or multiple source files to destination directory.                                                                           │
│ get      Download files from a modal.Volume object.                                                                                                                                                    │
│ ls       List files and directories in a modal.Volume volume.                                                                                                                                          │
│ put      Upload a file or directory to a modal.Volume.                                                                                                                                                 │
│ rm       Delete a file or directory from a modal.Volume.                                                                                                                                               │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
╭─ Management ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╮
│ create   Create a named, persistent modal.Volume.                                                                                                                                                      │
│ delete   Delete a named, persistent modal.Volume.                                                                                                                                                      │
│ list     List the details of all modal.Volume volumes in an Environment.                                                                                                                               │
╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯
```

For more details, see the
[reference documentation](/docs/reference/cli/volume)

.

Volume commits and reloads
--------------------------

Unlike a normal filesystem, you need to explicitly reload the Volume to see
changes made since it was first mounted. This reload is handled by invoking the
[`.reload()`](/docs/reference/modal.Volume#reload)

method on a Volume object.
Similarly, any Volume changes made within a container need to be committed for
those the changes to become visible outside the current container. This is handled
periodically by
[background commits](#background-commits)

and directly by invoking
the
[`.commit()`](/docs/reference/modal.Volume#commit)

method on a
`modal.Volume`
object.

At container creation time the latest state of an attached Volume is mounted. If
the Volume is then subsequently modified by a commit operation in another
running container, that Volume modification won’t become available until the
original container does a
[`.reload()`](/docs/reference/modal.Volume#reload)

.

Consider this example which demonstrates the effect of a reload:

```
import pathlib
import modal

app = modal.App()

volume = modal.Volume.from_name("my-volume")

p = pathlib.Path("/root/foo/bar.txt")

@app.function(volumes={"/root/foo": volume})
def f():
    p.write_text("hello")
    print(f"Created {p=}")
    volume.commit()  # Persist changes
    print(f"Committed {p=}")

@app.function(volumes={"/root/foo": volume})
def g(reload: bool = False):
    if reload:
        volume.reload()  # Fetch latest changes
    if p.exists():
        print(f"{p=} contains '{p.read_text()}'")
    else:
        print(f"{p=} does not exist!")

@app.local_entrypoint()
def main():
    g.remote()  # 1. container for `g` starts
    f.remote()  # 2. container for `f` starts, commits file
    g.remote(reload=False)  # 3. reuses container for `g`, no reload
    g.remote(reload=True)   # 4. reuses container, but reloads to see file.
```

The output for this example is this:

```
p=PosixPath('/root/foo/bar.txt') does not exist!
Created p=PosixPath('/root/foo/bar.txt')
Committed p=PosixPath('/root/foo/bar.txt')
p=PosixPath('/root/foo/bar.txt') does not exist!
p=PosixPath('/root/foo/bar.txt') contains hello
```

This code runs two containers, one for
`f`
and one for
`g`
. Only the last
function invocation reads the file created and committed by
`f`
because it was
configured to reload.

### Background commits

Modal Volumes run background commits:
every few seconds while your Function executes,
the contents of attached Volumes will be committed
without your application code calling
`.commit`
.
A final snapshot and commit is also automatically performed on container shutdown.

Being able to persist changes to Volumes without changing your application code
is especially useful when
[training or fine-tuning models using frameworks](#model-checkpointing)

.

Model serving
-------------

A single ML model can be served by simply baking it into a
`modal.Image`
at
build time using
[`run_function`](/docs/reference/modal.Image#run_function)

. But
if you have dozens of models to serve, or otherwise need to decouple image
builds from model storage and serving, use a
`modal.Volume`
.

Volumes can be used to save a large number of ML models and later serve any one
of them at runtime with much better performance than can be achieved with a
[`modal.NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)

.

This snippet below shows the basic structure of the solution.

```
import modal

app = modal.App()
volume = modal.Volume.from_name("model-store")
model_store_path = "/vol/models"

@app.function(volumes={model_store_path: volume}, gpu="any")
def run_training():
    model = train(...)
    save(model_store_path, model)
    volume.commit()  # Persist changes

@app.function(volumes={model_store_path: volume})
def inference(model_id: str, request):
    try:
        model = load_model(model_store_path, model_id)
    except NotFound:
        volume.reload()  # Fetch latest changes
        model = load_model(model_store_path, model_id)
    return model.run(request)
```

For more details, see our
[guide to storing model weights on Modal](/docs/guide/model-weights)

.

Model checkpointing
-------------------

Checkpoints are snapshots of an ML model and can be configured by the callback
functions of ML frameworks. You can use saved checkpoints to restart a training
job from the last saved checkpoint. This is particularly helpful in managing
[preemption](/docs/guide/preemption)

.

For more, see our
[example code for long-running training](/docs/examples/long-training)

.

### Hugging Face `transformers`

To periodically checkpoint into a
`modal.Volume`
, just set the
`Trainer`
’s
[`output_dir`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments.output_dir)

to a directory in the Volume.

```
import pathlib

volume = modal.Volume.from_name("my-volume")
VOL_MOUNT_PATH = pathlib.Path("/vol")

@app.function(
    gpu="A10G",
    timeout=2 * 60 * 60,  # run for at most two hours
    volumes={VOL_MOUNT_PATH: volume},
)
def finetune():
    from transformers import Seq2SeqTrainer
    ...

    training_args = Seq2SeqTrainingArguments(
        output_dir=str(VOL_MOUNT_PATH / "model"),
        # ... more args here
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_xsum_train,
        eval_dataset=tokenized_xsum_test,
    )
```

Volumes versus Network File Systems
-----------------------------------

Like the
[`modal.NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem)

,
Volumes can be simultaneously attached to multiple Modal Functions, supporting
concurrent reading and writing. But unlike the
`modal.NetworkFileSystem`
, the
`modal.Volume`
has been designed for fast reads and does not automatically
synchronize writes between mounted Volumes.

Volume performance
------------------

Volumes work best when they contain less than 50,000 files and directories. The
latency to attach or modify a Volume scales linearly with the number of files in
the Volume, and past a few tens of thousands of files the linear component
starts to dominate the fixed overhead.

There is currently a hard limit of 500,000 inodes (files, directories and
symbolic links) per Volume. If you reach this limit, any further attempts to
create new files or directories will error with
[`ENOSPC`
(No space left on device)](https://pubs.opengroup.org/onlinepubs/9799919799/)

.

Filesystem consistency
----------------------

### Concurrent modification

Concurrent modification from multiple containers is supported, but concurrent
modifications of the same files should be avoided. Last write wins in case of
concurrent modification of the same file — any data the last writer didn’t have
when committing changes will be lost!

The number of commits you can run concurrently is limited. If you run too many
concurrent commits each commit will take longer due to contention. If you are
committing small changes, avoid doing more than 5 concurrent commits (the number
of concurrent commits you can make is proportional to the size of the changes
being committed).

As a result, Volumes are typically not a good fit for use cases where you need
to make concurrent modifications to the same file (nor is distributed file
locking supported).

While a reload is in progress the Volume will appear empty to the container that
initiated the reload. That means you cannot read from or write to a Volume in a
container where a reload is ongoing (note that this only applies to the
container where the reload was issued, other containers remain unaffected).

### Busy Volume errors

You can only reload a Volume when there no open files on the Volume. If you have
open files on the Volume the
[`.reload()`](/docs/reference/modal.Volume#reload)

operation will fail with “volume busy”. The following is a simple example of how
a “volume busy” error can occur:

```
volume = modal.Volume.from_name("my-volume")

@app.function(volumes={"/vol": volume})
def reload_with_open_files():
    f = open("/vol/data.txt", "r")
    volume.reload()  # Cannot reload when files in the Volume are open.
```

### Can’t find file on Volume errors

When accessing files in your Volume, don’t forget to pre-pend where your Volume
is mounted in the container.

In the example below, where the Volume has been mounted at
`/data`
, “hello” is
being written to
`/data/xyz.txt`
.

```
import modal

app = modal.App()
vol = modal.Volume.from_name("my-volume")

@app.function(volumes={"/data": vol})
def run():
    with open("/data/xyz.txt", "w") as f:
        f.write("hello")
    vol.commit()
```

If you instead write to
`/xyz.txt`
, the file will be saved to the local disk of the Modal Function.
When you dump the contents of the Volume, you will not see the
`xyz.txt`
file.

Further examples
----------------

* [Character LoRA fine-tuning](/docs/examples/diffusers_lora_finetune)

  with model storage on a Volume
* [Protein folding](/docs/examples/chai1)

  with model weights and output files stored on Volumes
* [Dataset visualization with Datasette](/docs/example/cron_datasette)

  using a SQLite database on a Volume

[Volumes](#volumes)

[Creating a Volume](#creating-a-volume)

[Using a Volume on Modal](#using-a-volume-on-modal)

[Downloading a file from a Volume](#downloading-a-file-from-a-volume)

[Creating Volumes lazily from code](#creating-volumes-lazily-from-code)

[Using a Volume from outside of Modal](#using-a-volume-from-outside-of-modal)

[Using a Volume from local code](#using-a-volume-from-local-code)

[Using a Volume via the command line](#using-a-volume-via-the-command-line)

[Volume commits and reloads](#volume-commits-and-reloads)

[Background commits](#background-commits)

[Model serving](#model-serving)

[Model checkpointing](#model-checkpointing)

[Hugging Face transformers](#hugging-face-transformers)

[Volumes versus Network File Systems](#volumes-versus-network-file-systems)

[Volume performance](#volume-performance)

[Filesystem consistency](#filesystem-consistency)

[Concurrent modification](#concurrent-modification)

[Busy Volume errors](#busy-volume-errors)

[Can’t find file on Volume errors](#cant-find-file-on-volume-errors)

[Further examples](#further-examples)

See it in action

[Fine-tuning and serving custom image generation models](/docs/examples/diffusers_lora_finetune)

[Folding proteins](/docs/examples/chai1)

[Serving interactive visualizations for a SQLite database](/docs/examples/cron_datasette)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/webhook-proxy-auth
================================================================================

Proxy Auth Tokens
=================

Use Proxy Auth Tokens to prevent unauthorized clients from triggering your web endpoints.

```
import modal

image = modal.Image.debian_slim().pip_install("fastapi")
app = modal.App("proxy-auth-public", image=image)

@app.function()
@modal.fastapi_endpoint()
def public():
    return "hello world"

@app.function()
@modal.fastapi_endpoint(requires_proxy_auth=True)
def private():
    return "hello friend"
```

The
`public`
endpoint can be hit by any client over the Internet.

```
curl https://public-url--goes-here.modal.run
```

The
`private`
endpoint cannot.

```
curl --fail-with-body https://private-url--goes-here.modal.run
# modal-http: missing credentials for proxy authorization
# curl: (22) The requested URL returned error: 401
# https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401
```

Authorization is demonstrated via a Proxy Auth Token. You can create a Proxy Auth Token for your workspace
[here](/settings/proxy-auth-tokens)

.
In requests to the web endpoint, clients supply the Token ID and Token Secret in the
`Modal-Key`
and
`Modal-Secret`
HTTP headers.

```
export TOKEN_ID=wk-1234abcd
export TOKEN_SECRET=ws-1234abcd
curl -H "Modal-Key: $TOKEN_ID" \
     -H "Modal-Secret: $TOKEN_SECRET" \
     https://private-url--goes-here.modal.run
```

Proxy authorization can be added to
[web endpoints](/docs/guide/webhooks)

created by the
[`fastapi_endpoint`](/docs/reference/modal.fastapi_endpoint)

,
[`asgi_app`](/docs/reference/modal.asgi_app)

,
[`wsgi_app`](/docs/reference/modal.wsgi_app)

, or
[`web_server`](/docs/reference/modal.web_server)

decorators,
which are otherwise publicly available.

Everyone within the workspace of the web endpoint can manage its Proxy Auth Tokens.

[Proxy Auth Tokens](#proxy-auth-tokens)

See it in action

[Create a Proxy Auth Token](/settings/proxy-auth-tokens)

[Deploy a proxy authorized endpoint](/docs/examples/basic_web)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/webhook-timeouts
================================================================================

Request timeouts
================

Web endpoint (a.k.a. webhook) requests should complete quickly, ideally within a
few seconds. All web endpoint function types
(
[`web_endpoint`
,
`asgi_app`
,
`wsgi_app`](/docs/reference/modal.web_endpoint)

)
have a maximum HTTP request timeout of 150 seconds enforced. However, the
underlying Modal function can have a longer
[timeout](/docs/guide/timeouts)

.

In case the function takes more than 150 seconds to complete, a HTTP status 303
redirect response is returned pointing at the original URL with a special query
parameter linking it that request. This is the
*result URL*
for your function.
Most web browsers allow for up to 20 such redirects, effectively allowing up to
50 minutes (20 \* 150 s) for web endpoints before the request times out.

(
**Note:**
This does not work with requests that require
[CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)

, since the
response will not have been returned from your code in time for the server to
populate CORS headers.)

Some libraries and tools might require you to add a flag or option in order to
follow redirects automatically, e.g.
`curl -L ...`
or
`http --follow ...`
.

The
*result URL*
can be reloaded without triggering a new request. It will block
until the request completes.

(
**Note:**
As of March 2025, the Python standard library’s
`urllib`
module has the
maximum number of redirects to any single URL set to 4 by default (
[source](https://github.com/python/cpython/blob/main/Lib/urllib/request.py)

), which would limit the total timeout to 12.5 minutes (5 \* 150 s = 750 s) unless this setting is overridden.)

Polling solutions
-----------------

Sometimes it can be useful to be able to poll for results rather than wait for a
long running HTTP request. The easiest way to do this is to have your web
endpoint spawn a
`modal.Function`
call and return the function call id that
another endpoint can use to poll the submitted function’s status. Here is an
example:

```
import fastapi

import modal

image = modal.Image.debian_slim().pip_install("fastapi[standard]")
app = modal.App(image=image)

web_app = fastapi.FastAPI()

@app.function()
@modal.asgi_app()
def fastapi_app():
    return web_app

@app.function()
def slow_operation():
    ...

@web_app.post("/accept")
async def accept_job(request: fastapi.Request):
    call = slow_operation.spawn()
    return {"call_id": call.object_id}

@web_app.get("/result/{call_id}")
async def poll_results(call_id: str):
    function_call = modal.FunctionCall.from_id(call_id)
    try:
        return function_call.get(timeout=0)
    except TimeoutError:
        http_accepted_code = 202
        return fastapi.responses.JSONResponse({}, status_code=http_accepted_code)
```

[*Document OCR Web App*](/docs/examples/doc_ocr_webapp)

is an example that uses
this pattern.

[Request timeouts](#request-timeouts)

[Polling solutions](#polling-solutions)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/webhook-urls
================================================================================

Web endpoint URLs
=================

This guide documents the behavior of URLs for
[web endpoints](/docs/guide/webhooks)

on Modal: automatic generation, configuration, programmatic retrieval, and more.

Determine the URL of a web endpoint from code
---------------------------------------------

Modal Functions with the
[`fastapi_endpoint`](/docs/reference/modal.fastapi_endpoint)

,
[`asgi_app`](/docs/reference/modal.asgi_app)

,
[`wsgi_app`](/docs/reference/modal.wsgi_app)

,
or
[`web_server`](/docs/reference/modal.web_server)

decorator
are made available over the Internet when they are
[`serve`
d](/docs/reference/cli/serve)

or
[`deploy`
ed](/docs/reference/cli/deploy)

and so they have a URL.

This URL is displayed in the
`modal`
CLI output
and is available in the Modal
[dashboard](/apps)

for the Function.

To determine a Function’s URL programmatically,
check its
[`get_web_url()`](/docs/reference/modal.Function#get_web_url)

property:

```
@app.function(image=modal.Image.debian_slim().pip_install("fastapi[standard]"))
@modal.fastapi_endpoint(docs=True)
def show_url() -> str:
    return show_url.get_web_url()
```

For deployed Functions, this also works from other Python code!
You just need to do a
[`from_name`](/docs/reference/modal.Function#from_name)

based on the name of the Function and its
[App](/docs/guide/apps)

:

```
import requests

remote_function = modal.Function.from_name("app", "show_url")
remote_function.get_web_url() == requests.get(handle.get_web_url()).json()
```

Auto-generated URLs
-------------------

By default, Modal Functions
will be served from the
`modal.run`
domain.
The full URL will be constructed from a number of pieces of information
to uniquely identify the endpoint.

At a high-level, web endpoint URLs for deployed applications have the
following structure:
`https://<source>--<label>.modal.run`
.

The
`source`
component represents the workspace and environment where the App is
deployed. If your workspace has only a single environment, the
`source`
will
just be the workspace name. Multiple environments are disambiguated by an
[“environment suffix”](/docs/guide/environments#environment-web-suffixes)

, so
the full source would be
`<workspace>-<suffix>`
. However, one environment per
workspace is allowed to have a null suffix, in which case the source would just
be
`<workspace>`
.

The
`label`
component represents the specific App and Function that the endpoint
routes to. By default, these are concatenated with a hyphen, so the label would
be
`<app>-<function>`
.

These components are normalized to contain only lowercase letters, numerals, and dashes.

To put this all together, consider the following example. If a member of the
`ECorp`
workspace uses the
`main`
environment (which has
`prod`
as its web
suffix) to deploy the
`text_to_speech`
app with a webhook for the
`flask-app`
function, the URL will have the following components:

* *Source*
  :
  + *Workspace name slug*
    :
    `ECorp`
    →
    `ecorp`
  + *Environment web suffix slug*
    :
    `main`
    →
    `prod`
* *Label*
  :
  + *App name slug*
    :
    `text_to_speech`
    →
    `text-to-speech`
  + *Function name slug*
    :
    `flask_app`
    →
    `flask-app`

The full URL will be
`https://ecorp-prod--text-to-speech-flask-app.modal.run`
.

User-specified labels
---------------------

It’s also possible to customize the
`label`
used for each Function
by passing a parameter to the relevant endpoint decorator:

```
import modal

image = modal.Image.debian_slim().pip_install("fastapi")
app = modal.App(name="text_to_speech", image=image)

@app.function()
@modal.fastapi_endpoint(label="speechify")
def web_endpoint_handler():
    ...
```

Building on the example above, this code would produce the following URL:
`https://ecorp-prod--speechify.modal.run`
.

User-specified labels are not automatically normalized, but labels with
invalid characters will be rejected.

Ephemeral apps
--------------

To support development workflows, webhooks for ephemeral apps (i.e., apps
created with
`modal serve`
) will have a
`-dev`
suffix appended to their URL
label (regardless of whether the label is auto-generated or user-specified).
This prevents development work from interfering with deployed versions of the
same app.

If an ephemeral app is serving a webhook while another ephemeral webhook is
created seeking the same web endpoint label, the new function will
*steal*
the
running webhook’s label.

This ensures that the latest iteration of the ephemeral function is
serving requests and that older ones stop receiving web traffic.

Truncation
----------

If a generated subdomain label is longer than 63 characters, it will be
truncated.

For example, the following subdomain label is too long, 67 characters:
`ecorp--text-to-speech-really-really-realllly-long-function-name-dev`
.

The truncation happens by calculating a SHA-256 hash of the overlong label, then
taking the first 6 characters of this hash. The overlong subdomain label is
truncated to 56 characters, and then joined by a dash to the hash prefix. In
the above example, the resulting URL would be
`ecorp--text-to-speech-really-really-rea-1b964b-dev.modal.run`
.

The combination of the label hashing and truncation provides a unique list of 63
characters, complying with both DNS system limits and uniqueness requirements.

Custom domains
--------------

**Custom domains are available on our
[Team and Enterprise plans](/settings/plans)

.**

For more customization, you can use your own domain names with Modal web
endpoints. If your
[plan](/pricing)

supports custom domains, visit the
[Domains
tab](/settings/domains)

in your workspace settings to add a domain name to your
workspace.

You can use three kinds of domains with Modal:

* **Apex:**
  root domain names like
  `example.com`
* **Subdomain:**
  single subdomain entries such as
  `my-app.example.com`
  ,
  `api.example.com`
  , etc.
* **Wildcard domain:**
  either in a subdomain like
  `*.example.com`
  , or in a
  deeper level like
  `*.modal.example.com`

You’ll be asked to update your domain DNS records with your domain name
registrar and then validate the configuration in Modal. Once the records have
been properly updated and propagated, your custom domain will be ready to use.

You can assign any Modal web endpoint to any registered domain in your workspace
with the
`custom_domains`
argument.

```
import modal

app = modal.App("custom-domains-example")

@app.function()
@modal.fastapi_endpoint(custom_domains=["api.example.com"])
def hello(message: str):
    return {"message": f"hello {message}"}
```

You can then run
`modal deploy`
to put your web endpoint online, live.

```
$ curl -s https://api.example.com?message=world
{"message": "hello world"}
```

Note that Modal automatically generates and renews TLS certificates for your
custom domains. Since we do this when your domain is first accessed, there may
be an additional 1-2s latency on the first request. Additional requests use a
cached certificate.

You can also register multiple domain names and associate them with the same web
endpoint.

```
import modal

app = modal.App("custom-domains-example-2")

@app.function()
@modal.fastapi_endpoint(custom_domains=["api.example.com", "api.example.net"])
def hello(message: str):
    return {"message": f"hello {message}"}
```

For
**Wildcard**
domains, Modal will automatically resolve arbitrary custom
endpoints (and issue TLS certificates). For example, if you add the wildcard
domain
`*.example.com`
, then you can create any custom domains under
`example.com`
:

```
import random
import modal

app = modal.App("custom-domains-example-2")

random_domain_name = random.choice(range(10))

@app.function()
@modal.fastapi_endpoint(custom_domains=[f"{random_domain_name}.example.com"])
def hello(message: str):
    return {"message": f"hello {message}"}
```

Custom domains can also be used with
[ASGI](https://modal.com/docs/reference/modal.asgi_app#modalasgi_app)

or
[WSGI](https://modal.com/docs/reference/modal.wsgi_app)

apps using the same
`custom_domains`
argument.

[Web endpoint URLs](#web-endpoint-urls)

[Determine the URL of a web endpoint from code](#determine-the-url-of-a-web-endpoint-from-code)

[Auto-generated URLs](#auto-generated-urls)

[User-specified labels](#user-specified-labels)

[Ephemeral apps](#ephemeral-apps)

[Truncation](#truncation)

[Custom domains](#custom-domains)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/webhooks
================================================================================

Web endpoints
=============

This guide explains how to set up web endpoints with Modal.

All deployed Modal Functions can be
[invoked from any other Python application](/docs/guide/trigger-deployed-functions)

using the Modal client library. We additionally provide multiple ways to expose
your Functions over the web for non-Python clients.

You can
[turn any Python function into a web endpoint](#simple-endpoints)

with a single line
of code, you can
[serve a full app](#serving-asgi-and-wsgi-apps)

using
frameworks like FastAPI, Django, or Flask, or you can
[serve anything that speaks HTTP and listens on a port](#non-asgi-web-servers)

.

Below we walk through each method, assuming you’re familiar with web applications outside of Modal.
For a detailed walkthrough of basic web endpoints on Modal aimed at developers new to web applications,
see
[this tutorial](/docs/examples/basic_web)

.

Simple endpoints
----------------

The easiest way to create a web endpoint from an existing Python function is to use the
[`@modal.fastapi_endpoint`
decorator](/docs/reference/modal.fastapi_endpoint)

.

```
image = modal.Image.debian_slim().pip_install("fastapi[standard]")

@app.function(image=image)
@modal.fastapi_endpoint()
def f():
    return "Hello world!"
```

This decorator wraps the Modal Function in a
[FastAPI application](#how-do-web-endpoints-run-in-the-cloud)

.

*Note: Prior to v0.73.82, this function was named
`@modal.web_endpoint`*
.

### Developing with `modal serve`

You can run this code as an ephemeral app, by running the command

```
modal serve server_script.py
```

Where
`server_script.py`
is the file name of your code. This will create an
ephemeral app for the duration of your script (until you hit Ctrl-C to stop it).
It creates a temporary URL that you can use like any other REST endpoint. This
URL is on the public internet.

The
`modal serve`
command will live-update an app when any of its supporting
files change.

Live updating is particularly useful when working with apps containing web
endpoints, as any changes made to web endpoint handlers will show up almost
immediately, without requiring a manual restart of the app.

### Deploying with `modal deploy`

You can also deploy your app and create a persistent web endpoint in the cloud
by running
`modal deploy`
:

### Passing arguments to an endpoint

When using
`@modal.fastapi_endpoint`
, you can add
[query parameters](https://fastapi.tiangolo.com/tutorial/query-params/)

which
will be passed to your Function as arguments. For instance

```
image = modal.Image.debian_slim().pip_install("fastapi[standard]")

@app.function(image=image)
@modal.fastapi_endpoint()
def square(x: int):
    return {"square": x**2}
```

If you hit this with a URL-encoded query string with the
`x`
parameter present,
the Function will receive the value as an argument:

```
$ curl https://modal-labs--web-endpoint-square-dev.modal.run?x=42
{"square":1764}
```

If you want to use a
`POST`
request, you can use the
`method`
argument to
`@modal.fastapi_endpoint`
to set the HTTP verb. To accept any valid JSON object,
[use
`dict`
as your type annotation](https://fastapi.tiangolo.com/tutorial/body-nested-models/?h=dict#bodies-of-arbitrary-dicts)

and FastAPI will handle the rest.

```
image = modal.Image.debian_slim().pip_install("fastapi[standard]")

@app.function(image=image)
@modal.fastapi_endpoint(method="POST")
def square(item: dict):
    return {"square": item['x']**2}
```

This now creates an endpoint that takes a JSON body:

```
$ curl -X POST -H 'Content-Type: application/json' --data-binary '{"x": 42}' https://modal-labs--web-endpoint-square-dev.modal.run
{"square":1764}
```

This is often the easiest way to get started, but note that FastAPI recommends
that you use
[typed Pydantic models](https://fastapi.tiangolo.com/tutorial/body/)

in order to
get automatic validation and documentation. FastAPI also lets you pass data to
web endpoints in other ways, for instance as
[form data](https://fastapi.tiangolo.com/tutorial/request-forms/)

and
[file uploads](https://fastapi.tiangolo.com/tutorial/request-files/)

.

How do web endpoints run in the cloud?
--------------------------------------

Note that web endpoints, like everything else on Modal, only run when they need
to. When you hit the web endpoint the first time, it will boot up the container,
which might take a few seconds. Modal keeps the container alive for a short
period in case there are subsequent requests. If there are a lot of requests,
Modal might create more containers running in parallel.

For the shortcut
`@modal.fastapi_endpoint`
decorator, Modal wraps your function in a
[FastAPI](https://fastapi.tiangolo.com/)

application. This means that the
[Image](/docs/guide/images)

your Function uses must have FastAPI installed, and the Functions that you write
need to follow its request and response
[semantics](https://fastapi.tiangolo.com/tutorial)

. Web endpoint Functions can use
all of FastAPI’s powerful features, such as Pydantic models for automatic validation,
typed query and path parameters, and response types.

Here’s everything together, combining Modal’s abilities to run functions in
user-defined containers with the expressivity of FastAPI:

```
import modal
from fastapi.responses import HTMLResponse
from pydantic import BaseModel

image = modal.Image.debian_slim().pip_install("fastapi[standard]", "boto3")
app = modal.App(image=image)

class Item(BaseModel):
    name: str
    qty: int = 42

@app.function()
@modal.fastapi_endpoint(method="POST")
def f(item: Item):
    import boto3
    # do things with boto3...
    return HTMLResponse(f"<html>Hello, {item.name}!</html>")
```

This endpoint definition would be called like so:

```
curl -d '{"name": "Erik", "qty": 10}' \
    -H "Content-Type: application/json" \
    -X POST https://ecorp--web-demo-f-dev.modal.run
```

Or in Python with the
[`requests`](https://pypi.org/project/requests/)

library:

```
import requests

data = {"name": "Erik", "qty": 10}
requests.post("https://ecorp--web-demo-f-dev.modal.run", json=data, timeout=10.0)
```

Serving ASGI and WSGI apps
--------------------------

You can also serve any app written in an
[ASGI](https://asgi.readthedocs.io/en/latest/)

or
[WSGI](https://en.wikipedia.org/wiki/Web_Server_Gateway_Interface)

-compatible
web framework on Modal.

ASGI provides support for async web frameworks. WSGI provides support for
synchronous web frameworks.

### ASGI apps - FastAPI, FastHTML, Starlette

For ASGI apps, you can create a function decorated with
[`@modal.asgi_app`](/docs/reference/modal.asgi_app)

that returns a reference to
your web app:

```
image = modal.Image.debian_slim().pip_install("fastapi[standard]")

@app.function(image=image)
@modal.concurrent(max_inputs=100)
@modal.asgi_app()
def fastapi_app():
    from fastapi import FastAPI, Request

    web_app = FastAPI()

    @web_app.post("/echo")
    async def echo(request: Request):
        body = await request.json()
        return body

    return web_app
```

Now, as before, when you deploy this script as a Modal App, you get a URL for
your app that you can hit:

The
`@modal.concurrent`
decorator enables a single container
to process multiple inputs at once, taking advantage of the asynchronous
event loops in ASGI applications. See
[this guide](/docs/guide/concurrent-inputs)

for details.

#### ASGI Lifespan

While we recommend using
[`@modal.enter`](https://modal.com/docs/guide/lifecycle-functions#enter)

for defining container lifecycle hooks, we also support the
[ASGI lifespan protocol](https://asgi.readthedocs.io/en/latest/specs/lifespan.html)

. Lifespans begin when containers start, typically at the time of the first request. Here’s an example using
[FastAPI](https://fastapi.tiangolo.com/advanced/events/#lifespan)

:

```
import modal

app = modal.App("fastapi-lifespan-app")

image = modal.Image.debian_slim().pip_install("fastapi[standard]")

@app.function(image=image)
@modal.asgi_app()
def fastapi_app_with_lifespan():
    from fastapi import FastAPI, Request

    def lifespan(wapp: FastAPI):
        print("Starting")
        yield
        print("Shutting down")

    web_app = FastAPI(lifespan=lifespan)

    @web_app.get("/")
    async def hello(request: Request):
        return "hello"

    return web_app
```

### WSGI apps - Django, Flask

You can serve WSGI apps using the
[`@modal.wsgi_app`](/docs/reference/modal.wsgi_app)

decorator:

```
image = modal.Image.debian_slim().pip_install("flask")

@app.function(image=image)
@modal.concurrent(max_inputs=100)
@modal.wsgi_app()
def flask_app():
    from flask import Flask, request

    web_app = Flask(__name__)

    @web_app.post("/echo")
    def echo():
        return request.json

    return web_app
```

See
[Flask’s docs](https://flask.palletsprojects.com/en/2.1.x/deploying/asgi/)

for more information on using Flask as a WSGI app.

Because WSGI apps are synchronous, concurrent inputs will be run on separate
threads. See
[this guide](/docs/guide/concurrent-inputs)

for details.

Non-ASGI web servers
--------------------

Not all web frameworks offer an ASGI or WSGI interface. For example,
[`aiohttp`](https://docs.aiohttp.org/)

and
[`tornado`](https://www.tornadoweb.org/)

use their own asynchronous network binding, while others like
[`text-generation-inference`](https://github.com/huggingface/text-generation-inference)

actually expose a Rust-based HTTP server running as a subprocess.

For these cases, you can use the
[`@modal.web_server`](/docs/reference/modal.web_server)

decorator to “expose” a
port on the container:

```
@app.function()
@modal.concurrent(max_inputs=100)
@modal.web_server(8000)
def my_file_server():
    import subprocess
    subprocess.Popen("python -m http.server -d / 8000", shell=True)
```

Just like all web endpoints on Modal, this is only run on-demand. The function
is executed on container startup, creating a file server at the root directory.
When you hit the web endpoint URL, your request will be routed to the file
server listening on port
`8000`
.

For
`@web_server`
endpoints, you need to make sure that the application binds to
the external network interface, not just localhost. This usually means binding
to
`0.0.0.0`
instead of
`127.0.0.1`
.

See our examples of how to serve
[Streamlit](/docs/examples/serve_streamlit)

and
[ComfyUI](/docs/examples/comfyapp)

on Modal.

Serve many configurations with parametrized functions
-----------------------------------------------------

Python functions that launch ASGI/WSGI apps or web servers on Modal
cannot take arguments.

One simple pattern for allowing client-side configuration of these web endpoints
is to use
[parametrized functions](/docs/guide/parametrized-functions)

.
Each different choice for the values of the parameters will create a distinct
auto-scaling container pool.

```
@app.cls()
@modal.concurrent(max_inputs=100)
class Server:
    root: str = modal.parameter(default=".")

    @modal.web_server(8000)
    def files(self):
        import subprocess
        subprocess.Popen(f"python -m http.server -d {self.root} 8000", shell=True)
```

The values are provided in URLs as query parameters:

```
curl https://ecorp--server-files.modal.run		# use the default value
curl https://ecorp--server-files.modal.run?root=.cache  # use a different value
curl https://ecorp--server-files.modal.run?root=%2F	# don't forget to URL encode!
```

For details, see
[this guide to parametrized functions](/docs/guide/parametrized-functions)

.

WebSockets
----------

Functions annotated with
`@web_server`
,
`@asgi_app`
, or
`@wsgi_app`
also support
the WebSocket protocol. Consult your web framework for appropriate documentation
on how to use WebSockets with that library.

WebSockets on Modal maintain a single function call per connection, which can be
useful for keeping state around. Most of the time, you will want to set your
handler function to
[allow concurrent inputs](/docs/guide/concurrent-inputs)

,
which allows multiple simultaneous WebSocket connections to be handled by the
same container.

We support the full WebSocket protocol as per
[RFC 6455](https://www.rfc-editor.org/rfc/rfc6455)

, but we do not yet have
support for
[RFC 8441](https://www.rfc-editor.org/rfc/rfc8441)

(WebSockets over
HTTP/2) or
[RFC 7692](https://datatracker.ietf.org/doc/html/rfc7692)

(
`permessage-deflate`
extension). WebSocket messages can be up to 2 MiB each.

Performance and scaling
-----------------------

If you have no active containers when the web endpoint receives a request, it will
experience a “cold start”. Consult the guide page on
[cold start performance](/docs/guide/cold-start)

for more information on when
Functions will cold start and advice how to mitigate the impact.

If your Function uses
`@modal.concurrent`
, multiple requests to the same
endpoint may be handled by the same container. Beyond this limit, additional
containers will start up to scale your App horizontally. When you reach the
Function’s limit on containers, requests will queue for handling.

Each workspace on Modal has a rate limit on total operations. For a new account,
this is set to 200 function inputs or web endpoint requests per second, with a
burst multiplier of 5 seconds. If you reach the rate limit, excess requests to
web endpoints will return a
[429 status code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429)

,
and you’ll need to
[get in touch](mailto:support@modal.com)

with us about
raising the limit.

Web endpoint request bodies can be up to 4 GiB, and their response bodies are
unlimited in size.

Authentication
--------------

Modal offers first-class web endpoint protection via
[proxy auth tokens](https://modal.com/docs/guide/webhook-proxy-auth)

.
Proxy auth tokens protect web endpoints by requiring a key and token combination to be passed
in the
`Modal-Key`
and
`Modal-Secret`
headers.
Modal works as a proxy, rejecting requests that aren’t authorized to access
your endpoint.

We also support standard techniques for securing web servers.

### Token-based authentication

This is easy to implement in whichever framework you’re using. For example, if
you’re using
`@modal.fastapi_endpoint`
or
`@modal.asgi_app`
with FastAPI, you
can validate a Bearer token like this:

```
from fastapi import Depends, HTTPException, status, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

import modal

image = modal.Image.debian_slim().pip_install("fastapi[standard]")
app = modal.App("auth-example", image=image)

auth_scheme = HTTPBearer()

@app.function(secrets=[modal.Secret.from_name("my-web-auth-token")])
@modal.fastapi_endpoint()
async def f(request: Request, token: HTTPAuthorizationCredentials = Depends(auth_scheme)):
    import os

    print(os.environ["AUTH_TOKEN"])

    if token.credentials != os.environ["AUTH_TOKEN"]:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect bearer token",
            headers={"WWW-Authenticate": "Bearer"},
        )

    # Function body
    return "success!"
```

This assumes you have a
[Modal Secret](https://modal.com/secrets)

named
`my-web-auth-token`
created, with contents
`{AUTH_TOKEN: secret-random-token}`
.
Now, your endpoint will return a 401 status code except when you hit it with the
correct
`Authorization`
header set (note that you have to prefix the token with
`Bearer`
):

```
curl --header "Authorization: Bearer secret-random-token" https://modal-labs--auth-example-f.modal.run
```

### Client IP address

You can access the IP address of the client making the request. This can be used
for geolocation, whitelists, blacklists, and rate limits.

```
from fastapi import Request

import modal

image = modal.Image.debian_slim().pip_install("fastapi[standard]")
app = modal.App(image=image)

@app.function()
@modal.fastapi_endpoint()
def get_ip_address(request: Request):
    return f"Your IP address is {request.client.host}"
```

[Web endpoints](#web-endpoints)

[Simple endpoints](#simple-endpoints)

[Developing with modal serve](#developing-with-modal-serve)

[Deploying with modal deploy](#deploying-with-modal-deploy)

[Passing arguments to an endpoint](#passing-arguments-to-an-endpoint)

[How do web endpoints run in the cloud?](#how-do-web-endpoints-run-in-the-cloud)

[Serving ASGI and WSGI apps](#serving-asgi-and-wsgi-apps)

[ASGI apps - FastAPI, FastHTML, Starlette](#asgi-apps---fastapi-fasthtml-starlette)

[ASGI Lifespan](#asgi-lifespan)

[WSGI apps - Django, Flask](#wsgi-apps---django-flask)

[Non-ASGI web servers](#non-asgi-web-servers)

[Serve many configurations with parametrized functions](#serve-many-configurations-with-parametrized-functions)

[WebSockets](#websockets)

[Performance and scaling](#performance-and-scaling)

[Authentication](#authentication)

[Token-based authentication](#token-based-authentication)

[Client IP address](#client-ip-address)

Fully featured web apps

[LLM Voice Chat (React)](/docs/examples/llm-voice-chat)

[Stable Diffusion (Alpine)](/docs/examples/stable_diffusion_cli)

[Whisper Podcast Transcriber (React)](/docs/examples/whisper-transcriber)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/guide/workspaces
================================================================================

Workspaces
==========

A
**workspace**
is an area where a user can deploy Modal apps and other
resources. There are two types of workspaces: personal and shared. After a new
user has signed up to Modal, a personal workspace is automatically created for
them. The name of the personal workspace is based on your GitHub username, but
it might be randomly generated if already taken or invalid.

To collaborate with others, a new shared workspace needs to be created.

Create a Workspace
------------------

All additional workspaces are shared workspaces, meaning you can invite others
by email to collaborate with you. There are two ways to create a Modal workspace
on the
[settings](/settings/workspaces)

page.

![view of workspaces creation interface](https://modal-cdn.com/cdnbot/create-new-workspace-viewk0ka46_7_800f2053.webp)

1. Create from
   [GitHub organization](https://docs.github.com/en/organizations)

   . Allows members in GitHub organization to auto-join the workspace.
2. Create from scratch. You can invite anyone to your workspace.

If you’re interested in having a workspace associated with your Okta
organization, then check out our
[Okta SSO docs](/docs/guide/okta-sso)

.

If you’re interested in using SSO through Google or other providers, then please reach out to us at
[support@modal.com](mailto:support@modal.com)

.

Auto-joining a Workspace associated with a GitHub organization
--------------------------------------------------------------

Note: This is only relevant for Workspaces created from a GitHub organization.

Users can automatically join a Workspace on their
[Workspace settings page](/settings/workspaces)

if they are a member of the GitHub organization associated with the Workspace.

To turn off this functionality a Workspace Manager can disable it on the
**Workspace Management**
tab of their Workspace’s settings page.

Inviting new Workspace members
------------------------------

To invite a new Workspace member, you can visit the
[settings](/settings)

page
and navigate to the members tab for the appropriate workspace.

You can either send an email invite or share an invite link. Both existing Modal
users and non-existing users can use the links to join your workspace. If they
are a new user a Modal account will be created for them.

![invite member section](/_app/immutable/assets/invite-member.CHnml0eT.png)

Create a token for a Workspace
------------------------------

To interact with a Workspace’s resources programmatically, you need to add an
API token for that Workspace. Your existing API tokens are displayed on
[the settings page](/settings/tokens)

and new API tokens can be added for a
particular Workspace.

After adding a token for a Workspace to your Modal config file you can activate
that Workspace’s profile using the CLI (see below).

As an manager or workspace owner you can manage active tokens for a workspace on
[the member tokens page](/settings/tokens/member-tokens)

. For more information on API
token management see the
[documentation about configuration](/docs/reference/modal.config)

.

Switching active Workspace
--------------------------

When on the dashboard or using the CLI, the active profile determines which
personal or organizational Workspace is associated with your actions.

### Dashboard

You can switch between organization Workspaces and your Personal Workspace by
using the workspace selector at the top of
[the dashboard](/home)

.

### CLI

To switch the Workspace associated with CLI commands, use
`modal profile activate`
.

Administrating workspace members
--------------------------------

Workspaces have three different levels of access privileges:

* Owner
* Manager
* Member

The user that creates a workspace is automatically set as the
**Owner**
for that
workspace. The owner can assign any other roles within the workspace, as well as
remove other members of the workspace.

A
**Manager**
within a workspace can assign all roles except
**Owner**
and can
also remove other members of the workspace.

A
**Member**
of a workspace can not assign any access privileges within the
workspace but can otherwise perform any action like running and deploying apps
and modify Secrets.

As an Owner or Manager you can administrate the access privileges of other
members on the members tab in
[settings](/settings)

.

Leaving a Workspace
-------------------

To leave a workspace, navigate to
[the settings page](/settings/workspaces)

and
click “Leave” on a listed Workspace. There must be at least one owner assigned
to a workspace.

[Workspaces](#workspaces)

[Create a Workspace](#create-a-workspace)

[Auto-joining a Workspace associated with a GitHub organization](#auto-joining-a-workspace-associated-with-a-github-organization)

[Inviting new Workspace members](#inviting-new-workspace-members)

[Create a token for a Workspace](#create-a-token-for-a-workspace)

[Switching active Workspace](#switching-active-workspace)

[Dashboard](#dashboard)

[CLI](#cli)

[Administrating workspace members](#administrating-workspace-members)

[Leaving a Workspace](#leaving-a-workspace)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/agent
================================================================================

Build a coding agent with Modal Sandboxes and LangGraph
=======================================================

This example demonstrates how to build an LLM coding “agent” that can generate and execute Python code, using
documentation from the web to inform its approach.

Naturally, we use the agent to generate code that runs language models.

The agent is built with
[LangGraph](https://github.com/langchain-ai/langgraph)

, a library for building
directed graphs of computation popular with AI agent developers,
and uses models from the OpenAI API.

Setup
-----

```
import modal

from .src import edges, nodes, retrieval
from .src.common import COLOR, PYTHON_VERSION, image
```

You will need two
[Modal Secrets](https://modal.com/docs/guide/secrets)

to run this example:
one to access the OpenAI API and another to access the LangSmith API for logging the agent’s behavior.

To create them, head to the
[Secrets dashboard](https://modal.com/secrets)

, select “Create new secret”,
and use the provided templates for OpenAI and LangSmith.

```
app = modal.App(
    "example-code-langchain",
    image=image,
    secrets=[
        modal.Secret.from_name("openai-secret", required_keys=["OPENAI_API_KEY"]),
        modal.Secret.from_name("langsmith-secret", required_keys=["LANGCHAIN_API_KEY"]),
    ],
)
```

Creating a Sandbox
------------------

We execute the agent’s code in a Modal
[Sandbox](https://modal.com/docs/guide/sandbox)

, which allows us to
run arbitrary code in a safe environment. In this example, we will use the
[`transformers`](https://huggingface.co/docs/transformers/index)

library to generate text with a pre-trained model. Let’s create a Sandbox with the necessary dependencies.

```
def create_sandbox(app) -> modal.Sandbox:
    # Change this image (and the retrieval logic in the retrieval module)
    # if you want the agent to give coding advice on other libraries!
    agent_image = modal.Image.debian_slim(python_version=PYTHON_VERSION).pip_install(
        "torch==2.5.0",
        "transformers==4.46.0",
    )

    return modal.Sandbox.create(
        image=agent_image,
        timeout=60 * 10,  # 10 minutes
        app=app,
        # Modal sandboxes support GPUs!
        gpu="T4",
        # you can also pass secrets here -- note that the main app's secrets are not shared
    )
```

We also need a way to run our code in the sandbox. For this, we’ll write a simple wrapper
around the Modal Sandbox
`exec`
method. We use
`exec`
because it allows us to run code without spinning up a
new container. And we can reuse the same container for multiple runs, preserving state.

```
def run(code: str, sb: modal.Sandbox) -> tuple[str, str]:
    print(
        f"{COLOR['HEADER']}📦: Running in sandbox{COLOR['ENDC']}",
        f"{COLOR['GREEN']}{code}{COLOR['ENDC']}",
        sep="\n",
    )

    exc = sb.exec("python", "-c", code)
    exc.wait()

    stdout = exc.stdout.read()
    stderr = exc.stderr.read()

    if exc.returncode != 0:
        print(
            f"{COLOR['HEADER']}📦: Failed with exitcode {sb.returncode}{COLOR['ENDC']}"
        )

    return stdout, stderr
```

Constructing the agent’s graph
------------------------------

Now that we have the sandbox to execute code in, we can construct our agent’s graph. Our graph is
defined in the
`edges`
and
`nodes`
modules
[associated with this example](https://github.com/modal-labs/modal-examples/tree/main/13_sandboxes/codelangchain)

.
Nodes are actions that change the state. Edges are transitions between nodes.

The idea is simple: we start at the node
`generate`
, which invokes the LLM to generate code based off documentation.
The generated code is executed (in the sandbox) as part of an edge called
`check_code_execution`
and then the outputs are passed to the LLM for evaluation (the
`evaluate_execution`
node).
If the LLM determines that the code has executed correctly — which might mean that the code raised an exception! —
we pass along the
`decide_to_finish`
edge and finish.

```
def construct_graph(sandbox: modal.Sandbox, debug: bool = False):
    from langgraph.graph import StateGraph

    from .src.common import GraphState

    # Crawl the transformers documentation to inform our code generation
    context = retrieval.retrieve_docs(debug=debug)

    graph = StateGraph(GraphState)

    # Attach our nodes to the graph
    graph_nodes = nodes.Nodes(context, sandbox, run, debug=debug)
    for key, value in graph_nodes.node_map.items():
        graph.add_node(key, value)

    # Construct the graph by adding edges
    graph = edges.enrich(graph)

    # Set the starting and ending nodes of the graph
    graph.set_entry_point(key="generate")
    graph.set_finish_point(key="finish")

    return graph
```

We now set up the graph and compile it. See the
`src`
module for details
on the content of the graph and the nodes we’ve defined.

```
DEFAULT_QUESTION = "How do I generate Python code using a pre-trained model from the transformers library?"

@app.function()
def go(
    question: str = DEFAULT_QUESTION,
    debug: bool = False,
):
    """Compiles the Python code generation agent graph and runs it, returning the result."""
    sb = create_sandbox(app)

    graph = construct_graph(sb, debug=debug)
    runnable = graph.compile()
    result = runnable.invoke(
        {"keys": {"question": question, "iterations": 0}},
        config={"recursion_limit": 50},
    )

    sb.terminate()

    return result["keys"]["response"]
```

Running the Graph
-----------------

Now let’s call the agent from the command line!

We define a
`local_entrypoint`
that runs locally and triggers execution on Modal.

You can invoke it by executing following command from a folder that contains the
`codelangchain`
directory
[from our examples repo](https://github.com/modal-labs/modal-examples/tree/main/13_sandboxes/codelangchain)

:

```
modal run -m codelangchain.agent --question "How do I run a pre-trained model from the transformers library?"
```

```
@app.local_entrypoint()
def main(
    question: str = DEFAULT_QUESTION,
    debug: bool = False,
):
    """Sends a question to the Python code generation agent.

    Switch to debug mode for shorter context and smaller model."""
    if debug:
        if question == DEFAULT_QUESTION:
            question = "hi there, how are you?"

    print(go.remote(question, debug=debug))
```

If things are working properly, you should see output like the following:

```
$ modal run -m codelangchain.agent --question "generate some cool output with transformers"
---DECISION: FINISH---
---FINISHING---
To generate some cool output using transformers, we can use a pre-trained language model from the Hugging Face Transformers library. In this example, we'll use the GPT-2 model to generate text based on a given prompt. The GPT-2 model is a popular choice for text generation tasks due to its ability to produce coherent and contextually relevant text. We'll use the pipeline API from the Transformers library, which simplifies the process of using pre-trained models for various tasks, including text generation.

from transformers import pipeline
# Initialize the text generation pipeline with the GPT-2 model
generator = pipeline('text-generation', model='gpt2')

# Define a prompt for the model to generate text from
prompt = "Once upon a time in a land far, far away"

# Generate text using the model
output = generator(prompt, max_length=50, num_return_sequences=1)

# Print the generated text
print(output[0]['generated_text'])

Result of code execution:
Once upon a time in a land far, far away, and still inhabited even after all the human race, there would be one God: a perfect universal God who has always been and will ever be worshipped. All His acts and deeds are immutable,
```

[Build a coding agent with Modal Sandboxes and LangGraph](#build-a-coding-agent-with-modal-sandboxes-and-langgraph)

[Setup](#setup)

[Creating a Sandbox](#creating-a-sandbox)

[Constructing the agent’s graph](#constructing-the-agents-graph)

[Running the Graph](#running-the-graph)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run -m 13_sandboxes.codelangchain.agent --question 'Use gpt2 and transformers to generate text'
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/algolia_indexer
================================================================================

Algolia docsearch crawler
=========================

This tutorial shows you how to use Modal to run the
[Algolia docsearch
crawler](https://docsearch.algolia.com/docs/legacy/run-your-own/)

to index your
website and make it searchable. This is not just example code - we run the same
code in production to power search on this page (
`Ctrl+K`
to try it out!).

Basic setup
-----------

Let’s get the imports out of the way.

```
import json
import os
import subprocess

import modal
```

Modal lets you
[use and extend existing Docker images](https://modal.com/docs/guide/custom-container#use-an-existing-container-image-with-from_registry)

,
as long as they have
`python`
and
`pip`
available. We’ll use the official crawler image built by Algolia, with a small
adjustment: since this image has
`python`
symlinked to
`python3.6`
and Modal is not compatible with Python 3.6, we
install Python 3.11 and symlink that as the
`python`
executable instead.

```
algolia_image = modal.Image.from_registry(
    "algolia/docsearch-scraper:v1.16.0",
    add_python="3.11",
    setup_dockerfile_commands=["ENTRYPOINT []"],
)

app = modal.App("example-algolia-indexer")
```

Configure the crawler
---------------------

Now, let’s configure the crawler with the website we want to index, and which
CSS selectors we want to scrape. Complete documentation for crawler configuration is available
[here](https://docsearch.algolia.com/docs/legacy/config-file)

.

```
CONFIG = {
    "index_name": "modal_docs",
    "custom_settings": {
        "separatorsToIndex": "._",
        "synonyms": [["cls", "class"]],
    },
    "stop_urls": [
        "https://modal.com/docs/reference/modal.Stub",
        "https://modal.com/gpu-glossary",
        "https://modal.com/docs/reference/changelog",
    ],
    "start_urls": [
        {
            "url": "https://modal.com/docs/guide",
            "selectors_key": "default",
            "page_rank": 2,
        },
        {
            "url": "https://modal.com/docs/examples",
            "selectors_key": "examples",
            "page_rank": 1,
        },
        {
            "url": "https://modal.com/docs/reference",
            "selectors_key": "reference",
            "page_rank": 1,
        },
    ],
    "selectors": {
        "default": {
            "lvl0": {
                "selector": "header .navlink-active",
                "global": True,
            },
            "lvl1": "article h1",
            "lvl2": "article h2",
            "lvl3": "article h3",
            "text": "article p,article ol,article ul",
        },
        "examples": {
            "lvl0": {
                "selector": "header .navlink-active",
                "global": True,
            },
            "lvl1": "article h1",
            "text": "article p,article ol,article ul",
        },
        "reference": {
            "lvl0": {
                "selector": "//div[contains(@class, 'sidebar')]//a[contains(@class, 'active')]//preceding::a[contains(@class, 'header')][1]",
                "type": "xpath",
                "global": True,
                "default_value": "",
                "skip": {"when": {"value": ""}},
            },
            "lvl1": "article h1",
            "lvl2": "article h2",
            "lvl3": "article h3",
            "text": "article p,article ol,article ul",
        },
    },
}
```

Create an API key
-----------------

If you don’t already have one, sign up for an account on
[Algolia](https://www.algolia.com/)

. Set up
a project and create an API key with
`write`
access to your index, and with the ACL permissions
`addObject`
,
`editSettings`
and
`deleteIndex`
. Now, create a Secret on the Modal
[Secrets](https://modal.com/secrets)

page with the
`API_KEY`
and
`APPLICATION_ID`
you just created. You can name this anything you want,
but we named it
`algolia-secret`
and so that’s what the code below expects.

The actual function
-------------------

We want to trigger our crawler from our CI/CD pipeline, so we’re serving it as a
[web endpoint](https://modal.com/docs/guide/webhooks)

that can be triggered by a
`GET`
request during deploy.
You could also consider running the crawler on a
[schedule](https://modal.com/docs/guide/cron)

.

The Algolia crawler is written for Python 3.6 and needs to run in the
`pipenv`
created for it,
so we’re invoking it using a subprocess.

```
@app.function(
    image=algolia_image,
    secrets=[modal.Secret.from_name("algolia-secret")],
)
def crawl():
    # Installed with a 3.6 venv; Python 3.6 is unsupported by Modal, so use a subprocess instead.
    subprocess.run(
        ["pipenv", "run", "python", "-m", "src.index"],
        env={**os.environ, "CONFIG": json.dumps(CONFIG)},
    )
```

We want to be able to trigger this function through a webhook.

```
@app.function(image=modal.Image.debian_slim().pip_install("fastapi[standard]"))
@modal.fastapi_endpoint()
def crawl_webhook():
    crawl.remote()
    return "Finished indexing docs"
```

Deploy the indexer
------------------

That’s all the code we need! To deploy your application, run

```
modal deploy algolia_indexer.py
```

If successful, this will print a URL for your new webhook, that you can hit using
`curl`
or a browser. Logs from webhook invocations can be found from the
[apps](https://modal.com/apps)

page.

The indexed contents can be found at
<https://www.algolia.com/apps/APP_ID/explorer/browse/>

, for your
APP\_ID. Once you’re happy with the results, you can
[set up the
`docsearch`
package with your
website](https://docsearch.algolia.com/docs/docsearch-v3/)

, and create a search component that uses this index.

Entrypoint for development
--------------------------

To make it easier to test this, we also have an entrypoint for when you run
`modal run algolia_indexer.py`

```
@app.local_entrypoint()
def run():
    crawl.remote()
```

[Algolia docsearch crawler](#algolia-docsearch-crawler)

[Basic setup](#basic-setup)

[Configure the crawler](#configure-the-crawler)

[Create an API key](#create-an-api-key)

[The actual function](#the-actual-function)

[Deploy the indexer](#deploy-the-indexer)

[Entrypoint for development](#entrypoint-for-development)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/algolia_indexer.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/amazon_embeddings
================================================================================

Embed 30 million Amazon reviews at 575k tokens per second with Qwen2-7B
=======================================================================

This example demonstrates how to create embeddings for a large text dataset. This is
often necessary to enable semantic search, translation, and other language
processing tasks. Modal makes it easy to deploy large, capable embedding models and handles
all of the scaling to process very large datasets in parallel on many cloud GPUs.

We create a Modal Function that will handle all of the data loading and submit inputs to an
inference Cls that will automatically scale up to handle hundreds of large
batches in parallel.

Between the time a batch is submitted and the time it is fetched, it is stored via
Modal’s
`spawn`
system, which can hold onto up to one million inputs for up to a week.

```
import json
import subprocess
from pathlib import Path

import modal

app = modal.App(name="example-amazon-embeddings")
MINUTES = 60  # seconds
HOURS = 60 * MINUTES
```

We define our
`main`
function as a
`local_entrypoint`
. This is what we’ll call locally
to start the job on Modal.

You can run it with the command

```
modal run --detach amazon_embeddings.py
```

By default we
`down-scale`
to 1/100th of the data for demonstration purposes.
To launch the full job, set the
`--down-scale`
parameter to
`1`
.
But note that this will cost you!

The entrypoint starts the job and gets back a
`f`
unction
`c`
all ID for each batch.
We can use these IDs to retrieve the embeddings once the job is finished.
Modal will keep the results around for up to 7 days after completion. Take a look at our
[job processing guide](https://modal.com/docs/guide/job-queue)

for more details.

```
@app.local_entrypoint()
def main(
    dataset_name: str = "McAuley-Lab/Amazon-Reviews-2023",
    dataset_subset: str = "raw_review_Books",
    down_scale: float = 0.001,
):
    out_path = Path("/tmp") / "embeddings-example-fc-ids.json"
    function_ids = launch_job.remote(
        dataset_name=dataset_name, dataset_subset=dataset_subset, down_scale=down_scale
    )
    out_path.write_text(json.dumps(function_ids, indent=2) + "\n")
    print(f"output handles saved to {out_path}")
```

Load the data and start the inference job
-----------------------------------------

Next we define the Function that will do the data loading and feed it to our embedding model.
We define a container
[Image](https://modal.com/docs/guide/images)

with the data loading dependencies.

In it, we download the data we need and cache it to the container’s local disk,
which will disappear when the job is finished. We will be saving the review data
along with the embeddings, so we don’t need to keep the dataset around.

Embedding a large dataset like this can take some time, but we don’t need to wait
around for it to finish. We use
`spawn`
to invoke our embedding Function
and get back a handle with an ID that we can use to get the results later.
This can bottleneck on just sending data over the network for processing, so
we speed things up by using
`ThreadPoolExecutor`
to submit batches using multiple threads.

Once all of the batches have been sent for inference, we can return the function IDs
to the local client to save.

```
@app.function(
    image=modal.Image.debian_slim().pip_install("datasets==3.5.1"), timeout=2 * HOURS
)
def launch_job(dataset_name: str, dataset_subset: str, down_scale: float):
    import time
    from concurrent.futures import ThreadPoolExecutor, as_completed

    from datasets import load_dataset
    from tqdm import tqdm

    print("Loading dataset...")
    dataset = load_dataset(
        dataset_name,
        dataset_subset,
        split="full",
        trust_remote_code=True,
    )

    data_subset = dataset.select(range(int(len(dataset) * down_scale)))

    tei = TextEmbeddingsInference()
    batches = generate_batches_of_chunks(data_subset)

    start = time.perf_counter()
    with ThreadPoolExecutor() as executor:
        futures = [executor.submit(tei.embed.spawn, batch) for batch in tqdm(batches)]
        function_ids = []
        for future in tqdm(as_completed(futures), total=len(futures)):
            function_ids.append(future.result().object_id)

    print(f"Finished submitting job: {time.perf_counter() - start:.2f}s")

    return function_ids
```

Massively scaling up and scaling out embedding inference on many beefy GPUs
---------------------------------------------------------------------------

We’re going to spin up many containers to run inference, and we don’t want each
one to have to download the embedding model from Hugging Face. We can download and save it to a
Modal
[Volume](https://modal.com/docs/guide/volumes)

during the image build step using
`run_function`
.

We’ll use the
[GTE-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)

model from Alibaba, which performs well on the
[Massive Text Embedding Benchmark](https://huggingface.co/spaces/mteb/leaderboard)

.

```
MODEL_ID = "Alibaba-NLP/gte-Qwen2-7B-instruct"
MODEL_DIR = "/model"
MODEL_CACHE_VOLUME = modal.Volume.from_name(
    "embeddings-example-model-cache", create_if_missing=True
)

def download_model():
    from huggingface_hub import snapshot_download

    snapshot_download(MODEL_ID, cache_dir=MODEL_DIR)
```

For inference, we will use Hugging Face’s
[Text Embeddings Inference](https://github.com/huggingface/text-embeddings-inference)

framework for embedding model deployment.

Running lots of separate machines is “scaling out”. But we can also “scale up”
by running on large, high-performance machines.

We’ll use L40S GPUs for a good balance between cost and performance. Hugging Face has
prebuilt Docker images we can use as a base for our Modal Image.
We’ll use the one built for the L40S’s
[SM89/Ada Lovelace architecture](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

and install the rest of our dependencies on top.

```
tei_image = "ghcr.io/huggingface/text-embeddings-inference:89-1.7"

inference_image = (
    modal.Image.from_registry(tei_image, add_python="3.12")
    .dockerfile_commands("ENTRYPOINT []")
    .pip_install(
        "httpx==0.28.1",
        "huggingface_hub[hf_transfer]==0.30.2",
        "numpy==2.2.5",
        "tqdm==4.67.1",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HOME": MODEL_DIR})
    .run_function(download_model, volumes={MODEL_DIR: MODEL_CACHE_VOLUME})
)
```

Next we define our inference class. Modal will auto-scale the number of
containers ready to handle inputs based on the parameters we set in the
`@app.cls`
and
`@modal.concurrent`
decorators. Here we limit the total number of containers to
100 and the maximum number of concurrent inputs to 10, which caps us at 1000 concurrent batches.
On Modal’s Starter (free) and Team plans, the maximum number of concurrent GPUs is lower,
reducing the total number of concurrent batches and so the throughput.

Customers on Modal’s Enterprise Plan regularly scale up another order of magnitude above this.
If you’re interested in running on thousands of GPUs,
[get in touch](https://form.fillout.com/t/onUBuQZ5vCus)

.

Here we also specify the GPU type and attach the Modal Volume where we saved the
embedding model.

This class will spawn a local Text Embeddings Inference server when the container
starts, and process each batch by receiving the text data over HTTP, returning a list of
tuples with the batch text data and embeddings.

```
@app.cls(
    image=inference_image,
    gpu="L40S",
    volumes={MODEL_DIR: MODEL_CACHE_VOLUME},
    max_containers=100,
    scaledown_window=5 * MINUTES,  # idle for 5 min without inputs before scaling down
    retries=3,  # handle transient failures and storms in the cloud
    timeout=2 * HOURS,  # run for at most 2 hours
)
@modal.concurrent(max_inputs=10)
class TextEmbeddingsInference:
    @modal.enter()
    def open_connection(self):
        from httpx import AsyncClient

        print("Starting text embedding inference server...")
        self.process = spawn_server()
        self.client = AsyncClient(base_url="http://127.0.0.1:8000", timeout=30)

    @modal.exit()
    def terminate_connection(self):
        self.process.terminate()

    @modal.method()
    async def embed(self, batch):
        texts = [chunk[-1] for chunk in batch]
        res = await self.client.post("/embed", json={"inputs": texts})
        return [chunk + (embedding,) for chunk, embedding in zip(batch, res.json())]
```

Helper Functions
----------------

The book review dataset contains ~30M reviews with ~12B total characters,
indicating an average review length of ~500 characters. Some are much longer.
Embedding models have a limit on the number of tokens they can process in a single
input. We will need to split each review into chunks that are under this limit.

The proper way to split text data is to use a tokenizer to ensure that any
single request is under the models token limit, and to overlap chunks to provide
semantic context and preserve information. For the sake of this example, we’re going
just to split by a set character length (
`CHUNK_SIZE`
).

While the embedding model has a limit on the number of input tokens for a single
embedding, the number of chunks that we can process in a single batch is limited by
the VRAM of the GPU. We set the
`BATCH_SIZE`
accordingly.

```
BATCH_SIZE = 256
CHUNK_SIZE = 512

def generate_batches_of_chunks(
    dataset, chunk_size: int = CHUNK_SIZE, batch_size: int = BATCH_SIZE
):
    """Creates batches of chunks by naively slicing strings according to CHUNK_SIZE."""
    batch = []
    for entry_index, data in enumerate(dataset):
        product_id = data["asin"]
        user_id = data["user_id"]
        timestamp = data["timestamp"]
        title = data["title"]
        text = data["text"]
        for chunk_index, chunk_start in enumerate(range(0, len(text), chunk_size)):
            batch.append(
                (
                    entry_index,
                    chunk_index,
                    product_id,
                    user_id,
                    timestamp,
                    title,
                    text[chunk_start : chunk_start + chunk_size],
                )
            )
            if len(batch) == batch_size:
                yield batch
                batch = []
    if batch:
        yield batch

def spawn_server(
    model_id: str = MODEL_ID,
    port: int = 8000,
    max_client_batch_size: int = BATCH_SIZE,
    max_batch_tokens: int = BATCH_SIZE * CHUNK_SIZE,
    huggingface_hub_cache: str = MODEL_DIR,
):
    """Starts a text embedding inference server in a subprocess."""
    import socket

    LAUNCH_FLAGS = [
        "--model-id",
        model_id,
        "--port",
        str(port),
        "--max-client-batch-size",
        str(max_client_batch_size),
        "--max-batch-tokens",
        str(max_batch_tokens),
        "--huggingface-hub-cache",
        huggingface_hub_cache,
    ]

    process = subprocess.Popen(["text-embeddings-router"] + LAUNCH_FLAGS)
    # Poll until webserver at 127.0.0.1:8000 accepts connections before running inputs.
    while True:
        try:
            socket.create_connection(("127.0.0.1", port), timeout=1).close()
            print("Inference server ready!")
            return process
        except (socket.timeout, ConnectionRefusedError):
            retcode = process.poll()  # Check if the process has terminated.
            if retcode is not None:
                raise RuntimeError(f"Launcher exited unexpectedly with code {retcode}")
```

[Embed 30 million Amazon reviews at 575k tokens per second with Qwen2-7B](#embed-30-million-amazon-reviews-at-575k-tokens-per-second-with-qwen2-7b)

[Load the data and start the inference job](#load-the-data-and-start-the-inference-job)

[Massively scaling up and scaling out embedding inference on many beefy GPUs](#massively-scaling-up-and-scaling-out-embedding-inference-on-many-beefy-gpus)

[Helper Functions](#helper-functions)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run --detach 06_gpu_and_ml/embeddings/amazon_embeddings.py --dataset-subset raw_review_Magazine_Subscriptions
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/anthropic_computer_use
================================================================================

Run Anthropic’s computer use demo in a Modal Sandbox
====================================================

This example demonstrates how to run Anthropic’s
[Computer Use demo](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo)

in a Modal
[Sandbox](https://modal.com/docs/guide/sandbox)

.

Sandbox Setup
-------------

All Sandboxes are associated with an App.

We start by looking up an existing App by name, or creating one if it doesn’t exist.

```
import time
import urllib.request

import modal
import modal.experimental

app = modal.App.lookup("example-computer-use", create_if_missing=True)
```

The Computer Use
[quickstart](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo)

provides a prebuilt Docker image. We use this hosted image to create our sandbox environment.

```
sandbox_image = (
    modal.experimental.raw_registry_image(
        "ghcr.io/anthropics/anthropic-quickstarts:computer-use-demo-latest",
    )
    .env({"WIDTH": "1920", "HEIGHT": "1080"})
    .workdir("/home/computeruse")
    .entrypoint([])
)
```

We’ll provide the Anthropic API key via a Modal
[Secret](https://modal.com/docs/guide/secrets)

which the sandbox can access at runtime.

```
secret = modal.Secret.from_name("anthropic-secret", required_keys=["ANTHROPIC_API_KEY"])
```

Now, we can start our Sandbox.
We use
`modal.enable_output()`
to print the Sandbox’s image build logs to the console.
We’ll also expose the ports required for the demo’s interfaces:

* Port 8501 serves the Streamlit UI for interacting with the agent loop
* Port 6080 serves the VNC desktop view via a browser-based noVNC client

```
with modal.enable_output():
    sandbox = modal.Sandbox.create(
        "sudo",
        "--preserve-env=ANTHROPIC_API_KEY,DISPLAY_NUM,WIDTH,HEIGHT,PATH",
        "-u",
        "computeruse",
        "./entrypoint.sh",
        app=app,
        image=sandbox_image,
        secrets=[secret],
        encrypted_ports=[8501, 6080],
        timeout=60 * 60,  # stay alive for one hour, maximum one day
    )

print(f"🏖️  Sandbox ID: {sandbox.object_id}")
```

After starting the sandbox, we retrieve the public URLs for the exposed ports.

```
tunnels = sandbox.tunnels()
for port, tunnel in tunnels.items():
    print(f"Waiting for service on port {port} to start at {tunnel.url}")
```

We can check on each server’s status by making an HTTP request to the server’s URL
and verifying that it responds with a 200 status code.

```
def is_server_up(url):
    try:
        response = urllib.request.urlopen(url)
        return response.getcode() == 200
    except Exception:
        return False

timeout = 60  # seconds
start_time = time.time()
up_ports = set()
while time.time() - start_time < timeout:
    for port, tunnel in tunnels.items():
        if port not in up_ports and is_server_up(tunnel.url):
            print(f"🏖️  Server is up and running on port {port}!")
            up_ports.add(port)
    if len(up_ports) == len(tunnels):
        break
    time.sleep(1)
else:
    print("🏖️  Timed out waiting for server to start.")
```

You can now open the URLs in your browser to interact with the demo!
Note: The sandbox logs may mention
`localhost:8080`
.
Ignore this and use the printed tunnel URLs instead.

When finished, you can terminate the sandbox from your
[Modal dashboard](https://modal.com/containers)

or by running
`Sandbox.from_id(sandbox.object_id).terminate()`
.
The Sandbox will also spin down after one hour.

[Run Anthropic’s computer use demo in a Modal Sandbox](#run-anthropics-computer-use-demo-in-a-modal-sandbox)

[Sandbox Setup](#sandbox-setup)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
python 13_sandboxes/anthropic_computer_use.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/basic_web
================================================================================

Hello world wide web!
=====================

Modal makes it easy to turn your Python functions into serverless web services:
access them via a browser or call them from any client that speaks HTTP, all
without having to worry about setting up servers or managing infrastructure.

This tutorial shows the path with the shortest
[“time to 200”](https://shkspr.mobi/blog/2021/05/whats-your-apis-time-to-200/)

:
[`modal.fastapi_endpoint`](https://modal.com/docs/reference/modal.fastapi_endpoint)

.

On Modal, web endpoints have all the superpowers of Modal Functions:
they can be
[accelerated with GPUs](https://modal.com/docs/guide/gpu)

,
they can access
[Secrets](https://modal.com/docs/guide/secrets)

or
[Volumes](https://modal.com/docs/guide/volumes)

,
and they
[automatically scale](https://modal.com/docs/guide/cold-start)

to handle more traffic.

Under the hood, we use the
[FastAPI library](https://fastapi.tiangolo.com/)

,
which has
[high-quality documentation](https://fastapi.tiangolo.com/tutorial/)

,
linked throughout this tutorial.

Turn a Modal Function into an API endpoint with a single decorator
------------------------------------------------------------------

Modal Functions are already accessible remotely — when you add the
`@app.function`
decorator to a Python function
and run
`modal deploy`
, you make it possible for your
[other Python functions to call it](https://modal.com/docs/guide/trigger-deployed-functions)

.

That’s great, but it’s not much help if you want to share what you’ve written with someone running code in a different language —
or not running code at all!

And that’s where most of the power of the Internet comes from: sharing information and functionality across different computer systems.

So we provide the
`fastapi_endpoint`
decorator to wrap your Modal Functions in the lingua franca of the web: HTTP.
Here’s what that looks like:

```
import modal

image = modal.Image.debian_slim().pip_install("fastapi[standard]")
app = modal.App(name="example-lifecycle-web", image=image)

@app.function()
@modal.fastapi_endpoint(
    docs=True  # adds interactive documentation in the browser
)
def hello():
    return "Hello world!"
```

You can turn this function into a web endpoint by running
`modal serve basic_web.py`
.
In the output, you should see a URL that ends with
`hello-dev.modal.run`
.
If you navigate to this URL, you should see the
`"Hello world!"`
message appear in your browser.

You can also find interactive documentation, powered by OpenAPI and Swagger,
if you add
`/docs`
to the end of the URL.
From this documentation, you can interact with your endpoint, sending HTTP requests and receiving HTTP responses.
For more details, see the
[FastAPI documentation](https://fastapi.tiangolo.com/features/#automatic-docs)

.

By running the endpoint with
`modal serve`
, you created a temporary endpoint that will disappear if you interrupt your terminal.
These temporary endpoints are great for debugging — when you save a change to any of your dependent files, the endpoint will redeploy.
Try changing the message to something else, hitting save, and then hitting refresh in your browser or re-sending
the request from
`/docs`
or the command line. You should see the new message, along with logs in your terminal showing the redeploy and the request.

When you’re ready to deploy this endpoint permanently, run
`modal deploy basic_web.py`
.
Now, your function will be available even when you’ve closed your terminal or turned off your computer.

Send data to a web endpoint
---------------------------

The web endpoint above was a bit silly: it always returns the same message.

Most endpoints need an input to be useful. There are two ways to send data to a web endpoint:

* in the URL as a
  [query parameter](#sending-data-in-query-parameters)
* in the
  [body of the request](#sending-data-in-the-request-body)

  as JSON

### Sending data in query parameters

By default, your function’s arguments are treated as query parameters:
they are extracted from the end of the URL, where they should be added in the form
`?arg1=foo&arg2=bar`
.

From the Python side, there’s hardly anything to do:

```
@app.function()
@modal.fastapi_endpoint(docs=True)
def greet(user: str) -> str:
    return f"Hello {user}!"
```

If you are already running
`modal serve basic_web.py`
, this endpoint will be available at a URL, printed in your terminal, that ends with
`greet-dev.modal.run`
.

We provide Python type-hints to get type information in the docs and
[automatic validation](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/)

.
For example, if you navigate directly to the URL for
`greet`
, you will get a detailed error message
indicating that the
`user`
parameter is missing. Navigate instead to
`/docs`
to see how to invoke the endpoint properly.

You can read more about query parameters in the
[FastAPI documentation](https://fastapi.tiangolo.com/tutorial/query-params/)

.

### Sending data in the request body

For larger and more complex data, it is generally preferrable to send data in the body of the HTTP request.
This body is formatted as
[JSON](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON)

,
the most common data interchange format on the web.

To set up an endpoint that accepts JSON data, add an argument with a
`dict`
type-hint to your function.
This argument will be populated with the data sent in the request body.

```
@app.function()
@modal.fastapi_endpoint(method="POST", docs=True)
def goodbye(data: dict) -> str:
    name = data.get("name") or "world"
    return f"Goodbye {name}!"
```

Note that we gave a value of
`"POST"`
for the
`method`
argument here.
This argument defines the HTTP request method that the endpoint will respond to,
and it defaults to
`"GET"`
.
If you head to the URL for the
`goodbye`
endpoint in your browser,
you will get a 405 Method Not Allowed error, because browsers only send GET requests by default.
While this is technically a separate concern from query parameters versus request bodies
and you can define an endpoint that accepts GET requests and uses data from the body,
it is
[considered bad form](https://stackoverflow.com/a/983458)

.

Navigate to
`/docs`
for more on how to invoke the endpoint properly.
You will need to send a POST request with a JSON body containing a
`name`
key.
To get the same typing and validation benefits as with query parameters,
use a
[Pydantic model](https://fastapi.tiangolo.com/tutorial/body/)

for this argument.

You can read more about request bodies in the
[FastAPI documentation](https://fastapi.tiangolo.com/tutorial/body/)

.

Handle expensive startup with
`modal.Cls`
-----------------------------------------

Sometimes your endpoint needs to do something before it can handle its first request,
like get a value from a database or set the value of a variable.
If that step is expensive, like
[loading a large ML model](https://modal.com/docs/guide/model-weights)

,
it’d be a shame to have to do it every time a request comes in!

Web endpoints can be methods on a
[`modal.Cls`](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-functions-and-parameters)

,
which allows you to manage the container’s lifecycle independently from processing individual requests.

This example will only set the
`start_time`
instance variable once, on container startup.

```
@app.cls()
class WebApp:
    @modal.enter()
    def startup(self):
        from datetime import datetime, timezone

        print("🏁 Starting up!")
        self.start_time = datetime.now(timezone.utc)

    @modal.fastapi_endpoint(docs=True)
    def web(self):
        from datetime import datetime, timezone

        current_time = datetime.now(timezone.utc)
        return {"start_time": self.start_time, "current_time": current_time}
```

Protect web endpoints with proxy authentication
-----------------------------------------------

Sharing your Python functions on the web is great, but it’s not always a good idea
to make those functions available to just anyone.

For example, you might have a function like the one below that
is more expensive to run than to call (and so might be abused by your enemies)
or reveals information that you would rather keep secret.

To protect your Modal web endpoints so that they can’t be triggered except
by members of your
[Modal workspace](https://modal.com/docs/guide/workspaces)

,
add the
`requires_proxy_auth=True`
flag to the
`fastapi_endpoint`
decorator.

```
@app.function(gpu="h100")
@modal.fastapi_endpoint(requires_proxy_auth=True, docs=False)
def expensive_secret():
    return "I didn't care for 'The Godfather'. It insists upon itself."
```

The
`expensive-secret`
endpoint URL will still be printed to the output when you
`modal serve`
or
`modal deploy`
,
along with a ”🔑” emoji indicating that it is secured with proxy authentication.
If you head to that URL via the browser, you will get a
[`401 Unauthorized`](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/401)

error code in response.
You should also check the dashboard page for this app (at the URL printed at the very top of the
`modal`
command output)
so you can see that no containers were spun up to handle the request — this authorization is handled entirely inside Modal’s infrastructure.

You can trigger the web endpoint by
[creating a Proxy Auth Token](https://modal.com/settings/proxy-auth-tokens)

and then including the token ID and secret in the
`Modal-Key`
and
`Modal-Secret`
headers.

From the command line, that might look like

```
export TOKEN_ID=wk-1234abcd
export TOKEN_SECRET=ws-1234abcd
curl -H "Modal-Key: $TOKEN_ID" \
     -H "Modal-Secret: $TOKEN_SECRET" \
     https://your-workspace-name--expensive-secret.modal.run
```

For more details, see the
[guide to proxy authentication](https://modal.com/docs/guide/webhook-proxy-auth)

.

What next?
----------

Modal’s
`fastapi_endpoint`
decorator is opinionated and designed for relatively simple web applications —
one or a few independent Python functions that you want to expose to the web.

Three additional decorators allow you to serve more complex web applications with greater control:

* [`asgi_app`](https://modal.com/docs/guide/webhooks#asgi)

  to serve applications compliant with the ASGI standard,
  like
  [FastAPI](https://fastapi.tiangolo.com/)
* [`wsgi_app`](https://modal.com/docs/guide/webhooks#wsgi)

  to serve applications compliant with the WSGI standard,
  like
  [Flask](https://flask.palletsprojects.com/)
* [`web_server`](https://modal.com/docs/guide/webhooks#non-asgi-web-servers)

  to serve any application that listens on a port

[Hello world wide web!](#hello-world-wide-web)

[Turn a Modal Function into an API endpoint with a single decorator](#turn-a-modal-function-into-an-api-endpoint-with-a-single-decorator)

[Send data to a web endpoint](#send-data-to-a-web-endpoint)

[Sending data in query parameters](#sending-data-in-query-parameters)

[Sending data in the request body](#sending-data-in-the-request-body)

[Handle expensive startup with modal.Cls](#handle-expensive-startup-with-modalcls)

[Protect web endpoints with proxy authentication](#protect-web-endpoints-with-proxy-authentication)

[What next?](#what-next)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 07_web_endpoints/basic_web.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/batched_whisper
================================================================================

Fast Whisper inference using dynamic batching
=============================================

In this example, we demonstrate how to run
[dynamically batched inference](https://modal.com/docs/guide/dynamic-batching)

for OpenAI’s speech recognition model,
[Whisper](https://openai.com/index/whisper/)

, on Modal.
Batching multiple audio samples together or batching chunks of a single audio sample can help to achieve a 2.8x increase
in inference throughput on an A10G!

We will be running the
[Whisper Large V3](https://huggingface.co/openai/whisper-large-v3)

model.
To run
[any of the other HuggingFace Whisper models](https://huggingface.co/models?search=openai/whisper)

,
simply replace the
`MODEL_NAME`
and
`MODEL_REVISION`
variables.

Setup
-----

Let’s start by importing the Modal client and defining the model that we want to serve.

```
from typing import Optional

import modal

MODEL_DIR = "/model"
MODEL_NAME = "openai/whisper-large-v3"
MODEL_REVISION = "afda370583db9c5359511ed5d989400a6199dfe1"
```

Define a container image
------------------------

We’ll start with Modal’s baseline
`debian_slim`
image and install the relevant libraries.

```
image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "torch==2.5.1",
        "transformers==4.47.1",
        "hf-transfer==0.1.8",
        "huggingface_hub==0.27.0",
        "librosa==0.10.2",
        "soundfile==0.12.1",
        "accelerate==1.2.1",
        "datasets==3.2.0",
    )
    # Use the barebones `hf-transfer` package for maximum download speeds. No progress bar, but expect 700MB/s.
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HUB_CACHE": MODEL_DIR})
)

model_cache = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)
app = modal.App(
    "example-batched-whisper",
    image=image,
    volumes={MODEL_DIR: model_cache},
)
```

Caching the model weights
-------------------------

We’ll define a function to download the model and cache it in a volume.
You can
`modal run`
against this function prior to deploying the App.

```
@app.function()
def download_model():
    from huggingface_hub import snapshot_download
    from transformers.utils import move_cache

    snapshot_download(
        MODEL_NAME,
        ignore_patterns=["*.pt", "*.bin"],  # Using safetensors
        revision=MODEL_REVISION,
    )
    move_cache()
```

The model class
---------------

The inference function is best represented using Modal’s
[class syntax](https://modal.com/docs/guide/lifecycle-functions)

.

We define a
`@modal.enter`
method to load the model when the container starts, before it picks up any inputs.
The weights will be loaded from the Hugging Face cache volume so that we don’t need to download them when
we start a new container.

We also define a
`transcribe`
method that uses the
`@modal.batched`
decorator to enable dynamic batching.
This allows us to invoke the function with individual audio samples, and the function will automatically batch them
together before running inference. Batching is critical for making good use of the GPU, since GPUs are designed
for running parallel operations at high throughput.

The
`max_batch_size`
parameter limits the maximum number of audio samples combined into a single batch.
We used a
`max_batch_size`
of
`64`
, the largest power-of-2 batch size that can be accommodated by the 24 A10G GPU memory.
This number will vary depending on the model and the GPU you are using.

The
`wait_ms`
parameter sets the maximum time to wait for more inputs before running the batched transcription.
To tune this parameter, you can set it to the target latency of your application minus the execution time of an inference batch.
This allows the latency of any request to stay within your target latency.

```
@app.cls(
    gpu="a10g",  # Try using an A100 or H100 if you've got a large model or need big batches!
    max_containers=10,  # default max GPUs for Modal's free tier
)
class Model:
    @modal.enter()
    def load_model(self):
        import torch
        from transformers import (
            AutoModelForSpeechSeq2Seq,
            AutoProcessor,
            pipeline,
        )

        self.processor = AutoProcessor.from_pretrained(MODEL_NAME)
        self.model = AutoModelForSpeechSeq2Seq.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16,
            low_cpu_mem_usage=True,
            use_safetensors=True,
        ).to("cuda")

        self.model.generation_config.language = "<|en|>"

        # Create a pipeline for preprocessing and transcribing speech data
        self.pipeline = pipeline(
            "automatic-speech-recognition",
            model=self.model,
            tokenizer=self.processor.tokenizer,
            feature_extractor=self.processor.feature_extractor,
            torch_dtype=torch.float16,
            device="cuda",
        )

    @modal.batched(max_batch_size=64, wait_ms=1000)
    def transcribe(self, audio_samples):
        import time

        start = time.monotonic_ns()
        print(f"Transcribing {len(audio_samples)} audio samples")
        transcriptions = self.pipeline(audio_samples, batch_size=len(audio_samples))
        end = time.monotonic_ns()
        print(
            f"Transcribed {len(audio_samples)} samples in {round((end - start) / 1e9, 2)}s"
        )
        return transcriptions
```

Transcribe a dataset
--------------------

In this example, we use the
[librispeech\_asr\_dummy dataset](https://huggingface.co/datasets/hf-internal-testing/librispeech_asr_dummy)

from Hugging Face’s Datasets library to test the model.

We use
[`map.aio`](https://modal.com/docs/reference/modal.Function#map)

to asynchronously map over the audio files.
This allows us to invoke the batched transcription method on each audio sample in parallel.

```
@app.function()
async def transcribe_hf_dataset(dataset_name):
    from datasets import load_dataset

    print("📂 Loading dataset", dataset_name)
    ds = load_dataset(dataset_name, "clean", split="validation")
    print("📂 Dataset loaded")
    batched_whisper = Model()
    print("📣 Sending data for transcription")
    async for transcription in batched_whisper.transcribe.map.aio(ds["audio"]):
        yield transcription
```

Run the model
-------------

We define a
[`local_entrypoint`](https://modal.com/docs/guide/apps#entrypoints-for-ephemeral-apps)

to run the transcription. You can run this locally with
`modal run batched_whisper.py`
.

```
@app.local_entrypoint()
async def main(dataset_name: Optional[str] = None):
    if dataset_name is None:
        dataset_name = "hf-internal-testing/librispeech_asr_dummy"
    for result in transcribe_hf_dataset.remote_gen(dataset_name):
        print(result["text"])
```

[Fast Whisper inference using dynamic batching](#fast-whisper-inference-using-dynamic-batching)

[Setup](#setup)

[Define a container image](#define-a-container-image)

[Caching the model weights](#caching-the-model-weights)

[The model class](#the-model-class)

[Transcribe a dataset](#transcribe-a-dataset)

[Run the model](#run-the-model)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/speech-to-text/batched_whisper.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/blender_video
================================================================================

Render a video with Blender on many GPUs or CPUs in parallel
============================================================

This example shows how you can render an animated 3D scene using
[Blender](https://www.blender.org/)

’s Python interface.

You can run it on CPUs to scale out on one hundred containers
or run it on GPUs to get higher throughput per node.
Even for this simple scene, GPUs render >10x faster than CPUs.

The final render looks something like this:

[

](https://modal-cdn.com/modal-blender-video.mp4)

Defining a Modal app
--------------------

```
from pathlib import Path

import modal
```

Modal runs your Python functions for you in the cloud.
You organize your code into apps, collections of functions that work together.

```
app = modal.App("examples-blender-video")
```

We need to define the environment each function runs in — its container image.
The block below defines a container image, starting from a basic Debian Linux image
adding Blender’s system-level dependencies
and then installing the
`bpy`
package, which is Blender’s Python API.

```
rendering_image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("xorg", "libxkbcommon0")  # X11 (Unix GUI) dependencies
    .pip_install("bpy==4.1.0")  # Blender as a Python package
)
```

Rendering a single frame
------------------------

We define a function that renders a single frame. We’ll scale this function out on Modal later.

Functions in Modal are defined along with their hardware and their dependencies.
This function can be run with GPU acceleration or without it, and we’ll use a global flag in the code to switch between the two.

```
WITH_GPU = (
    True  # try changing this to False to run rendering massively in parallel on CPUs!
)
```

We decorate the function with
`@app.function`
to define it as a Modal function.
Note that in addition to defining the hardware requirements of the function,
we also specify the container image that the function runs in (the one we defined above).

The details of the scene aren’t too important for this example, but we’ll load
a .blend file that we created earlier. This scene contains a rotating
Modal logo made of a transmissive ice-like material, with a generated displacement map. The
animation keyframes were defined in Blender.

```
@app.function(
    gpu="L40S" if WITH_GPU else None,
    # default limits on Modal free tier
    max_containers=10 if WITH_GPU else 100,
    image=rendering_image,
)
def render(blend_file: bytes, frame_number: int = 0) -> bytes:
    """Renders the n-th frame of a Blender file as a PNG."""
    import bpy

    input_path = "/tmp/input.blend"
    output_path = f"/tmp/output-{frame_number}.png"

    # Blender requires input as a file.
    Path(input_path).write_bytes(blend_file)

    bpy.ops.wm.open_mainfile(filepath=input_path)
    bpy.context.scene.frame_set(frame_number)
    bpy.context.scene.render.filepath = output_path
    configure_rendering(bpy.context, with_gpu=WITH_GPU)
    bpy.ops.render.render(write_still=True)

    # Blender renders image outputs to a file as well.
    return Path(output_path).read_bytes()
```

### Rendering with acceleration

We can configure the rendering process to use GPU acceleration with NVIDIA CUDA.
We select the
[Cycles rendering engine](https://www.cycles-renderer.org/)

, which is compatible with CUDA,
and then activate the GPU.

```
def configure_rendering(ctx, with_gpu: bool):
    # configure the rendering process
    ctx.scene.render.engine = "CYCLES"
    ctx.scene.render.resolution_x = 3000
    ctx.scene.render.resolution_y = 2000
    ctx.scene.render.resolution_percentage = 50
    ctx.scene.cycles.samples = 128

    cycles = ctx.preferences.addons["cycles"]

    # Use GPU acceleration if available.
    if with_gpu:
        cycles.preferences.compute_device_type = "CUDA"
        ctx.scene.cycles.device = "GPU"

        # reload the devices to update the configuration
        cycles.preferences.get_devices()
        for device in cycles.preferences.devices:
            device.use = True

    else:
        ctx.scene.cycles.device = "CPU"

    # report rendering devices -- a nice snippet for debugging and ensuring the accelerators are being used
    for dev in cycles.preferences.devices:
        print(f"ID:{dev['id']} Name:{dev['name']} Type:{dev['type']} Use:{dev['use']}")
```

Combining frames into a video
-----------------------------

Rendering 3D images is fun, and GPUs can make it faster, but rendering 3D videos is better!
We add another function to our app, running on a different, simpler container image
and different hardware, to combine the frames into a video.

```
combination_image = modal.Image.debian_slim(python_version="3.11").apt_install("ffmpeg")
```

The function to combine the frames into a video takes a sequence of byte sequences, one for each rendered frame,
and converts them into a single sequence of bytes, the MP4 file.

```
@app.function(image=combination_image)
def combine(frames_bytes: list[bytes], fps: int = 60) -> bytes:
    import subprocess
    import tempfile

    with tempfile.TemporaryDirectory() as tmpdir:
        for i, frame_bytes in enumerate(frames_bytes):
            frame_path = Path(tmpdir) / f"frame_{i:05}.png"
            frame_path.write_bytes(frame_bytes)
        out_path = Path(tmpdir) / "output.mp4"
        subprocess.run(
            f"ffmpeg -framerate {fps} -pattern_type glob -i '{tmpdir}/*.png' -c:v libx264 -pix_fmt yuv420p {out_path}",
            shell=True,
        )
        return out_path.read_bytes()
```

Rendering in parallel in the cloud from the comfort of the command line
-----------------------------------------------------------------------

With these two functions defined, we need only a few more lines to run our rendering at scale on Modal.

First, we need a function that coordinates our functions to
`render`
frames and
`combine`
them.
We decorate that function with
`@app.local_entrypoint`
so that we can run it with
`modal run blender_video.py`
.

In that function, we use
`render.map`
to map the
`render`
function over the range of frames.

We give the
`local_entrypoint`
two parameters to control the render — the number of frames to render and how many frames to skip.
These demonstrate a basic pattern for controlling Functions on Modal from a local client.

We collect the bytes from each frame into a
`list`
locally and then send it to
`combine`
with
`.remote`
.

The bytes for the video come back to our local machine, and we write them to a file.

The whole rendering process (for four seconds of 1080p 60 FPS video) takes about three minutes to run on 10 L40S GPUs,
with a per-frame latency of about six seconds, and about five minutes to run on 100 CPUs, with a per-frame latency of about one minute.

```
@app.local_entrypoint()
def main(frame_count: int = 250, frame_skip: int = 1):
    output_directory = Path("/tmp") / "render"
    output_directory.mkdir(parents=True, exist_ok=True)

    input_path = Path(__file__).parent / "IceModal.blend"
    blend_bytes = input_path.read_bytes()
    args = [(blend_bytes, frame) for frame in range(1, frame_count + 1, frame_skip)]
    images = list(render.starmap(args))
    for i, image in enumerate(images):
        frame_path = output_directory / f"frame_{i + 1}.png"
        frame_path.write_bytes(image)
        print(f"Frame saved to {frame_path}")

    video_path = output_directory / "output.mp4"
    video_bytes = combine.remote(images)
    video_path.write_bytes(video_bytes)
    print(f"Video saved to {video_path}")
```

[Render a video with Blender on many GPUs or CPUs in parallel](#render-a-video-with-blender-on-many-gpus-or-cpus-in-parallel)

[Defining a Modal app](#defining-a-modal-app)

[Rendering a single frame](#rendering-a-single-frame)

[Rendering with acceleration](#rendering-with-acceleration)

[Combining frames into a video](#combining-frames-into-a-video)

[Rendering in parallel in the cloud from the comfort of the command line](#rendering-in-parallel-in-the-cloud-from-the-comfort-of-the-command-line)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/blender/blender_video.py --frame-skip 2
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/boltz_predict
================================================================================

Fold proteins with Boltz-2
==========================

![Boltz-2](https://modal-cdn.com/cdnbot/boltz_examplecd5u3m0j_9fa47e43.webp)

*Example of Boltz-2 protein structure prediction
of a
[protein-ligand complex](https://github.com/jwohlwend/boltz/blob/main/examples/affinity.yaml)*

Boltz-2 is an open source molecular structure prediction model.
In contrast to previous models like Boltz-1,
[Chai-1](https://modal.com/docs/examples/chai1)

, and AlphaFold-3, it not only predicts protein structures but also the
[binding affinities](https://en.wikipedia.org/wiki/Ligand_(biochemistry)#Receptor/ligand_binding_affinity)

between proteins and
[ligands](https://en.wikipedia.org/wiki/Ligand_(biochemistry))

.
It was created by the
[MIT Jameel Clinic](https://jclinic.mit.edu/boltz-2/)

.
For details, see
[their technical report](https://jeremywohlwend.com/assets/boltz2.pdf)

.

Here, we demonstrate how to run Boltz-2 on Modal.

Setup
-----

```
from pathlib import Path
from typing import Optional

import modal

here = Path(__file__).parent  # the directory of this file

MINUTES = 60  # seconds

app = modal.App(name="example-boltz-predict")
```

Fold a protein from the command line
------------------------------------

The logic for running Boltz-2 is encapsulated in the function below,
which you can trigger from the command line by running

```
modal run boltz_predict.py
```

This will set up the environment for running Boltz-2 inference in Modal’s cloud,
run it, and then save the results locally as a
[tarball](https://computing.help.inf.ed.ac.uk/FAQ/whats-tarball-or-how-do-i-unpack-or-create-tgz-or-targz-file)

.
That tarball archive contains, among other things, the predicted structure as a
[Crystallographic Information File](https://en.wikipedia.org/wiki/Crystallographic_Information_File)

,
which you can render with the online
[Molstar Viewer](https://molstar.org/viewer)

.

You can pass any options for the
[`boltz predict`
command line tool](https://github.com/jwohlwend/boltz/blob/main/docs/prediction.md)

as a string, like

```
modal run boltz_predict.py --args "--sampling_steps 10"
```

To see more options, run the command with the
`--help`
flag.

To learn how it works, read on!

```
@app.local_entrypoint()
def main(
    force_download: bool = False, input_yaml_path: Optional[str] = None, args: str = ""
):
    print("🧬 loading model remotely")
    download_model.remote(force_download)

    if input_yaml_path is None:
        input_yaml_path = here / "data" / "boltz_affinity.yaml"
    input_yaml = input_yaml_path.read_text()

    print(f"🧬 running boltz with input from {input_yaml_path}")
    output = boltz_inference.remote(input_yaml)

    output_path = Path("/tmp") / "boltz" / "boltz_result.tar.gz"
    output_path.parent.mkdir(exist_ok=True, parents=True)
    print(f"🧬 writing output to {output_path}")
    output_path.write_bytes(output)
```

Installing Boltz-2 Python dependencies on Modal
-----------------------------------------------

Code running on Modal runs inside containers built from
[container images](https://modal.com/docs/guide/images)

that include that code’s dependencies.

Because Modal images include
[GPU drivers](https://modal.com/docs/guide/cuda)

by default,
installation of higher-level packages like
`boltz`
that require GPUs is painless.

Here, we do it in a few lines, using the
`uv`
package manager for extra speed.

```
image = modal.Image.debian_slim(python_version="3.12").run_commands(
    "uv pip install --system --compile-bytecode boltz==2.1.1"
)
```

Storing Boltz-2 model weights on Modal with Volumes
---------------------------------------------------

Not all “dependencies” belong in a container image. Boltz-2, for example, depends on
the weights of the model and a
[Chemical Component Dictionary](https://www.wwpdb.org/data/ccd)

(CCD) file.

Rather than loading them dynamically at run-time (which would add several minutes of GPU time to each inference),
or installing them into the image (which would require they be re-downloaded any time the other dependencies changed),
we load them onto a
[Modal Volume](https://modal.com/docs/guide/volumes)

.
A Modal Volume is a file system that all of your code running on Modal (or elsewhere!) can access.
For more on storing model weights on Modal, see
[this guide](https://modal.com/docs/guide/model-weights)

.
For details on how we download the weights in this case, see the
[Addenda](#addenda)

.

```
boltz_model_volume = modal.Volume.from_name("boltz-models", create_if_missing=True)
models_dir = Path("/models/boltz")
```

Running Boltz-2 on Modal
------------------------

To run inference on Modal we wrap our function in a decorator,
`@app.function`
.
We provide that decorator with some arguments that describe the infrastructure our code needs to run:
the Volume we created, the Image we defined, and of course a fast GPU!

Note that the
`boltz`
command-line tool we use takes the path to a
[specially-formatted YAML file](https://github.com/jwohlwend/boltz/blob/main/docs/prediction.md#yaml-format)

that includes definitions of molecules to predict the structures of and optionally paths to
[Multiple Sequence Alignment](https://en.wikipedia.org/wiki/Multiple_sequence_alignment)

(MSA) files
for any protein molecules. We pass the
[—use\_msa\_server](https://github.com/jwohlwend/boltz/blob/main/docs/prediction.md)

flag to auto-generate the MSA using the mmseqs2 server.

```
@app.function(
    image=image,
    volumes={models_dir: boltz_model_volume},
    timeout=10 * MINUTES,
    gpu="H100",
)
def boltz_inference(boltz_input_yaml: str, args="") -> bytes:
    import shlex
    import subprocess

    input_path = Path("input.yaml")
    input_path.write_text(boltz_input_yaml)

    args = shlex.split(args)

    print(f"🧬 predicting structure using boltz model from {models_dir}")
    subprocess.run(
        ["boltz", "predict", input_path, "--use_msa_server", "--cache", str(models_dir)]
        + args,
        check=True,
    )

    print("🧬 packaging up outputs")
    output_bytes = package_outputs(f"boltz_results_{input_path.with_suffix('').name}")

    return output_bytes
```

Addenda
-------

Above, we glossed over just how we got hold of the model weights —
the
`local_entrypoint`
just called a function named
`download_model`
.

Here’s the implementation of that function. For details, see our
[guide to storing model weights on Modal](https://modal.com/docs/guide/model-weights)

.

```
download_image = (
    modal.Image.debian_slim()
    .pip_install("huggingface_hub[hf_transfer]==0.26.3")
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})  # and enable it
)

@app.function(
    volumes={models_dir: boltz_model_volume},
    timeout=20 * MINUTES,
    image=download_image,
)
def download_model(
    force_download: bool = False,
    revision: str = "6fdef46d763fee7fbb83ca5501ccceff43b85607",
):
    from huggingface_hub import snapshot_download

    snapshot_download(
        repo_id="boltz-community/boltz-2",
        revision=revision,
        local_dir=models_dir,
        force_download=force_download,
    )
    boltz_model_volume.commit()

    print(f"🧬 model downloaded to {models_dir}")
```

We package the outputs into a tarball which contains the predicted structure as a
[Crystallographic Information File](https://en.wikipedia.org/wiki/Crystallographic_Information_File)

and the binding affinity as a JSON file.
You can render the structure with the online
[Molstar Viewer](https://molstar.org/viewer)

.

```
def package_outputs(output_dir: str) -> bytes:
    import io
    import tarfile

    tar_buffer = io.BytesIO()

    with tarfile.open(fileobj=tar_buffer, mode="w:gz") as tar:
        tar.add(output_dir, arcname=output_dir)

    return tar_buffer.getvalue()
```

[Fold proteins with Boltz-2](#fold-proteins-with-boltz-2)

[Setup](#setup)

[Fold a protein from the command line](#fold-a-protein-from-the-command-line)

[Installing Boltz-2 Python dependencies on Modal](#installing-boltz-2-python-dependencies-on-modal)

[Storing Boltz-2 model weights on Modal with Volumes](#storing-boltz-2-model-weights-on-modal-with-volumes)

[Running Boltz-2 on Modal](#running-boltz-2-on-modal)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/protein-folding/boltz_predict.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/chai1
================================================================================

Fold proteins with Chai-1
=========================

In biology, function follows form quite literally:
the physical shapes of proteins dictate their behavior.
Measuring those shapes directly is difficult
and first-principles physical simulation prohibitively expensive.

And so predicting protein shape from content —
determining how the one-dimensional chain of amino acids encoded by DNA
*folds*
into a 3D object —
has emerged as a key application for machine learning and neural networks in biology.

In this example, we demonstrate how to run the open source
[Chai-1](https://github.com/chaidiscovery/chai-lab/)

protein structure prediction model on Modal’s flexible serverless infrastructure.
For details on how the Chai-1 model works and what it can be used for,
see the authors’
[technical report on bioRxiv](https://www.biorxiv.org/content/10.1101/2024.10.10.615955)

.

This simple script is meant as a starting point showing how to handle fiddly bits
like installing dependencies, loading weights, and formatting outputs so that you can get on with the fun stuff.
To experience the full power of Modal, try scaling inference up and running on hundreds or thousands of structures!

[[

](https://modal-cdn.com/example-chai1-folding.mp4)](https://molstar.org/viewer)

Setup
-----

```
import hashlib
import json
from pathlib import Path
from typing import Optional
from uuid import uuid4

import modal

here = Path(__file__).parent  # the directory of this file

MINUTES = 60  # seconds

app = modal.App(name="example-chai1-inference")
```

Fold a protein from the command line
------------------------------------

The logic for running Chai-1 is encapsulated in the function below,
which you can trigger from the command line by running

```
modal run chai1
```

This will set up the environment for running Chai-1 inference in Modal’s cloud,
run it, and then save the results remotely and locally. The results are returned in the
[Crystallographic Information File](https://en.wikipedia.org/wiki/Crystallographic_Information_File)

format,
which you can render with the online
[Molstar Viewer](https://molstar.org/)

.

To see more options, run the command with the
`--help`
flag.

To learn how it works, read on!

```
@app.local_entrypoint()
def main(
    force_redownload: bool = False,
    fasta_file: Optional[str] = None,
    inference_config_file: Optional[str] = None,
    output_dir: Optional[str] = None,
    run_id: Optional[str] = None,
):
    print("🧬 checking inference dependencies")
    download_inference_dependencies.remote(force=force_redownload)

    if fasta_file is None:
        fasta_file = here / "data" / "chai1_default_input.fasta"
    print(f"🧬 running Chai inference on {fasta_file}")
    fasta_content = Path(fasta_file).read_text()

    if inference_config_file is None:
        inference_config_file = here / "data" / "chai1_default_inference.json"
    print(f"🧬 loading Chai inference config from {inference_config_file}")
    inference_config = json.loads(Path(inference_config_file).read_text())

    if run_id is None:
        run_id = hashlib.sha256(uuid4().bytes).hexdigest()[:8]  # short id
    print(f"🧬 running inference with {run_id=}")

    results = chai1_inference.remote(fasta_content, inference_config, run_id)

    if output_dir is None:
        output_dir = Path("/tmp/chai1")
        output_dir.mkdir(parents=True, exist_ok=True)

    print(f"🧬 saving results to disk locally in {output_dir}")
    for ii, (scores, cif) in enumerate(results):
        (Path(output_dir) / f"{run_id}-scores.model_idx_{ii}.npz").write_bytes(scores)
        (Path(output_dir) / f"{run_id}-preds.model_idx_{ii}.cif").write_text(cif)
```

Installing Chai-1 Python dependencies on Modal
----------------------------------------------

Code running on Modal runs inside containers built from
[container images](https://modal.com/docs/guide/images)

that include that code’s dependencies.

Because Modal images include
[GPU drivers](https://modal.com/docs/guide/cuda)

by default,
installation of higher-level packages like
`chai_lab`
that require GPUs is painless.

Here, we do it with one line, using the
`uv`
package manager for extra speed.

```
image = modal.Image.debian_slim(python_version="3.12").run_commands(
    "uv pip install --system --compile-bytecode chai_lab==0.5.0 hf_transfer==0.1.8"
)
```

Storing Chai-1 model weights on Modal with Volumes
--------------------------------------------------

Not all “dependencies” belong in a container image. Chai-1, for example, depends on
the weights of several models.

Rather than loading them dynamically at run-time (which would add several minutes of GPU time to each inference),
or installing them into the image (which would require they be re-downloaded any time the other dependencies changed),
we load them onto a
[Modal Volume](https://modal.com/docs/guide/volumes)

.
A Modal Volume is a file system that all of your code running on Modal (or elsewhere!) can access.
For more on storing model weights on Modal, see
[this guide](https://modal.com/docs/guide/model-weights)

.

```
chai_model_volume = (
    modal.Volume.from_name(  # create distributed filesystem for model weights
        "chai1-models",
        create_if_missing=True,
    )
)
models_dir = Path("/models/chai1")
```

The details of how we handle the download here (e.g. running concurrently for extra speed)
are in the
[Addenda](#addenda)

.

```
image = image.env(  # update the environment variables in the image to...
    {
        "CHAI_DOWNLOADS_DIR": str(models_dir),  # point the chai code to it
        "HF_HUB_ENABLE_HF_TRANSFER": "1",  # speed up downloads
    }
)
```

Storing Chai-1 outputs on Modal Volumes
---------------------------------------

Chai-1 produces its outputs by writing to disk —
the model’s scores for the structure and the structure itself along with rich metadata.

But Modal is a
*serverless*
platform, and the filesystem your Modal Functions write to
is not persistent. Any file can be converted into bytes and sent back from a Modal Function
— and we mean any! You can send files that are gigabytes in size that way.
So we do that below.

But for larger jobs, like folding every protein in the PDB, storing bytes on a local client
like a laptop won’t cut it.

So we again lean on Modal Volumes, which can store thousands of files each.
We attach a Volume to a Modal Function that runs Chai-1 and the inference code
saves the results to distributed storage, without any fuss or source code changes.

```
chai_preds_volume = modal.Volume.from_name("chai1-preds", create_if_missing=True)
preds_dir = Path("/preds")
```

Running Chai-1 on Modal
-----------------------

Now we’re ready to define a Modal Function that runs Chai-1.

We put our function on Modal by wrapping it in a decorator,
`@app.function`
.
We provide that decorator with some arguments that describe the infrastructure our code needs to run:
the Volumes we created, the Image we defined, and of course a fast GPU!

Note that Chai-1 takes a file path as input —
specifically, a path to a file in the
[FASTA format](https://en.wikipedia.org/wiki/FASTA_format)

.
We pass the file contents to the function as a string and save them to disk so they can be picked up by the inference code.

Because Modal is serverless, we don’t need to worry about cleaning up these resources:
the disk is ephemeral and the GPU only costs you money when you’re using it.

```
@app.function(
    timeout=15 * MINUTES,
    gpu="H100",
    volumes={models_dir: chai_model_volume, preds_dir: chai_preds_volume},
    image=image,
)
def chai1_inference(
    fasta_content: str, inference_config: dict, run_id: str
) -> list[(bytes, str)]:
    from pathlib import Path

    import torch
    from chai_lab import chai1

    N_DIFFUSION_SAMPLES = 5  # hard-coded in chai-1

    fasta_file = Path("/tmp/inputs.fasta")
    fasta_file.write_text(fasta_content.strip())

    output_dir = Path("/preds") / run_id

    chai1.run_inference(
        fasta_file=fasta_file,
        output_dir=output_dir,
        device=torch.device("cuda"),
        **inference_config,
    )

    print(
        f"🧬 done, results written to /{output_dir.relative_to('/preds')} on remote volume"
    )

    results = []
    for ii in range(N_DIFFUSION_SAMPLES):
        scores = (output_dir / f"scores.model_idx_{ii}.npz").read_bytes()
        cif = (output_dir / f"pred.model_idx_{ii}.cif").read_text()

        results.append((scores, cif))

    return results
```

Addenda
-------

Above, we glossed over just how we got hold of the model weights —
the
`local_entrypoint`
just called a function named
`download_inference_dependencies`
.

Here’s that function’s implementation.

A few highlights:

* This Modal Function can access the model weights Volume, like the inference Function,
  but it can’t access the model predictions Volume.
* This Modal Function has a different Image (the default!) and doesn’t use a GPU. Modal helps you
  separate the concerns, and the costs, of your infrastructure’s components.
* We use the
  `async`
  keyword here so that we can run the download for each model file
  as a separate task, concurrently. We don’t need to worry about this use of
  `async`
  spreading to the rest of our code — Modal launches just this Function in an async runtime.

```
@app.function(volumes={models_dir: chai_model_volume})
async def download_inference_dependencies(force=False):
    import asyncio

    import aiohttp

    base_url = "https://chaiassets.com/chai1-inference-depencencies/"  # sic
    inference_dependencies = [
        "conformers_v1.apkl",
        "models_v2/trunk.pt",
        "models_v2/token_embedder.pt",
        "models_v2/feature_embedding.pt",
        "models_v2/diffusion_module.pt",
        "models_v2/confidence_head.pt",
    ]

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"
    }

    # launch downloads concurrently
    async with aiohttp.ClientSession(headers=headers) as session:
        tasks = []
        for dep in inference_dependencies:
            local_path = models_dir / dep
            if force or not local_path.exists():
                url = base_url + dep
                print(f"🧬 downloading {dep}")
                tasks.append(download_file(session, url, local_path))

        # run all of the downloads and await their completion
        await asyncio.gather(*tasks)

    chai_model_volume.commit()  # ensures models are visible on remote filesystem before exiting, otherwise takes a few seconds, racing with inference

async def download_file(session, url: str, local_path: Path):
    async with session.get(url) as response:
        response.raise_for_status()
        local_path.parent.mkdir(parents=True, exist_ok=True)
        with open(local_path, "wb") as f:
            while chunk := await response.content.read(8192):
                f.write(chunk)
```

[Fold proteins with Chai-1](#fold-proteins-with-chai-1)

[Setup](#setup)

[Fold a protein from the command line](#fold-a-protein-from-the-command-line)

[Installing Chai-1 Python dependencies on Modal](#installing-chai-1-python-dependencies-on-modal)

[Storing Chai-1 model weights on Modal with Volumes](#storing-chai-1-model-weights-on-modal-with-volumes)

[Storing Chai-1 outputs on Modal Volumes](#storing-chai-1-outputs-on-modal-volumes)

[Running Chai-1 on Modal](#running-chai-1-on-modal)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/protein-folding/chai1.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/chat_with_pdf_vision
================================================================================

Chat with PDF: RAG with ColQwen2
================================

In this example, we demonstrate how to use the the
[ColQwen2](https://huggingface.co/vidore/colqwen2-v0.1)

model to build a simple
“Chat with PDF” retrieval-augmented generation (RAG) app.
The ColQwen2 model is based on
[ColPali](https://huggingface.co/blog/manu/colpali)

but uses the
[Qwen2-VL-2B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-2B-Instruct)

vision-language model.
ColPali is in turn based on the late-interaction embedding approach pioneered in
[ColBERT](https://dl.acm.org/doi/pdf/10.1145/3397271.3401075)

.

Vision-language models with high-quality embeddings obviate the need for complex pre-processing pipelines.
See
[this blog post from Jo Bergum of Vespa](https://blog.vespa.ai/announcing-colbert-embedder-in-vespa/)

for more.

Setup
-----

First, we’ll import the libraries we need locally and define some constants.

```
from pathlib import Path
from typing import Optional
from urllib.request import urlopen
from uuid import uuid4

import modal

MINUTES = 60  # seconds

app = modal.App("chat-with-pdf")
```

Setting up dependenices
-----------------------

In Modal, we define
[container images](https://modal.com/docs/guide/custom-container)

that run our serverless workloads.
We install the packages required for our application in those images.

```
CACHE_DIR = "/hf-cache"

model_image = (
    modal.Image.debian_slim(python_version="3.12")
    .apt_install("git")
    .pip_install(
        [
            "git+https://github.com/illuin-tech/colpali.git@782edcd50108d1842d154730ad3ce72476a2d17d",  # we pin the commit id
            "hf_transfer==0.1.8",
            "qwen-vl-utils==0.0.8",
            "torchvision==0.19.1",
        ]
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HUB_CACHE": CACHE_DIR})
)
```

These dependencies are only installed remotely, so we can’t import them locally.
Use the
`.imports`
context manager to import them only on Modal instead.

```
with model_image.imports():
    import torch
    from colpali_engine.models import ColQwen2, ColQwen2Processor
    from qwen_vl_utils import process_vision_info
    from transformers import AutoProcessor, Qwen2VLForConditionalGeneration
```

Specifying the ColQwen2 model
-----------------------------

Vision-language models (VLMs) for embedding and generation add another layer of simplification
to RAG apps based on vector search: we only need one model.

```
MODEL_NAME = "Qwen/Qwen2-VL-2B-Instruct"
MODEL_REVISION = "aca78372505e6cb469c4fa6a35c60265b00ff5a4"
```

Managing state with Modal Volumes and Dicts
-------------------------------------------

Chat services are stateful:
the response to an incoming user message depends on past user messages in a session.

RAG apps add even more state:
the documents being retrieved from and the index over those documents,
e.g. the embeddings.

Modal Functions are stateless in and of themselves.
They don’t retain information from input to input.
That’s what enables Modal Functions to automatically scale up and down
[based on the number of incoming requests](https://modal.com/docs/guide/cold-start)

.

### Managing chat sessions with Modal Dicts

In this example, we use a
[`modal.Dict`](https://modal.com/docs/guide/dicts-and-queues)

to store state information between Function calls.

Modal Dicts behave similarly to Python dictionaries,
but they are backed by remote storage and accessible to all of your Modal Functions.
They can contain any Python object
that can be serialized using
[`cloudpickle`](https://github.com/cloudpipe/cloudpickle)

.

A Dict can hold a few gigabytes across keys of size up to 100 MiB,
so it works well for our chat session state, which is a few KiB per session,
and for our embeddings, which are a few hundred KiB per PDF page,
up to about 100,000 pages of PDFs.

At a larger scale, we’d need to replace this with a database, like Postgres,
or push more state to the client.

```
sessions = modal.Dict.from_name("colqwen-chat-sessions", create_if_missing=True)

class Session:
    def __init__(self):
        self.images = None
        self.messages = []
        self.pdf_embeddings = None
```

### Storing PDFs on a Modal Volume

Images extracted from PDFs are larger than our session state or embeddings
— low tens of MiB per page.

So we store them on a
[Modal Volume](https://modal.com/docs/guide/volumes)

,
which can store terabytes (or more!) of data across tens of thousands of files.

Volumes behave like a remote file system:
we read and write from them much like a local file system.

```
pdf_volume = modal.Volume.from_name("colqwen-chat-pdfs", create_if_missing=True)
PDF_ROOT = Path("/vol/pdfs/")
```

### Caching the model weights

We’ll also use a Volume to cache the model weights.

```
cache_volume = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)
```

Running this function will download the model weights to the cache volume.
Otherwise, the model weights will be downloaded on the first query.

```
@app.function(
    image=model_image, volumes={CACHE_DIR: cache_volume}, timeout=20 * MINUTES
)
def download_model():
    from huggingface_hub import snapshot_download

    result = snapshot_download(
        MODEL_NAME,
        revision=MODEL_REVISION,
        ignore_patterns=["*.pt", "*.bin"],  # using safetensors
    )
    print(f"Downloaded model weights to {result}")
```

Defining a Chat with PDF service
--------------------------------

To deploy an autoscaling “Chat with PDF” vision-language model service on Modal,
we just need to wrap our Python logic in a
[Modal App](https://modal.com/docs/guide/apps)

:

It uses
[Modal
`@app.cls`](https://modal.com/docs/guide/lifecycle-functions)

decorators
to organize the “lifecycle” of the app:
loading the model on container start (
`@modal.enter`
) and running inference on request (
`@modal.method`
).

We include in the arguments to the
`@app.cls`
decorator
all the information about this service’s infrastructure:
the container image, the remote storage, and the GPU requirements.

```
@app.cls(
    image=model_image,
    gpu="A100-80GB",
    scaledown_window=10 * MINUTES,  # spin down when inactive
    volumes={"/vol/pdfs/": pdf_volume, CACHE_DIR: cache_volume},
)
class Model:
    @modal.enter()
    def load_models(self):
        self.colqwen2_model = ColQwen2.from_pretrained(
            "vidore/colqwen2-v0.1",
            torch_dtype=torch.bfloat16,
            device_map="cuda:0",
        )
        self.colqwen2_processor = ColQwen2Processor.from_pretrained(
            "vidore/colqwen2-v0.1"
        )
        self.qwen2_vl_model = Qwen2VLForConditionalGeneration.from_pretrained(
            MODEL_NAME,
            revision=MODEL_REVISION,
            torch_dtype=torch.bfloat16,
        )
        self.qwen2_vl_model.to("cuda:0")
        self.qwen2_vl_processor = AutoProcessor.from_pretrained(
            "Qwen/Qwen2-VL-2B-Instruct", trust_remote_code=True
        )

    @modal.method()
    def index_pdf(self, session_id, target: bytes | list):
        # We store concurrent user chat sessions in a modal.Dict

        # For simplicity, we assume that each user only runs one session at a time

        session = sessions.get(session_id)
        if session is None:
            session = Session()

        if isinstance(target, bytes):
            images = convert_pdf_to_images.remote(target)
        else:
            images = target

        # Store images on a Volume for later retrieval
        session_dir = PDF_ROOT / f"{session_id}"
        session_dir.mkdir(exist_ok=True, parents=True)
        for ii, image in enumerate(images):
            filename = session_dir / f"{str(ii).zfill(3)}.jpg"
            image.save(filename)

        # Generated embeddings from the image(s)
        BATCH_SZ = 4
        pdf_embeddings = []
        batches = [images[i : i + BATCH_SZ] for i in range(0, len(images), BATCH_SZ)]
        for batch in batches:
            batch_images = self.colqwen2_processor.process_images(batch).to(
                self.colqwen2_model.device
            )
            pdf_embeddings += list(self.colqwen2_model(**batch_images).to("cpu"))

        # Store the image embeddings in the session, for later retrieval
        session.pdf_embeddings = pdf_embeddings

        # Write embeddings back to the modal.Dict
        sessions[session_id] = session

    @modal.method()
    def respond_to_message(self, session_id, message):
        session = sessions.get(session_id)
        if session is None:
            session = Session()

        pdf_volume.reload()  # make sure we have the latest data

        images = (PDF_ROOT / str(session_id)).glob("*.jpg")
        images = list(sorted(images, key=lambda p: int(p.stem)))

        # Nothing to chat about without a PDF!
        if not images:
            return "Please upload a PDF first"
        elif session.pdf_embeddings is None:
            return "Indexing PDF..."

        # RAG, Retrieval-Augmented Generation, is two steps:

        # _Retrieval_ of the most relevant data to answer the user's query
        relevant_image = self.get_relevant_image(message, session, images)

        # _Generation_ based on the retrieved data
        output_text = self.generate_response(message, session, relevant_image)

        # Update session state for future chats
        append_to_messages(message, session, user_type="user")
        append_to_messages(output_text, session, user_type="assistant")
        sessions[session_id] = session

        return output_text

    # Retrieve the most relevant image from the PDF for the input query
    def get_relevant_image(self, message, session, images):
        import PIL

        batch_queries = self.colqwen2_processor.process_queries([message]).to(
            self.colqwen2_model.device
        )
        query_embeddings = self.colqwen2_model(**batch_queries)

        # This scores our query embedding against the image embeddings from index_pdf
        scores = self.colqwen2_processor.score_multi_vector(
            query_embeddings, session.pdf_embeddings
        )[0]

        # Select the best matching image
        max_index = max(range(len(scores)), key=lambda index: scores[index])
        return PIL.Image.open(images[max_index])

    # Pass the query and retrieved image along with conversation history into the VLM for a response
    def generate_response(self, message, session, image):
        chatbot_message = get_chatbot_message_with_image(message, image)
        query = self.qwen2_vl_processor.apply_chat_template(
            [*session.messages, chatbot_message],
            tokenize=False,
            add_generation_prompt=True,
        )
        image_inputs, _ = process_vision_info([chatbot_message])
        inputs = self.qwen2_vl_processor(
            text=[query],
            images=image_inputs,
            padding=True,
            return_tensors="pt",
        )
        inputs = inputs.to("cuda:0")

        generated_ids = self.qwen2_vl_model.generate(**inputs, max_new_tokens=512)
        generated_ids_trimmed = [
            out_ids[len(in_ids) :]
            for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
        ]
        output_text = self.qwen2_vl_processor.batch_decode(
            generated_ids_trimmed,
            skip_special_tokens=True,
            clean_up_tokenization_spaces=False,
        )[0]
        return output_text
```

Loading PDFs as images
----------------------

Vision-Language Models operate on images, not PDFs directly,
so we need to convert our PDFs into images first.

We separate this from our indexing and chatting logic —
we run on a different container with different dependencies.

```
pdf_image = (
    modal.Image.debian_slim(python_version="3.12")
    .apt_install("poppler-utils")
    .pip_install("pdf2image==1.17.0", "pillow==10.4.0")
)

@app.function(image=pdf_image)
def convert_pdf_to_images(pdf_bytes):
    from pdf2image import convert_from_bytes

    images = convert_from_bytes(pdf_bytes, fmt="jpeg")
    return images
```

Chatting with a PDF from the terminal
-------------------------------------

Before deploying in a UI, we can test our service from the terminal.

Just run

```
modal run chat_with_pdf_vision.py
```

and optionally pass in a path to or URL of a PDF with the
`--pdf-path`
argument
and specify a question with the
`--question`
argument.

Continue a previous chat by passing the session ID printed to the terminal at start
with the
`--session-id`
argument.

```
@app.local_entrypoint()
def main(
    question: Optional[str] = None,
    pdf_path: Optional[str] = None,
    session_id: Optional[str] = None,
):
    model = Model()
    if session_id is None:
        session_id = str(uuid4())
        print("Starting a new session with id", session_id)

        if pdf_path is None:
            pdf_path = "https://arxiv.org/pdf/1706.03762"  # all you need

        if pdf_path.startswith("http"):
            pdf_bytes = urlopen(pdf_path).read()
        else:
            pdf_bytes = Path(pdf_path).read_bytes()

        print("Indexing PDF from", pdf_path)
        model.index_pdf.remote(session_id, pdf_bytes)
    else:
        if pdf_path is not None:
            raise ValueError("Start a new session to chat with a new PDF")
        print("Resuming session with id", session_id)

    if question is None:
        question = "What is this document about?"

    print("QUESTION:", question)
    print(model.respond_to_message.remote(session_id, question))
```

A hosted Gradio interface
-------------------------

With the
[Gradio](https://gradio.app)

library, we can create a simple web interface around our class in Python,
then use Modal to host it for anyone to try out.

To deploy your own, run

```
modal deploy chat_with_pdf_vision.py
```

and navigate to the URL that appears in your teriminal.
If you’re editing the code, use
`modal serve`
instead to see changes hot-reload.

```
web_image = pdf_image.pip_install(
    "fastapi[standard]==0.115.4",
    "pydantic==2.9.2",
    "starlette==0.41.2",
    "gradio==4.44.1",
    "pillow==10.4.0",
    "gradio-pdf==0.0.15",
    "pdf2image==1.17.0",
)

@app.function(
    image=web_image,
    # gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow it to scale to 1000 concurrent inputs
    max_containers=1,
)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def ui():
    import uuid

    import gradio as gr
    from fastapi import FastAPI
    from gradio.routes import mount_gradio_app
    from gradio_pdf import PDF
    from pdf2image import convert_from_path

    web_app = FastAPI()

    # Since this Gradio app is running from its own container,
    # allowing us to run the inference service via .remote() methods.
    model = Model()

    def upload_pdf(path, session_id):
        if session_id == "" or session_id is None:
            # Generate session id if new client
            session_id = str(uuid.uuid4())

        images = convert_from_path(path)
        # Call to our remote inference service to index the PDF
        model.index_pdf.remote(session_id, images)

        return session_id

    def respond_to_message(message, _, session_id):
        # Call to our remote inference service to run RAG
        return model.respond_to_message.remote(session_id, message)

    with gr.Blocks(theme="soft") as demo:
        session_id = gr.State("")

        gr.Markdown("# Chat with PDF")
        with gr.Row():
            with gr.Column(scale=1):
                gr.ChatInterface(
                    fn=respond_to_message,
                    additional_inputs=[session_id],
                    retry_btn=None,
                    undo_btn=None,
                    clear_btn=None,
                )
            with gr.Column(scale=1):
                pdf = PDF(
                    label="Upload a PDF",
                )
                pdf.upload(upload_pdf, [pdf, session_id], session_id)

    return mount_gradio_app(app=web_app, blocks=demo, path="/")
```

Addenda
-------

The remainder of this code consists of utility functions and boiler plate used in the
main code above.

```
def get_chatbot_message_with_image(message, image):
    return {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": message},
        ],
    }

def append_to_messages(message, session, user_type="user"):
    session.messages.append(
        {
            "role": user_type,
            "content": {"type": "text", "text": message},
        }
    )
```

[Chat with PDF: RAG with ColQwen2](#chat-with-pdf-rag-with-colqwen2)

[Setup](#setup)

[Setting up dependenices](#setting-up-dependenices)

[Specifying the ColQwen2 model](#specifying-the-colqwen2-model)

[Managing state with Modal Volumes and Dicts](#managing-state-with-modal-volumes-and-dicts)

[Managing chat sessions with Modal Dicts](#managing-chat-sessions-with-modal-dicts)

[Storing PDFs on a Modal Volume](#storing-pdfs-on-a-modal-volume)

[Caching the model weights](#caching-the-model-weights)

[Defining a Chat with PDF service](#defining-a-chat-with-pdf-service)

[Loading PDFs as images](#loading-pdfs-as-images)

[Chatting with a PDF from the terminal](#chatting-with-a-pdf-from-the-terminal)

[A hosted Gradio interface](#a-hosted-gradio-interface)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-serving/chat_with_pdf_vision.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/chatterbox_tts
================================================================================

Create a Chatterbox TTS API on Modal
====================================

This example demonstrates how to deploy a text-to-speech (TTS) API using the Chatterbox TTS model on Modal.
The API accepts text prompts and returns generated audio as WAV files through a FastAPI endpoint.
We use Modal’s class-based approach with GPU acceleration to provide fast, scalable TTS inference.

Setup
-----

Import the necessary modules for Modal deployment and TTS functionality.

```
import io

import modal
```

Define a container image
------------------------

We start with Modal’s baseline
`debian_slim`
image and install the required packages.

* `chatterbox-tts`
  : The TTS model library
* `fastapi`
  : Web framework for creating the API endpoint

```
image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "chatterbox-tts==0.1.1", "fastapi[standard]"
)
app = modal.App("chatterbox-api-example", image=image)
```

Import the required libraries within the image context to ensure they’re available
when the container runs. This includes audio processing and the TTS model itself.

```
with image.imports():
    import torchaudio as ta
    from chatterbox.tts import ChatterboxTTS
    from fastapi.responses import StreamingResponse
```

The TTS model class
-------------------

The TTS service is implemented using Modal’s class syntax with GPU acceleration.
We configure the class to use an A10G GPU with additional parameters:

* `scaledown_window=60 * 5`
  : Keep containers alive for 5 minutes after last request
* `enable_memory_snapshot=True`
  : Enable
  [memory snapshots](https://modal.com/docs/guide/memory-snapshot)

  to optimize cold boot times
* `@modal.concurrent(max_inputs=10)`
  : Allow up to 10 concurrent requests per container

```
@app.cls(gpu="a10g", scaledown_window=60 * 5, enable_memory_snapshot=True)
@modal.concurrent(max_inputs=10)
class Chatterbox:
    @modal.enter()
    def load(self):
        self.model = ChatterboxTTS.from_pretrained(device="cuda")

    @modal.fastapi_endpoint(docs=True, method="POST")
    def generate(self, prompt: str):
        # Generate audio waveform from the input text
        wav = self.model.generate(prompt)

        # Create an in-memory buffer to store the WAV file
        buffer = io.BytesIO()

        # Save the generated audio to the buffer in WAV format
        # Uses the model's sample rate and WAV format
        ta.save(buffer, wav, self.model.sr, format="wav")

        # Reset buffer position to the beginning for reading
        buffer.seek(0)

        # Return the audio as a streaming response with appropriate MIME type.
        # This allows for browsers to playback audio directly.
        return StreamingResponse(
            io.BytesIO(buffer.read()),
            media_type="audio/wav",
        )
```

Now deploy the Chatterbox API with:

```
modal deploy chatterbox_tts.py
```

And query the endpoint with:

```
mkdir -p /tmp/chatterbox-tts  # create tmp directory

curl -X POST --get "<YOUR-ENDPOINT-URL>" \
  --data-urlencode "prompt=Chatterbox running on Modal"
  --output /tmp/chatterbox-tts/output.wav
```

You’ll receive a WAV file named
`/tmp/chatterbox-tts/output.wav`
containing the generated audio.

This app takes about 30 seconds to cold boot, mostly dominated by loading
the Chatterbox model into GPU memory. It takes 2-3s to generate a 5s audio clip.

[Create a Chatterbox TTS API on Modal](#create-a-chatterbox-tts-api-on-modal)

[Setup](#setup)

[Define a container image](#define-a-container-image)

[The TTS model class](#the-tts-model-class)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 06_gpu_and_ml/test-to-audio/chatterbox_tts.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/ci-on-modal
================================================================================

Run Continuous Integration (CI) Tests on Modal
==============================================

[This example repo](https://github.com/modal-labs/ci-on-modal)

is a
demonstration of one pattern for running tests on Modal: bring your existing
package and test suite (here
`my_pkg`
and
`tests`
) and add a Modal App
(
`my_pkg.ci`
) with a Function (
`pytest`
) that runs
`pytest`
.

That’s as straightforward as

```
# my_pkg/ci.py

@app.function(gpu="any")
def pytest():
    import subprocess

    subprocess.run(["pytest", "-vs"], check=True, cwd="/root")
```

Setup
-----

* Create a Python virtual environment
* `pip install modal`
* That’s it 😎

Usage
-----

All commands below are run from the root of the repository.

### Run tests remotely on Modal

```
modal run -m my_pkg.ci
```

On the first execution, the
[container image](https://modal.com/docs/guide/images)

for your application will be built.

This image will be cached on Modal and only rebuilt if one of its dependencies,
like the
`requirements.txt`
file, changes.

### Run tests on Modal from GitHub Actions

The same command can be executed from inside a CI runner on another platform.
We provide a sample GitHub Actions workflow in
`.github/workflows/ci.yml`
.

To run these tests on GitHub Actions, fork this repo and
[create a new GitHub Actions secret](https://docs.github.com/en/actions/security-guides/using-secrets-in-github-actions)

that contains your
`MODAL_TOKEN_ID`
and
`MODAL_TOKEN_SECRET`
.
You can find this info in the
`.modal.toml`
file in your home directory.

Now you can
[manually trigger the tests to run on GitHub Actions](https://docs.github.com/en/actions/using-workflows/manually-running-a-workflow)

or trigger them by making a change on our fork and pushing to
`main`
or making a pull request.

### Debug tests running remotely

To debug the tests, you can open a shell
in the exact same environment that the tests are run in:

```
modal shell -m my_pkg.ci
```

We used the
`shell`
feature heavily while developing this pattern!

*Note*
: On the Modal worker, the
`pytest`
command is run from the home directory,
`/root`
,
which contains the
`tests`
folder, but the
`modal shell`
command will
drop you at the top of the filesystem,
`/`
.

[Run Continuous Integration (CI) Tests on Modal](#run-continuous-integration-ci-tests-on-modal)

[Setup](#setup)

[Usage](#usage)

[Run tests remotely on Modal](#run-tests-remotely-on-modal)

[Run tests on Modal from GitHub Actions](#run-tests-on-modal-from-github-actions)

[Debug tests running remotely](#debug-tests-running-remotely)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/cloud_bucket_mount_loras
================================================================================

LoRAs Galore: Create a LoRA Playground with Modal, Gradio, and S3
=================================================================

This example shows how to mount an S3 bucket in a Modal app using
[`CloudBucketMount`](https://modal.com/docs/reference/modal.CloudBucketMount)

.
We will download a bunch of LoRA adapters from the
[HuggingFace Hub](https://huggingface.co/models)

into our S3 bucket
then read from that bucket, on the fly, when doing inference.

By default, we use the
[IKEA instructions LoRA](https://huggingface.co/ostris/ikea-instructions-lora-sdxl)

as an example,
which produces the following image when prompted to generate “IKEA instructions for building a GPU rig for deep learning”:

![IKEA instructions for building a GPU rig for deep learning](/_app/immutable/assets/ikea-instructions-for-building-a-gpu-rig-for-deep-learning.DcGj0diD.png)

By the end of this example, we’ve deployed a “playground” app where anyone with a browser can try
out these custom models. That’s the power of Modal: custom, autoscaling AI applications, deployed in seconds.
You can try out our deployment
[here](https://modal-labs-examples--loras-galore-ui.modal.run)

.

Basic setup
-----------

```
import io
import os
from pathlib import Path
from typing import Optional

import modal
```

You will need to have an S3 bucket and AWS credentials to run this example. Refer to the documentation
for the detailed
[IAM permissions](https://modal.com/docs/guide/cloud-bucket-mounts#iam-permissions)

those credentials will need.

After you are done creating a bucket and configuring IAM settings,
you now need to create a
[Modal Secret](https://modal.com/docs/guide/secrets)

. Navigate to the “Secrets” tab and
click on the AWS card, then fill in the fields with the AWS key and secret created
previously. Name the Secret
`s3-bucket-secret`
.

```
bucket_secret = modal.Secret.from_name(
    "s3-bucket-secret",
    required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"],
)

MOUNT_PATH: Path = Path("/mnt/bucket")
LORAS_PATH: Path = MOUNT_PATH / "loras/v5"

BASE_MODEL = "stabilityai/stable-diffusion-xl-base-1.0"
CACHE_DIR = "/hf-cache"
```

Modal runs serverless functions inside containers.
The environments those functions run in are defined by
the container
`Image`
. The line below constructs an image
with the dependencies we need — no need to install them locally.

```
image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "huggingface_hub==0.21.4",
        "transformers==4.38.2",
        "diffusers==0.26.3",
        "peft==0.9.0",
        "accelerate==0.27.2",
    )
    .env({"HF_HUB_CACHE": CACHE_DIR})
)

with image.imports():
    # we import these dependencies only inside the container
    import diffusers
    import huggingface_hub
    import torch
```

We attach the S3 bucket to all the Modal functions in this app by mounting it on the filesystem they see,
passing a
`CloudBucketMount`
to the
`volumes`
dictionary argument. We can read and write to this mounted bucket
(almost) as if it were a local directory.

```
app = modal.App(
    "loras-galore",
    image=image,
    volumes={
        MOUNT_PATH: modal.CloudBucketMount(
            "modal-s3mount-test-bucket",
            secret=bucket_secret,
        )
    },
)
```

For the base model, we’ll use a modal.Volume to store the Hugging Face cache.

```
cache_volume = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.function(image=image, volumes={CACHE_DIR: cache_volume})
def download_model():
    loc = huggingface_hub.snapshot_download(repo_id=BASE_MODEL)
    print(f"Saved model to {loc}")
```

Acquiring LoRA weights
----------------------

`search_loras()`
will use the Hub API to search for LoRAs. We limit LoRAs
to a maximum size to avoid downloading very large model weights.
We went with 800 MiB, but feel free to adapt to what works best for you.

```
@app.function(secrets=[bucket_secret])
def search_loras(limit: int, max_model_size: int = 1024 * 1024 * 1024):
    api = huggingface_hub.HfApi()

    model_ids: list[str] = []
    for model in api.list_models(
        tags=["lora", f"base_model:{BASE_MODEL}"],
        library="diffusers",
        sort="downloads",  # sort by most downloaded
    ):
        try:
            model_size = 0
            for file in api.list_files_info(model.id):
                model_size += file.size

        except huggingface_hub.utils.GatedRepoError:
            print(f"gated model ({model.id}); skipping")
            continue

        # Skip models that are larger than file limit.
        if model_size > max_model_size:
            print(f"model {model.id} is too large; skipping")
            continue

        model_ids.append(model.id)
        if len(model_ids) >= limit:
            return model_ids

    return model_ids
```

We want to take the LoRA weights we found and move them from Hugging Face onto S3,
where they’ll be accessible, at short latency and high throughput, for our Modal functions.
Downloading files in this mount will automatically upload files to S3.
To speed things up, we will run this function in parallel using Modal’s
[`map`](https://modal.com/docs/reference/modal.Function#map)

.

```
@app.function()
def download_lora(repository_id: str) -> Optional[str]:
    os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

    # CloudBucketMounts will report 0 bytes of available space leading to many
    # unnecessary warnings, so we patch the method that emits those warnings.
    from huggingface_hub import file_download

    file_download._check_disk_space = lambda x, y: False

    repository_path = LORAS_PATH / repository_id
    try:
        # skip models we've already downloaded
        if not repository_path.exists():
            huggingface_hub.snapshot_download(
                repository_id,
                local_dir=repository_path.as_posix().replace(".", "_"),
                allow_patterns=["*.safetensors"],
            )
        downloaded_lora = len(list(repository_path.rglob("*.safetensors"))) > 0
    except OSError:
        downloaded_lora = False
    except FileNotFoundError:
        downloaded_lora = False
    if downloaded_lora:
        return repository_id
    else:
        return None
```

Inference with LoRAs
--------------------

We define a
`StableDiffusionLoRA`
class to organize our inference code.
We load Stable Diffusion XL 1.0 as a base model, then, when doing inference,
we load whichever LoRA the user specifies from the S3 bucket.
For more on the decorators we use on the methods below to speed up building and booting,
check out the
[container lifecycle hooks guide](https://modal.com/docs/guide/lifecycle-functions)

.

```
@app.cls(
    gpu="a10g",  # A10G GPUs are great for inference
    volumes={CACHE_DIR: cache_volume},  # We cache the base model
)
class StableDiffusionLoRA:
    @modal.enter()  # when a new container starts, we load the base model into the GPU
    def load(self):
        self.pipe = diffusers.DiffusionPipeline.from_pretrained(
            BASE_MODEL, torch_dtype=torch.float16
        ).to("cuda")

    @modal.method()  # at inference time, we pull in the LoRA weights and pass the final model the prompt
    def run_inference_with_lora(
        self, lora_id: str, prompt: str, seed: int = 8888
    ) -> bytes:
        for file in (LORAS_PATH / lora_id).rglob("*.safetensors"):
            self.pipe.load_lora_weights(lora_id, weight_name=file.name)
            break

        lora_scale = 0.9
        image = self.pipe(
            prompt,
            num_inference_steps=10,
            cross_attention_kwargs={"scale": lora_scale},
            generator=torch.manual_seed(seed),
        ).images[0]

        buffer = io.BytesIO()
        image.save(buffer, format="PNG")

        return buffer.getvalue()
```

Try it locally!
---------------

To use our inference code from our local command line, we add a
`local_entrypoint`
to our
`app`
.
Run it using
`modal run cloud_bucket_mount_loras.py`
, and pass
`--help`
to see the available options.

The inference code will run on our machines, but the results will be available on yours.

```
@app.local_entrypoint()
def main(
    limit: int = 100,
    example_lora: str = "ostris/ikea-instructions-lora-sdxl",
    prompt: str = "IKEA instructions for building a GPU rig for deep learning",
    seed: int = 8888,
):
    # Download LoRAs in parallel.
    lora_model_ids = [example_lora]
    lora_model_ids += search_loras.remote(limit)

    downloaded_loras = []
    for model in download_lora.map(lora_model_ids):
        if model:
            downloaded_loras.append(model)

    print(f"downloaded {len(downloaded_loras)} loras => {downloaded_loras}")

    # Run inference using one of the downloaded LoRAs.
    byte_stream = StableDiffusionLoRA().run_inference_with_lora.remote(
        example_lora, prompt, seed
    )
    dir = Path("/tmp/stable-diffusion-xl")
    if not dir.exists():
        dir.mkdir(exist_ok=True, parents=True)

    output_path = dir / f"{as_slug(prompt.lower())}.png"
    print(f"Saving it to {output_path}")
    with open(output_path, "wb") as f:
        f.write(byte_stream)
```

LoRA Exploradora: A hosted Gradio interface
-------------------------------------------

Command line tools are cool, but we can do better!
With the Gradio library by Hugging Face, we can create a simple web interface
around our Python inference function, then use Modal to host it for anyone to try out.

To set up your own, run
`modal deploy cloud_bucket_mount_loras.py`
and navigate to the URL it prints out.
If you’re playing with the code, use
`modal serve`
instead to see changes live.

```
web_image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "fastapi[standard]==0.115.4",
    "gradio~=5.7.1",
    "pillow~=10.2.0",
)

@app.function(
    image=web_image,
    min_containers=1,
    scaledown_window=60 * 20,
    # gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow it to scale to 100 concurrent inputs
    max_containers=1,
)
@modal.concurrent(max_inputs=100)
@modal.asgi_app()
def ui():
    """A simple Gradio interface around our LoRA inference."""
    import io

    import gradio as gr
    from fastapi import FastAPI
    from gradio.routes import mount_gradio_app
    from PIL import Image

    # determine which loras are available
    lora_ids = [
        f"{lora_dir.parent.stem}/{lora_dir.stem}" for lora_dir in LORAS_PATH.glob("*/*")
    ]

    # pick one to be default, set a default prompt
    default_lora_id = (
        "ostris/ikea-instructions-lora-sdxl"
        if "ostris/ikea-instructions-lora-sdxl" in lora_ids
        else lora_ids[0]
    )
    default_prompt = (
        "IKEA instructions for building a GPU rig for deep learning"
        if default_lora_id == "ostris/ikea-instructions-lora-sdxl"
        else "text"
    )

    # the simple path to making an app on Gradio is an Interface: a UI wrapped around a function.
    def go(lora_id: str, prompt: str, seed: int) -> Image:
        return Image.open(
            io.BytesIO(
                StableDiffusionLoRA().run_inference_with_lora.remote(
                    lora_id, prompt, seed
                )
            ),
        )

    iface = gr.Interface(
        go,
        inputs=[  # the inputs to go/our inference function
            gr.Dropdown(choices=lora_ids, value=default_lora_id, label="👉 LoRA ID"),
            gr.Textbox(default_prompt, label="🎨 Prompt"),
            gr.Number(value=8888, label="🎲 Random Seed"),
        ],
        outputs=gr.Image(label="Generated Image"),
        # some extra bits to make it look nicer
        title="LoRAs Galore",
        description="# Try out some of the top custom SDXL models!"
        "\n\nPick a LoRA finetune of SDXL from the dropdown, then prompt it to generate an image."
        "\n\nCheck out [the code on GitHub](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/cloud_bucket_mount_loras.py)"
        " if you want to create your own version or just see how it works."
        "\n\nPowered by [Modal](https://modal.com) 🚀",
        theme="soft",
        allow_flagging="never",
    )

    return mount_gradio_app(app=FastAPI(), blocks=iface, path="/")

def as_slug(name):
    """Converts a string, e.g. a prompt, into something we can use as a filename."""
    import re

    s = str(name).strip().replace(" ", "-")
    s = re.sub(r"(?u)[^-\w.]", "", s)
    return s
```

[LoRAs Galore: Create a LoRA Playground with Modal, Gradio, and S3](#loras-galore-create-a-lora-playground-with-modal-gradio-and-s3)

[Basic setup](#basic-setup)

[Acquiring LoRA weights](#acquiring-lora-weights)

[Inference with LoRAs](#inference-with-loras)

[Try it locally!](#try-it-locally)

[LoRA Exploradora: A hosted Gradio interface](#lora-exploradora-a-hosted-gradio-interface)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/cloud_bucket_mount_loras.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/comfyapp
================================================================================

Run Flux on ComfyUI as an API
=============================

In this example, we show you how to turn a
[ComfyUI](https://github.com/comfyanonymous/ComfyUI)

workflow into a scalable API endpoint.

Quickstart
----------

To run this simple text-to-image
[Flux Schnell workflow](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/comfyui/workflow_api.json)

as an API:

1. Deploy ComfyUI behind a web endpoint:

```
modal deploy 06_gpu_and_ml/comfyui/comfyapp.py
```

2. In another terminal, run inference:

```
python 06_gpu_and_ml/comfyui/comfyclient.py --modal-workspace $(modal profile current) --prompt "Surreal dreamscape with floating islands, upside-down waterfalls, and impossible geometric structures, all bathed in a soft, ethereal light"
```

![example comfyui image](https://modal-cdn.com/cdnbot/flux_gen_imagesenr_0w3_209b7170.webp)

The first inference will take ~1m since the container needs to launch the ComfyUI server and load Flux into memory. Successive calls on a warm container should take a few seconds.

Installing ComfyUI
------------------

We use
[comfy-cli](https://github.com/Comfy-Org/comfy-cli)

to install ComfyUI and its dependencies.

```
import json
import subprocess
import uuid
from pathlib import Path
from typing import Dict

import modal
import modal.experimental

image = (  # build up a Modal Image to run ComfyUI, step by step
    modal.Image.debian_slim(  # start from basic Linux with Python
        python_version="3.11"
    )
    .apt_install("git")  # install git to clone ComfyUI
    .pip_install("fastapi[standard]==0.115.4")  # install web dependencies
    .pip_install("comfy-cli==1.4.1")  # install comfy-cli
    .run_commands(  # use comfy-cli to install ComfyUI and its dependencies
        "comfy --skip-prompt install --fast-deps --nvidia --version 0.3.41"
    )
)
```

Downloading custom nodes
------------------------

We’ll also use
`comfy-cli`
to download custom nodes, in this case the popular
[WAS Node Suite](https://github.com/WASasquatch/was-node-suite-comfyui)

.

Use the
[ComfyUI Registry](https://registry.comfy.org/)

to find the specific custom node name to use with this command.

```
image = (
    image.run_commands(  # download a custom node
        "comfy node install --fast-deps was-node-suite-comfyui@1.0.2"
    )
    # Add .run_commands(...) calls for any other custom nodes you want to download
)
```

See
[this post](https://modal.com/blog/comfyui-custom-nodes)

for more examples
on how to install popular custom nodes like ComfyUI Impact Pack and ComfyUI IPAdapter Plus.

Downloading models
------------------

`comfy-cli`
also supports downloading models, but we’ve found it’s faster to use
[`hf_hub_download`](https://huggingface.co/docs/huggingface_hub/en/guides/download#download-a-single-file)

directly by:

1. Enabling
   [faster downloads](https://huggingface.co/docs/huggingface_hub/en/guides/download#faster-downloads)
2. Mounting the cache directory to a
   [Volume](https://modal.com/docs/guide/volumes)

By persisting the cache to a Volume, you avoid re-downloading the models every time you rebuild your image.

```
def hf_download():
    from huggingface_hub import hf_hub_download

    flux_model = hf_hub_download(
        repo_id="Comfy-Org/flux1-schnell",
        filename="flux1-schnell-fp8.safetensors",
        cache_dir="/cache",
    )

    # symlink the model to the right ComfyUI directory
    subprocess.run(
        f"ln -s {flux_model} /root/comfy/ComfyUI/models/checkpoints/flux1-schnell-fp8.safetensors",
        shell=True,
        check=True,
    )

vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

image = (
    # install huggingface_hub with hf_transfer support to speed up downloads
    image.pip_install("huggingface_hub[hf_transfer]==0.30.0")
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
    .run_function(
        hf_download,
        # persist the HF cache to a Modal Volume so future runs don't re-download models
        volumes={"/cache": vol},
    )
)
```

Lastly, copy the ComfyUI workflow JSON to the container.

```
image = image.add_local_file(
    Path(__file__).parent / "workflow_api.json", "/root/workflow_api.json"
)
```

Running ComfyUI interactively
-----------------------------

Spin up an interactive ComfyUI server by wrapping the
`comfy launch`
command in a Modal Function
and serving it as a
[web server](https://modal.com/docs/guide/webhooks#non-asgi-web-servers)

.

```
app = modal.App(name="example-comfyui", image=image)

@app.function(
    max_containers=1,  # limit interactive session to 1 container
    gpu="L40S",  # good starter GPU for inference
    volumes={"/cache": vol},  # mounts our cached models
)
@modal.concurrent(
    max_inputs=10
)  # required for UI startup process which runs several API calls concurrently
@modal.web_server(8000, startup_timeout=60)
def ui():
    subprocess.Popen("comfy launch -- --listen 0.0.0.0 --port 8000", shell=True)
```

At this point you can run
`modal serve 06_gpu_and_ml/comfyui/comfyapp.py`
and open the UI in your browser for the classic ComfyUI experience.

Remember to
**close your UI tab**
when you are done developing.
This will close the connection with the container serving ComfyUI and you will stop being charged.

Running ComfyUI as an API
-------------------------

To run a workflow as an API:

1. Stand up a “headless” ComfyUI server in the background when the app starts.
2. Define an
   `infer`
   method that takes in a workflow path and runs the workflow on the ComfyUI server.
3. Create a web handler
   `api`
   as a web endpoint, so that we can run our workflow as a service and accept inputs from clients.

We group all these steps into a single Modal
`cls`
object, which we’ll call
`ComfyUI`
.

```
@app.cls(
    scaledown_window=300,  # 5 minute container keep alive after it processes an input
    gpu="L40S",
    volumes={"/cache": vol},
)
@modal.concurrent(max_inputs=5)  # run 5 inputs per container
class ComfyUI:
    port: int = 8000

    @modal.enter()
    def launch_comfy_background(self):
        # launch the ComfyUI server exactly once when the container starts
        cmd = f"comfy launch --background -- --port {self.port}"
        subprocess.run(cmd, shell=True, check=True)

    @modal.method()
    def infer(self, workflow_path: str = "/root/workflow_api.json"):
        # sometimes the ComfyUI server stops responding (we think because of memory leaks), so this makes sure it's still up
        self.poll_server_health()

        # runs the comfy run --workflow command as a subprocess
        cmd = f"comfy run --workflow {workflow_path} --wait --timeout 1200 --verbose"
        subprocess.run(cmd, shell=True, check=True)

        # completed workflows write output images to this directory
        output_dir = "/root/comfy/ComfyUI/output"

        # looks up the name of the output image file based on the workflow
        workflow = json.loads(Path(workflow_path).read_text())
        file_prefix = [
            node.get("inputs")
            for node in workflow.values()
            if node.get("class_type") == "SaveImage"
        ][0]["filename_prefix"]

        # returns the image as bytes
        for f in Path(output_dir).iterdir():
            if f.name.startswith(file_prefix):
                return f.read_bytes()

    @modal.fastapi_endpoint(method="POST")
    def api(self, item: Dict):
        from fastapi import Response

        workflow_data = json.loads(
            (Path(__file__).parent / "workflow_api.json").read_text()
        )

        # insert the prompt
        workflow_data["6"]["inputs"]["text"] = item["prompt"]

        # give the output image a unique id per client request
        client_id = uuid.uuid4().hex
        workflow_data["9"]["inputs"]["filename_prefix"] = client_id

        # save this updated workflow to a new file
        new_workflow_file = f"{client_id}.json"
        json.dump(workflow_data, Path(new_workflow_file).open("w"))

        # run inference on the currently running container
        img_bytes = self.infer.local(new_workflow_file)

        return Response(img_bytes, media_type="image/jpeg")

    def poll_server_health(self) -> Dict:
        import socket
        import urllib

        try:
            # check if the server is up (response should be immediate)
            req = urllib.request.Request(f"http://127.0.0.1:{self.port}/system_stats")
            urllib.request.urlopen(req, timeout=5)
            print("ComfyUI server is healthy")
        except (socket.timeout, urllib.error.URLError) as e:
            # if no response in 5 seconds, stop the container
            print(f"Server health check failed: {str(e)}")
            modal.experimental.stop_fetching_inputs()

            # all queued inputs will be marked "Failed", so you need to catch these errors in your client and then retry
            raise Exception("ComfyUI server is not healthy, stopping container")
```

This serves the
`workflow_api.json`
in this repo. When deploying your own workflows, make sure you select the “Export (API)” option in the ComfyUI menu:

![comfyui menu](https://modal-cdn.com/cdnbot/comfyui_menugo5j8ahx_27d72c45.webp)

More resources
--------------

* Use
  [memory snapshots](https://modal.com/docs/guide/memory-snapshot)

  to speed up cold starts (check out the
  `memory_snapshot`
  directory on
  [Github](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/comfyui)

  )
* Run a ComfyUI workflow as a
  [Python script](https://modal.com/blog/comfyui-prototype-to-production)
* When to use
  [A1111 vs ComfyUI](https://modal.com/blog/a1111-vs-comfyui)
* Understand tradeoffs of parallel processing strategies when
  [scaling ComfyUI](https://modal.com/blog/scaling-comfyui)

[Run Flux on ComfyUI as an API](#run-flux-on-comfyui-as-an-api)

[Quickstart](#quickstart)

[Installing ComfyUI](#installing-comfyui)

[Downloading custom nodes](#downloading-custom-nodes)

[Downloading models](#downloading-models)

[Running ComfyUI interactively](#running-comfyui-interactively)

[Running ComfyUI as an API](#running-comfyui-as-an-api)

[More resources](#more-resources)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 06_gpu_and_ml/comfyui/comfyapp.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/controlnet_gradio_demos
================================================================================

Play with the ControlNet demos
==============================

This example allows you to play with all 10 demonstration Gradio apps from the new and amazing ControlNet project.
ControlNet provides a minimal interface allowing users to use images to constrain StableDiffusion’s generation process.
With ControlNet, users can easily condition the StableDiffusion image generation with different spatial contexts
including a depth maps, segmentation maps, scribble drawings, and keypoints!

[

](https://user-images.githubusercontent.com/12058921/222927911-3ab52dd1-f2ee-4fb8-97e8-dafbf96ed5c5.mp4)

Imports and config preamble
---------------------------

```
import importlib
import os
import pathlib
from dataclasses import dataclass, field

import modal
from fastapi import FastAPI
```

Below are the configuration objects for all
**10**
demos provided in the original
[lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet)

repo.
The demos each depend on their own custom pretrained StableDiffusion model, and these models are 5-6GB each.
We can only run one demo at a time, so this module avoids downloading the model and ‘detector’ dependencies for
all 10 demos and instead uses the demo configuration object to download only what’s necessary for the chosen demo.

Even just limiting our dependencies setup to what’s required for one demo, the resulting container image is
*huge*
.

```
@dataclass(frozen=True)
class DemoApp:
    """Config object defining a ControlNet demo app's specific dependencies."""

    name: str
    model_files: list[str]
    detector_files: list[str] = field(default_factory=list)

demos = [
    DemoApp(
        name="canny2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth"
        ],
    ),
    DemoApp(
        name="depth2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt"
        ],
    ),
    DemoApp(
        name="fake_scribble2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth"
        ],
    ),
    DemoApp(
        name="hed2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth"
        ],
    ),
    DemoApp(
        name="hough2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth",
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth",
        ],
    ),
    DemoApp(
        name="normal2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth"
        ],
    ),
    DemoApp(
        name="pose2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth",
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth",
        ],
    ),
    DemoApp(
        name="scribble2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth"
        ],
    ),
    DemoApp(
        name="scribble2image_interactive",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth"
        ],
    ),
    DemoApp(
        name="seg2image",
        model_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth"
        ],
        detector_files=[
            "https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth"
        ],
    ),
]
demos_map: dict[str, DemoApp] = {d.name: d for d in demos}
```

Pick a demo, any demo
---------------------

Simply by changing the
`DEMO_NAME`
below, you can change which ControlNet demo app is setup
and run by this Modal script.

```
DEMO_NAME = "scribble2image"  # Change this value to change the active demo app.
selected_demo = demos_map[DEMO_NAME]
```

Setting up the dependencies
---------------------------

ControlNet requires
*a lot*
of dependencies which could be fiddly to setup manually, but Modal’s programmatic
container image building Python APIs handle this complexity straightforwardly and automatically.

To run any of the 10 demo apps, we need the following:

1. a base Python 3 Linux image (we use Debian Slim)
2. a bunch of third party PyPi packages
3. `git`
   , so that we can download the ControlNet source code (there’s no
   `controlnet`
   PyPi package)
4. some image process Linux system packages, including
   `ffmpeg`
5. and demo specific pre-trained model and detector
   `.pth`
   files

That’s a lot! Fortunately, the code below is already written for you that stitches together a working container image
ready to produce remarkable ControlNet images.

**Note:**
a ControlNet model pipeline is
[now available in Huggingface’s
`diffusers`
package](https://huggingface.co/blog/controlnet)

. But this does not contain the demo apps.

```
def download_file(url: str, output_path: pathlib.Path):
    import httpx
    from tqdm import tqdm

    with open(output_path, "wb") as download_file:
        with httpx.stream("GET", url, follow_redirects=True) as response:
            total = int(response.headers["Content-Length"])
            with tqdm(
                total=total, unit_scale=True, unit_divisor=1024, unit="B"
            ) as progress:
                num_bytes_downloaded = response.num_bytes_downloaded
                for chunk in response.iter_bytes():
                    download_file.write(chunk)
                    progress.update(
                        response.num_bytes_downloaded - num_bytes_downloaded
                    )
                    num_bytes_downloaded = response.num_bytes_downloaded

def download_demo_files() -> None:
    """
    The ControlNet repo instructs: 'Make sure that SD models are put in "ControlNet/models".'
    'ControlNet' is just the repo root, so we place in /root/models.

    The ControlNet repo also instructs: 'Make sure that... detectors are put in "ControlNet/annotator/ckpts".'
    'ControlNet' is just the repo root, so we place in /root/annotator/ckpts.
    """
    demo = demos_map[os.environ["DEMO_NAME"]]
    models_dir = pathlib.Path("/root/models")
    for url in demo.model_files:
        filepath = pathlib.Path(url).name
        download_file(url=url, output_path=models_dir / filepath)
        print(f"download complete for {filepath}")

    detectors_dir = pathlib.Path("/root/annotator/ckpts")
    for url in demo.detector_files:
        filepath = pathlib.Path(url).name
        download_file(url=url, output_path=detectors_dir / filepath)
        print(f"download complete for {filepath}")
    print("🎉 finished baking demo file(s) into image.")

image = (
    modal.Image.debian_slim(python_version="3.10")
    .pip_install(
        "fastapi[standard]==0.115.4",
        "pydantic==2.9.1",
        "starlette==0.41.2",
        "gradio==3.16.2",
        "albumentations==1.3.0",
        "opencv-contrib-python",
        "imageio==2.9.0",
        "imageio-ffmpeg==0.4.2",
        "pytorch-lightning==1.5.0",
        "omegaconf==2.1.1",
        "test-tube>=0.7.5",
        "streamlit==1.12.1",
        "einops==0.3.0",
        "transformers==4.19.2",
        "webdataset==0.2.5",
        "kornia==0.6",
        "open_clip_torch==2.0.2",
        "invisible-watermark>=0.1.5",
        "streamlit-drawable-canvas==0.8.0",
        "torchmetrics==0.6.0",
        "timm==0.6.12",
        "addict==2.4.0",
        "yapf==0.32.0",
        "prettytable==3.6.0",
        "safetensors==0.2.7",
        "basicsr==1.4.2",
        "tqdm~=4.64.1",
    )
    # xformers library offers performance improvement.
    .pip_install("xformers", pre=True)
    .apt_install("git")
    # Here we place the latest ControlNet repository code into /root.
    # Because /root is almost empty, but not entirely empty, `git clone` won't work,
    # so this `init` then `checkout` workaround is used.
    .run_commands(
        "cd /root && git init .",
        "cd /root && git remote add --fetch origin https://github.com/lllyasviel/ControlNet.git",
        "cd /root && git checkout main",
    )
    .apt_install("ffmpeg", "libsm6", "libxext6")
    .run_function(
        download_demo_files,
        secrets=[modal.Secret.from_dict({"DEMO_NAME": DEMO_NAME})],
    )
)
app = modal.App(name="example-controlnet", image=image)

web_app = FastAPI()
```

Serving the Gradio web UI
-------------------------

Each ControlNet gradio demo module exposes a
`block`
Gradio interface running in queue-mode,
which is initialized in module scope on import and served on
`0.0.0.0`
. We want the block interface object,
but the queueing and launched webserver aren’t compatible with Modal’s serverless web endpoint interface,
so in the
`import_gradio_app_blocks`
function we patch out these behaviors.

```
def import_gradio_app_blocks(demo: DemoApp):
    from gradio import blocks

    # The ControlNet repo demo scripts are written to be run as
    # standalone scripts, and have a lot of code that executes
    # in global scope on import, including the launch of a Gradio web server.
    # We want Modal to control the Gradio web app serving, so we
    # monkeypatch the .launch() function to be a no-op.
    blocks.Blocks.launch = lambda self, server_name: print(
        "launch() has been monkeypatched to do nothing."
    )

    # each demo app module is a file like gradio_{name}.py
    module_name = f"gradio_{demo.name}"
    mod = importlib.import_module(module_name)
    blocks = mod.block
    # disable queueing mode, which is incompatible with our Modal web app setup.
    blocks.enable_queue = False
    return blocks
```

Because the ControlNet gradio apps are so time and compute intensive to cold-start,
the web app function is limited to running just 1 warm container (max\_containers=1).
This way, while playing with the demos we can pay the cold-start cost once and have
all web requests hit the same warm container.
Spinning up extra containers to handle additional requests would not be efficient
given the cold-start time.
We set the scaledown\_window to 600 seconds so the container will be kept
running for 10 minutes after the last request, to keep the app responsive in case
of continued experimentation.

```
@app.function(
    gpu="A10G",
    max_containers=1,
    scaledown_window=600,
)
@modal.asgi_app()
def run():
    from gradio.routes import mount_gradio_app

    # mount for execution on Modal
    return mount_gradio_app(
        app=web_app,
        blocks=import_gradio_app_blocks(demo=selected_demo),
        path="/",
    )
```

Have fun!
---------

Serve your chosen demo app with
`modal serve controlnet_gradio_demos.py`
. If you don’t have any images ready at hand,
try one that’s in the
`06_gpu_and_ml/controlnet/demo_images/`
folder.

StableDiffusion was already impressive enough, but ControlNet’s ability to so accurately and intuitively constrain
the image generation process is sure to put a big, dumb grin on your face.

[Play with the ControlNet demos](#play-with-the-controlnet-demos)

[Imports and config preamble](#imports-and-config-preamble)

[Pick a demo, any demo](#pick-a-demo-any-demo)

[Setting up the dependencies](#setting-up-the-dependencies)

[Serving the Gradio web UI](#serving-the-gradio-web-ui)

[Have fun!](#have-fun)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 06_gpu_and_ml/controlnet/controlnet_gradio_demos.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/count_faces
================================================================================

Run OpenCV face detection on an image
=====================================

This example shows how you can use OpenCV on Modal to detect faces in an image. We use
the
`opencv-python`
package to load the image and the
`opencv`
library to
detect faces. The function
`count_faces`
takes an image as input and returns
the number of faces detected in the image.

The code below also shows how you can create wrap this function
in a simple FastAPI server to create a web interface.

```
import os

import modal

app = modal.App("example-count-faces")

open_cv_image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("python3-opencv")
    .pip_install(
        "fastapi[standard]==0.115.4",
        "opencv-python~=4.10.0",
        "numpy<2",
    )
)

@app.function(image=open_cv_image)
def count_faces(image_bytes: bytes) -> int:
    import cv2
    import numpy as np

    # Example borrowed from https://towardsdatascience.com/face-detection-in-2-minutes-using-opencv-python-90f89d7c0f81
    # Load the cascade
    face_cascade = cv2.CascadeClassifier(
        os.path.join(cv2.data.haarcascades, "haarcascade_frontalface_default.xml")
    )
    # Read the input image
    np_bytes = np.frombuffer(image_bytes, dtype=np.uint8)
    img = cv2.imdecode(np_bytes, cv2.IMREAD_COLOR)
    # Convert into grayscale
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    # Detect faces
    faces = face_cascade.detectMultiScale(gray, 1.1, 4)
    return len(faces)

@app.function(
    image=modal.Image.debian_slim(python_version="3.11").pip_install("inflect")
)
@modal.asgi_app()
def web():
    import inflect
    from fastapi import FastAPI, File, HTTPException, UploadFile
    from fastapi.responses import HTMLResponse

    app = FastAPI()

    @app.get("/", response_class=HTMLResponse)
    async def index():
        """
        Render an HTML form for file upload.
        """
        return """
        <html>
            <head>
                <title>Face Counter</title>
            </head>
            <body>
                <h1>Upload an Image to Count Faces</h1>
                <form action="/process" method="post" enctype="multipart/form-data">
                    <input type="file" name="file" id="file" accept="image/*" required />
                    <button type="submit">Upload</button>
                </form>
            </body>
        </html>
        """

    @app.post("/process", response_class=HTMLResponse)
    async def process(file: UploadFile = File(...)):
        """
        Process the uploaded image and return the number of faces detected.
        """
        try:
            file_content = await file.read()
            num_faces = await count_faces.remote.aio(file_content)
            return f"""
            <html>
                <head>
                    <title>Face Counter Result</title>
                </head>
                <body>
                    <h1>{inflect.engine().number_to_words(num_faces).title()} {"Face" if num_faces == 1 else "Faces"} Detected</h1>
                    <h2>{"😀" * num_faces}</h2>
                    <a href="/">Go back</a>
                </body>
            </html>
            """
        except Exception as e:
            raise HTTPException(
                status_code=400, detail=f"Error processing image: {str(e)}"
            )

    return app
```

[Run OpenCV face detection on an image](#run-opencv-face-detection-on-an-image)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 07_web_endpoints/count_faces.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/cron_datasette
================================================================================

Publish interactive datasets with Datasette
===========================================

![Datasette user interface](https://modal-cdn.com/cdnbot/imdb_datasetteqzaj3q9d_a83d82fd.webp)

Build and deploy an interactive movie database that automatically updates daily with the latest IMDb data.
This example shows how to serve a Datasette application on Modal with millions of movie and TV show records.

Try it out for yourself
[here](https://modal-labs-examples--example-cron-datasette-ui.modal.run)

.

Along the way, we will learn how to use the following Modal features:

* [Volumes](https://modal.com/docs/guide/volumes)

  : a persisted volume lets us store and grow the published dataset over time.
* [Scheduled functions](https://modal.com/docs/guide/cron)

  : the underlying dataset is refreshed daily, so we schedule a function to run daily.
* [Web endpoints](https://modal.com/docs/guide/webhooks)

  : exposes the Datasette application for web browser interaction and API requests.

Basic setup
-----------

Let’s get started writing code.
For the Modal container image we need a few Python packages.

```
import asyncio
import gzip
import pathlib
import shutil
import tempfile
from datetime import datetime
from urllib.request import urlretrieve

import modal

app = modal.App("example-cron-datasette")
cron_image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "datasette==0.65.1", "sqlite-utils==3.38", "tqdm~=4.67.1", "setuptools<80"
)
```

Persistent dataset storage
--------------------------

To separate database creation and maintenance from serving, we’ll need the underlying
database file to be stored persistently. To achieve this we use a
[Volume](https://modal.com/docs/guide/volumes)

.

```
volume = modal.Volume.from_name(
    "example-cron-datasette-cache-vol", create_if_missing=True
)
DB_FILENAME = "imdb.db"
VOLUME_DIR = "/cache-vol"
DATA_DIR = pathlib.Path(VOLUME_DIR, "imdb-data")
DB_PATH = pathlib.Path(VOLUME_DIR, DB_FILENAME)
```

Getting a dataset
-----------------

[IMDb Datasets](https://datasets.imdbws.com/)

are available publicly and are updated daily.
We will download the title.basics.tsv.gz file which contains basic information about all titles (movies, TV shows, etc.).
Since we are serving an interactive database which updates daily, we will download the files into a temporary directory and then move them to the volume to prevent downtime.

```
BASE_URL = "https://datasets.imdbws.com/"
IMDB_FILES = [
    "title.basics.tsv.gz",
]

@app.function(
    image=cron_image,
    volumes={VOLUME_DIR: volume},
    retries=2,
    timeout=1800,
)
def download_dataset(force_refresh=False):
    """Download IMDb dataset files."""
    if DATA_DIR.exists() and not force_refresh:
        print(
            f"Dataset already present and force_refresh={force_refresh}. Skipping download."
        )
        return

    TEMP_DATA_DIR = pathlib.Path(VOLUME_DIR, "imdb-data-temp")
    if TEMP_DATA_DIR.exists():
        shutil.rmtree(TEMP_DATA_DIR)

    TEMP_DATA_DIR.mkdir(parents=True, exist_ok=True)

    print("Downloading IMDb dataset...")

    try:
        for filename in IMDB_FILES:
            print(f"Downloading {filename}...")
            url = BASE_URL + filename
            output_path = TEMP_DATA_DIR / filename

            urlretrieve(url, output_path)
            print(f"Successfully downloaded {filename}")

        if DATA_DIR.exists():
            # move the current data to a backup location
            OLD_DATA_DIR = pathlib.Path(VOLUME_DIR, "imdb-data-old")
            if OLD_DATA_DIR.exists():
                shutil.rmtree(OLD_DATA_DIR)
            shutil.move(DATA_DIR, OLD_DATA_DIR)

            # move the new data into place
            shutil.move(TEMP_DATA_DIR, DATA_DIR)

            # clean up the old data
            shutil.rmtree(OLD_DATA_DIR)
        else:
            shutil.move(TEMP_DATA_DIR, DATA_DIR)

        volume.commit()
        print("Finished downloading dataset.")

    except Exception as e:
        print(f"Error during download: {e}")
        if TEMP_DATA_DIR.exists():
            shutil.rmtree(TEMP_DATA_DIR)
        raise
```

Data processing
---------------

This dataset is no swamp, but a bit of data cleaning is still in order.
The following function reads a .tsv file, cleans the data and yields batches of records.

```
def parse_tsv_file(filepath, batch_size=50000, filter_year=None):
    """Parse a gzipped TSV file and yield batches of records."""
    import csv

    with gzip.open(filepath, "rt", encoding="utf-8") as gz_file:
        reader = csv.DictReader(gz_file, delimiter="\t")
        batch = []
        total_processed = 0

        for row in reader:
            # map missing values to None
            row = {k: (None if v == "\\N" else v) for k, v in row.items()}

            # remove nsfw data
            if row.get("isAdult") == "1":
                continue

            if filter_year:
                start_year = int(row.get("startYear", 0) or 0)
                if start_year < filter_year:
                    continue

            batch.append(row)
            total_processed += 1

            if len(batch) >= batch_size:
                yield batch
                batch = []

        # Yield any remaining records
        if batch:
            yield batch

        print(f"Finished processing {total_processed:,} titles.")
```

Inserting into SQLite
---------------------

With the TSV processing out of the way, we’re ready to create a SQLite database and feed data into it.

Importantly, the
`prep_db`
function mounts the same volume used by
`download_dataset`
, and rows are batch inserted with progress logged after each batch,
as the full IMDb dataset has millions of rows and does take some time to be fully inserted.

A more sophisticated implementation would only load new data instead of performing a full refresh,
but we’re keeping things simple for this example!
We will also create indexes for the titles table to speed up queries.

```
@app.function(
    image=cron_image,
    volumes={VOLUME_DIR: volume},
    timeout=900,
)
def prep_db(filter_year=None):
    """Process IMDb data files and create SQLite database."""
    import sqlite_utils
    import tqdm

    volume.reload()

    # Create database in a temporary directory first
    with tempfile.TemporaryDirectory() as tmpdir:
        tmpdir_path = pathlib.Path(tmpdir)
        tmp_db_path = tmpdir_path / DB_FILENAME

        db = sqlite_utils.Database(tmp_db_path)

        # Process title.basics.tsv.gz
        titles_file = DATA_DIR / "title.basics.tsv.gz"

        if titles_file.exists():
            titles_table = db["titles"]
            batch_count = 0
            total_processed = 0

            with tqdm.tqdm(desc="Processing titles", unit="batch", leave=True) as pbar:
                for i, batch in enumerate(
                    parse_tsv_file(
                        titles_file, batch_size=50000, filter_year=filter_year
                    )
                ):
                    titles_table.insert_all(batch, batch_size=50000, truncate=(i == 0))
                    batch_count += len(batch)
                    total_processed += len(batch)
                    pbar.update(1)
                    pbar.set_postfix({"titles": f"{total_processed:,}"})

            print(f"Total titles in database: {batch_count:,}")

            # Create indexes for titles so we can query the database faster
            print("Creating indexes...")
            titles_table.create_index(["tconst"], if_not_exists=True, unique=True)
            titles_table.create_index(["primaryTitle"], if_not_exists=True)
            titles_table.create_index(["titleType"], if_not_exists=True)
            titles_table.create_index(["startYear"], if_not_exists=True)
            titles_table.create_index(["genres"], if_not_exists=True)
            print("Created indexes for titles table")

        db.close()

        # Copy the database to the volume
        DB_PATH.parent.mkdir(parents=True, exist_ok=True)
        shutil.copyfile(tmp_db_path, DB_PATH)

    print("Syncing DB with volume.")
    volume.commit()
    print("Volume changes committed.")
```

Keep it fresh
-------------

IMDb updates their data daily, so we set up
a
[scheduled](https://modal.com/docs/guide/cron)

function to automatically refresh the database
every 24 hours.

```
@app.function(schedule=modal.Period(hours=24), timeout=4000)
def refresh_db():
    """Scheduled function to refresh the database daily."""
    print(f"Running scheduled refresh at {datetime.now()}")
    download_dataset.remote(force_refresh=True)
    prep_db.remote()
```

Web endpoint
------------

Hooking up the SQLite database to a Modal webhook is as simple as it gets.
The Modal
`@asgi_app`
decorator wraps a few lines of code: one
`import`
and a few
lines to instantiate the
`Datasette`
instance and return its app server.

First, let’s define a metadata object for the database.
This will be used to configure Datasette to display a custom UI with some pre-defined queries.

```
columns = {
    "tconst": "Unique identifier",
    "titleType": "Type (movie, tvSeries, short, etc.)",
    "primaryTitle": "Main title",
    "originalTitle": "Original language title",
    "startYear": "Release year",
    "endYear": "End year (for TV series)",
    "runtimeMinutes": "Runtime in minutes",
    "genres": "Comma-separated genres",
}

queries = {
    "movies_2024": {
        "sql": """
                        SELECT
                            primaryTitle as title,
                            genres,
                            runtimeMinutes as runtime
                        FROM titles
                        WHERE titleType = 'movie'
                        AND startYear = 2024
                        ORDER BY primaryTitle
                        LIMIT 100
                    """,
        "title": "Movies Released in 2024",
    },
    "longest_movies": {
        "sql": """
                        SELECT
                            primaryTitle as title,
                            startYear as year,
                            runtimeMinutes as runtime,
                            genres
                        FROM titles
                        WHERE titleType = 'movie'
                        AND runtimeMinutes IS NOT NULL
                        AND runtimeMinutes > 180
                        ORDER BY runtimeMinutes DESC
                        LIMIT 50
                    """,
        "title": "Longest Movies (3+ hours)",
    },
    "genre_breakdown": {
        "sql": """
                        SELECT
                            genres,
                            COUNT(*) as count
                        FROM titles
                        WHERE titleType = 'movie'
                        AND genres IS NOT NULL
                        GROUP BY genres
                        ORDER BY count DESC
                        LIMIT 25
                    """,
        "title": "Popular Genres",
    },
}

metadata = {
    "title": "IMDb Database Explorer",
    "description": "Explore IMDb movie and TV show data",
    "databases": {
        "imdb": {
            "tables": {
                "titles": {
                    "description": "Basic information about all titles (movies, TV shows, etc.)",
                    "columns": columns,
                }
            },
            "queries": {
                "movies_2024": queries["movies_2024"],
                "longest_movies": queries["longest_movies"],
                "genre_breakdown": queries["genre_breakdown"],
            },
        }
    },
}
```

Now we can define the web endpoint that will serve the Datasette application

```
@app.function(
    image=cron_image,
    volumes={VOLUME_DIR: volume},
)
@modal.concurrent(max_inputs=16)
@modal.asgi_app()
def ui():
    """Web endpoint for Datasette UI."""
    from datasette.app import Datasette

    ds = Datasette(
        files=[DB_PATH],
        settings={
            "sql_time_limit_ms": 60000,
            "max_returned_rows": 10000,
            "allow_download": True,
            "facet_time_limit_ms": 5000,
            "allow_facet": True,
        },
        metadata=metadata,
    )
    asyncio.run(ds.invoke_startup())
    return ds.app()
```

Publishing to the web
---------------------

Run this script using
`modal run cron_datasette.py`
and it will create the database under 5 minutes!

If you would like to force a refresh of the dataset, you can use:

`modal run cron_datasette.py --force-refresh`

If you would like to filter the data to be after a specific year, you can use:

`modal run cron_datasette.py --filter-year year`

You can then use
`modal serve cron_datasette.py`
to create a short-lived web URL
that exists until you terminate the script.

When publishing the interactive Datasette app you’ll want to create a persistent URL.
Just run
`modal deploy cron_datasette.py`
and your app will be deployed in seconds!

```
@app.local_entrypoint()
def run(force_refresh: bool = False, filter_year: int = None):
    if force_refresh:
        print("Force refreshing the dataset...")

    if filter_year:
        print(f"Filtering data to be after {filter_year}")

    print("Downloading IMDb dataset...")
    download_dataset.remote(force_refresh=force_refresh)
    print("Processing data and creating SQLite DB...")
    prep_db.remote(filter_year=filter_year)
    print("\nDatabase ready! You can now run:")
    print("  modal serve cron_datasette.py  # For development")
    print("  modal deploy cron_datasette.py  # For production deployment")
```

You can explore the data at the
[deployed web endpoint](https://modal-labs-examples--example-cron-datasette-ui.modal.run)

.

[Publish interactive datasets with Datasette](#publish-interactive-datasets-with-datasette)

[Basic setup](#basic-setup)

[Persistent dataset storage](#persistent-dataset-storage)

[Getting a dataset](#getting-a-dataset)

[Data processing](#data-processing)

[Inserting into SQLite](#inserting-into-sqlite)

[Keep it fresh](#keep-it-fresh)

[Web endpoint](#web-endpoint)

[Publishing to the web](#publishing-to-the-web)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/cron_datasette.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/db_to_sheet
================================================================================

Write to Google Sheets from Postgres
====================================

In this tutorial, we’ll show how to use Modal to schedule a daily report in a spreadsheet on Google Sheets
that combines data from a PostgreSQL database with data from an external API.

In particular, we’ll extract the city of each user from the database, look up the current weather in that city,
and then build a count/histogram of how many users are experiencing each type of weather.

Entering credentials
--------------------

We begin by setting up some credentials that we’ll need in order to access our database and output
spreadsheet. To do that in a secure manner, we log in to our Modal account on the web and go to
the
[Secrets](https://modal.com/secrets)

section.

### Database

First we will enter our database credentials. The easiest way to do this is to click
**New
secret**
and select the
**Postgres compatible**
Secret preset and fill in the requested
information. Then we press
**Next**
and name our Secret
`postgres-secret`
and click
**Create**
.

### Google Sheets/GCP

We’ll now add another Secret for Google Sheets access through Google Cloud Platform. Click
**New
secret**
and select the Google Sheets preset.

In order to access the Google Sheets API, we’ll need to create a
*Service Account*
in Google Cloud
Platform. You can skip this step if you already have a Service Account json file.

1. Sign up to Google Cloud Platform or log in if you haven’t
   (
   <https://cloud.google.com/>

   ).
2. Go to
   <https://console.cloud.google.com/>

   .
3. In the navigation pane on the left, go to
   **IAM & Admin**
   >
   **Service Accounts**
   .
4. Click the
   **+ CREATE SERVICE ACCOUNT**
   button.
5. Give the service account a suitable name, like “sheet-access-bot”. Click
   **Done**
   . You don’t
   have to grant it any specific access privileges at this time.
6. Click your new service account in the list view that appears and navigate to the
   **Keys**
   section.
7. Click
   **Add key**
   and choose
   **Create new key**
   . Use the
   **JSON**
   key type and confirm by
   clicking
   **Create**
   .
8. A json key file should be downloaded to your computer at this point. Copy the contents of that
   file and use it as the value for the
   `SERVICE_ACCOUNT_JSON`
   field in your new secret.

We’ll name this other Secret
`"gsheets-secret"`
.

Now you can access the values of your Secrets from Modal Functions that you annotate with the
corresponding
`modal.Secret`
s, e.g.:

```
import os

import modal

app = modal.App("example-db-to-sheet")

@app.function(secrets=[modal.Secret.from_name("postgres-secret")])
def show_host():
    # automatically filled from the specified secret
    print("Host is " + os.environ["PGHOST"])
```

Because these Secrets are Python objects, you can construct and manipulate them in your code.
We’ll do that below by defining a variable to hold our Secret for accessing Postgres

You can additionally specify

```
pg_secret = modal.Secret.from_name(
    "postgres-secret",
    required_keys=["PGHOST", "PGPORT", "PGDATABASE", "PGUSER", "PGPASSWORD"],
)
```

In order to connect to the database, we’ll use the
`psycopg2`
Python package. To make it available
to your Modal Function you need to supply it with an
`image`
argument that tells Modal how to
build the container image that contains that package. We’ll base it off of the
`Image.debian_slim`
base
image that’s built into Modal, and make sure to install the required binary packages as well as
the
`psycopg2`
package itself:

```
pg_image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("libpq-dev")
    .pip_install("psycopg2~=2.9.9")
)
```

Since the default keynames for a
**Postgres compatible**
secret correspond to the environment
variables that
`psycopg2`
looks for, we can now easily connect to the database even without
explicit credentials in your code. We’ll create a simple function that queries the city for each
user in the
`users`
table.

```
@app.function(image=pg_image, secrets=[pg_secret])
def get_db_rows(verbose=True):
    import psycopg2

    conn = psycopg2.connect()  # no explicit credentials needed
    cur = conn.cursor()
    cur.execute("SELECT city FROM users")
    results = [row[0] for row in cur.fetchall()]
    if verbose:
        print(results)
    return results
```

Note that we import
`psycopg2`
inside our function instead of the global scope. This allows us to
run this Modal Function even from an environment where
`psycopg2`
is not installed. We can test run
this function using the
`modal run`
shell command:
`modal run db_to_sheet.py::app.get_db_rows`
.

To run this function, make sure there is a table called
`users`
in your database with a column called
`city`
.
You can populate the table with some example data using the following SQL commands:

```
CREATE TABLE users (city TEXT);
INSERT INTO users VALUES ('Stockholm,,Sweden');
INSERT INTO users VALUES ('New York,NY,USA');
INSERT INTO users VALUES ('Tokyo,,Japan');
```

Applying Python logic
---------------------

For each row in our source data we’ll run an online lookup of the current weather using the
<http://openweathermap.org>

API. To do this, we’ll add the API key to
another Modal Secret. We’ll use a custom secret called “weather-secret” with the key
`OPENWEATHER_API_KEY`
containing our API key for OpenWeatherMap.

```
requests_image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "requests~=2.31.0"
)

@app.function(
    image=requests_image,
    secrets=[
        modal.Secret.from_name("weather-secret", required_keys=["OPENWEATHER_API_KEY"])
    ],
)
def city_weather(city):
    import requests

    url = "https://api.openweathermap.org/data/2.5/weather"
    params = {"q": city, "appid": os.environ["OPENWEATHER_API_KEY"]}
    response = requests.get(url, params=params)
    weather_label = response.json()["weather"][0]["main"]
    return weather_label
```

We’ll make use of Modal’s built-in
`function.map`
method to create our report.
`function.map`
makes it really easy to parallelize work by executing a Function on every element in a sequence of
data. For this example we’ll just do a simple count of rows per weather type —
answering the question “how many of our users are experiencing each type of weather?“.

```
from collections import Counter

@app.function()
def create_report(cities):
    # run city_weather for each city in parallel
    user_weather = city_weather.map(cities)
    count_users_by_weather = Counter(user_weather).items()
    return count_users_by_weather
```

Let’s try to run this! To make it simple to trigger the function with some
predefined input data, we create a “local entrypoint” that can be
run from the command line with

```
modal run db_to_sheet.py
```

```
@app.local_entrypoint()
def main():
    cities = [
        "Stockholm,,Sweden",
        "New York,NY,USA",
        "Tokyo,,Japan",
    ]
    print(create_report.remote(cities))
```

Running the local entrypoint using
`modal run db_to_sheet.py`
should print something like:
`dict_items([('Clouds', 3)])`
.
Note that since this file only has a single app, and the app has only one local entrypoint
we only have to specify the file to run it - the function/entrypoint is inferred.

In this case the logic is quite simple, but in a real world context you could have applied a
machine learning model or any other tool you could build into a container to transform the data.

Sending output to a Google Sheet
--------------------------------

We’ll set up a new Google Sheet to send our report to. Using the “Sharing” dialog in Google
Sheets, share the document to the service account’s email address (the value of the
`client_email`
field in the json file)
and make the service account an editor of the document.

You may also need to enable the Google Sheets API for your project in the Google Cloud Platform console.
If so, the URL will be printed inside the message of a 403 Forbidden error when you run the function.
It begins with
<https://console.developers.google.com/apis/api/sheets.googleapis.com/overview>

.

Lastly, we need to point our code to the correct Google Sheet. We’ll need the
*key*
of the document.
You can find the key in the URL of the Google Sheet. It appears after the
`/d/`
in the URL, like:
`https://docs.google.com/spreadsheets/d/1wOktal......IJR77jD8Do`
.

We’ll make use of the
`pygsheets`
python package to authenticate with
Google Sheets and then update the spreadsheet with information from the report we just created:

```
pygsheets_image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "pygsheets~=2.0.6"
)

@app.function(
    image=pygsheets_image,
    secrets=[
        modal.Secret.from_name("gsheets-secret", required_keys=["SERVICE_ACCOUNT_JSON"])
    ],
)
def update_sheet_report(rows):
    import pygsheets

    gc = pygsheets.authorize(service_account_env_var="SERVICE_ACCOUNT_JSON")
    document_key = "1JxhGsht4wltyPFFOd2hP0eIv6lxZ5pVxJN_ZwNT-l3c"
    sh = gc.open_by_key(document_key)
    worksheet = sh.sheet1
    worksheet.clear("A2")

    worksheet.update_values("A2", [list(row) for row in rows])
```

At this point, we have everything we need in order to run the full program. We can put it all together in
another Modal Function, and add a
[`schedule`](https://modal.com/docs/guide/cron)

argument so it runs every day automatically:

```
@app.function(schedule=modal.Period(days=1))
def db_to_sheet():
    rows = get_db_rows.remote()
    report = create_report.remote(rows)
    update_sheet_report.remote(report)
    print("Updated sheet with new weather distribution")
    for weather, count in report:
        print(f"{weather}: {count}")
```

This entire app can now be deployed using
`modal deploy db_to_sheet.py`
. The
[apps page](https://modal.com/apps)

shows our cron job’s execution history and lets you navigate to each invocation’s logs.
To trigger a manual run from your local code during development, you can also trigger this function using the cli:
`modal run db_to_sheet.py::db_to_sheet`

Note that all of the
`@app.function()`
annotated functions above run remotely in isolated containers that are specified per
function, but they are called as seamlessly as if we were using regular Python functions. This is a simple
showcase of how you can mix and match Modal Functions that use different environments and have them feed
into each other or even call each other as if they were all functions in the same local program.

[Write to Google Sheets from Postgres](#write-to-google-sheets-from-postgres)

[Entering credentials](#entering-credentials)

[Database](#database)

[Google Sheets/GCP](#google-sheetsgcp)

[Applying Python logic](#applying-python-logic)

[Sending output to a Google Sheet](#sending-output-to-a-google-sheet)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 04_secrets/db_to_sheet.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/dbt_duckdb
================================================================================

Build your own data warehouse with DuckDB, DBT, and Modal
=========================================================

This example contains a minimal but capable
[data warehouse](https://en.wikipedia.org/wiki/Data_warehouse)

.
It’s comprised of the following:

* [DuckDB](https://duckdb.org)

  as the warehouse’s
  [OLAP](https://en.wikipedia.org/wiki/Online_analytical_processing)

  database engine
* [AWS S3](https://aws.amazon.com/s3/)

  as the data storage provider
* [DBT](https://docs.getdbt.com/docs/introduction)

  as the data transformation tool

Meet your new serverless cloud data warehouse, powered by Modal!

Configure Modal, S3, and DBT
----------------------------

The only thing in the source code that you must update is the S3 bucket name.
AWS S3 bucket names are globally unique, and the one in this source is used by us to host this example.

Update the
`BUCKET_NAME`
variable below and also any references to the original value
within
`sample_proj_duckdb_s3/models/`
. The AWS IAM policy below also includes the bucket
name and that must be updated.

```
from pathlib import Path

import modal

BUCKET_NAME = "modal-example-dbt-duckdb-s3"
LOCAL_DBT_PROJECT = (  # local path
    Path(__file__).parent / "sample_proj_duckdb_s3"
)
PROJ_PATH = "/root/dbt"  # remote paths
PROFILES_PATH = "/root/dbt_profile"
TARGET_PATH = "/root/target"
```

Most of the DBT code and configuration is taken directly from the classic
[Jaffle Shop](https://github.com/dbt-labs/jaffle_shop)

demo and modified to support
using
`dbt-duckdb`
with an S3 bucket.

The DBT
`profiles.yml`
configuration is taken from
[the
`dbt-duckdb`
docs](https://github.com/jwills/dbt-duckdb#configuring-your-profile)

.

We also define the environment our application will run in —
a container image, as in Docker.
See
[this guide](https://modal.com/docs/guide/custom-container)

for details.

```
dbt_image = (  # start from a slim Linux image
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(  # install python packages
        "boto3~=1.34",  # aws client sdk
        "dbt-duckdb~=1.8.1",  # dbt and duckdb and a connector
        "pandas~=2.2.2",  # dataframes
        "pyarrow~=16.1.0",  # columnar data lib
        "fastapi[standard]~=0.115.4",  # web app
    )
    .env(  # configure DBT environment variables
        {
            "DBT_PROJECT_DIR": PROJ_PATH,
            "DBT_PROFILES_DIR": PROFILES_PATH,
            "DBT_TARGET_PATH": TARGET_PATH,
        }
    )
    # Here we add all local code and configuration into the Modal Image
    # so that it will be available when we run DBT on Modal.
    .add_local_dir(LOCAL_DBT_PROJECT, remote_path=PROJ_PATH)
    .add_local_file(
        LOCAL_DBT_PROJECT / "profiles.yml",
        remote_path=f"{PROFILES_PATH}/profiles.yml",
    )
)

app = modal.App(name="example-dbt-duckdb-s3", image=dbt_image)

dbt_target = modal.Volume.from_name("dbt-target-vol", create_if_missing=True)
```

We’ll also need to authenticate with AWS to store data in S3.

```
s3_secret = modal.Secret.from_name(
    "modal-examples-aws-user",
    required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY", "AWS_REGION"],
)
```

Create this Secret using the “AWS” template from the
[Secrets dashboard](https://modal.com/secrets)

.
Below we will use the provided credentials in a Modal Function to create an S3 bucket and
populate it with
`.parquet`
data, so be sure to provide credentials for a user
with permission to create S3 buckets and read & write data from them.

The policy required for this example is the following.
Not that you
*must*
update the bucket name listed in the policy to your
own bucket name.

```
{
    "Statement": [
        {
            "Action": "s3:*",
            "Effect": "Allow",
            "Resource": [
                "arn:aws:s3:::modal-example-dbt-duckdb-s3/*",
                "arn:aws:s3:::modal-example-dbt-duckdb-s3"
            ],
            "Sid": "duckdbs3access"
        }
    ],
    "Version": "2012-10-17"
}
```

Upload seed data
----------------

In order to provide source data for DBT to ingest and transform,
we have the below
`create_source_data`
function which creates an AWS S3 bucket and
populates it with Parquet files based off the CSV data in the
`seeds/`
directory.

You can kick it off by running this script on Modal:

```
modal run dbt_duckdb.py
```

This script also runs the full data warehouse setup, and the whole process takes a minute or two.
We’ll walk through the rest of the steps below. See the
`app.local_entrypoint`
below for details.

Note that this is not the typical way that
`seeds/`
data is used, but it’s useful for this
demonstration. See
[the DBT docs](https://docs.getdbt.com/docs/build/seeds)

for more info.

```
@app.function(
    secrets=[s3_secret],
)
def create_source_data():
    import boto3
    import pandas as pd
    from botocore.exceptions import ClientError

    s3_client = boto3.client("s3")
    s3_client.create_bucket(Bucket=BUCKET_NAME)

    for seed_csv_path in Path(PROJ_PATH, "seeds").glob("*.csv"):
        print(f"Found seed file {seed_csv_path}")
        name = seed_csv_path.stem
        parquet_filename = f"{name}.parquet"
        object_key = f"sources/{parquet_filename}"
        try:
            s3_client.head_object(Bucket=BUCKET_NAME, Key=object_key)
            print(
                f"File '{object_key}' already exists in bucket '{BUCKET_NAME}'. Skipping."
            )
        except ClientError:
            df = pd.read_csv(seed_csv_path)
            df.to_parquet(parquet_filename)
            print(f"Uploading '{object_key}' to S3 bucket '{BUCKET_NAME}'")
            s3_client.upload_file(parquet_filename, BUCKET_NAME, object_key)
            print(f"File '{object_key}' uploaded successfully.")
```

Run DBT on the cloud with Modal
-------------------------------

Modal makes it easy to run Python code in the cloud.
And DBT is a Python tool, so it’s easy to run DBT with Modal:
below, we import the
`dbt`
library’s
`dbtRunner`
to pass commands from our
Python code, running on Modal, the same way we’d pass commands on a command line.

Note that this Modal Function has access to our AWS S3 Secret,
the local files associated with our DBT project and profiles,
and a remote Modal Volume that acts as a distributed file system.

```
@app.function(
    secrets=[s3_secret],
    volumes={TARGET_PATH: dbt_target},
)
def run(command: str) -> None:
    from dbt.cli.main import dbtRunner

    res = dbtRunner().invoke(command.split(" "))
    if res.exception:
        print(res.exception)
```

You can run this Modal Function from the command line with

`modal run dbt_duckdb.py::run --command run`

A successful run will log something like the following:

```
03:41:04  Running with dbt=1.5.0
03:41:05  Found 5 models, 8 tests, 0 snapshots, 0 analyses, 313 macros, 0 operations, 3 seed files, 3 sources, 0 exposures, 0 metrics, 0 groups
03:41:05
03:41:06  Concurrency: 1 threads (target='modal')
03:41:06
03:41:06  1 of 5 START sql table model main.stg_customers ................................ [RUN]
03:41:06  1 of 5 OK created sql table model main.stg_customers ........................... [OK in 0.45s]
03:41:06  2 of 5 START sql table model main.stg_orders ................................... [RUN]
03:41:06  2 of 5 OK created sql table model main.stg_orders .............................. [OK in 0.34s]
03:41:06  3 of 5 START sql table model main.stg_payments ................................. [RUN]
03:41:07  3 of 5 OK created sql table model main.stg_payments ............................ [OK in 0.36s]
03:41:07  4 of 5 START sql external model main.customers ................................. [RUN]
03:41:07  4 of 5 OK created sql external model main.customers ............................ [OK in 0.72s]
03:41:07  5 of 5 START sql table model main.orders ....................................... [RUN]
03:41:08  5 of 5 OK created sql table model main.orders .................................. [OK in 0.22s]
03:41:08
03:41:08  Finished running 4 table models, 1 external model in 0 hours 0 minutes and 3.15 seconds (3.15s).
03:41:08  Completed successfully
03:41:08
03:41:08  Done. PASS=5 WARN=0 ERROR=0 SKIP=0 TOTAL=5
```

Look for the
`'materialized='external'`
DBT config in the SQL templates
to see how
`dbt-duckdb`
is able to write back the transformed data to AWS S3!

After running the
`run`
command and seeing it succeed, check what’s contained
under the bucket’s
`out/`
key prefix. You’ll see that DBT has run the transformations
defined in
`sample_proj_duckdb_s3/models/`
and produced output
`.parquet`
files.

Serve fresh data documentation with FastAPI and Modal
-----------------------------------------------------

DBT also automatically generates
[rich, interactive data docs](https://docs.getdbt.com/docs/collaborate/explore-projects)

.
You can serve these docs on Modal.
Just define a simple
[FastAPI](https://fastapi.tiangolo.com/)

app:

```
@app.function(volumes={TARGET_PATH: dbt_target})
@modal.concurrent(max_inputs=100)
@modal.asgi_app()  # wrap a function that returns a FastAPI app in this decorator to host on Modal
def serve_dbt_docs():
    import fastapi
    from fastapi.staticfiles import StaticFiles

    web_app = fastapi.FastAPI()
    web_app.mount(
        "/",
        StaticFiles(  # dbt docs are automatically generated and sitting in the Volume
            directory=TARGET_PATH, html=True
        ),
        name="static",
    )

    return web_app
```

And deploy that app to Modal with

```
modal deploy dbt_duckdb.py
# ...
# Created web function serve_dbt_docs => <output-url>
```

If you navigate to the output URL, you should see something like
[![example dbt docs](/_app/immutable/assets/dbt_docs.BwfMuDI8.png)](https://modal-labs-examples--example-dbt-duckdb-s3-serve-dbt-docs.modal.run)

You can also check out our instance of the docs
[here](https://modal-labs-examples--example-dbt-duckdb-s3-serve-dbt-docs.modal.run)

.
The app will be served “serverlessly” — it will automatically scale up or down
during periods of increased or decreased usage, and you won’t be charged at all
when it has scaled to zero.

Schedule daily updates
----------------------

The following
`daily_build`
function
[runs on a schedule](https://modal.com/docs/guide/cron)

to keep the DuckDB data warehouse up-to-date. It is also deployed by the same
`modal deploy`
command for the docs app.

The source data for this warehouse is static,
so the daily executions don’t really “update” anything, just re-build. But this example could be extended
to have sources which continually provide new data across time.
It will also generate the DBT docs daily to keep them fresh.

```
@app.function(
    schedule=modal.Period(days=1),
    secrets=[s3_secret],
    volumes={TARGET_PATH: dbt_target},
)
def daily_build() -> None:
    run.remote("build")
    run.remote("docs generate")

@app.local_entrypoint()
def main():
    create_source_data.remote()
    run.remote("run")
    daily_build.remote()
```

[Build your own data warehouse with DuckDB, DBT, and Modal](#build-your-own-data-warehouse-with-duckdb-dbt-and-modal)

[Configure Modal, S3, and DBT](#configure-modal-s3-and-dbt)

[Upload seed data](#upload-seed-data)

[Run DBT on the cloud with Modal](#run-dbt-on-the-cloud-with-modal)

[Serve fresh data documentation with FastAPI and Modal](#serve-fresh-data-documentation-with-fastapi-and-modal)

[Schedule daily updates](#schedule-daily-updates)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/dbt/dbt_duckdb.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/dicts_and_queues
================================================================================

Use Modal Dicts and Queues together
===================================

Modal Dicts and Queues store and communicate objects in distributed applications on Modal.

To illustrate how Dicts and Queues can interact together in a simple distributed
system, consider the following example program that crawls the web, starting
from some initial page and traversing links to many sites in breadth-first order.

The Modal Queue acts as a job queue, accepting new pages to crawl as they are discovered
by the crawlers and doling them out to be crawled via
[`.spawn`](https://modal.com/docs/reference/modal.Function#spawn)

.

The Dict is used to coordinate termination once the maximum number of URLs to crawl is reached.

Starting from Wikipedia, this spawns several dozen containers (auto-scaled on
demand) and crawls about 100,000 URLs per minute.

```
import queue
import sys
from datetime import datetime

import modal

app = modal.App(
    image=modal.Image.debian_slim().pip_install(
        "requests~=2.32.4", "beautifulsoup4~=4.13.4"
    )
)

def extract_links(url: str) -> list[str]:
    """Extract links from a given URL."""
    import urllib.parse

    import requests
    from bs4 import BeautifulSoup

    resp = requests.get(url, timeout=10)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")
    links = []
    for link in soup.find_all("a"):
        links.append(urllib.parse.urljoin(url, link.get("href")))
    return links

@app.function()
def crawl_pages(q: modal.Queue, d: modal.Dict, urls: set[str]) -> None:
    for url in urls:
        if "stop" in d:
            return
        try:
            s = datetime.now()
            links = extract_links(url)
            print(f"Crawled: {url} in {datetime.now() - s}, with {len(links)} links")
            q.put_many(links)
        except Exception as exc:
            print(
                f"Failed to crawl: {url} with error {exc}, skipping...", file=sys.stderr
            )

@app.function()
def scrape(url: str, max_urls: int = 50_000):
    start_time = datetime.now()

    # Create ephemeral dicts and queues
    with modal.Dict.ephemeral() as d, modal.Queue.ephemeral() as q:
        # The dict is used to signal the scraping to stop
        # The queue contains the URLs that have been crawled

        # Initialize queue with a starting URL
        q.put(url)

        # Crawl until the queue is empty, or reaching some number of URLs
        visited = set()
        max_urls = min(max_urls, 50_000)
        while True:
            try:
                next_urls = q.get_many(2000, timeout=5)
            except queue.Empty:
                break
            new_urls = set(next_urls) - visited
            visited |= new_urls
            if len(visited) < max_urls:
                crawl_pages.spawn(q, d, new_urls)
            else:
                d["stop"] = True

        elapsed = (datetime.now() - start_time).total_seconds()
        print(f"Crawled {len(visited)} URLs in {elapsed:.2f} seconds")

@app.local_entrypoint()
def main(starting_url=None, max_urls: int = 10_000):
    starting_url = starting_url or "https://www.wikipedia.org/"
    scrape.remote(starting_url, max_urls=max_urls)
```

[Use Modal Dicts and Queues together](#use-modal-dicts-and-queues-together)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 09_job_queues/dicts_and_queues.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/diffusers_lora_finetune
================================================================================

Fine-tune Flux on your pet using LoRA
=====================================

This example finetunes the
[Flux.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev)

on images of a pet (by default, a puppy named Qwerty)
using a technique called textual inversion from
[the “Dreambooth” paper](https://dreambooth.github.io/)

.
Effectively, it teaches a general image generation model a new “proper noun”,
allowing for the personalized generation of art and photos.
We supplement textual inversion with low-rank adaptation (LoRA)
for increased efficiency during training.

It then makes the model shareable with others — without costing $25/day for a GPU server—
by hosting a
[Gradio app](https://gradio.app/)

on Modal.

It demonstrates a simple, productive, and cost-effective pathway
to building on large pretrained models using Modal’s building blocks, like
[GPU-accelerated](https://modal.com/docs/guide/gpu)

Modal Functions and Clses for compute-intensive work,
[Volumes](https://modal.com/docs/guide/volumes)

for storage,
and
[web endpoints](https://modal.com/docs/guide/webhooks)

for serving.

And with some light customization, you can use it to generate images of your pet!

![Gradio.app image generation interface](/_app/immutable/assets/gradio-image-generate.DJVgtpVQ.png)

You can find a video walkthrough of this example on the Modal YouTube channel
[here](https://www.youtube.com/watch?v=df-8fiByXMI)

.

Imports and setup
-----------------

We start by importing the necessary libraries and setting up the environment.

```
from dataclasses import dataclass
from pathlib import Path

import modal
```

Building up the environment
---------------------------

Machine learning environments are complex, and the dependencies can be hard to manage.
Modal makes creating and working with environments easy via
[containers and container images](https://modal.com/docs/guide/custom-container)

.

We start from a base image and specify all of our dependencies.
We’ll call out the interesting ones as they come up below.
Note that these dependencies are not installed locally
— they are only installed in the remote environment where our Modal App runs.

```
app = modal.App(name="example-lora-flux")

image = modal.Image.debian_slim(python_version="3.10").pip_install(
    "accelerate==0.31.0",
    "datasets~=2.13.0",
    "fastapi[standard]==0.115.4",
    "ftfy~=6.1.0",
    "gradio~=5.5.0",
    "huggingface-hub==0.26.2",
    "hf_transfer==0.1.8",
    "numpy<2",
    "peft==0.11.1",
    "pydantic==2.9.2",
    "sentencepiece>=0.1.91,!=0.1.92",
    "smart_open~=6.4.0",
    "starlette==0.41.2",
    "transformers~=4.41.2",
    "torch~=2.2.0",
    "torchvision~=0.16",
    "triton~=2.2.0",
    "wandb==0.17.6",
)
```

### Downloading scripts and installing a git repo with `run_commands`

We’ll use an example script from the
`diffusers`
library to train the model.
We acquire it from GitHub and install it in our environment with a series of commands.
The container environments Modal Functions run in are highly flexible —
see
[the docs](https://modal.com/docs/guide/custom-container)

for more details.

```
GIT_SHA = "e649678bf55aeaa4b60bd1f68b1ee726278c0304"  # specify the commit to fetch

image = (
    image.apt_install("git")
    # Perform a shallow fetch of just the target `diffusers` commit, checking out
    # the commit in the container's home directory, /root. Then install `diffusers`
    .run_commands(
        "cd /root && git init .",
        "cd /root && git remote add origin https://github.com/huggingface/diffusers",
        f"cd /root && git fetch --depth=1 origin {GIT_SHA} && git checkout {GIT_SHA}",
        "cd /root && pip install -e .",
    )
)
```

### Configuration with `dataclass` es

Machine learning apps often have a lot of configuration information.
We collect up all of our configuration into dataclasses to avoid scattering special/magic values throughout code.

```
@dataclass
class SharedConfig:
    """Configuration information shared across project components."""

    # The instance name is the "proper noun" we're teaching the model
    instance_name: str = "Qwerty"
    # That proper noun is usually a member of some class (person, bird),
    # and sharing that information with the model helps it generalize better.
    class_name: str = "Golden Retriever"
    # identifier for pretrained models on Hugging Face
    model_name: str = "black-forest-labs/FLUX.1-dev"
```

### Storing data created by our app with `modal.Volume`

The tools we’ve used so far work well for fetching external information,
which defines the environment our app runs in,
but what about data that we create or modify during the app’s execution?
A persisted
[`modal.Volume`](https://modal.com/docs/guide/volumes)

can store and share data across Modal Apps and Functions.

We’ll use one to store both the original and fine-tuned weights we create during training
and then load them back in for inference.

```
volume = modal.Volume.from_name(
    "dreambooth-finetuning-volume-flux", create_if_missing=True
)
MODEL_DIR = "/model"
```

Note that access to the Flux.1-dev model on Hugging Face is
[gated by a license agreement](https://huggingface.co/docs/hub/en/models-gated)

which
you must agree to
[here](https://huggingface.co/black-forest-labs/FLUX.1-dev)

.
After you have accepted the license,
[create a Modal Secret](https://modal.com/secrets)

with the name
`huggingface-secret`
following the instructions in the template.

```
huggingface_secret = modal.Secret.from_name(
    "huggingface-secret", required_keys=["HF_TOKEN"]
)

image = image.env(
    {"HF_HUB_ENABLE_HF_TRANSFER": "1"}  # turn on faster downloads from HF
)

@app.function(
    volumes={MODEL_DIR: volume},
    image=image,
    secrets=[huggingface_secret],
    timeout=600,  # 10 minutes
)
def download_models(config):
    import torch
    from diffusers import DiffusionPipeline
    from huggingface_hub import snapshot_download

    snapshot_download(
        config.model_name,
        local_dir=MODEL_DIR,
        ignore_patterns=["*.pt", "*.bin"],  # using safetensors
    )

    DiffusionPipeline.from_pretrained(MODEL_DIR, torch_dtype=torch.bfloat16)
```

### Load fine-tuning dataset

Part of the magic of the low-rank fine-tuning is that we only need 3-10 images for fine-tuning.
So we can fetch just a few images, stored on consumer platforms like Imgur or Google Drive,
whenever we need them — no need for expensive, hard-to-maintain data pipelines.

```
def load_images(image_urls: list[str]) -> Path:
    import PIL.Image
    from smart_open import open

    img_path = Path("/img")

    img_path.mkdir(parents=True, exist_ok=True)
    for ii, url in enumerate(image_urls):
        with open(url, "rb") as f:
            image = PIL.Image.open(f)
            image.save(img_path / f"{ii}.png")
    print(f"{ii + 1} images loaded")

    return img_path
```

Low-Rank Adapation (LoRA) fine-tuning for a text-to-image model
---------------------------------------------------------------

The base model we start from is trained to do a sort of “reverse
[ekphrasis](https://en.wikipedia.org/wiki/Ekphrasis)

”:
it attempts to recreate a visual work of art or image from only its description.

We can use the model to synthesize wholly new images
by combining the concepts it has learned from the training data.

We use a pretrained model, the Flux model from Black Forest Labs.
In this example, we “finetune” Flux, making only small adjustments to the weights.
Furthermore, we don’t change all the weights in the model.
Instead, using a technique called
[*low-rank adaptation*](https://arxiv.org/abs/2106.09685)

,
we change a much smaller matrix that works “alongside” the existing weights, nudging the model in the direction we want.

We can get away with such a small and simple training process because we’re just teach the model the meaning of a single new word: the name of our pet.

The result is a model that can generate novel images of our pet:
as an astronaut in space, as painted by Van Gogh or Bastiat, etc.

### Finetuning with Hugging Face 🧨 Diffusers and Accelerate

The model weights, training libraries, and training script are all provided by
[🤗 Hugging Face](https://huggingface.co)

.

You can kick off a training job with the command
`modal run dreambooth_app.py::app.train`
.
It should take about ten minutes.

Training machine learning models takes time and produces a lot of metadata —
metrics for performance and resource utilization,
metrics for model quality and training stability,
and model inputs and outputs like images and text.
This is especially important if you’re fiddling around with the configuration parameters.

This example can optionally use
[Weights & Biases](https://wandb.ai)

to track all of this training information.
Just sign up for an account, switch the flag below, and add your API key as a
[Modal Secret](https://modal.com/secrets)

.

```
USE_WANDB = False
```

You can see an example W&B dashboard
[here](https://wandb.ai/cfrye59/dreambooth-lora-sd-xl)

.
Check out
[this run](https://wandb.ai/cfrye59/dreambooth-lora-sd-xl/runs/ca3v1lsh?workspace=user-cfrye59)

,
which
[despite having high GPU utilization](https://wandb.ai/cfrye59/dreambooth-lora-sd-xl/runs/ca3v1lsh/system)

suffered from numerical instability during training and produced only black images — hard to debug without experiment management logs!

You can read more about how the values in
`TrainConfig`
are chosen and adjusted
[in this blog post on Hugging Face](https://huggingface.co/blog/dreambooth)

.
To run training on images of your own pet, upload the images to separate URLs and edit the contents of the file at
`TrainConfig.instance_example_urls_file`
to point to them.

Tip: if the results you’re seeing don’t match the prompt too well, and instead produce an image
of your subject without taking the prompt into account, the model has likely overfit. In this case, repeat training with a lower
value of
`max_train_steps`
. If you used W&B, look back at results earlier in training to determine where to stop.
On the other hand, if the results don’t look like your subject, you might need to increase
`max_train_steps`
.

```
@dataclass
class TrainConfig(SharedConfig):
    """Configuration for the finetuning step."""

    # training prompt looks like `{PREFIX} {INSTANCE_NAME} the {CLASS_NAME} {POSTFIX}`
    prefix: str = "a photo of"
    postfix: str = ""

    # locator for plaintext file with urls for images of target instance
    instance_example_urls_file: str = str(
        Path(__file__).parent / "instance_example_urls.txt"
    )

    # Hyperparameters/constants from the huggingface training example
    resolution: int = 512
    train_batch_size: int = 3
    rank: int = 16  # lora rank
    gradient_accumulation_steps: int = 1
    learning_rate: float = 4e-4
    lr_scheduler: str = "constant"
    lr_warmup_steps: int = 0
    max_train_steps: int = 500
    checkpointing_steps: int = 1000
    seed: int = 117

@app.function(
    image=image,
    gpu="A100-80GB",  # fine-tuning is VRAM-heavy and requires a high-VRAM GPU
    volumes={MODEL_DIR: volume},  # stores fine-tuned model
    timeout=1800,  # 30 minutes
    secrets=[huggingface_secret]
    + (
        [modal.Secret.from_name("wandb-secret", required_keys=["WANDB_API_KEY"])]
        if USE_WANDB
        else []
    ),
)
def train(instance_example_urls, config):
    import subprocess

    from accelerate.utils import write_basic_config

    # load data locally
    img_path = load_images(instance_example_urls)

    # set up hugging face accelerate library for fast training
    write_basic_config(mixed_precision="bf16")

    # define the training prompt
    instance_phrase = f"{config.instance_name} the {config.class_name}"
    prompt = f"{config.prefix} {instance_phrase} {config.postfix}".strip()

    # the model training is packaged as a script, so we have to execute it as a subprocess, which adds some boilerplate
    def _exec_subprocess(cmd: list[str]):
        """Executes subprocess and prints log to terminal while subprocess is running."""
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
        )
        with process.stdout as pipe:
            for line in iter(pipe.readline, b""):
                line_str = line.decode()
                print(f"{line_str}", end="")

        if exitcode := process.wait() != 0:
            raise subprocess.CalledProcessError(exitcode, "\n".join(cmd))

    # run training -- see huggingface accelerate docs for details
    print("launching dreambooth training script")
    _exec_subprocess(
        [
            "accelerate",
            "launch",
            "examples/dreambooth/train_dreambooth_lora_flux.py",
            "--mixed_precision=bf16",  # half-precision floats most of the time for faster training
            f"--pretrained_model_name_or_path={MODEL_DIR}",
            f"--instance_data_dir={img_path}",
            f"--output_dir={MODEL_DIR}",
            f"--instance_prompt={prompt}",
            f"--resolution={config.resolution}",
            f"--train_batch_size={config.train_batch_size}",
            f"--gradient_accumulation_steps={config.gradient_accumulation_steps}",
            f"--learning_rate={config.learning_rate}",
            f"--lr_scheduler={config.lr_scheduler}",
            f"--lr_warmup_steps={config.lr_warmup_steps}",
            f"--max_train_steps={config.max_train_steps}",
            f"--checkpointing_steps={config.checkpointing_steps}",
            f"--seed={config.seed}",  # increased reproducibility by seeding the RNG
        ]
        + (
            [
                "--report_to=wandb",
                # validation output tracking is useful, but currently broken for Flux LoRA training
                # f"--validation_prompt={prompt} in space",  # simple test prompt
                # f"--validation_epochs={config.max_train_steps // 5}",
            ]
            if USE_WANDB
            else []
        ),
    )
    # The trained model information has been output to the volume mounted at `MODEL_DIR`.
    # To persist this data for use in our web app, we 'commit' the changes
    # to the volume.
    volume.commit()
```

Running our model
-----------------

To generate images from prompts using our fine-tuned model, we define a Modal Function called
`inference`
.

Naively, this would seem to be a bad fit for the flexible, serverless infrastructure of Modal:
wouldn’t you need to include the steps to load the model and spin it up in every function call?

In order to initialize the model just once on container startup,
we use Modal’s
[container lifecycle](https://modal.com/docs/guide/lifecycle-functions)

features, which require the function to be part
of a class. Note that the
`modal.Volume`
we saved the model to is mounted here as well,
so that the fine-tuned model created by
`train`
is available to us.

```
@app.cls(image=image, gpu="A100", volumes={MODEL_DIR: volume})
class Model:
    @modal.enter()
    def load_model(self):
        import torch
        from diffusers import DiffusionPipeline

        # Reload the modal.Volume to ensure the latest state is accessible.
        volume.reload()

        # set up a hugging face inference pipeline using our model
        pipe = DiffusionPipeline.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.bfloat16,
        ).to("cuda")
        pipe.load_lora_weights(MODEL_DIR)
        self.pipe = pipe

    @modal.method()
    def inference(self, text, config):
        image = self.pipe(
            text,
            num_inference_steps=config.num_inference_steps,
            guidance_scale=config.guidance_scale,
        ).images[0]

        return image
```

Wrap the trained model in a Gradio web UI
-----------------------------------------

[Gradio](https://gradio.app)

makes it super easy to expose a model’s functionality
in an easy-to-use, responsive web interface.

This model is a text-to-image generator,
so we set up an interface that includes a user-entry text box
and a frame for displaying images.

We also provide some example text inputs to help
guide users and to kick-start their creative juices.

And we couldn’t resist adding some Modal style to it as well!

You can deploy the app on Modal with the command
`modal deploy dreambooth_app.py`
.
You’ll be able to come back days, weeks, or months later and find it still ready to go,
even though you don’t have to pay for a server to run while you’re not using it.

```
@dataclass
class AppConfig(SharedConfig):
    """Configuration information for inference."""

    num_inference_steps: int = 50
    guidance_scale: float = 6

web_image = image.add_local_dir(
    # Add local web assets to the image
    Path(__file__).parent / "assets",
    remote_path="/assets",
)

@app.function(
    image=web_image,
    max_containers=1,
)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def fastapi_app():
    import gradio as gr
    from fastapi import FastAPI
    from fastapi.responses import FileResponse
    from gradio.routes import mount_gradio_app

    web_app = FastAPI()

    # Call out to the inference in a separate Modal environment with a GPU
    def go(text=""):
        if not text:
            text = example_prompts[0]
        return Model().inference.remote(text, config)

    # set up AppConfig
    config = AppConfig()

    instance_phrase = f"{config.instance_name} the {config.class_name}"

    example_prompts = [
        f"{instance_phrase}",
        f"a painting of {instance_phrase.title()} With A Pearl Earring, by Vermeer",
        f"oil painting of {instance_phrase} flying through space as an astronaut",
        f"a painting of {instance_phrase} in cyberpunk city. character design by cory loftis. volumetric light, detailed, rendered in octane",
        f"drawing of {instance_phrase} high quality, cartoon, path traced, by studio ghibli and don bluth",
    ]

    modal_docs_url = "https://modal.com/docs"
    modal_example_url = f"{modal_docs_url}/examples/dreambooth_app"

    description = f"""Describe what they are doing or how a particular artist or style would depict them. Be fantastical! Try the examples below for inspiration.

### Learn how to make a "Dreambooth" for your own pet [here]({modal_example_url}).
    """

    # custom styles: an icon, a background, and a theme
    @web_app.get("/favicon.ico", include_in_schema=False)
    async def favicon():
        return FileResponse("/assets/favicon.svg")

    @web_app.get("/assets/background.svg", include_in_schema=False)
    async def background():
        return FileResponse("/assets/background.svg")

    with open("/assets/index.css") as f:
        css = f.read()

    theme = gr.themes.Default(
        primary_hue="green", secondary_hue="emerald", neutral_hue="neutral"
    )

    # add a gradio UI around inference
    with gr.Blocks(
        theme=theme,
        css=css,
        title=f"Generate images of {config.instance_name} on Modal",
    ) as interface:
        gr.Markdown(
            f"# Generate images of {instance_phrase}.\n\n{description}",
        )
        with gr.Row():
            inp = gr.Textbox(  # input text component
                label="",
                placeholder=f"Describe the version of {instance_phrase} you'd like to see",
                lines=10,
            )
            out = gr.Image(  # output image component
                height=512, width=512, label="", min_width=512, elem_id="output"
            )
        with gr.Row():
            btn = gr.Button("Dream", variant="primary", scale=2)
            btn.click(
                fn=go, inputs=inp, outputs=out
            )  # connect inputs and outputs with inference function

            gr.Button(  # shameless plug
                "⚡️ Powered by Modal",
                variant="secondary",
                link="https://modal.com",
            )

        with gr.Column(variant="compact"):
            # add in a few examples to inspire users
            for ii, prompt in enumerate(example_prompts):
                btn = gr.Button(prompt, variant="secondary")
                btn.click(fn=lambda idx=ii: example_prompts[idx], outputs=inp)

    # mount for execution on Modal
    return mount_gradio_app(
        app=web_app,
        blocks=interface,
        path="/",
    )
```

Running your fine-tuned model from the command line
---------------------------------------------------

You can use the
`modal`
command-line interface to set up, customize, and deploy this app:

* `modal run diffusers_lora_finetune.py`
  will train the model. Change the
  `instance_example_urls_file`
  to point to your own pet’s images.
* `modal serve diffusers_lora_finetune.py`
  will
  [serve](https://modal.com/docs/guide/webhooks#developing-with-modal-serve)

  the Gradio interface at a temporary location. Great for iterating on code!
* `modal shell diffusers_lora_finetune.py`
  is a convenient helper to open a bash
  [shell](https://modal.com/docs/guide/developing-debugging#interactive-shell)

  in our image. Great for debugging environment issues.

Remember, once you’ve trained your own fine-tuned model, you can deploy it permanently — for no cost when it is not being used! —
using
`modal deploy diffusers_lora_finetune.py`
.

If you just want to try the app out, you can find our deployment
[here](https://modal-labs--example-lora-flux-fastapi-app.modal.run)

.

```
@app.local_entrypoint()
def run(  # add more config params here to make training configurable
    max_train_steps: int = 250,
):
    print("🎨 loading model")
    download_models.remote(SharedConfig())
    print("🎨 setting up training")
    config = TrainConfig(max_train_steps=max_train_steps)
    instance_example_urls = (
        Path(TrainConfig.instance_example_urls_file).read_text().splitlines()
    )
    train.remote(instance_example_urls, config)
    print("🎨 training finished")
```

[Fine-tune Flux on your pet using LoRA](#fine-tune-flux-on-your-pet-using-lora)

[Imports and setup](#imports-and-setup)

[Building up the environment](#building-up-the-environment)

[Downloading scripts and installing a git repo with run\_commands](#downloading-scripts-and-installing-a-git-repo-with-run_commands)

[Configuration with dataclasses](#configuration-with-dataclasses)

[Storing data created by our app with modal.Volume](#storing-data-created-by-our-app-with-modalvolume)

[Load fine-tuning dataset](#load-fine-tuning-dataset)

[Low-Rank Adapation (LoRA) fine-tuning for a text-to-image model](#low-rank-adapation-lora-fine-tuning-for-a-text-to-image-model)

[Finetuning with Hugging Face 🧨 Diffusers and Accelerate](#finetuning-with-hugging-face--diffusers-and-accelerate)

[Running our model](#running-our-model)

[Wrap the trained model in a Gradio web UI](#wrap-the-trained-model-in-a-gradio-web-ui)

[Running your fine-tuned model from the command line](#running-your-fine-tuned-model-from-the-command-line)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/dreambooth/diffusers_lora_finetune.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/discord_bot
================================================================================

Serve a Discord Bot on Modal
============================

In this example we will demonstrate how to use Modal to build and serve a Discord bot that uses
[slash commands](https://discord.com/developers/docs/interactions/application-commands)

.

Slash commands send information from Discord server members to a service at a URL.
Here, we set up a simple
[FastAPI app](https://fastapi.tiangolo.com/)

to run that service and deploy it easily Modal’s
[`@asgi_app`](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps)

decorator.

As our example service, we hit a simple free API:
the
[Free Public APIs API](https://www.freepublicapis.com/api)

,
a directory of free public APIs.

[Try it out on Discord](https://discord.gg/PmG7P47EPQ)

!

Set up our App and its Image
----------------------------

First, we define the
[container image](https://modal.com/docs/guide/images)

that all the pieces of our bot will run in.

We set that as the default image for a Modal
[App](https://modal.com/docs/guide/apps)

.
The App is where we’ll attach all the components of our bot.

```
import json
from enum import Enum

import modal

image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "fastapi[standard]==0.115.4", "pynacl~=1.5.0", "requests~=2.32.3"
)

app = modal.App("example-discord-bot", image=image)
```

Hit the Free Public APIs API
----------------------------

We start by defining the core service that our bot will provide.

In a real application, this might be
[music generation](https://modal.com/docs/examples/musicgen)

,
a
[chatbot](https://modal.com/docs/examples/chat_with_pdf_vision)

,
or
[interacting with a database](https://modal.com/docs/examples/cron_datasette)

.

Here, we just hit a simple free public API:
the
[Free Public APIs](https://www.freepublicapis.com)

API,
an “API of APIs” that returns information about free public APIs,
like the
[Global Shark Attack API](https://www.freepublicapis.com/global-shark-attack-api)

and the
[Corporate Bullshit Generator](https://www.freepublicapis.com/corporate-bullshit-generator)

.
We convert the response into a Markdown-formatted message.

We turn our Python function into a Modal Function by attaching the
`app.function`
decorator.
We make the function
`async`
and add
`@modal.concurrent()`
with a large
`max_inputs`
value, because
communicating with an external API is a classic case for better performance from asynchronous execution.
Modal handles things like the async event loop for us.

```
@app.function()
@modal.concurrent(max_inputs=1000)
async def fetch_api() -> str:
    import aiohttp

    url = "https://www.freepublicapis.com/api/random"

    async with aiohttp.ClientSession() as session:
        try:
            async with session.get(url) as response:
                response.raise_for_status()
                data = await response.json()
                message = (
                    f"# {data.get('emoji') or '🤖'} [{data['title']}]({data['source']})"
                )
                message += f"\n _{''.join(data['description'].splitlines())}_"
        except Exception as e:
            message = f"# 🤖: Oops! {e}"

    return message
```

This core component has nothing to do with Discord,
and it’s nice to be able to interact with and test it in isolation.

For that, we add a
`local_entrypoint`
that calls the Modal Function.
Notice that we add
`.remote`
to the function’s name.

Later, when you replace this component of the app with something more interesting,
test it by triggering this entrypoint with
`modal run discord_bot.py`
.

```
@app.local_entrypoint()
def test_fetch_api():
    result = fetch_api.remote()
    if result.startswith("# 🤖: Oops! "):
        raise Exception(result)
    else:
        print(result)
```

Integrate our Modal Function with Discord Interactions
------------------------------------------------------

Now we need to map this function onto Discord’s interface —
in particular the
[Interactions API](https://discord.com/developers/docs/interactions/overview)

.

Reviewing the documentation, we see that we need to send a JSON payload
to a specific API URL that will include an
`app_id`
that identifies our bot
and a
`token`
that identifies the interaction (loosely, message) that we’re participating in.

So let’s write that out. This function doesn’t need to live on Modal,
since it’s just encapsulating some logic — we don’t want to turn it into a service or an API on its own.
That means we don’t need any Modal decorators.

```
async def send_to_discord(payload: dict, app_id: str, interaction_token: str):
    import aiohttp

    interaction_url = f"https://discord.com/api/v10/webhooks/{app_id}/{interaction_token}/messages/@original"

    async with aiohttp.ClientSession() as session:
        async with session.patch(interaction_url, json=payload) as resp:
            print("🤖 Discord response: " + await resp.text())
```

Other parts of our application might want to both hit the Free Public APIs API and send the result to Discord,
so we both write a Python function for this and we promote it to a Modal Function with a decorator.

Notice that we use the
`.local`
suffix to call our
`fetch_api`
Function. That means we run
the Function the same way we run all the other Python functions, rather than treating it as a special
Modal Function. This reduces a bit of extra latency, but couples these two Functions more tightly.

```
@app.function()
@modal.concurrent(max_inputs=1000)
async def reply(app_id: str, interaction_token: str):
    message = await fetch_api.local()
    await send_to_discord({"content": message}, app_id, interaction_token)
```

Set up a Discord app
--------------------

Now, we need to actually connect to Discord.
We start by creating an application on the Discord Developer Portal.

1. Go to the
   [Discord Developer Portal](https://discord.com/developers/applications)

   and
   log in with your Discord account.
2. On the portal, go to
   **Applications**
   and create a new application by
   clicking
   **New Application**
   in the top right next to your profile picture.
3. [Create a custom Modal Secret](https://modal.com/docs/guide/secrets)

   for your Discord bot.
   On Modal’s Secret creation page, select ‘Discord’. Copy your Discord application’s
   **Public Key**
   and
   **Application ID**
   (from the
   **General Information**
   tab in the Discord Developer Portal)
   and paste them as the value of
   `DISCORD_PUBLIC_KEY`
   and
   `DISCORD_CLIENT_ID`
   .
   Additionally, head to the
   **Bot**
   tab and use the
   **Reset Token**
   button to create a new bot token.
   Paste this in the value of an additional key in the Secret,
   `DISCORD_BOT_TOKEN`
   .
   Name this Secret
   `discord-secret`
   .

We access that Secret in code like so:

```
discord_secret = modal.Secret.from_name(
    "discord-secret",
    required_keys=[  # included so we get nice error messages if we forgot a key
        "DISCORD_BOT_TOKEN",
        "DISCORD_CLIENT_ID",
        "DISCORD_PUBLIC_KEY",
    ],
)
```

Register a Slash Command
------------------------

Next, we’re going to register a
[Slash Command](https://discord.com/developers/docs/interactions/application-commands#slash-commands)

for our Discord app. Slash Commands are triggered by users in servers typing
`/`
and the name of the command.

The Modal Function below will register a Slash Command for your bot named
`bored`
.
More information about Slash Commands can be found in the Discord docs
[here](https://discord.com/developers/docs/interactions/application-commands)

.

You can run this Function with

```
modal run discord_bot::create_slash_command
```

```
@app.function(secrets=[discord_secret], image=image)
def create_slash_command(force: bool = False):
    """Registers the slash command with Discord. Pass the force flag to re-register."""
    import os

    import requests

    BOT_TOKEN = os.getenv("DISCORD_BOT_TOKEN")
    CLIENT_ID = os.getenv("DISCORD_CLIENT_ID")

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bot {BOT_TOKEN}",
    }
    url = f"https://discord.com/api/v10/applications/{CLIENT_ID}/commands"

    command_description = {
        "name": "api",
        "description": "Information about a random free, public API",
    }

    # first, check if the command already exists
    response = requests.get(url, headers=headers)
    try:
        response.raise_for_status()
    except Exception as e:
        raise Exception("Failed to create slash command") from e

    commands = response.json()
    command_exists = any(
        command.get("name") == command_description["name"] for command in commands
    )

    # and only recreate it if the force flag is set
    if command_exists and not force:
        print(f"🤖: command {command_description['name']} exists")
        return

    response = requests.post(url, headers=headers, json=command_description)
    try:
        response.raise_for_status()
    except Exception as e:
        raise Exception("Failed to create slash command") from e
    print(f"🤖: command {command_description['name']} created")
```

Host a Discord Interactions endpoint on Modal
---------------------------------------------

If you look carefully at the definition of the Slash Command above,
you’ll notice that it doesn’t know anything about our bot besides an ID.

To hook the Slash Commands in the Discord UI up to our logic for hitting the Bored API,
we need to set up a service that listens at some URL and follows a specific protocol,
described
[here](https://discord.com/developers/docs/interactions/overview#configuring-an-interactions-endpoint-url)

.

Here are some of the most important facets:

1. We’ll need to respond within five seconds or Discord will assume we are dead.
   Modal’s fast-booting serverless containers usually start faster than that,
   but it’s not guaranteed. So we’ll add the
   `min_containers`
   parameter to our
   Function so that there’s at least one live copy ready to respond quickly at any time.
   Modal charges a minimum of about 2¢ an hour for live containers (pricing details
   [here](https://modal.com/pricing)

   ).
   Note that that still fits within Modal’s $30/month of credits on the free tier.
2. We have to respond to Discord that quickly, but we don’t have to respond to the user that quickly.
   We instead send an acknowledgement so that they know we’re alive and they can close their connection to us.
   We also trigger our
   `reply`
   Modal Function, which will respond to the user via Discord’s Interactions API,
   but we don’t wait for the result, we just
   `spawn`
   the call.
3. The protocol includes some authentication logic that is mandatory
   and checked by Discord. We’ll explain in more detail in the next section.

We can set up our interaction endpoint by deploying a FastAPI app on Modal.
This is as easy as creating a Python Function that returns a FastAPI app
and adding the
`modal.asgi_app`
decorator.
For more details on serving Python web apps on Modal, see
[this guide](https://modal.com/docs/guide/webhooks)

.

```
@app.function(secrets=[discord_secret], min_containers=1)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def web_app():
    from fastapi import FastAPI, HTTPException, Request
    from fastapi.middleware.cors import CORSMiddleware

    web_app = FastAPI()

    # must allow requests from other domains, e.g. from Discord's servers
    web_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    @web_app.post("/api")
    async def get_api(request: Request):
        body = await request.body()

        # confirm this is a request from Discord
        authenticate(request.headers, body)

        print("🤖: parsing request")
        data = json.loads(body.decode())
        if data.get("type") == DiscordInteractionType.PING.value:
            print("🤖: acking PING from Discord during auth check")
            return {"type": DiscordResponseType.PONG.value}

        if data.get("type") == DiscordInteractionType.APPLICATION_COMMAND.value:
            print("🤖: handling slash command")
            app_id = data["application_id"]
            interaction_token = data["token"]

            # kick off request asynchronously, will respond when ready
            reply.spawn(app_id, interaction_token)

            # respond immediately with defer message
            return {
                "type": DiscordResponseType.DEFERRED_CHANNEL_MESSAGE_WITH_SOURCE.value
            }

        print(f"🤖: unable to parse request with type {data.get('type')}")
        raise HTTPException(status_code=400, detail="Bad request")

    return web_app
```

The authentication for Discord is a bit involved and there aren’t,
to our knowledge, any good Python libraries for it.

So we have to implement the protocol “by hand”.

Essentially, Discord sends a header in their request
that we can use to verify the request comes from them.
For that, we use the
`DISCORD_PUBLIC_KEY`
from
our Application Information page.

The details aren’t super important, but they appear in the
`authenticate`
function below
(which defers the real cryptography work to
[PyNaCl](https://pypi.org/project/PyNaCl/)

,
a Python wrapper for
[`libsodium`](https://github.com/jedisct1/libsodium)

).

Discord will also check that we reject unauthorized requests,
so we have to be sure to get this right!

```
def authenticate(headers, body):
    import os

    from fastapi.exceptions import HTTPException
    from nacl.exceptions import BadSignatureError
    from nacl.signing import VerifyKey

    print("🤖: authenticating request")
    # verify the request is from Discord using their public key
    public_key = os.getenv("DISCORD_PUBLIC_KEY")
    verify_key = VerifyKey(bytes.fromhex(public_key))

    signature = headers.get("X-Signature-Ed25519")
    timestamp = headers.get("X-Signature-Timestamp")

    message = timestamp.encode() + body

    try:
        verify_key.verify(message, bytes.fromhex(signature))
    except BadSignatureError:
        # either an unauthorized request or Discord's "negative control" check
        raise HTTPException(status_code=401, detail="Invalid request")
```

The code above used a few enums to abstract bits of the Discord protocol.
Now that we’ve walked through all of it,
we’re in a position to understand what those are
and so the code for them appears below.

```
class DiscordInteractionType(Enum):
    PING = 1  # hello from Discord during auth check
    APPLICATION_COMMAND = 2  # an actual command

class DiscordResponseType(Enum):
    PONG = 1  # hello back during auth check
    DEFERRED_CHANNEL_MESSAGE_WITH_SOURCE = 5  # we'll send a message later
```

Deploy on Modal
---------------

You can deploy this app on Modal by running the following commands:

```
modal run discord_bot.py  # checks the API wrapper, little test
modal run discord_bot.py::create_slash_command  # creates the slash command, if missing
modal deploy discord_bot.py  # deploys the web app and the API wrapper
```

Copy the Modal URL that is printed in the output and go back to the
**General Information**
section on the
[Discord Developer Portal](https://discord.com/developers/applications)

.
Paste the URL, making sure to append the path of your
`POST`
route (here,
`/api`
), in the
**Interactions Endpoint URL**
field, then click
**Save Changes**
. If your
endpoint URL is incorrect or if authentication is incorrectly implemented,
Discord will refuse to save the URL. Once it saves, you can start
handling interactions!

Finish setting up Discord bot
-----------------------------

To start using the Slash Command you just set up, you need to invite the bot to
a Discord server. To do so, go to your application’s
**Installation**
section on the
[Discord Developer Portal](https://discord.com/developers/applications)

.
Copy the
**Discored Provided Link**
and visit it to invite the bot to your bot to the server.

Now you can open your Discord server and type
`/api`
in a channel to trigger the bot.
You can see a working version
[in our test Discord server](https://discord.gg/PmG7P47EPQ)

.

[Serve a Discord Bot on Modal](#serve-a-discord-bot-on-modal)

[Set up our App and its Image](#set-up-our-app-and-its-image)

[Hit the Free Public APIs API](#hit-the-free-public-apis-api)

[Integrate our Modal Function with Discord Interactions](#integrate-our-modal-function-with-discord-interactions)

[Set up a Discord app](#set-up-a-discord-app)

[Register a Slash Command](#register-a-slash-command)

[Host a Discord Interactions endpoint on Modal](#host-a-discord-interactions-endpoint-on-modal)

[Deploy on Modal](#deploy-on-modal)

[Finish setting up Discord bot](#finish-setting-up-discord-bot)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 07_web_endpoints/discord_bot.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/doc_ocr_jobs
================================================================================

Run a job queue for GOT-OCR
===========================

This tutorial shows you how to use Modal as an infinitely scalable job queue
that can service async tasks from a web app. For the purpose of this tutorial,
we’ve also built a
[React + FastAPI web app on Modal](https://modal.com/docs/examples/doc_ocr_webapp)

that works together with it, but note that you don’t need a web app running on Modal
to use this pattern. You can submit async tasks to Modal from any Python
application (for example, a regular Django app running on Kubernetes).

Our job queue will handle a single task: running OCR transcription for images of receipts.
We’ll make use of a pre-trained model:
the
[General OCR Theory (GOT) 2.0 model](https://huggingface.co/stepfun-ai/GOT-OCR2_0)

.

Try it out for yourself
[here](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)

.

[![Webapp frontend](https://modal-cdn.com/doc_ocr_frontend.jpg)](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)

Define an App
-------------

Let’s first import
`modal`
and define an
[`App`](https://modal.com/docs/reference/modal.App)

.
Later, we’ll use the name provided for our
`App`
to find it from our web app and submit tasks to it.

```
from typing import Optional

import modal

app = modal.App("example-doc-ocr-jobs")
```

We also define the dependencies for our Function by specifying an
[Image](https://modal.com/docs/guide/images)

.

```
inference_image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "accelerate==0.28.0",
    "huggingface_hub[hf_transfer]==0.27.1",
    "numpy<2",
    "tiktoken==0.6.0",
    "torch==2.5.1",
    "torchvision==0.20.1",
    "transformers==4.48.0",
    "verovio==4.3.1",
)
```

Cache the pre-trained model on a Modal Volume
---------------------------------------------

We can obtain the pre-trained model we want to run from Hugging Face
using its name and a revision identifier.

```
MODEL_NAME = "ucaslcl/GOT-OCR2_0"
MODEL_REVISION = "cf6b7386bc89a54f09785612ba74cb12de6fa17c"
```

The logic for loading the model based on this information
is encapsulated in the
`setup`
function below.

```
def setup():
    import warnings

    from transformers import AutoModel, AutoTokenizer

    with warnings.catch_warnings():  # filter noisy warnings from GOT modeling code
        warnings.simplefilter("ignore")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME, revision=MODEL_REVISION, trust_remote_code=True
        )

        model = AutoModel.from_pretrained(
            MODEL_NAME,
            revision=MODEL_REVISION,
            trust_remote_code=True,
            device_map="cuda",
            use_safetensors=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    return tokenizer, model
```

The
`.from_pretrained`
methods from Hugging Face are smart enough
to only download models if they haven’t been downloaded before.
But in Modal’s serverless environment, filesystems are ephemeral,
and so using this code alone would mean that models need to get downloaded
on every request.

So instead, we create a Modal
[Volume](https://modal.com/docs/guide/volumes)

to store the model — a durable filesystem that any Modal Function can access.

```
model_cache = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)
```

We also update the environment variables for our Function
to include this new path for the model cache —
and to enable fast downloads with the
`hf_transfer`
library.

```
MODEL_CACHE_PATH = "/root/models"
inference_image = inference_image.env(
    {"HF_HUB_CACHE": MODEL_CACHE_PATH, "HF_HUB_ENABLE_HF_TRANSFER": "1"}
)
```

Run OCR inference on Modal by wrapping with
`app.function`
----------------------------------------------------------

Now let’s set up the actual OCR inference.

Using the
[`@app.function`](https://modal.com/docs/reference/modal.App#function)

decorator, we set up a Modal
[Function](https://modal.com/docs/reference/modal.Function)

.
We provide arguments to that decorator to customize the hardware, scaling, and other features
of the Function.

Here, we say that this Function should use NVIDIA L40S
[GPUs](https://modal.com/docs/guide/gpu)

,
automatically
[retry](https://modal.com/docs/guide/retries#function-retries)

failures up to 3 times,
and have access to our
[shared model cache](https://modal.com/docs/guide/volumes)

.

```
@app.function(
    gpu="l40s",
    retries=3,
    volumes={MODEL_CACHE_PATH: model_cache},
    image=inference_image,
)
def parse_receipt(image: bytes) -> str:
    from tempfile import NamedTemporaryFile

    tokenizer, model = setup()

    with NamedTemporaryFile(delete=False, mode="wb+") as temp_img_file:
        temp_img_file.write(image)
        output = model.chat(tokenizer, temp_img_file.name, ocr_type="format")

    print("Result: ", output)

    return output
```

Deploy
------

Now that we have a function, we can publish it by deploying the app:

```
modal deploy doc_ocr_jobs.py
```

Once it’s published, we can
[look up](https://modal.com/docs/guide/trigger-deployed-functions)

this Function
from another Python process and submit tasks to it:

```
fn = modal.Function.from_name("example-doc-ocr-jobs", "parse_receipt")
fn.spawn(my_image)
```

Modal will auto-scale to handle all the tasks queued, and
then scale back down to 0 when there’s no work left. To see how you could use this from a Python web
app, take a look at the
[receipt parser frontend](https://modal.com/docs/examples/doc_ocr_webapp)

tutorial.

Run manually
------------

We can also trigger
`parse_receipt`
manually for easier debugging:

```
modal run doc_ocr_jobs
```

To try it out, you can find some
example receipts
[here](https://drive.google.com/drive/folders/1S2D1gXd4YIft4a5wDtW99jfl38e85ouW)

.

```
@app.local_entrypoint()
def main(receipt_filename: Optional[str] = None):
    import urllib.request
    from pathlib import Path

    if receipt_filename is None:
        receipt_filename = Path(__file__).parent / "receipt.png"
    else:
        receipt_filename = Path(receipt_filename)

    if receipt_filename.exists():
        image = receipt_filename.read_bytes()
        print(f"running OCR on {receipt_filename}")
    else:
        receipt_url = "https://modal-cdn.com/cdnbot/Brandys-walmart-receipt-8g68_a_hk_f9c25fce.webp"
        request = urllib.request.Request(receipt_url)
        with urllib.request.urlopen(request) as response:
            image = response.read()
        print(f"running OCR on sample from URL {receipt_url}")
    print(parse_receipt.remote(image))
```

[Run a job queue for GOT-OCR](#run-a-job-queue-for-got-ocr)

[Define an App](#define-an-app)

[Cache the pre-trained model on a Modal Volume](#cache-the-pre-trained-model-on-a-modal-volume)

[Run OCR inference on Modal by wrapping with app.function](#run-ocr-inference-on-modal-by-wrapping-with-appfunction)

[Deploy](#deploy)

[Run manually](#run-manually)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 09_job_queues/doc_ocr_jobs.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/doc_ocr_webapp
================================================================================

Serve a document OCR web app
============================

This tutorial shows you how to use Modal to deploy a fully serverless
[React](https://reactjs.org/)

+
[FastAPI](https://fastapi.tiangolo.com/)

application.
We’re going to build a simple “Receipt Parser” web app that submits OCR transcription
tasks to a separate Modal app defined in
[another example](https://modal.com/docs/examples/doc_ocr_jobs)

,
polls until the task is completed, and displays
the results. Try it out for yourself
[here](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)

.

[![Webapp frontend](https://modal-cdn.com/doc_ocr_frontend.jpg)](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)

Basic setup
-----------

Let’s get the imports out of the way and define an
[`App`](https://modal.com/docs/reference/modal.App)

.

```
from pathlib import Path

import fastapi
import fastapi.staticfiles
import modal

app = modal.App("example-doc-ocr-webapp")
```

Modal works with any
[ASGI](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps)

or
[WSGI](https://modal.com/docs/guide/webhooks#wsgi)

web framework. Here, we choose to use
[FastAPI](https://fastapi.tiangolo.com/)

.

```
web_app = fastapi.FastAPI()
```

Define endpoints
----------------

We need two endpoints: one to accept an image and submit it to the Modal job queue,
and another to poll for the results of the job.

In
`parse`
, we’re going to submit tasks to the function defined in the
[Job
Queue tutorial](https://modal.com/docs/examples/doc_ocr_jobs)

, so we import it first using
[`Function.lookup`](https://modal.com/docs/reference/modal.Function#lookup)

.

We call
[`.spawn()`](https://modal.com/docs/reference/modal.Function#spawn)

on the function handle
we imported above to kick off our function without blocking on the results.
`spawn`
returns
a unique ID for the function call, which we then use
to poll for its result.

```
@web_app.post("/parse")
async def parse(request: fastapi.Request):
    parse_receipt = modal.Function.from_name("example-doc-ocr-jobs", "parse_receipt")

    form = await request.form()
    receipt = await form["receipt"].read()  # type: ignore
    call = parse_receipt.spawn(receipt)
    return {"call_id": call.object_id}
```

`/result`
uses the provided
`call_id`
to instantiate a
`modal.FunctionCall`
object, and attempt
to get its result. If the call hasn’t finished yet, we return a
`202`
status code, which indicates
that the server is still working on the job.

```
@web_app.get("/result/{call_id}")
async def poll_results(call_id: str):
    function_call = modal.functions.FunctionCall.from_id(call_id)
    try:
        result = function_call.get(timeout=0)
    except TimeoutError:
        return fastapi.responses.JSONResponse(content="", status_code=202)

    return result
```

Now that we’ve defined our endpoints, we’re ready to host them on Modal.
First, we specify our dependencies — here, a basic Debian Linux
environment with FastAPI installed.

```
image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "fastapi[standard]==0.115.4"
)
```

Then, we add the static files for our front-end. We’ve made
[a simple React
app](https://github.com/modal-labs/modal-examples/tree/main/09_job_queues/doc_ocr_frontend)

that hits the two endpoints defined above. To package these files with our app, we use
`add_local_dir`
with the local directory of the assets, and specify that we want them
in the
`/assets`
directory inside our container (the
`remote_path`
). Then, we instruct FastAPI to
[serve
this static file directory](https://fastapi.tiangolo.com/tutorial/static-files/)

at our root path.

```
local_assets_path = Path(__file__).parent / "doc_ocr_frontend"
image = image.add_local_dir(local_assets_path, remote_path="/assets")

@app.function(image=image)
@modal.asgi_app()
def wrapper():
    web_app.mount("/", fastapi.staticfiles.StaticFiles(directory="/assets", html=True))
    return web_app
```

Running
-------

While developing, you can run this as an ephemeral app by executing the command

```
modal serve doc_ocr_webapp.py
```

Modal watches all the mounted files and updates the app if anything changes.
See
[these docs](https://modal.com/docs/guide/webhooks#developing-with-modal-serve)

for more details.

Deploy
------

To deploy your application, run

```
modal deploy doc_ocr_webapp.py
```

That’s all!

If successful, this will print a URL for your app that you can navigate to in
your browser 🎉 .

[![Webapp frontend](https://modal-cdn.com/doc_ocr_frontend.jpg)](https://modal-labs-examples--example-doc-ocr-webapp-wrapper.modal.run/)

[Serve a document OCR web app](#serve-a-document-ocr-web-app)

[Basic setup](#basic-setup)

[Define endpoints](#define-endpoints)

[Running](#running)

[Deploy](#deploy)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 09_job_queues/doc_ocr_webapp.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/esm3
================================================================================

Build a protein folding dashboard with ESM3, Molstar, and Gradio
================================================================

![Image of dashboard UI for ESM3 protein folding](https://modal-cdn.com/example-esm3-ui.png)

There are perhaps a quadrillion distinct proteins on the planet Earth,
each one a marvel of nanotechnology discovered by painstaking evolution.
We know the amino acid sequence of nearly a billion but we only
know the three-dimensional structure of a few hundred thousand,
gathered by slow, difficult observational methods like X-ray crystallography.
Built upon this data are machine learning models like
EvolutionaryScale’s
[ESM3](https://www.evolutionaryscale.ai/blog/esm3-release)

that can predict the structure of any sequence in seconds.

In this example, we’ll show how you can use Modal to not
just run the latest protein-folding model but also build tools around it for
you and your team of scientists to understand and analyze the results.

Basic Setup
-----------

```
import base64
import io
from pathlib import Path
from typing import Optional

import modal

MINUTES = 60  # seconds

app = modal.App("example-esm3-dashboard")
```

### Create a Volume to store ESM3 model weights and Entrez sequence data

To minimize cold start times, we’ll store the ESM3 model weights on a Modal
[Volume](https://modal.com/docs/guide/volumes)

.
For patterns and best practices for storing model weights on Modal, see
[this guide](https://modal.com/docs/guide/model-weights)

.
We’ll use this same distributed storage primitive to store sequence data.

```
volume = modal.Volume.from_name("example-esm3-dashboard", create_if_missing=True)
VOLUME_PATH = Path("/vol")
MODELS_PATH = VOLUME_PATH / "models"
DATA_PATH = VOLUME_PATH / "data"
```

### Define dependencies in container images

The container image for structure inference is based on Modal’s default slim Debian
Linux image with
`esm`
for loading and running the model,
`gemmi`
for
managing protein structure file conversions, and
`hf_transfer`
for faster downloading of the model weights from Hugging Face.

```
esm3_image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install(
        "esm==3.1.1",
        "torch==2.4.1",
        "gemmi==0.7.0",
        "huggingface_hub[hf_transfer]==0.26.2",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HOME": str(MODELS_PATH)})
)
```

We’ll also define a separate image, with different dependencies,
for the part of our app that hosts the dashboard.
This helps reduce the complexity of Python dependency management
by “walling off” the different parts, e.g. separating
functions that depend on finicky ML packages
from those that depend on pedantic web packages.
Dependencies include
`gradio`
for building a web UI in Python and
`biotite`
for extracting sequences from UniProt accession numbers.

You can read more about how to configure container images on Modal in
[this guide](https://modal.com/docs/guide/images)

.

```
web_app_image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install("gradio~=4.44.0", "biotite==0.41.2", "fastapi[standard]==0.115.4")
    .add_local_dir(Path(__file__).parent / "frontend", remote_path="/assets")
)
```

Here we “pre-import” libraries that will be used by the functions we run
on Modal in a given image using the
`with image.imports`
context manager.

```
with esm3_image.imports():
    import tempfile

    import gemmi
    import torch
    from esm.models.esm3 import ESM3
    from esm.sdk.api import ESMProtein, GenerationConfig

with web_app_image.imports():
    import biotite.database.entrez as entrez
    import biotite.sequence.io.fasta as fasta
    from fastapi import FastAPI
```

Define a
`Model`
inference class for ESM3
-----------------------------------------

Next, we map the model’s setup and inference code onto Modal.

1. For setup code that only needs to run once, we put it in a method
   decorated with
   `@enter`
   , which runs on container start. For details,
   see
   [this guide](https://modal.com/docs/guide/cold-start)

   .
2. The rest of the inference code goes in a method decorated with
   `@method`
   .
3. We accelerate the compute-intensive inference with a GPU, specifically an A10G.
   For more on using GPUs on Modal, see
   [this guide](https://modal.com/docs/guide/gpu)

   .

```
@app.cls(
    image=esm3_image,
    volumes={VOLUME_PATH: volume},
    secrets=[modal.Secret.from_name("huggingface-secret")],
    gpu="A10G",
    timeout=20 * MINUTES,
)
class Model:
    @modal.enter()
    def enter(self):
        self.model = ESM3.from_pretrained("esm3_sm_open_v1")
        self.model.to("cuda")

        print("using half precision and tensor cores for fast ESM3 inference")
        self.model = self.model.half()
        torch.backends.cuda.matmul.allow_tf32 = True

        self.max_steps = 250
        print(f"setting max ESM steps to: {self.max_steps}")

    def convert_protein_to_MMCIF(self, esm_protein, output_path):
        structure = gemmi.read_pdb_string(esm_protein.to_pdb_string())
        doc = structure.make_mmcif_document()
        doc.write_file(str(output_path), gemmi.cif.WriteOptions())

    def get_generation_config(self, num_steps):
        return GenerationConfig(track="structure", num_steps=num_steps)

    @modal.method()
    def inference(self, sequence: str):
        num_steps = min(len(sequence), self.max_steps)

        print(f"running ESM3 inference with num_steps={num_steps}")
        esm_protein = self.model.generate(
            ESMProtein(sequence=sequence), self.get_generation_config(num_steps)
        )

        print("checking for errors in output")
        if hasattr(esm_protein, "error_msg"):
            raise ValueError(esm_protein.error_msg)

        print("converting ESMProtein into MMCIF file")
        save_path = Path(tempfile.mktemp() + ".mmcif")
        self.convert_protein_to_MMCIF(esm_protein, save_path)

        print("returning MMCIF bytes")
        return io.BytesIO(save_path.read_bytes())
```

Serve a dashboard as an
`asgi_app`
----------------------------------

In this section we’ll create a web interface around the ESM3 model
that can help scientists and stakeholders understand and interrogate the results of the model.

You can deploy this UI, along with the backing inference endpoint,
with the following command:

```
modal deploy esm3.py
```

### Integrating Modal Functions

The integration between our dashboard and our inference backend
is made simple by the Modal SDK:
because the definition of the
`Model`
class is available in the same Python
context as the defintion of the web UI,
we can instantiate an instance and call its methods with
`.remote`
.

The inference runs in a GPU-accelerated container with all of ESM3’s
dependencies, while this code executes in a CPU-only container
with only our web dependencies.

```
def run_esm(sequence: str) -> str:
    sequence = sequence.strip()

    print("running ESM")
    mmcif_buffer = Model().inference.remote(sequence)

    print("converting mmCIF bytes to base64 for compatibility with HTML")
    mmcif_content = mmcif_buffer.read().decode()
    mmcif_base64 = base64.b64encode(mmcif_content.encode()).decode()

    return get_molstar_html(mmcif_base64)
```

### Building a UI in Python with Gradio

We’ll visualize the results using
[Mol\*](https://molstar.org/)

.
Mol\* (pronounced “molstar”) is an open-source toolkit for
visualizing and analyzing large-scale molecular data, including secondary structures
and residue-specific positions of proteins.

Second, we’ll create links to lookup the metadata and structure of known
proteins using the
[Universal Protein Resource](https://www.uniprot.org/)

database from the UniProt consortium which is supported by the European
Bioinformatics Institute, the National Human Genome Research
Institute, and the Swiss Institute of Bioinformatics. UniProt
is also a hub that links to many other databases, like the RCSB Protein
Data Bank.

To pull sequence data, we’ll use the
[Biotite](https://www.biotite-python.org/)

library to pull
[FASTA](https://en.wikipedia.org/wiki/FASTA_format)

files from
UniProt which contain labelled sequences.

You should see the URL for this UI in the output of
`modal deploy`
or on your
[Modal app dashboard](https://modal.com/apps)

for this app.

```
@app.function(
    image=web_app_image,
    volumes={VOLUME_PATH: volume},
    max_containers=1,  # Gradio requires sticky sessions
)
@modal.concurrent(max_inputs=1000)  # Gradio can handle many async inputs
@modal.asgi_app()
def ui():
    import gradio as gr
    from fastapi.responses import FileResponse
    from gradio.routes import mount_gradio_app

    web_app = FastAPI()

    # custom styles: an icon, a background, and some CSS
    @web_app.get("/favicon.ico", include_in_schema=False)
    async def favicon():
        return FileResponse("/assets/favicon.svg")

    @web_app.get("/assets/background.svg", include_in_schema=False)
    async def background():
        return FileResponse("/assets/background.svg")

    css = Path("/assets/index.css").read_text()

    theme = gr.themes.Default(
        primary_hue="green", secondary_hue="emerald", neutral_hue="neutral"
    )

    title = "Predict & Visualize Protein Structures"

    with gr.Blocks(theme=theme, css=css, title=title, js=always_dark()) as interface:
        gr.Markdown(f"# {title}")

        with gr.Row():
            with gr.Column():
                gr.Markdown("## Enter UniProt ID ")
                uniprot_num_box = gr.Textbox(
                    label="Enter UniProt ID or select one on the right",
                    placeholder="e.g. P02768, P69905,  etc.",
                )
                get_sequence_button = gr.Button(
                    "Retrieve Sequence from UniProt ID", variant="primary"
                )

                uniprot_link_button = gr.Button(value="View protein on UniProt website")
                uniprot_link_button.click(
                    fn=None,
                    inputs=uniprot_num_box,
                    js=get_js_for_uniprot_link(),
                )

            with gr.Column():
                example_uniprots = get_uniprot_examples()

                def extract_uniprot_num(example_idx):
                    uniprot = example_uniprots[example_idx]
                    return uniprot[uniprot.index("[") + 1 : uniprot.index("]")]

                gr.Markdown("## Example UniProt Accession Numbers")
                with gr.Row():
                    half_len = int(len(example_uniprots) / 2)
                    with gr.Column():
                        for i, uniprot in enumerate(example_uniprots[:half_len]):
                            btn = gr.Button(uniprot, variant="secondary")
                            btn.click(
                                fn=lambda j=i: extract_uniprot_num(j),
                                outputs=uniprot_num_box,
                            )

                    with gr.Column():
                        for i, uniprot in enumerate(example_uniprots[half_len:]):
                            btn = gr.Button(uniprot, variant="secondary")
                            btn.click(
                                fn=lambda j=i + half_len: extract_uniprot_num(j),
                                outputs=uniprot_num_box,
                            )

        gr.Markdown("## Enter Sequence")
        sequence_box = gr.Textbox(
            label="Enter a sequence or retrieve it from a UniProt ID",
            placeholder="e.g. MVTRLE..., PVTTIMHALL..., etc.",
        )
        get_sequence_button.click(
            fn=get_sequence, inputs=[uniprot_num_box], outputs=[sequence_box]
        )

        run_esm_button = gr.Button("Run ESM3 Folding", variant="primary")

        gr.Markdown("## ESM3 Predicted Structure")
        molstar_html = gr.HTML()

        run_esm_button.click(fn=run_esm, inputs=sequence_box, outputs=molstar_html)

    # return a FastAPI app for Modal to serve
    return mount_gradio_app(app=web_app, blocks=interface, path="/")
```

Folding from the command line
-----------------------------

If you want to quickly run the ESM3 model without the web interface, you can
run it from the command line like this:

```
modal run esm3
```

This will run the same inference code above on Modal. The results are
returned in the
[Crystallographic Information File](https://en.wikipedia.org/wiki/Crystallographic_Information_File)

format, which you can render with the online
[Molstar Viewer](https://molstar.org/viewer/)

.

```
@app.local_entrypoint()
def main(sequence: Optional[str] = None, output_dir: Optional[str] = None):
    if sequence is None:
        print("using sequence for insulin [P01308]")
        sequence = "MRTPMLLALLALATLCLAGRADAKPGDAESGKGAAFVSKQEGSEVVKRLRRYLDHWLGAPAPYPDPLEPKREVCELNPDCDELADHIGFQEAYRRFYGPV"

    if output_dir is None:
        output_dir = Path("/tmp/esm3")
        output_dir.mkdir(parents=True, exist_ok=True)
    output_path = output_dir / "output.mmcif"

    print("starting inference on Modal")
    results_buffer = Model().inference.remote(sequence)

    print(f"writing results to {output_path}")
    output_path.write_bytes(results_buffer.read())
```

Addenda
-------

The remainder of this code is boilerplate.

### Extracting Sequences from UniProt Accession Numbers

To retrieve sequence information we’ll utilize the
`biotite`
library which
will allow us to fetch
[fasta](https://en.wikipedia.org/wiki/FASTA_format)

sequence files from the
[National Center for Biotechnology Information (NCBI) Entrez database](https://www.ncbi.nlm.nih.gov/Web/Search/entrezfs.html)

.

```
def get_sequence(uniprot_num: str) -> str:
    try:
        DATA_PATH.mkdir(parents=True, exist_ok=True)

        uniprot_num = uniprot_num.strip()
        fasta_path = DATA_PATH / f"{uniprot_num}.fasta"

        print(f"Fetching {fasta_path} from the entrez database")
        entrez.fetch_single_file(
            uniprot_num, fasta_path, db_name="protein", ret_type="fasta"
        )
        fasta_file = fasta.FastaFile.read(fasta_path)

        protein_sequence = fasta.get_sequence(fasta_file)
        return str(protein_sequence)

    except Exception as e:
        return f"Error: {e}"
```

### Supporting functions for the Gradio app

The following Python code is used to enhance the Gradio app,
mostly by generating some extra HTML & JS and handling styling.

```
def get_js_for_uniprot_link():
    url = "https://www.uniprot.org/uniprotkb/"
    end = "/entry#structure"
    return f"""(uni_id) => {{ if (!uni_id) return; window.open("{url}" + uni_id + "{end}"); }}"""

def get_molstar_html(mmcif_base64):
    return f"""
    <iframe
        id="molstar_frame"
        style="width: 100%; height: 600px; border: none;"
        srcdoc='
            <!DOCTYPE html>
            <html>
                <head>
                    <script src="https://cdn.jsdelivr.net/npm/@rcsb/rcsb-molstar/build/dist/viewer/rcsb-molstar.js"></script>
                    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@rcsb/rcsb-molstar/build/dist/viewer/rcsb-molstar.css">
                </head>
                <body>
                    <div id="protein-viewer" style="width: 1200px; height: 400px; position: center"></div>
                    <script>
                        console.log("Initializing viewer...");
                        (async function() {{
                            // Create plugin instance
                            const viewer = new rcsbMolstar.Viewer("protein-viewer");

                            // CIF data in base64
                            const mmcifData = "{mmcif_base64}";

                            // Convert base64 to blob
                            const blob = new Blob(
                                [atob(mmcifData)],
                                {{ type: "text/plain" }}
                            );

                            // Create object URL
                            const url = URL.createObjectURL(blob);

                            try {{
                                // Load structure
                                await viewer.loadStructureFromUrl(url, "mmcif");
                            }} catch (error) {{
                                console.error("Error loading structure:", error);
                            }}
                      }})();
                    </script>
                </body>
            </html>
        '>
    </iframe>"""

def get_uniprot_examples():
    return [
        "Albumin [P02768]",
        "Insulin [P01308]",
        "Hemoglobin [P69905]",
        "Lysozyme [P61626]",
        "BRCA1 [P38398]",
        "Immunoglobulin [P01857]",
        "Actin [P60709]",
        "Ribonuclease [P07998]",
    ]

def always_dark():
    return """
    function refresh() {
        const url = new URL(window.location);

        if (url.searchParams.get('__theme') !== 'dark') {
            url.searchParams.set('__theme', 'dark');
            window.location.href = url.href;
        }
    }
    """
```

[Build a protein folding dashboard with ESM3, Molstar, and Gradio](#build-a-protein-folding-dashboard-with-esm3-molstar-and-gradio)

[Basic Setup](#basic-setup)

[Create a Volume to store ESM3 model weights and Entrez sequence data](#create-a-volume-to-store-esm3-model-weights-and-entrez-sequence-data)

[Define dependencies in container images](#define-dependencies-in-container-images)

[Define a Model inference class for ESM3](#define-a-model-inference-class-for-esm3)

[Serve a dashboard as an asgi\_app](#serve-a-dashboard-as-an-asgi_app)

[Integrating Modal Functions](#integrating-modal-functions)

[Building a UI in Python with Gradio](#building-a-ui-in-python-with-gradio)

[Folding from the command line](#folding-from-the-command-line)

[Addenda](#addenda)

[Extracting Sequences from UniProt Accession Numbers](#extracting-sequences-from-uniprot-accession-numbers)

[Supporting functions for the Gradio app](#supporting-functions-for-the-gradio-app)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/protein-folding/esm3.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/fasthtml_app
================================================================================

Deploy a FastHTML app with Modal
================================

This example shows how you can deploy a FastHTML app with Modal.
[FastHTML](https://www.fastht.ml/)

is a Python library built on top of
[HTMX](https://htmx.org/)

which allows you to create entire web applications using only Python.

The integration is pretty simple, thanks to the ASGI standard.
You just need to define a function returns your FastHTML app
and is decorated with
`app.function`
and
`modal.asgi_app`
.

```
import modal

app = modal.App("example-fasthtml")

@app.function(
    image=modal.Image.debian_slim(python_version="3.12").pip_install(
        "python-fasthtml==0.5.2"
    )
)
@modal.asgi_app()
def serve():
    import fasthtml.common as fh

    app = fh.FastHTML()

    @app.get("/")
    def home():
        return fh.Div(fh.P("Hello World!"), hx_get="/change")

    return app
```

[Deploy a FastHTML app with Modal](#deploy-a-fasthtml-app-with-modal)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 07_web_endpoints/fasthtml_app.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/fasthtml_checkboxes
================================================================================

Deploy 100,000 multiplayer checkboxes on Modal with FastHTML
============================================================

[![Screenshot of FastHTML Checkboxes UI](/_app/immutable/assets/ui.BaSTrcQW.png)](https://modal-labs-examples--example-checkboxes-web.modal.run)

This example shows how you can deploy a multiplayer checkbox game with FastHTML on Modal.

[FastHTML](https://www.fastht.ml/)

is a Python library built on top of
[HTMX](https://htmx.org/)

which allows you to create entire web applications using only Python.
For a simpler template for using FastHTML with Modal, check out
[this example](https://modal.com/docs/examples/fasthtml_app)

.

Our example is inspired by
[1 Million Checkboxes](https://onemillioncheckboxes.com/)

.

```
import time
from asyncio import Lock
from pathlib import Path
from uuid import uuid4

import modal

from .constants import N_CHECKBOXES

app = modal.App("example-checkboxes")
db = modal.Dict.from_name("example-checkboxes-db", create_if_missing=True)

css_path_local = Path(__file__).parent / "styles.css"
css_path_remote = "/assets/styles.css"

@app.function(
    image=modal.Image.debian_slim(python_version="3.12")
    .pip_install("python-fasthtml==0.12.21", "inflect~=7.4.0")
    .add_local_file(css_path_local, remote_path=css_path_remote),
    max_containers=1,  # we currently maintain state in memory, so we restrict the server to one worker
)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def web():
    import fasthtml.common as fh
    import inflect

    # Connected clients are tracked in-memory
    clients = {}
    clients_mutex = Lock()

    # We keep all checkbox fasthtml elements in memory during operation, and persist to modal dict across restarts
    checkboxes = db.get("checkboxes", [])
    checkbox_mutex = Lock()

    if len(checkboxes) == N_CHECKBOXES:
        print("Restored checkbox state from previous session.")
    else:
        print("Initializing checkbox state.")
        checkboxes = []
        for i in range(N_CHECKBOXES):
            checkboxes.append(
                fh.Input(
                    id=f"cb-{i}",
                    type="checkbox",
                    checked=False,
                    # when clicked, that checkbox will send a POST request to the server with its index
                    hx_post=f"/checkbox/toggle/{i}",
                    hx_swap_oob="true",  # allows us to later push diffs to arbitrary checkboxes by id
                )
            )

    async def on_shutdown():
        # Handle the shutdown event by persisting current state to modal dict
        async with checkbox_mutex:
            db["checkboxes"] = checkboxes
        print("Checkbox state persisted.")

    style = open(css_path_remote, "r").read()
    app, _ = fh.fast_app(
        # FastHTML uses the ASGI spec, which allows handling of shutdown events
        on_shutdown=[on_shutdown],
        hdrs=[fh.Style(style)],
    )

    # handler run on initial page load
    @app.get("/")
    async def get():
        # register a new client
        client = Client()
        async with clients_mutex:
            clients[client.id] = client

        return (
            fh.Title(f"{N_CHECKBOXES // 1000}k Checkboxes"),
            fh.Main(
                fh.H1(
                    f"{inflect.engine().number_to_words(N_CHECKBOXES).title()} Checkboxes"
                ),
                fh.Div(
                    *checkboxes,
                    id="checkbox-array",
                ),
                cls="container",
                # use HTMX to poll for diffs to apply
                hx_trigger="every 1s",  # poll every second
                hx_get=f"/diffs/{client.id}",  # call the diffs endpoint
                hx_swap="none",  # don't replace the entire page
            ),
        )

    # users submitting checkbox toggles
    @app.post("/checkbox/toggle/{i}")
    async def toggle(i: int):
        async with checkbox_mutex:
            cb = checkboxes[i]
            cb.checked = not cb.checked
            checkboxes[i] = cb

        async with clients_mutex:
            expired = []
            for client in clients.values():
                # clean up old clients
                if not client.is_active():
                    expired.append(client.id)

                # add diff to client for when they next poll
                client.add_diff(i)

            for client_id in expired:
                del clients[client_id]
        return

    # clients polling for any outstanding diffs
    @app.get("/diffs/{client_id}")
    async def diffs(client_id: str):
        # we use the `hx_swap_oob='true'` feature to
        # push updates only for the checkboxes that changed
        async with clients_mutex:
            client = clients.get(client_id, None)
            if client is None or len(client.diffs) == 0:
                return

            client.heartbeat()
            diffs = client.pull_diffs()

        async with checkbox_mutex:
            diff_array = [checkboxes[i] for i in diffs]

        return diff_array

    return app
```

Class for tracking state to push out to connected clients

```
class Client:
    def __init__(self):
        self.id = str(uuid4())
        self.diffs = []
        self.inactive_deadline = time.time() + 30

    def is_active(self):
        return time.time() < self.inactive_deadline

    def heartbeat(self):
        self.inactive_deadline = time.time() + 30

    def add_diff(self, i):
        if i not in self.diffs:
            self.diffs.append(i)

    def pull_diffs(self):
        # return a copy of the diffs and clear them
        diffs = self.diffs
        self.diffs = []
        return diffs
```

[Deploy 100,000 multiplayer checkboxes on Modal with FastHTML](#deploy-100000-multiplayer-checkboxes-on-modal-with-fasthtml)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve -m 07_web_endpoints.fasthtml-checkboxes.fasthtml_checkboxes
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/fastrtc_flip_webcam
================================================================================

Run a FastRTC app on Modal
==========================

[FastRTC](https://fastrtc.org/)

is a Python library for real-time communication on the web.
This example demonstrates how to run a simple FastRTC app in the cloud on Modal.

It’s intended to help you get up and running with real-time streaming applications on Modal
as quickly as possible. If you’re interested in running a production-grade WebRTC app on Modal,
see
[this example](https://modal.com/docs/examples/webrtc_yolo)

.

In this example, we stream webcam video from a browser to a container on Modal,
where the video is flipped, annotated, and sent back with under 100ms of delay.
You can try it out
[here](https://modal-labs-examples--fastrtc-flip-webcam-ui.modal.run/)

or just dive straight into the code to run it yourself.

Set up FastRTC on Modal
-----------------------

First, we import the
`modal`
SDK
and use it to define a
[container image](https://modal.com/docs/guide/images)

with FastRTC and related dependencies.

```
import modal

web_image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "fastapi[standard]==0.115.4",
    "fastrtc==0.0.23",
    "gradio==5.7.1",
    "opencv-python-headless==4.11.0.86",
)
```

Then, we set that as the default Image on our Modal
[App](https://modal.com/docs/guide/apps)

.

```
app = modal.App("fastrtc-flip-webcam", image=web_image)
```

### Configure WebRTC streaming on Modal

Under the hood, FastRTC uses the WebRTC
[APIs](https://www.w3.org/TR/webrtc/)

and
[protocols](https://datatracker.ietf.org/doc/html/rfc8825)

.

WebRTC provides low latency (“real-time”) peer-to-peer communication
for Web applications, focusing on audio and video.
Considering that the Web is a platform originally designed
for high-latency, client-server communication of text and images,
that’s no mean feat!

In addition to protocols that implement this communication,
WebRTC includes APIs for describing and manipulating audio/video streams.
In this demo, we set a few simple parameters, like the direction of the webcam
and the minimum frame rate. See the
[MDN Web Docs for
`MediaTrackConstraints`](https://developer.mozilla.org/en-US/docs/Web/API/MediaTrackConstraints)

for more.

```
TRACK_CONSTRAINTS = {
    "width": {"exact": 640},
    "height": {"exact": 480},
    "frameRate": {"min": 30},
    "facingMode": {  # https://developer.mozilla.org/en-US/docs/Web/API/MediaTrackSettings/facingMode
        "ideal": "user"
    },
}
```

In theory, the Internet is designed for peer-to-peer communication
all the way down to its heart, the Internet Protocol (IP): just send packets between IP addresses.
In practice, peer-to-peer communication on the contemporary Internet is fraught with difficulites,
from restrictive firewalls to finicky work-arounds for
[the exhaustion of IPv4 addresses](https://www.a10networks.com/glossary/what-is-ipv4-exhaustion/)

,
like
[Carrier-Grade Network Address Translation (CGNAT)](https://en.wikipedia.org/wiki/Carrier-grade_NAT)

.

So establishing peer-to-peer connections can be quite involved.
The protocol for doing so is called Interactive Connectivity Establishment (ICE).
It is described in
[this RFC](https://datatracker.ietf.org/doc/html/rfc8445#section-2)

.

ICE involves the peers exchanging a list of connections that might be used.
We use a fairly simple setup here, where our peer on Modal uses the
[Session Traversal Utilities for NAT (STUN)](https://datatracker.ietf.org/doc/html/rfc5389)

server provided by Google. A STUN server basically just reflects back to a client what their
IP address and port number appear to be when they talk to it. The peer on Modal communicates
that information to the other peer trying to connect to it — in this case, a browser trying to share a webcam feed.
Note the use of
`stun`
and port
`19302`
in the URL in place of
something more familiar, like
`http`
and port
`80`
.

```
RTC_CONFIG = {"iceServers": [{"url": "stun:stun.l.google.com:19302"}]}
```

Running a FastRTC app on Modal
------------------------------

FastRTC builds on top of the
[Gradio](https://www.gradio.app/docs)

library for defining Web UIs in Python.
Gradio in turn is compatible with the
[Asynchronous Server Gateway Interface (ASGI)](https://asgi.readthedocs.io/en/latest/)

protocol for asynchronous Python web servers, like
[FastAPI](https://fastrtc.org/userguide/streams/)

,
so we can host it on Modal’s cloud platform using the
[`modal.asgi_app`
decorator](https://modal.com/docs/guide/webhooks#serving-asgi-and-wsgi-apps)

with
[Modal Function](https://modal.com/docs/guide/apps)

.

But before we do that, we need to consider limits:
on how many peers can connect to one instance on Modal
and on how long they can stay connected.
We picked some sensible defaults to show how they interact
with the deployment parameters of the Modal Function.
You’ll want to tune these for your application!

```
MAX_CONCURRENT_STREAMS = 10  # number of peers per instance on Modal

MINUTES = 60  # seconds
TIME_LIMIT = 10 * MINUTES  # time limit

@app.function(
    # gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow that container to handle concurrent streams
    max_containers=1,
    scaledown_window=TIME_LIMIT + 1 * MINUTES,  # add a small buffer to time limit
)
@modal.concurrent(max_inputs=MAX_CONCURRENT_STREAMS)  # inputs per container
@modal.asgi_app()  # ASGI on Modal
def ui():
    import fastrtc  # WebRTC in Gradio
    import gradio as gr  # WebUIs in Python
    from fastapi import FastAPI  # asynchronous ASGI server framework
    from gradio.routes import mount_gradio_app  # connects Gradio and FastAPI

    with gr.Blocks() as blocks:  # block-wise UI definition
        gr.HTML(  # simple HTML header
            "<h1 style='text-align: center'>"
            "Streaming Video Processing with Modal and FastRTC"
            "</h1>"
        )

        with gr.Column():  # a column of UI elements
            fastrtc.Stream(  # high-level media streaming UI element
                modality="video",
                mode="send-receive",
                handler=flip_vertically,  # handler -- handle incoming frame, produce outgoing frame
                ui_args={"title": "Click 'Record' to flip your webcam in the cloud"},
                rtc_configuration=RTC_CONFIG,
                track_constraints=TRACK_CONSTRAINTS,
                concurrency_limit=MAX_CONCURRENT_STREAMS,  # limit simultaneous connections
                time_limit=TIME_LIMIT,  # limit time per connection
            )

    return mount_gradio_app(app=FastAPI(), blocks=blocks, path="/")
```

To try this out for yourself, run

```
modal serve 07_web_endpoints/fastrtc_flip_webcam.py
```

and head to the
`modal.run`
URL that appears in your terminal.
You can also check on the application’s dashboard
via the
`modal.com`
URL thatappears below it.

The
`modal serve`
command produces a hot-reloading development server —
try editing the
`title`
in the
`ui_args`
above and watch the server redeploy.

This temporary deployment is tied to your terminal session.
To deploy permanently, run

```
modal deploy 07_web_endponts/fastrtc_flip_webcam.py
```

Note that Modal is a serverless platform with
[usage-based pricing](https://modal.com/pricing)

,
so this application will spin down and cost you nothing when it is not in use.

Addenda
-------

This FastRTC app is very much the “hello world” or “echo server”
of FastRTC: it just flips the incoming webcam stream and adds a “hello” message.
That logic appears below.

```
def flip_vertically(image):
    import cv2
    import numpy as np

    image = image.astype(np.uint8)

    if image is None:
        print("failed to decode image")
        return

    # flip vertically and caption to show video was processed on Modal
    image = cv2.flip(image, 0)
    lines = ["Hello from Modal!"]
    caption_image(image, lines)

    return image

def caption_image(
    img, lines, font_scale=0.8, thickness=2, margin=10, font=None, color=None
):
    import cv2

    if font is None:
        font = cv2.FONT_HERSHEY_SIMPLEX
    if color is None:
        color = (127, 238, 100, 128)  # Modal Green

    # get text sizes
    sizes = [cv2.getTextSize(line, font, font_scale, thickness)[0] for line in lines]
    if not sizes:
        return

    # position text in bottom right
    pos_xs = [img.shape[1] - size[0] - margin for size in sizes]

    pos_ys = [img.shape[0] - margin]
    for _width, height in reversed(sizes[:-1]):
        next_pos = pos_ys[-1] - 2 * height
        pos_ys.append(next_pos)

    for line, pos in zip(lines, zip(pos_xs, reversed(pos_ys))):
        cv2.putText(img, line, pos, font, font_scale, color, thickness)
```

[Run a FastRTC app on Modal](#run-a-fastrtc-app-on-modal)

[Set up FastRTC on Modal](#set-up-fastrtc-on-modal)

[Configure WebRTC streaming on Modal](#configure-webrtc-streaming-on-modal)

[Running a FastRTC app on Modal](#running-a-fastrtc-app-on-modal)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 07_web_endpoints/fastrtc_flip_webcam.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/finetune_yolo
================================================================================

Fine-tune open source YOLO models for object detection
======================================================

Example by
[@Erik-Dunteman](https://github.com/erik-dunteman)

and
[@AnirudhRahul](https://github.com/AnirudhRahul/)

.

The popular “You Only Look Once” (YOLO) model line provides high-quality object detection in an economical package.
In this example, we use the
[YOLOv10](https://docs.ultralytics.com/models/yolov10/)

model, released on May 23, 2024.

We will:

* Download two custom datasets from the
  [Roboflow](https://roboflow.com/)

  computer vision platform: a dataset of birds and a dataset of bees
* Fine-tune the model on those datasets, in parallel, using the
  [Ultralytics package](https://docs.ultralytics.com/)
* Run inference with the fine-tuned models on single images and on streaming frames

For commercial use, be sure to consult the
[Ultralytics software license options](https://docs.ultralytics.com/#yolo-licenses-how-is-ultralytics-yolo-licensed)

,
which include AGPL-3.0.

Set up the environment
----------------------

```
import warnings
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path

import modal
```

Modal runs your code in the cloud inside containers. So to use it, we have to define the dependencies
of our code as part of the container’s
[image](https://modal.com/docs/guide/custom-container)

.

```
image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install(  # install system libraries for graphics handling
        ["libgl1-mesa-glx", "libglib2.0-0"]
    )
    .pip_install(  # install python libraries for computer vision
        ["ultralytics~=8.2.68", "roboflow~=1.1.37", "opencv-python~=4.10.0"]
    )
    .pip_install(  # add an optional extra that renders images in the terminal
        "term-image==0.7.1"
    )
)
```

We also create a persistent
[Volume](https://modal.com/docs/guide/volumes)

for storing datasets, trained weights, and inference outputs.

```
volume = modal.Volume.from_name("yolo-finetune", create_if_missing=True)
volume_path = (  # the path to the volume from within the container
    Path("/root") / "data"
)
```

We attach both of these to a Modal
[App](https://modal.com/docs/guide/apps)

.

```
app = modal.App("yolo-finetune", image=image, volumes={volume_path: volume})
```

Download a dataset
------------------

We’ll be downloading our data from the
[Roboflow](https://roboflow.com/)

computer vision platform, so to follow along you’ll need to:

* Create a free account on
  [Roboflow](https://app.roboflow.com/)
* [Generate a Private API key](https://app.roboflow.com/settings/api)
* Set up a Modal
  [Secret](https://modal.com/docs/guide/secrets)

  called
  `roboflow-api-key`
  in the Modal UI
  [here](https://modal.com/secrets)

  ,
  setting the
  `ROBOFLOW_API_KEY`
  to the value of your API key.

You’re also free to bring your own dataset with a config in YOLOv10-compatible yaml format.

We’ll be training on the medium size model, but you’re free to experiment with
[other model sizes](https://docs.ultralytics.com/models/yolov10/#model-variants)

.

```
@dataclass
class DatasetConfig:
    """Information required to download a dataset from Roboflow."""

    workspace_id: str
    project_id: str
    version: int
    format: str
    target_class: str

    @property
    def id(self) -> str:
        return f"{self.workspace_id}/{self.project_id}/{self.version}"

@app.function(
    secrets=[
        modal.Secret.from_name("roboflow-api-key", required_keys=["ROBOFLOW_API_KEY"])
    ]
)
def download_dataset(config: DatasetConfig):
    import os

    from roboflow import Roboflow

    rf = Roboflow(api_key=os.getenv("ROBOFLOW_API_KEY"))
    project = (
        rf.workspace(config.workspace_id)
        .project(config.project_id)
        .version(config.version)
    )
    dataset_dir = volume_path / "dataset" / config.id
    project.download(config.format, location=str(dataset_dir))
```

Train a model
-------------

We train the model on a single A100 GPU. Training usually takes only a few minutes.

```
MINUTES = 60

TRAIN_GPU_COUNT = 1
TRAIN_GPU = f"A100:{TRAIN_GPU_COUNT}"
TRAIN_CPU_COUNT = 4

@app.function(
    gpu=TRAIN_GPU,
    cpu=TRAIN_CPU_COUNT,
    timeout=60 * MINUTES,
)
def train(
    model_id: str,
    dataset: DatasetConfig,
    model_size="yolov10m.pt",
    quick_check=False,
):
    from ultralytics import YOLO

    volume.reload()  # make sure volume is synced

    model_path = volume_path / "runs" / model_id
    model_path.mkdir(parents=True, exist_ok=True)

    data_path = volume_path / "dataset" / dataset.id / "data.yaml"

    model = YOLO(model_size)
    model.train(
        # dataset config
        data=data_path,
        fraction=0.4
        if not quick_check
        else 0.04,  # fraction of dataset to use for training/validation
        # optimization config
        device=list(range(TRAIN_GPU_COUNT)),  # use the GPU(s)
        epochs=8 if not quick_check else 1,  # pass over entire dataset this many times
        batch=0.95,  # automatic batch size to target fraction of GPU util
        seed=117,  # set seed for reproducibility
        # data processing config
        workers=max(
            TRAIN_CPU_COUNT // TRAIN_GPU_COUNT, 1
        ),  # split CPUs evenly across GPUs
        cache=False,  # cache preprocessed images in RAM?
        # model saving config
        project=f"{volume_path}/runs",
        name=model_id,
        exist_ok=True,  # overwrite previous model if it exists
        verbose=True,  # detailed logs
    )
```

Run inference on single inputs and on streams
---------------------------------------------

We demonstrate two different ways to run inference — on single images and on a stream of images.

The images we use for inference are loaded from the test set, which was added to our Volume when we downloaded the dataset.
Each image read takes ~50ms, and inference can take ~5ms, so the disk read would be our biggest bottleneck if we just looped over the image paths.
To avoid it, we parallelize the disk reads across many workers using Modal’s
[`.map`](https://modal.com/docs/guide/scale)

,
streaming the images to the model. This roughly mimics the behavior of an interactive object detection pipeline.
This can increase throughput up to ~60 images/s, or ~17 milliseconds/image, depending on image size.

```
@app.function()
def read_image(image_path: str):
    import cv2

    source = cv2.imread(image_path)
    return source
```

We use the
`@enter`
feature of
[`modal.Cls`](https://modal.com/docs/guide/lifecycle-functions)

to load the model only once on container start and reuse it for future inferences.
We use a generator to stream images to the model.

```
@app.cls(gpu="a10g")
class Inference:
    weights_path: str = modal.parameter()

    @modal.enter()
    def load_model(self):
        from ultralytics import YOLO

        self.model = YOLO(self.weights_path)

    @modal.method()
    def predict(self, model_id: str, image_path: str, display: bool = False):
        """A simple method for running inference on one image at a time."""
        results = self.model.predict(
            image_path,
            half=True,  # use fp16
            save=True,
            exist_ok=True,
            project=f"{volume_path}/predictions/{model_id}",
        )
        if display:
            from term_image.image import from_file

            terminal_image = from_file(results[0].path)
            terminal_image.draw()
        # you can view the output file via the Volumes UI in the Modal dashboard -- https://modal.com/storage

    @modal.method()
    def streaming_count(self, batch_dir: str, threshold: float | None = None):
        """Counts the number of objects in a directory of images.

        Intended as a demonstration of high-throughput streaming inference."""
        import os
        import time

        image_files = [os.path.join(batch_dir, f) for f in os.listdir(batch_dir)]

        completed, start = 0, time.monotonic_ns()
        for image in read_image.map(image_files):
            # note that we run predict on a single input at a time.
            # each individual inference is usually done before the next image arrives, so there's no throughput benefit to batching.
            results = self.model.predict(
                image,
                half=True,  # use fp16
                save=False,  # don't save to disk, as it slows down the pipeline significantly
                verbose=False,
            )
            completed += 1
            for res in results:
                for conf in res.boxes.conf:
                    if threshold is None:
                        yield 1
                        continue
                    if conf.item() >= threshold:
                        yield 1
            yield 0

        elapsed_seconds = (time.monotonic_ns() - start) / 1e9
        print(
            "Inferences per second:",
            round(completed / elapsed_seconds, 2),
        )
```

Running the example
-------------------

We’ll kick off our parallel training jobs and run inference from the command line.

```
modal run finetune_yolo.py
```

This runs the training in
`quick_check`
mode, useful for debugging the pipeline and getting a feel for it.
To do a longer run that actually meaningfully improves performance, use:

```
modal run finetune_yolo.py --no-quick-check
```

```
@app.local_entrypoint()
def main(quick_check: bool = True, inference_only: bool = False):
    """Run fine-tuning and inference on two datasets.

    Args:
        quick_check: fine-tune on a small subset. Lower quality results, but faster iteration.
        inference_only: skip fine-tuning and only run inference
    """

    birds = DatasetConfig(
        workspace_id="birds-s35xe",
        project_id="birds-u8mti",
        version=2,
        format="yolov9",
        target_class="🐥",
    )
    bees = DatasetConfig(
        workspace_id="bees-tbdsg",
        project_id="bee-counting",
        version=11,
        format="yolov9",
        target_class="🐝",
    )
    datasets = [birds, bees]

    # .for_each runs a function once on each element of the input iterators
    # here, that means download each dataset, in parallel
    if not inference_only:
        download_dataset.for_each(datasets)

    today = datetime.now().strftime("%Y-%m-%d")
    model_ids = [dataset.id + f"/{today}" for dataset in datasets]

    if not inference_only:
        train.for_each(model_ids, datasets, kwargs={"quick_check": quick_check})

    # let's run inference!
    for model_id, dataset in zip(model_ids, datasets):
        inference = Inference(
            weights_path=str(volume_path / "runs" / model_id / "weights" / "best.pt")
        )

        # predict on a single image and save output to the volume
        test_images = volume.listdir(
            str(Path("dataset") / dataset.id / "test" / "images")
        )
        # run inference on the first 5 images
        for ii, image in enumerate(test_images):
            print(f"{model_id}: Single image prediction on image", image.path)
            inference.predict.remote(
                model_id=model_id,
                image_path=f"{volume_path}/{image.path}",
                display=(
                    ii == 0  # display inference results only on first image
                ),
            )
            if ii >= 4:
                break

        # streaming inference on images from the test set
        print(f"{model_id}: Streaming inferences on all images in the test set...")
        count = 0
        for detection in inference.streaming_count.remote_gen(
            batch_dir=f"{volume_path}/dataset/{dataset.id}/test/images"
        ):
            if detection:
                print(f"{dataset.target_class}", end="")
                count += 1
            else:
                print("🎞️", end="", flush=True)
        print(f"\n{model_id}: Counted {count} {dataset.target_class}s!")
```

Addenda
-------

The rest of the code in this example is utility code.

```
warnings.filterwarnings(  # filter warning from the terminal image library
    "ignore",
    message="It seems this process is not running within a terminal. Hence, some features will behave differently or be disabled.",
    category=UserWarning,
)
```

[Fine-tune open source YOLO models for object detection](#fine-tune-open-source-yolo-models-for-object-detection)

[Set up the environment](#set-up-the-environment)

[Download a dataset](#download-a-dataset)

[Train a model](#train-a-model)

[Run inference on single inputs and on streams](#run-inference-on-single-inputs-and-on-streams)

[Running the example](#running-the-example)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/yolo/finetune_yolo.py --no-quick-check
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/flan_t5_finetune
================================================================================

Finetuning Flan-T5
==================

Example by
[@anishpdalal](https://github.com/anishpdalal)

[Flan-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5)

is a highly versatile model that’s been instruction-tuned to
perform well on a variety of text-based tasks such as question answering and summarization. There are smaller model variants available which makes
Flan-T5 a great base model to use for finetuning on a specific instruction dataset with just a single GPU. In this example, we’ll
finetune Flan-T5 on the
[Extreme Sum (“XSum”)](https://huggingface.co/datasets/xsum)

dataset to summarize news articles.

Defining dependencies
---------------------

The example uses the
`dataset`
package from HuggingFace to load the xsum dataset. It also uses the
`transformers`
and
`accelerate`
packages with a PyTorch backend to finetune and serve the model. Finally, we also
install
`tensorboard`
and serve it via a web app. All packages are installed into a Debian Slim base image
using the
`pip_install`
function.

```
from pathlib import Path

import modal

VOL_MOUNT_PATH = Path("/vol")
```

Other Flan-T5 models can be found
[here](https://huggingface.co/docs/transformers/model_doc/flan-t5)

```
BASE_MODEL = "google/flan-t5-base"

image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "accelerate",
    "transformers",
    "torch",
    "datasets",
    "tensorboard",
)

app = modal.App(name="example-news-summarizer", image=image)
output_vol = modal.Volume.from_name("finetune-volume", create_if_missing=True)
```

### Handling preemption

As this finetuning job is long-running it’s possible that it experiences a preemption.
The training code is robust to preemption events by periodically saving checkpoints and restoring
from checkpoint on restart. But it’s also helpful to observe in logs when a preemption restart has occurred,
so we track restarts with a
`modal.Dict`
.

See the
[guide on preemptions](https://modal.com/docs/guide/preemption#preemption)

for more details on preemption handling.

```
restart_tracker_dict = modal.Dict.from_name(
    "finetune-restart-tracker", create_if_missing=True
)

def track_restarts(restart_tracker: modal.Dict) -> int:
    if not restart_tracker.contains("count"):
        preemption_count = 0
        print(f"Starting first time. {preemption_count=}")
        restart_tracker["count"] = preemption_count
    else:
        preemption_count = restart_tracker.get("count") + 1
        print(f"Restarting after pre-emption. {preemption_count=}")
        restart_tracker["count"] = preemption_count
    return preemption_count
```

Finetuning Flan-T5 on XSum dataset
----------------------------------

Each row in the dataset has a
`document`
(input news article) and
`summary`
column.

```
@app.function(
    gpu="A10g",
    timeout=7200,
    volumes={VOL_MOUNT_PATH: output_vol},
)
def finetune(num_train_epochs: int = 1, size_percentage: int = 10):
    from datasets import load_dataset
    from transformers import (
        AutoModelForSeq2SeqLM,
        AutoTokenizer,
        DataCollatorForSeq2Seq,
        Seq2SeqTrainer,
        Seq2SeqTrainingArguments,
    )

    restarts = track_restarts(restart_tracker_dict)

    # Use size percentage to retrieve subset of the dataset to iterate faster
    if size_percentage:
        xsum_train = load_dataset("xsum", split=f"train[:{size_percentage}%]")
        xsum_test = load_dataset("xsum", split=f"test[:{size_percentage}%]")

    # Load the whole dataset
    else:
        xsum = load_dataset("xsum")
        xsum_train = xsum["train"]
        xsum_test = xsum["test"]

    # Load the tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL)

    # Replace all padding tokens with a large negative number so that the loss function ignores them in
    # its calculation
    padding_token_id = -100

    batch_size = 8

    def preprocess(batch):
        # prepend summarize: prefix to document to convert the example to a summarization instruction
        inputs = ["summarize: " + doc for doc in batch["document"]]

        model_inputs = tokenizer(
            inputs, max_length=512, truncation=True, padding="max_length"
        )

        labels = tokenizer(
            text_target=batch["summary"],
            max_length=128,
            truncation=True,
            padding="max_length",
        )

        labels["input_ids"] = [
            [l if l != tokenizer.pad_token_id else padding_token_id for l in label]
            for label in labels["input_ids"]
        ]

        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_xsum_train = xsum_train.map(
        preprocess, batched=True, remove_columns=["document", "summary", "id"]
    )

    tokenized_xsum_test = xsum_test.map(
        preprocess, batched=True, remove_columns=["document", "summary", "id"]
    )

    data_collator = DataCollatorForSeq2Seq(
        tokenizer,
        model=model,
        label_pad_token_id=padding_token_id,
        pad_to_multiple_of=batch_size,
    )

    training_args = Seq2SeqTrainingArguments(
        # Save checkpoints to the mounted volume
        output_dir=str(VOL_MOUNT_PATH / "model"),
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        predict_with_generate=True,
        learning_rate=3e-5,
        num_train_epochs=num_train_epochs,
        logging_strategy="steps",
        logging_steps=100,
        evaluation_strategy="steps",
        save_strategy="steps",
        save_steps=100,
        save_total_limit=2,
        load_best_model_at_end=True,
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_xsum_train,
        eval_dataset=tokenized_xsum_test,
    )

    try:
        resume = restarts > 0
        if resume:
            print("resuming from checkpoint")
        trainer.train(resume_from_checkpoint=resume)
    except KeyboardInterrupt:  # handle possible preemption
        print("received interrupt; saving state and model")
        trainer.save_state()
        trainer.save_model()
        raise

    # Save the trained model and tokenizer to the mounted volume
    model.save_pretrained(str(VOL_MOUNT_PATH / "model"))
    tokenizer.save_pretrained(str(VOL_MOUNT_PATH / "tokenizer"))
    output_vol.commit()
    print("✅ done")
```

Monitoring Finetuning with Tensorboard
--------------------------------------

Tensorboard is an application for visualizing training loss. In this example we
serve it as a Modal WSGI app.

```
@app.function(volumes={VOL_MOUNT_PATH: output_vol})
@modal.wsgi_app()
def monitor():
    import tensorboard

    board = tensorboard.program.TensorBoard()
    board.configure(logdir=f"{VOL_MOUNT_PATH}/logs")
    (data_provider, deprecated_multiplexer) = board._make_data_provider()
    wsgi_app = tensorboard.backend.application.TensorBoardWSGIApp(
        board.flags,
        board.plugin_loaders,
        data_provider,
        board.assets_zip_provider,
        deprecated_multiplexer,
    )
    return wsgi_app
```

Model Inference
---------------

```
@app.cls(volumes={VOL_MOUNT_PATH: output_vol})
class Summarizer:
    @modal.enter()
    def load_model(self):
        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline

        # Load saved tokenizer and finetuned from training run
        tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL, cache_dir=VOL_MOUNT_PATH / "tokenizer/"
        )
        model = AutoModelForSeq2SeqLM.from_pretrained(
            BASE_MODEL, cache_dir=VOL_MOUNT_PATH / "model/"
        )

        self.summarizer = pipeline("summarization", tokenizer=tokenizer, model=model)

    @modal.method()
    def generate(self, input: str) -> str:
        return self.summarizer(input)[0]["summary_text"]

@app.local_entrypoint()
def main():
    input = """
    The 14-time major champion, playing in his first full PGA Tour event for almost 18 months,
    carded a level-par second round of 72, but missed the cut by four shots after his first-round 76.
    World number one Jason Day and US Open champion Dustin Johnson also missed the cut at Torrey Pines in San Diego.
    Overnight leader Rose carded a one-under 71 to put him on eight under. Canada's
    Adam Hadwin and USA's Brandt Snedeker are tied in second on seven under, while US PGA champion
    Jimmy Walker missed the cut as he finished on three over. Woods is playing in just his
    second tournament since 15 months out with a back injury. "It's frustrating not being
    able to have a chance to win the tournament," said the 41-year-old, who won his last major,
    the US Open, at the same course in 2008. "Overall today was a lot better than yesterday.
    I hit it better, I putted well again. I hit a lot of beautiful putts that didn't go in, but
    I hit it much better today, which was nice." Scotland's Martin Laird and England's Paul Casey
    are both on two under, while Ireland's Shane Lowry is on level par.
    """
    model = Summarizer()
    response = model.generate.remote(input)
    print(response)
```

Run via the CLI
---------------

Trigger model finetuning using the following command:

```
modal run --detach flan_t5_finetune.py::finetune --num-train-epochs=1 --size-percentage=10
View the tensorboard logs at https://<username>--example-news-summarizer-monitor-dev.modal.run
```

Then, you can invoke inference via the
`local_entrypoint`
with this command:

```
modal run flan_t5_finetune.py
World number one Tiger Woods missed the cut at the US Open as he failed to qualify for the final round of the event in Los Angeles.
```

[Finetuning Flan-T5](#finetuning-flan-t5)

[Defining dependencies](#defining-dependencies)

[Handling preemption](#handling-preemption)

[Finetuning Flan-T5 on XSum dataset](#finetuning-flan-t5-on-xsum-dataset)

[Monitoring Finetuning with Tensorboard](#monitoring-finetuning-with-tensorboard)

[Model Inference](#model-inference)

[Run via the CLI](#run-via-the-cli)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/flan_t5/flan_t5_finetune.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/flux
================================================================================

`torch.compile`
===========================================

*Update: To speed up inference by another >2x, check out the additional optimization
techniques we tried in
[this blog post](https://modal.com/blog/flux-3x-faster)

!*

In this guide, we’ll run Flux as fast as possible on Modal using open source tools.
We’ll use
`torch.compile`
and NVIDIA H100 GPUs.

Setting up the image and dependencies
-------------------------------------

```
import time
from io import BytesIO
from pathlib import Path

import modal
```

We’ll make use of the full
[CUDA toolkit](https://modal.com/docs/guide/cuda)

in this example, so we’ll build our container image off of the
`nvidia/cuda`
base.

```
cuda_version = "12.4.0"  # should be no greater than host CUDA version
flavor = "devel"  # includes full CUDA toolkit
operating_sys = "ubuntu22.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"

cuda_dev_image = modal.Image.from_registry(
    f"nvidia/cuda:{tag}", add_python="3.11"
).entrypoint([])
```

Now we install most of our dependencies with
`apt`
and
`pip`
.
For Hugging Face’s
[Diffusers](https://github.com/huggingface/diffusers)

library
we install from GitHub source and so pin to a specific commit.

PyTorch added [faster attention kernels for Hopper GPUs in version 2.5

```
diffusers_commit_sha = "81cf3b2f155f1de322079af28f625349ee21ec6b"

flux_image = (
    cuda_dev_image.apt_install(
        "git",
        "libglib2.0-0",
        "libsm6",
        "libxrender1",
        "libxext6",
        "ffmpeg",
        "libgl1",
    )
    .pip_install(
        "invisible_watermark==0.2.0",
        "transformers==4.44.0",
        "huggingface_hub[hf_transfer]==0.26.2",
        "accelerate==0.33.0",
        "safetensors==0.4.4",
        "sentencepiece==0.2.0",
        "torch==2.5.0",
        f"git+https://github.com/huggingface/diffusers.git@{diffusers_commit_sha}",
        "numpy<2",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1", "HF_HUB_CACHE": "/cache"})
)
```

Later, we’ll also use
`torch.compile`
to increase the speed further.
Torch compilation needs to be re-executed when each new container starts,
So we turn on some extra caching to reduce compile times for later containers.

```
flux_image = flux_image.env(
    {
        "TORCHINDUCTOR_CACHE_DIR": "/root/.inductor-cache",
        "TORCHINDUCTOR_FX_GRAPH_CACHE": "1",
    }
)
```

Finally, we construct our Modal
[App](https://modal.com/docs/reference/modal.App)

,
set its default image to the one we just constructed,
and import
`FluxPipeline`
for downloading and running Flux.1.

```
app = modal.App("example-flux", image=flux_image)

with flux_image.imports():
    import torch
    from diffusers import FluxPipeline
```

Defining a parameterized
`Model`
inference class
------------------------------------------------

Next, we map the model’s setup and inference code onto Modal.

1. We the model setun in the method decorated with
   `@modal.enter()`
   . This includes loading the
   weights and moving them to the GPU, along with an optional
   `torch.compile`
   step (see details below).
   The
   `@modal.enter()`
   decorator ensures that this method runs only once, when a new container starts,
   instead of in the path of every call.
2. We run the actual inference in methods decorated with
   `@modal.method()`
   .

```
MINUTES = 60  # seconds
VARIANT = "schnell"  # or "dev", but note [dev] requires you to accept terms and conditions on HF
NUM_INFERENCE_STEPS = 4  # use ~50 for [dev], smaller for [schnell]

@app.cls(
    gpu="H100",  # fastest GPU on Modal
    scaledown_window=20 * MINUTES,
    timeout=60 * MINUTES,  # leave plenty of time for compilation
    volumes={  # add Volumes to store serializable compilation artifacts, see section on torch.compile below
        "/cache": modal.Volume.from_name("hf-hub-cache", create_if_missing=True),
        "/root/.nv": modal.Volume.from_name("nv-cache", create_if_missing=True),
        "/root/.triton": modal.Volume.from_name("triton-cache", create_if_missing=True),
        "/root/.inductor-cache": modal.Volume.from_name(
            "inductor-cache", create_if_missing=True
        ),
    },
)
class Model:
    compile: bool = (  # see section on torch.compile below for details
        modal.parameter(default=False)
    )

    @modal.enter()
    def enter(self):
        pipe = FluxPipeline.from_pretrained(
            f"black-forest-labs/FLUX.1-{VARIANT}", torch_dtype=torch.bfloat16
        ).to("cuda")  # move model to GPU
        self.pipe = optimize(pipe, compile=self.compile)

    @modal.method()
    def inference(self, prompt: str) -> bytes:
        print("🎨 generating image...")
        out = self.pipe(
            prompt,
            output_type="pil",
            num_inference_steps=NUM_INFERENCE_STEPS,
        ).images[0]

        byte_stream = BytesIO()
        out.save(byte_stream, format="JPEG")
        return byte_stream.getvalue()
```

Calling our inference function
------------------------------

To generate an image we just need to call the
`Model`
’s
`generate`
method
with
`.remote`
appended to it.
You can call
`.generate.remote`
from any Python environment that has access to your Modal credentials.
The local environment will get back the image as bytes.

Here, we wrap the call in a Modal
[`local_entrypoint`](https://modal.com/docs/reference/modal.App#local_entrypoint)

so that it can be run with
`modal run`
:

```
modal run flux.py
```

By default, we call
`generate`
twice to demonstrate how much faster
the inference is after cold start. In our tests, clients received images in about 1.2 seconds.
We save the output bytes to a temporary file.

```
@app.local_entrypoint()
def main(
    prompt: str = "a computer screen showing ASCII terminal art of the"
    " word 'Modal' in neon green. two programmers are pointing excitedly"
    " at the screen.",
    twice: bool = True,
    compile: bool = False,
):
    t0 = time.time()
    image_bytes = Model(compile=compile).inference.remote(prompt)
    print(f"🎨 first inference latency: {time.time() - t0:.2f} seconds")

    if twice:
        t0 = time.time()
        image_bytes = Model(compile=compile).inference.remote(prompt)
        print(f"🎨 second inference latency: {time.time() - t0:.2f} seconds")

    output_path = Path("/tmp") / "flux" / "output.jpg"
    output_path.parent.mkdir(exist_ok=True, parents=True)
    print(f"🎨 saving output to {output_path}")
    output_path.write_bytes(image_bytes)
```

Speeding up Flux with
`torch.compile`
-------------------------------------

By default, we do some basic optimizations, like adjusting memory layout
and re-expressing the attention head projections as a single matrix multiplication.
But there are additional speedups to be had!

PyTorch 2 added a compiler that optimizes the
compute graphs created dynamically during PyTorch execution.
This feature helps close the gap with the performance of static graph frameworks
like TensorRT and TensorFlow.

Here, we follow the suggestions from Hugging Face’s
[guide to fast diffusion inference](https://huggingface.co/docs/diffusers/en/tutorials/fast_diffusion)

,
which we verified with our own internal benchmarks.
Review that guide for detailed explanations of the choices made below.

The resulting compiled Flux
`schnell`
deployment returns images to the client in under a second (~700 ms), according to our testing.
*Super schnell*
!

Compilation takes up to twenty minutes on first iteration.
As of time of writing in late 2024,
the compilation artifacts cannot be fully serialized,
so some compilation work must be re-executed every time a new container is started.
That includes when scaling up an existing deployment or the first time a Function is invoked with
`modal run`
.

We cache compilation outputs from
`nvcc`
,
`triton`
, and
`inductor`
,
which can reduce compilation time by up to an order of magnitude.
For details see
[this tutorial](https://pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html)

.

You can turn on compilation with the
`--compile`
flag.
Try it out with:

```
modal run flux.py --compile
```

The
`compile`
option is passed by a
[`modal.parameter`](https://modal.com/docs/reference/modal.parameter#modalparameter)

on our class.
Each different choice for a
`parameter`
creates a
[separate auto-scaling deployment](https://modal.com/docs/guide/parameterized-functions)

.
That means your client can use arbitrary logic to decide whether to hit a compiled or eager endpoint.

```
def optimize(pipe, compile=True):
    # fuse QKV projections in Transformer and VAE
    pipe.transformer.fuse_qkv_projections()
    pipe.vae.fuse_qkv_projections()

    # switch memory layout to Torch's preferred, channels_last
    pipe.transformer.to(memory_format=torch.channels_last)
    pipe.vae.to(memory_format=torch.channels_last)

    if not compile:
        return pipe

    # set torch compile flags
    config = torch._inductor.config
    config.disable_progress = False  # show progress bar
    config.conv_1x1_as_mm = True  # treat 1x1 convolutions as matrix muls
    # adjust autotuning algorithm
    config.coordinate_descent_tuning = True
    config.coordinate_descent_check_all_directions = True
    config.epilogue_fusion = False  # do not fuse pointwise ops into matmuls

    # tag the compute-intensive modules, the Transformer and VAE decoder, for compilation
    pipe.transformer = torch.compile(
        pipe.transformer, mode="max-autotune", fullgraph=True
    )
    pipe.vae.decode = torch.compile(
        pipe.vae.decode, mode="max-autotune", fullgraph=True
    )

    # trigger torch compilation
    print("🔦 running torch compilation (may take up to 20 minutes)...")

    pipe(
        "dummy prompt to trigger torch compilation",
        output_type="pil",
        num_inference_steps=NUM_INFERENCE_STEPS,  # use ~50 for [dev], smaller for [schnell]
    ).images[0]

    print("🔦 finished torch compilation")

    return pipe
```

[Run Flux fast on H100s with torch.compile](#run-flux-fast-on-h100s-with-torchcompile)

[Setting up the image and dependencies](#setting-up-the-image-and-dependencies)

[Defining a parameterized Model inference class](#defining-a-parameterized-model-inference-class)

[Calling our inference function](#calling-our-inference-function)

[Speeding up Flux with torch.compile](#speeding-up-flux-with-torchcompile)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/stable_diffusion/flux.py --no-compile
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/gpu_fallbacks
================================================================================

Set “fallback” GPUs
===================

GPU availabilities on Modal can fluctuate, especially for
tightly-constrained requests, like for eight co-located GPUs
in a specific region.

If your code can run on multiple different GPUs, you can specify
your GPU request as a list, in order of preference, and whenever
your Function scales up, we will try to schedule it on each requested GPU type in order.

The code below demonstrates the usage of the
`gpu`
parameter with a list of GPUs.

```
import subprocess

import modal

app = modal.App("example-gpu-fallbacks")

@app.function(
    gpu=["h100", "a100", "any"],  # "any" means any of L4, A10, or T4
    max_inputs=1,  # new container each input, so we re-roll the GPU dice every time
)
async def remote(_idx):
    gpu = subprocess.run(
        ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
        check=True,
        text=True,
        stdout=subprocess.PIPE,
    ).stdout.strip()
    print(gpu)
    return gpu

@app.local_entrypoint()
def local(count: int = 32):
    from collections import Counter

    gpu_counter = Counter(remote.map([i for i in range(count)], order_outputs=False))
    print(f"ran {gpu_counter.total()} times")
    print(f"on the following {len(gpu_counter.keys())} GPUs:", end="\n")
    print(
        *[f"{gpu.rjust(32)}: {'🔥' * ct}" for gpu, ct in gpu_counter.items()],
        sep="\n",
    )
```

[Set “fallback” GPUs](#set-fallback-gpus)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/gpu_fallbacks.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/hackernews_alerts
================================================================================

Run cron jobs in the cloud to search Hacker News
================================================

In this example, we use Modal to deploy a cron job that periodically queries Hacker News for
new posts matching a given search term, and posts the results to Slack.

Import and define the app
-------------------------

Let’s start off with imports, and defining a Modal app.

```
import os
from datetime import datetime, timedelta

import modal

app = modal.App("example-hn-bot")
```

Now, let’s define an image that has the
`slack-sdk`
package installed, in which we can run a function
that posts a slack message.

```
slack_sdk_image = modal.Image.debian_slim().pip_install("slack-sdk")
```

Defining the function and importing the secret
----------------------------------------------

Our Slack bot will need access to a bot token.
We can use Modal’s
[Secrets](https://modal.com/secrets)

interface to accomplish this.
To quickly create a Slack bot secret, click the “Create new secret” button.
Then, select the Slack secret template from the list options,
and follow the instructions in the “Where to find the credentials?” panel.
Name your secret
`hn-bot-slack.`

Now, we define the function
`post_to_slack`
, which simply instantiates the Slack client using our token,
and then uses it to post a message to a given channel name.

```
@app.function(
    image=slack_sdk_image,
    secrets=[modal.Secret.from_name("hn-bot-slack", required_keys=["SLACK_BOT_TOKEN"])],
)
async def post_to_slack(message: str):
    import slack_sdk

    client = slack_sdk.WebClient(token=os.environ["SLACK_BOT_TOKEN"])
    client.chat_postMessage(channel="hn-alerts", text=message)
```

Searching Hacker News
---------------------

We are going to use Algolia’s
[Hacker News Search API](https://hn.algolia.com/api)

to query for posts
matching a given search term in the past X days. Let’s define our search term and query period.

```
QUERY = "serverless"
WINDOW_SIZE_DAYS = 1
```

Let’s also define an image that has the
`requests`
package installed, so we can query the API.

```
requests_image = modal.Image.debian_slim().pip_install("requests")
```

We can now define our main entrypoint, that queries Algolia for the term, and calls
`post_to_slack`
on all the results. We specify a
[schedule](https://modal.com/docs/guide/cron)

in the function decorator, which means that our function will run automatically at the given interval.

```
@app.function(image=requests_image)
def search_hackernews():
    import requests

    url = "http://hn.algolia.com/api/v1/search"

    threshold = datetime.utcnow() - timedelta(days=WINDOW_SIZE_DAYS)

    params = {
        "query": QUERY,
        "numericFilters": f"created_at_i>{threshold.timestamp()}",
    }

    response = requests.get(url, params, timeout=10).json()
    urls = [item["url"] for item in response["hits"] if item.get("url")]

    print(f"Query returned {len(urls)} items.")

    post_to_slack.for_each(urls)
```

Test running
------------

We can now test run our scheduled function as follows:
`modal run hackernews_alerts.py::app.search_hackernews`

Defining the schedule and deploying
-----------------------------------

Let’s define a function that will be called by Modal every day

```
@app.function(schedule=modal.Period(days=1))
def run_daily():
    search_hackernews.remote()
```

In order to deploy this as a persistent cron job, you can run
`modal deploy hackernews_alerts.py`
,

Once the job is deployed, visit the
[apps page](https://modal.com/apps)

page to see
its execution history, logs and other stats.

[Run cron jobs in the cloud to search Hacker News](#run-cron-jobs-in-the-cloud-to-search-hacker-news)

[Import and define the app](#import-and-define-the-app)

[Defining the function and importing the secret](#defining-the-function-and-importing-the-secret)

[Searching Hacker News](#searching-hacker-news)

[Test running](#test-running)

[Defining the schedule and deploying](#defining-the-schedule-and-deploying)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 05_scheduling/hackernews_alerts.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/hello_world
================================================================================

Hello, world!
=============

This tutorial demonstrates some core features of Modal:

* You can run functions on Modal just as easily as you run them locally.
* Running functions in parallel on Modal is simple and fast.
* Logs and errors show up immediately, even for functions running on Modal.

Importing Modal and setting up
------------------------------

We start by importing
`modal`
and creating a
`App`
.
We build up this
`App`
to
[define our application](https://modal.com/docs/guide/apps)

.

```
import sys

import modal

app = modal.App("example-hello-world")
```

Defining a function
-------------------

Modal takes code and runs it in the cloud.

So first we’ve got to write some code.

Let’s write a simple function that takes in an input,
prints a log or an error to the console,
and then returns an output.

To make this function work with Modal, we just wrap it in a decorator,
[`@app.function`](https://modal.com/docs/reference/modal.App#function)

.

```
@app.function()
def f(i):
    if i % 2 == 0:
        print("hello", i)
    else:
        print("world", i, file=sys.stderr)

    return i * i
```

Running our function locally, remotely, and in parallel
-------------------------------------------------------

Now let’s see three different ways we can call that function:

1. As a regular call on your
   `local`
   machine, with
   `f.local`
2. As a
   `remote`
   call that runs in the cloud, with
   `f.remote`
3. By
   `map`
   ping many copies of
   `f`
   in the cloud over many inputs, with
   `f.map`

We call
`f`
in each of these ways inside the
`main`
function below.

```
@app.local_entrypoint()
def main():
    # run the function locally
    print(f.local(1000))

    # run the function remotely on Modal
    print(f.remote(1000))

    # run the function in parallel and remotely on Modal
    total = 0
    for ret in f.map(range(200)):
        total += ret

    print(total)
```

Enter
`modal run hello_world.py`
in a shell, and you’ll see a Modal app initialize.
You’ll then see the
`print`
ed logs of
the
`main`
function and, mixed in with them, all the logs of
`f`
as it is run
locally, then remotely, and then remotely and in parallel.

That’s all triggered by adding the
[`@app.local_entrypoint`](https://modal.com/docs/reference/modal.App#local_entrypoint)

decorator on
`main`
, which defines it as the function to start from locally when we invoke
`modal run`
.

What just happened?
-------------------

When we called
`.remote`
on
`f`
, the function was executed
*in the cloud*
, on Modal’s infrastructure, not on the local machine.

In short, we took the function
`f`
, put it inside a container,
sent it the inputs, and streamed back the logs and outputs.

But why does this matter?
-------------------------

Try one of these things next to start seeing the full power of Modal!

### You can change the code and run it again

For instance, change the
`print`
statement in the function
`f`
to print
`"spam"`
and
`"eggs"`
instead and run the app again.
You’ll see that that your new code is run with no extra work from you —
and it should even run faster!

Modal’s goal is to make running code in the cloud feel like you’re
running code locally. That means no waiting for long image builds when you’ve just moved a comma,
no fiddling with container image pushes, and no context-switching to a web UI to inspect logs.

### You can map over more data

Change the
`map`
range from
`200`
to some large number, like
`1170`
. You’ll see
Modal create and run even more containers in parallel this time.

And it’ll happen lightning fast!

### You can run a more interesting function

The function
`f`
is a bit silly and doesn’t do much, but in its place
imagine something that matters to you, like:

* Running
  [language model inference](https://modal.com/docs/examples/vllm_inference)

  or
  [fine-tuning](https://modal.com/docs/examples/slack-finetune)
* Manipulating
  [audio](https://modal.com/docs/examples/musicgen)

  or
  [images](https://modal.com/docs/examples/diffusers_lora_finetune)
* [Embedding huge text datasets](https://modal.com/docs/examples/amazon_embeddings)

  at lightning fast speeds

Modal lets you parallelize that operation effortlessly by running hundreds or
thousands of containers in the cloud.

[Hello, world!](#hello-world)

[Importing Modal and setting up](#importing-modal-and-setting-up)

[Defining a function](#defining-a-function)

[Running our function locally, remotely, and in parallel](#running-our-function-locally-remotely-and-in-parallel)

[What just happened?](#what-just-happened)

[But why does this matter?](#but-why-does-this-matter)

[You can change the code and run it again](#you-can-change-the-code-and-run-it-again)

[You can map over more data](#you-can-map-over-more-data)

[You can run a more interesting function](#you-can-run-a-more-interesting-function)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 01_getting_started/hello_world.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/hp_sweep_gpt
================================================================================

Train an SLM from scratch with early-stopping grid search over hyperparameters
==============================================================================

![Split-Panel Image. Left: AI generated picture of Shakespeare. Right: SLM generated text](/_app/immutable/assets/shakespeare.Cu7OiGC9.jpg)

When you want a language model that performs well on your task, there are three options,
ordered by the degree of customization:

* [**Prompt Engineering**](https://en.wikipedia.org/wiki/Prompt_engineering)

  :
  large and capable language models understand tasks in natural language, so you can
  carefully design a natural language “prompt” to elicit the desired behavior.
* [**Fine-Tuning**](https://modal.com/docs/examples/llm-finetuning)

  :
  those same language models were trained by gradient descent on data sets representing tasks,
  and they can be further trained by gradient descent on data sets representative of your task.
* **Training from Scratch**
  :
  if you have enough data for your task, you can throw the pretrained model away and make your own.

Each step adds additional engineering complexity, but also leads to a superior cost-performance Pareto frontier
for your tasks. Fine-tuned models at one-tenth the size regularly outperform more generic models,
and models trained from scratch outperform them.

Because these models are so much smaller than the Large Language Models that power generic
assistant chatbots like ChatGPT and Claude, they are often called
*Small Language Models*
(SLMs).

In this example, we will explore training an SLM from scratch on Modal.

In fact, we’ll train 8 SLMs in parallel with different hyperparameters
and then select the best one for additional training.

We’ll monitor this training live and serve our training and trained models
as web endpoints and simple browser UIs.

Along the way we’ll use many features of the Modal platform:
[distributed volumes](https://modal.com/docs/guide/volumes)

,
multiple
[web endpoints](https://modal.com/docs/guide/webhooks)

,
and
[parallel container execution](https://modal.com/docs/guide/scale#parallel-execution-of-inputs)

.

Together, these features give every machine learning and AI team
the same infrastructural capabilities that the most sophisticated companies
have in their internal platforms.

Basic Setup
-----------

```
import logging as L
import urllib.request
from dataclasses import dataclass
from pathlib import Path, PosixPath
from typing import Optional

import modal
from pydantic import BaseModel

MINUTES = 60  # seconds
HOURS = 60 * MINUTES

app_name = "example-hp-sweep-gpt"
app = modal.App(app_name)
```

We’ll use A10G GPUs for training, which are able to train the model to recognizably improved performance
in ~15 minutes while keeping costs under ~$1.

```
gpu = "A10G"
```

### Create a Volume to store data, weights, and logs

Since we’ll be coordinating training across multiple machines we’ll use a
distributed
[Volume](https://modal.com/docs/guide/volumes)

to store the data, checkpointed models, and TensorBoard logs.

```
volume = modal.Volume.from_name("example-hp-sweep-gpt-volume", create_if_missing=True)
volume_path = PosixPath("/vol/data")
model_filename = "nano_gpt_model.pt"
best_model_filename = "best_nano_gpt_model.pt"
tb_log_path = volume_path / "tb_logs"
model_save_path = volume_path / "models"
```

### Define dependencies in container images

The container image for training is based on Modal’s default slim Debian Linux image with
`torch`
for defining and running our neural network and
`tensorboard`
for monitoring training.

```
base_image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "pydantic==2.9.1"
)

torch_image = base_image.pip_install(
    "torch==2.1.2",
    "tensorboard==2.17.1",
    "numpy<2",
)
```

We also have some local dependencies that we’ll need to import into the remote environment.
We add them into the remote container.

```
torch_image = torch_image.add_local_dir(
    Path(__file__).parent / "src", remote_path="/root/src"
)
```

We’ll serve a simple web endpoint:

```
web_image = base_image.pip_install("fastapi[standard]==0.115.4", "starlette==0.41.2")
```

And we’ll deploy a web UI for interacting with our trained models using Gradio.

```
assets_path = Path(__file__).parent / "assets"
ui_image = web_image.pip_install("gradio~=4.44.0").add_local_dir(
    assets_path, remote_path="/assets"
)
```

We can also “pre-import” libraries that will be used by the functions we run on Modal in a given image
using the
`with image.imports`
context manager.

```
with torch_image.imports():
    import glob
    import os
    from timeit import default_timer as timer

    import tensorboard
    import torch
    from src.dataset import Dataset
    from src.logs_manager import LogsManager
    from src.model import AttentionModel
    from src.tokenizer import Tokenizer
```

Running SLM training on Modal
-----------------------------

Here we define the training function, wrapping it in a decorator
that specifies the infrastructural parameters, like the container
`image`
we want to use,
which
`volume`
to mount where, the
`gpu`
we’re using, and so on.

Training consists of specifying optimization parameters, loading the
`dataset`
, building the
`model`
, setting up TensorBoard logging &
checkpointing, and then finally executing the
`training_loop`
itself.

```
@app.function(
    image=torch_image,
    volumes={volume_path: volume},
    gpu=gpu,
    timeout=1 * HOURS,
)
def train_model(
    node_rank,
    n_nodes,
    hparams,
    experiment_name,
    run_to_first_save=False,
    n_steps=3000,
    n_steps_before_eval=None,
    n_steps_before_checkpoint=None,
):
    # optimizer, data, and model prep
    batch_size = 64
    learning_rate = 3e-4

    n_eval_steps = 100
    if n_steps_before_eval is None:
        n_steps_before_eval = int(n_steps / 8)  # eval eight times per run
    if n_steps_before_checkpoint is None:
        n_steps_before_checkpoint = int(n_steps / 4)  # save four times per run

    train_percent = 0.9

    L.basicConfig(
        level=L.INFO,
        format=f"\033[0;32m%(asctime)s %(levelname)s [%(filename)s.%(funcName)s:%(lineno)d] [Node {node_rank + 1}] %(message)s\033[0m",
        datefmt="%b %d %H:%M:%S",
    )

    # use GPU if available
    device = "cuda" if torch.cuda.is_available() else "cpu"
    L.info("Remote Device: %s // GPU: %s", device, gpu)

    input_file_path = volume_path / "shakespeare_char.txt"
    text = prepare_data(input_file_path, volume)

    # construct tokenizer & dataset
    tokenizer = Tokenizer(text)
    dataset = Dataset(
        tokenizer.encode(text),
        train_percent,
        batch_size,
        hparams.context_size,
        device,
    )

    # build the model
    model = build_model(hparams, tokenizer.vocab_size, device)
    num_parameters = sum(p.numel() for p in model.parameters())
    L.info(f"Num parameters: {num_parameters}")

    optimizer = setup_optimizer(model, learning_rate)

    # TensorBoard logging & checkpointing prep
    logs_manager = LogsManager(experiment_name, hparams, num_parameters, tb_log_path)
    L.info(f"Model name: {logs_manager.model_name}")

    model_save_dir = model_save_path / experiment_name / logs_manager.model_name
    if model_save_dir.exists():
        L.info("Loading model from checkpoint...")
        checkpoint = torch.load(str(model_save_dir / model_filename))
        is_best_model = not run_to_first_save
        if is_best_model:
            make_best_symbolic_link(model_save_dir, model_filename, experiment_name)
        model.load_state_dict(checkpoint["model"])
        start_step = checkpoint["steps"] + 1
    else:
        model_save_dir.mkdir(parents=True, exist_ok=True)
        start_step = 0
        checkpoint = init_checkpoint(model, tokenizer, optimizer, start_step, hparams)

    checkpoint_path = model_save_dir / model_filename

    out = training_loop(
        start_step,
        n_steps,
        n_steps_before_eval,
        n_steps_before_checkpoint,
        n_eval_steps,
        dataset,
        tokenizer,
        model,
        optimizer,
        logs_manager,
        checkpoint,
        checkpoint_path,
        run_to_first_save,
    )

    return node_rank, float(out["val"]), hparams
```

Launch a hyperparameter sweep from a
`local_entrypoint`
-------------------------------------------------------

The main entry point coordinates the hyperparameter optimization.
First we specify the default hyperparameters for the model, taken from
[Andrej Karpathy’s walkthrough](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=5976s)

.
For better performance, you can increase the
`context_size`
and scale up the GPU accordingly.

```
@dataclass
class ModelHyperparameters:
    n_heads: int = 6
    n_embed: int = 384
    n_blocks: int = 6
    context_size: int = 256
    dropout: float = 0.2
```

Next we define the local entrypoint: the code we run locally to coordinate training.

It will train 8 models in parallel across 8 containers, each
with different hyperparameters, varying the number of heads (
`n_heads`
), the
`context_size`
(called the “block size” by Karpathy), and the dropout rate (
`dropout`
). To run in
parallel we need to use the
[`starmap`
method](https://modal.com/docs/guide/scale#parallel-execution-of-inputs)

.

We train all of the models until the first checkpoint and then stop early so we
can compare the validation losses.

Then we restart training for the best model and train it to completion.

You can kick off training with the following command:

```
modal run 06_gpu_and_ml/hyperparameter-sweep/hp_sweep_gpt.py
```

The output will look something like this:

```
Sep 16 21:20:39 INFO [hp_sweep_gpt.py.train_model:127] [Node 1]  Remote Device: cuda // GPU: A10G
Sep 16 21:20:40 INFO [hp_sweep_gpt.py.train_model:149] [Node 1]  Num parameters: 10693697
Sep 16 21:20:40 INFO [hp_sweep_gpt.py.train_model:156] [Node 1]  Model Name: E2024-0916-142031.618259_context_size=8_n_heads=1_dropout=0.1
Sep 16 21:20:41 INFO [hp_sweep_gpt.py.train_model:225] [Node 1]      0) //  1.03s // Train Loss: 3.58 // Val Loss: 3.60
Sep 16 21:20:41 INFO [hp_sweep_gpt.py.train_model:127] [Node 2]  Remote Device: cuda // GPU: A10G
...
```

The
`local_entrypoint`
code is below. Note that the arguments to it can also be passed via the command line.
Use
`--help`
for details.

```
@app.local_entrypoint()
def main(
    n_steps: int = 3000,
    n_steps_before_checkpoint: Optional[int] = None,
    n_steps_before_eval: Optional[int] = None,
):
    from datetime import datetime
    from itertools import product

    experiment_name = f"E{datetime.now().strftime('%Y-%m-%d-%H%M%S.%f')}"
    default_hparams = ModelHyperparameters()

    # build list of hyperparameters to train & validate
    nheads_options = (1, default_hparams.n_heads)
    context_size_options = (8, default_hparams.context_size)
    dropout_options = (0.1, default_hparams.dropout)

    hparams_list = [
        ModelHyperparameters(n_heads=h, context_size=c, dropout=d)
        for h, c, d in product(nheads_options, context_size_options, dropout_options)
    ]

    # run training for each hyperparameter setting
    results = []
    stop_early = True  # stop early so we can compare val losses
    print(f"Testing {len(hparams_list)} hyperparameter settings")
    n_nodes = len(hparams_list)
    static_params = (
        experiment_name,
        stop_early,
        n_steps,
        n_steps_before_eval,
        n_steps_before_checkpoint,
    )
    for result in train_model.starmap(
        [(i, n_nodes, h, *static_params) for i, h in enumerate(hparams_list)],
        order_outputs=False,
    ):
        # result = (node_rank, val_loss, hparams)
        node_rank = result[0]
        results.append(result)
        print(
            f"[Node {node_rank + 1}/{n_nodes}] Finished. Early stop val loss result: {result[1:]}"
        )

    # find the model and hparams with the lowest validation loss
    best_result = min(results, key=lambda x: x[1])
    print(f"Best early stop val loss result: {best_result}")
    best_hparams = best_result[-1]

    # finish training with best hparams
    node_rank = 0
    n_nodes = 1  # only one node for final training run
    train_model.remote(
        node_rank,
        n_nodes,
        best_hparams,
        experiment_name,
        not stop_early,
        n_steps,
        n_steps_before_eval,
        n_steps_before_checkpoint,
    )
```

### Monitor experiments with TensorBoard

To monitor our training we will create a TensorBoard WSGI web app, which will
display the progress of our training across all 8 models. We’ll use the latest
logs for the most recent experiment written to the Volume.

To ensure we have the latest data we add some
[WSGI Middleware](https://peps.python.org/pep-3333/)

that checks the Modal Volume for updates when the page is reloaded.

```
class VolumeMiddleware:
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        if (route := environ.get("PATH_INFO")) in ["/", "/modal-volume-reload"]:
            try:
                volume.reload()
            except Exception as e:
                print("Exception while re-loading traces: ", e)
            if route == "/modal-volume-reload":
                environ["PATH_INFO"] = "/"  # redirect
        return self.app(environ, start_response)
```

To ensure a unique color per experiment you can click the palette (🎨) icon
under TensorBoard > Time Series > Run and use the Regex:
`E(\d{4})-(\d{2})-(\d{2})-(\d{6})\.(\d{6})`

You can deploy this TensorBoard service by running

```
modal deploy 06_gpu_and_ml/hyperparameter-sweep/hp_sweep_gpt.py
```

and visit it at the URL that ends with
`-monitor-training.modal.run`
.

After training finishes, your TensorBoard UI will look something like this:

![8 lines on a graph, validation loss on y-axis, time step on x-axis. All lines go down over the first 1000 time steps, and one goes to 5000 time steps with a final loss of 1.52](/_app/immutable/assets/tensorboard.dLnBGsm0.png)

You can also find some sample text generated by the model in the “Text” tab.

```
@app.function(
    image=torch_image,
    volumes={volume_path: volume},
)
@modal.concurrent(max_inputs=1000)
@modal.wsgi_app()
def monitor_training():
    board = tensorboard.program.TensorBoard()
    board.configure(logdir=str(tb_log_path))
    (data_provider, deprecated_multiplexer) = board._make_data_provider()
    wsgi_app = tensorboard.backend.application.TensorBoardWSGIApp(
        board.flags,
        board.plugin_loaders,
        data_provider,
        board.assets_zip_provider,
        deprecated_multiplexer,
        experimental_middlewares=[VolumeMiddleware],
    )
    return wsgi_app
```

Notice that there are 8 models training, and the one with the lowest
validation loss at step 600 continues training to 3000 steps.

Serving SLMs on Modal during and after training
-----------------------------------------------

Because our weights are stored in a distributed Volume,
we can deploy an inference endpoint based off of them without any extra work —
and we can even check in on models while we’re still training them!

### Remote inference with Modal `Cls` es

We wrap our inference in a Modal
`Cls`
called
`ModelInference`
.
The user of
`ModelInference`
can control which model is used by providing the
`experiment_name`
. Each unique choice creates a separate
[auto-scaling deployment](https://modal.com/docs/guide/parameterized-functions)

.
If the user does not specify an
`experiment_name`
, the latest experiment
is used.

```
@app.cls(image=torch_image, volumes={volume_path: volume}, gpu=gpu)
class ModelInference:
    experiment_name: str = modal.parameter(default="")

    def get_latest_available_model_dirs(self, n_last):
        """Find the latest models that have a best model checkpoint saved."""
        save_model_dirs = glob.glob(f"{model_save_path}/*")
        sorted_model_dirs = sorted(save_model_dirs, key=os.path.getctime, reverse=True)

        valid_model_dirs = []
        for latest_model_dir in sorted_model_dirs:
            if Path(f"{latest_model_dir}/{best_model_filename}").exists():
                valid_model_dirs.append(Path(latest_model_dir))
            if len(valid_model_dirs) >= n_last:
                return valid_model_dirs
        return valid_model_dirs

    @modal.method()
    def get_latest_available_experiment_names(self, n_last):
        return [d.name for d in self.get_latest_available_model_dirs(n_last)]

    def load_model_impl(self):
        from .src.model import AttentionModel
        from .src.tokenizer import Tokenizer

        if self.experiment_name != "":  # user selected model
            use_model_dir = f"{model_save_path}/{self.experiment_name}"
        else:  # otherwise, pick latest
            try:
                use_model_dir = self.get_latest_available_model_dirs(1)[0]
            except IndexError:
                raise ValueError("No models available to load.")

        if self.use_model_dir == use_model_dir and self.is_fully_trained:
            return  # already loaded fully trained model.

        print(f"Loading experiment: {Path(use_model_dir).name}...")
        checkpoint = torch.load(f"{use_model_dir}/{best_model_filename}")

        self.use_model_dir = use_model_dir
        hparams = checkpoint["hparams"]
        key = (  # for backwards compatibility
            "unique_chars" if "unique_chars" in checkpoint else "chars"
        )
        unique_chars = checkpoint[key]
        steps = checkpoint["steps"]
        val_loss = checkpoint["val_loss"]
        self.is_fully_trained = checkpoint["finished_training"]

        print(
            f"Loaded model with {steps} train steps"
            f" and val loss of {val_loss:.2f}"
            f" (fully_trained={self.is_fully_trained})"
        )

        self.tokenizer = Tokenizer(unique_chars)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        self.model = AttentionModel(self.tokenizer.vocab_size, hparams, self.device)
        self.model.load_state_dict(checkpoint["model"])
        self.model.to(self.device)

    @modal.enter()
    def load_model(self):
        self.use_model_dir = None
        self.is_fully_trained = False
        self.load_model_impl()

    @modal.method()
    def generate(self, prompt):
        self.load_model_impl()  # load updated model if available

        n_new_tokens = 1000
        return self.model.generate_from_text(self.tokenizer, prompt, n_new_tokens)
```

### Adding a simple web endpoint

The
`ModelInference`
class above is available for use
from any other Python environment with the right Modal credentials
and the
`modal`
package installed — just use
[`lookup`](https://modal.com/docs/reference/modal.Cls#lookup)

.

But we can also expose it as a web endpoint for easy access
from anywhere, including other programming languages or the command line.

```
class GenerationRequest(BaseModel):
    prompt: str

@app.function(image=web_image)
@modal.fastapi_endpoint(method="POST", docs=True)
def web_generate(request: GenerationRequest):
    output = ModelInference().generate.remote(request.prompt)
    return {"output": output}
```

This endpoint can be deployed on Modal with
`modal deploy`
.
That will allow us to generate text via a simple
`curl`
command like this:

```
curl -X POST -H 'Content-Type: application/json' --data-binary '{"prompt": "\n"}' https://your-workspace-name--modal-nano-gpt-web-generate.modal.run
```

which will return something like:

```
{
"output":
   "BRUTUS:
    The broy trefore anny pleasory to
    wip me state of villoor so:
    Fortols listhey for brother beat the else
    Be all, ill of lo-love in igham;
    Ah, here all that queen and hould you father offer"
}
```

It’s not exactly Shakespeare, but at least it shows our model learned something!

You can choose which model to use by specifying the
`experiment_name`
in the query parameters of the request URL.

### Serving a Gradio UI with `asgi_app`

Second, we create a Gradio web app for generating text via a graphical user interface in the browser.
That way our fellow team members and stakeholders can easily interact with the model and give feedback,
even when we’re still training the model.

You should see the URL for this UI in the output of
`modal deploy`
or on your
[Modal app dashboard](https://modal.com/apps)

for this app.

The Gradio UI will look something like this:

![Image of Gradio Web App. Top shows model selection dropdown. Left side shows input prompt textbox. Right side shows SLM generated output. Bottom has button for starting generation process](/_app/immutable/assets/gradio.1X4m84IP.png)

```
@app.function(
    image=ui_image,
    max_containers=1,
    volumes={volume_path: volume},
)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def ui():
    import gradio as gr
    from fastapi import FastAPI
    from fastapi.responses import FileResponse
    from gradio.routes import mount_gradio_app

    # call out to the inference in a separate Modal environment with a GPU
    def generate(text="", experiment_name=""):
        if not text:
            text = "\n"
        generated = ModelInference(experiment_name=experiment_name).generate.remote(
            text
        )
        return text + generated

    example_prompts = [
        "DUKE OF YORK:\nWhere art thou Lucas?",
        "ROMEO:\nWhat is a man?",
        "CLARENCE:\nFair is foul and foul is fair, but who are you?",
        "Brevity is the soul of wit, so what is the soul of foolishness?",
    ]

    web_app = FastAPI()

    # custom styles: an icon, a background, and a theme
    @web_app.get("/favicon.ico", include_in_schema=False)
    async def favicon():
        return FileResponse("/assets/favicon.svg")

    @web_app.get("/assets/background.svg", include_in_schema=False)
    async def background():
        return FileResponse("/assets/background.svg")

    with open("/assets/index.css") as f:
        css = f.read()

    n_last = 20
    experiment_names = ModelInference().get_latest_available_experiment_names.remote(
        n_last
    )
    theme = gr.themes.Default(
        primary_hue="green", secondary_hue="emerald", neutral_hue="neutral"
    )

    # add a Gradio UI around inference
    with gr.Blocks(theme=theme, css=css, title="SLM") as interface:
        # title
        gr.Markdown("# GPT-style Shakespeare text generation.")

        # Model Selection
        with gr.Row():
            gr.Markdown("## Model Version")
        with gr.Row():
            experiment_dropdown = gr.Dropdown(
                experiment_names, label="Select Model Version"
            )

        # input and output
        with gr.Row():
            with gr.Column():
                gr.Markdown("## Input:")
                input_box = gr.Textbox(  # input text component
                    label="",
                    placeholder="Write some Shakespeare like text or keep it empty!",
                    lines=10,
                )
            with gr.Column():
                gr.Markdown("## Output:")
                output_box = gr.Textbox(  # output text component
                    label="",
                    lines=10,
                )

        # button to trigger inference and a link to Modal
        with gr.Row():
            generate_button = gr.Button("Generate", variant="primary", scale=2)
            generate_button.click(
                fn=generate,
                inputs=[input_box, experiment_dropdown],
                outputs=output_box,
            )  # connect inputs and outputs with inference function

            gr.Button(  # shameless plug
                " Powered by Modal",
                variant="secondary",
                link="https://modal.com",
            )

        # example prompts
        with gr.Column(variant="compact"):
            # add in a few examples to inspire users
            for ii, prompt in enumerate(example_prompts):
                btn = gr.Button(prompt, variant="secondary")
                btn.click(fn=lambda idx=ii: example_prompts[idx], outputs=input_box)

    # mount for execution on Modal
    return mount_gradio_app(
        app=web_app,
        blocks=interface,
        path="/",
    )
```

Addenda
-------

The remainder of this code is boilerplate.

### Training Loop

There’s quite a lot of code for just the training loop! If you’d rather not write this stuff yourself,
consider a training framework like
[PyTorch Lightning](https://lightning.ai/docs/pytorch/stable)

or
[Hugging Face](https://huggingface.co/transformers/main_classes/trainer.html)

.

```
def training_loop(
    start_step,
    n_steps,
    n_steps_before_eval,
    n_steps_before_checkpoint,
    n_eval_steps,
    dataset,
    tokenizer,
    model,
    optimizer,
    logs_manager,
    checkpoint,
    checkpoint_path,
    run_to_first_save,
):
    @torch.no_grad()
    def eval_model(model, dataset, tokenizer, n_eval_steps):
        """Evaluate model on train and validation data."""
        out = {}
        model.eval()  # Turn off gradients
        for split in ("train", "val"):
            losses = torch.zeros(n_eval_steps)
            for k in range(n_eval_steps):
                xb, yb = dataset.get_batch(split)
                logits, loss = model.forward(xb, yb)
                losses[k] = loss
            out[split] = losses.mean()

        # Generate some output samples
        out["sample"] = model.generate_from_text(tokenizer, "\n", 1000)

        model.train()  # Turn on gradients
        return out

    t_last = timer()
    for step in range(start_step, n_steps + 1):
        # sample a batch of data
        xb, yb = dataset.get_batch("train")

        # evaluate the loss, calculate & apply gradients
        logits, loss = model.forward(xb, yb)
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        # log training loss
        logs_manager.add_train_scalar("Cross Entropy Loss", loss.item(), step)

        # evaluate model on validation set
        if step % n_steps_before_eval == 0:
            out = eval_model(model, dataset, tokenizer, n_eval_steps)
            log_evals(out, step, t_last, logs_manager)
            t_last = timer()

        # save model with checkpoint information
        if step > 0 and step % n_steps_before_checkpoint == 0:
            checkpoint["steps"] = step
            checkpoint["val_loss"] = out["val"]

            # mark as finished if we hit n steps.
            checkpoint["finished_training"] = step >= n_steps

            L.info(
                f"Saving checkpoint to {checkpoint_path}\t {checkpoint['finished_training']})"
            )
            save_checkpoint(checkpoint, checkpoint_path)

            if run_to_first_save:
                L.info("Stopping early...")
                break
    return out

def save_checkpoint(checkpoint, checkpoint_path):
    torch.save(checkpoint, checkpoint_path)
    volume.commit()

def build_model(hparams, vocab_size, device):
    """Initialize the model and move it to the device."""
    model = AttentionModel(vocab_size, hparams, device)
    model.to(device)
    return model

def setup_optimizer(model, learning_rate):
    """Set up the optimizer for the model."""
    return torch.optim.AdamW(model.parameters(), lr=learning_rate)
```

### Miscellaneous

The remaining code includes small helper functions for training the model.

```
def prepare_data(input_file_path: Path, volume: modal.Volume) -> str:
    """Download and read the dataset."""
    volume.reload()
    if not input_file_path.exists():
        L.info("Downloading Shakespeare dataset...")
        data_url = "https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
        urllib.request.urlretrieve(data_url, input_file_path)
        volume.commit()
    return input_file_path.read_text()

def make_best_symbolic_link(model_save_dir, model_filename, experiment_name):
    # create symlink to the best model so it's easy to find for web serving
    os.symlink(
        str(model_save_dir / model_filename),
        str(model_save_path / experiment_name / best_model_filename),
    )
    volume.commit()  # commit the symlink

def init_checkpoint(model, tokenizer, optimizer, start_step, hparams):
    return {
        "model": model.state_dict(),
        "unique_chars": tokenizer.unique_chars,
        "optimizer": optimizer.state_dict(),
        "val_loss": float("inf"),
        "steps": start_step,
        "hparams": hparams,
        "finished_training": False,
    }

def log_evals(result, step, t_last, logs_manager):
    runtime_s = timer() - t_last
    L.info(
        f"{step:5d}) // {runtime_s:>5.2f}s // Train Loss: {result['train']:.2f} // Val Loss: {result['val']:.2f}"
    )
    logs_manager.add_val_scalar("Cross Entropy Loss", result["val"], step)
    logs_manager.add_val_text("Sample Output", result["sample"], step)
    logs_manager.flush()
    volume.commit()  # Make sure TensorBoard container will see it.

    return result
```

[Train an SLM from scratch with early-stopping grid search over hyperparameters](#train-an-slm-from-scratch-with-early-stopping-grid-search-over-hyperparameters)

[Basic Setup](#basic-setup)

[Create a Volume to store data, weights, and logs](#create-a-volume-to-store-data-weights-and-logs)

[Define dependencies in container images](#define-dependencies-in-container-images)

[Running SLM training on Modal](#running-slm-training-on-modal)

[Launch a hyperparameter sweep from a local\_entrypoint](#launch-a-hyperparameter-sweep-from-a-local_entrypoint)

[Monitor experiments with TensorBoard](#monitor-experiments-with-tensorboard)

[Serving SLMs on Modal during and after training](#serving-slms-on-modal-during-and-after-training)

[Remote inference with Modal Clses](#remote-inference-with-modal-clses)

[Adding a simple web endpoint](#adding-a-simple-web-endpoint)

[Serving a Gradio UI with asgi\_app](#serving-a-gradio-ui-with-asgi_app)

[Addenda](#addenda)

[Training Loop](#training-loop)

[Miscellaneous](#miscellaneous)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/hyperparameter-sweep/hp_sweep_gpt.py --n-steps 200 --n-steps-before-checkpoint 50 --n-steps-before-eval 50
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/image_to_image
================================================================================

Edit images with Flux Kontext
=============================

In this example, we run the Flux Kontext model in
*image-to-image*
mode:
the model takes in a prompt and an image and edits the image to better match the prompt.

For example, the model edited the first image into the second based on the prompt
”
*A cute dog wizard inspired by Gandalf from Lord of the Rings, featuring detailed fantasy elements in Studio Ghibli style*
“.

![A photo of a dog transformed into a cartoon of a cute dog wizard](https://modal-cdn.com/dog-wizard-ghibli-flux-kontext.jpg)

The model is Black Forest Labs’
[FLUX.1-Kontext-dev](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)

.
Learn more about the model
[here](https://bfl.ai/announcements/flux-1-kontext-dev)

.

Define a container image
------------------------

First, we define the environment the model inference will run in,
the
[container image](https://modal.com/docs/guide/custom-container)

.

```
from io import BytesIO
from pathlib import Path

import modal

diffusers_commit_sha = "00f95b9755718aabb65456e791b8408526ae6e76"

image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.8.1-devel-ubuntu22.04",
        add_python="3.12",
    )
    .entrypoint([])  # remove verbose logging by base image on entry
    .apt_install("git")
    .pip_install("uv")
    .run_commands(
        f"uv pip install --system --compile-bytecode --index-strategy unsafe-best-match accelerate~=1.8.1 git+https://github.com/huggingface/diffusers.git@{diffusers_commit_sha} huggingface-hub[hf-transfer]~=0.33.1 Pillow~=11.2.1 safetensors~=0.5.3 transformers~=4.53.0 sentencepiece~=0.2.0 torch==2.7.1 optimum-quanto==0.2.7 --extra-index-url https://download.pytorch.org/whl/cu128"
    )
)

MODEL_NAME = "black-forest-labs/FLUX.1-Kontext-dev"
MODEL_REVISION = "f9fdd1a95e0dfd7653cb0966cda2486745122695"

CACHE_DIR = Path("/cache")
cache_volume = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)
volumes = {CACHE_DIR: cache_volume}

secrets = [modal.Secret.from_name("huggingface-secret")]

image = image.env(
    {
        "HF_HUB_ENABLE_HF_TRANSFER": "1",  # Allows faster model downloads
        "HF_HOME": str(CACHE_DIR),  # Points the Hugging Face cache to a Volume
    }
)

app = modal.App("image-to-image")

with image.imports():
    import torch
    from diffusers import FluxKontextPipeline
    from diffusers.utils import load_image
    from PIL import Image
```

Setting up and running Flux Kontext
-----------------------------------

The Modal
`Cls`
defined below contains all the logic to set up and run Flux Kontext.

The
[container lifecycle](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-beta)

decorator
(
`@modal.enter()`
) ensures that the model is loaded into memory when a container starts, before it picks up any inputs.

The
`inference`
method runs the actual model inference. It takes in an image as a collection of
`bytes`
and a string
`prompt`
and returns
a new image (also as a collection of
`bytes`
).

To avoid excessive cold-starts, we set the
`scaledown_window`
to 240 seconds, meaning once a GPU has loaded the model it will stay
online for 4 minutes before spinning down.

```
@app.cls(
    image=image, gpu="B200", volumes=volumes, secrets=secrets, scaledown_window=240
)
class Model:
    @modal.enter()
    def enter(self):
        print(f"Downloading {MODEL_NAME} if necessary...")

        dtype = torch.bfloat16

        self.seed = 42
        self.device = "cuda"

        self.pipe = FluxKontextPipeline.from_pretrained(
            MODEL_NAME,
            revision=MODEL_REVISION,
            torch_dtype=dtype,
            cache_dir=CACHE_DIR,
        ).to(self.device)

    @modal.method()
    def inference(
        self,
        image_bytes: bytes,
        prompt: str,
        guidance_scale: float = 3.5,
        num_inference_steps: int = 20,
    ) -> bytes:
        init_image = load_image(Image.open(BytesIO(image_bytes))).resize((512, 512))

        image = self.pipe(
            image=init_image,
            prompt=prompt,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            output_type="pil",
            generator=torch.Generator(device=self.device).manual_seed(self.seed),
        ).images[0]

        byte_stream = BytesIO()
        image.save(byte_stream, format="PNG")
        image_bytes = byte_stream.getvalue()

        return image_bytes
```

Running the model from the command line
---------------------------------------

You can run the model from the command line with

```
modal run image_to_image.py
```

Use
`--help`
for additional details.

```
@app.local_entrypoint()
def main(
    image_path=Path(__file__).parent / "demo_images/dog.png",
    prompt="A cute dog wizard inspired by Gandalf from Lord of the Rings, featuring detailed fantasy elements in Studio Ghibli style",
    strength=0.9,  # increase to favor the prompt over the baseline image
):
    print(f"🎨 reading input image from {image_path}")
    input_image_bytes = Path(image_path).read_bytes()
    print(f"🎨 editing image with prompt {prompt}")
    output_image_bytes = Model().inference.remote(input_image_bytes, prompt)

    dir = Path("/tmp/stable-diffusion")
    dir.mkdir(exist_ok=True, parents=True)

    output_path = dir / "output.png"
    print(f"🎨 saving output image to {output_path}")
    output_path.write_bytes(output_image_bytes)
```

[Edit images with Flux Kontext](#edit-images-with-flux-kontext)

[Define a container image](#define-a-container-image)

[Setting up and running Flux Kontext](#setting-up-and-running-flux-kontext)

[Running the model from the command line](#running-the-model-from-the-command-line)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/stable_diffusion/image_to_image.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/image_to_video
================================================================================

Animate images with Lightricks LTX-Video via CLI, API, and web UI
=================================================================

This example shows how to run
[LTX-Video](https://huggingface.co/Lightricks/LTX-Video)

on Modal
to generate videos from your local command line, via an API, and in a web UI.

Generating a 5 second video takes ~1 minute from cold start.
Once the container is warm, a 5 second video takes ~15 seconds.

Here is a sample we generated:

[

](https://modal-cdn.com/example_image_to_video.mp4)

Basic setup
-----------

```
import io
import random
import time
from pathlib import Path
from typing import Annotated, Optional

import fastapi
import modal
```

All Modal programs need an
[`App`](https://modal.com/docs/reference/modal.App)

—
an object that acts as a recipe for the application.

```
app = modal.App("example-image-to-video")
```

### Configuring dependencies

The model runs remotely, on Modal’s cloud, which means we need to
[define the environment it runs in](https://modal.com/docs/guide/images)

.

Below, we start from a lightweight base Linux image
and then install our system and Python dependencies,
like Hugging Face’s
`diffusers`
library and
`torch`
.

```
image = (
    modal.Image.debian_slim(python_version="3.12")
    .apt_install("python3-opencv")
    .pip_install(
        "accelerate==1.4.0",
        "diffusers==0.32.2",
        "fastapi[standard]==0.115.8",
        "huggingface-hub[hf_transfer]==0.29.1",
        "imageio==2.37.0",
        "imageio-ffmpeg==0.6.0",
        "opencv-python==4.11.0.86",
        "pillow==11.1.0",
        "sentencepiece==0.2.0",
        "torch==2.6.0",
        "torchvision==0.21.0",
        "transformers==4.49.0",
    )
)
```

Storing model weights on Modal
------------------------------

We also need the parameters of the model remotely.
They can be loaded at runtime from Hugging Face,
based on a repository ID and a revision (aka a commit SHA).

```
MODEL_ID = "Lightricks/LTX-Video"
MODEL_REVISION_ID = "a6d59ee37c13c58261aa79027d3e41cd41960925"
```

Hugging Face will also cache the weights to disk once they’re downloaded.
But Modal Functions are serverless, and so even disks are ephemeral,
which means the weights would get re-downloaded every time we spin up a new instance.

We can fix this — without any modifications to Hugging Face’s model loading code! —
by pointing the Hugging Face cache at a
[Modal Volume](https://modal.com/docs/guide/volumes)

.

```
model_volume = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

MODEL_PATH = "/models"  # where the Volume will appear on our Functions' filesystems

image = image.env(
    {
        "HF_HUB_ENABLE_HF_TRANSFER": "1",  # faster downloads
        "HF_HUB_CACHE": MODEL_PATH,
    }
)
```

Storing model outputs on Modal
------------------------------

Contemporary video models can take a long time to run and they produce large outputs.
That makes them a great candidate for storage on Modal Volumes as well.
Python code running outside of Modal can also access this storage, as we’ll see below.

```
OUTPUT_PATH = "/outputs"
output_volume = modal.Volume.from_name("outputs", create_if_missing=True)
```

Implementing LTX-Video inference on Modal
-----------------------------------------

We wrap the inference logic in a Modal
[Cls](https://modal.com/docs/guide/lifecycle-functions)

that ensures models are loaded and then moved to the GPU once when a new instance
starts, rather than every time we run it.

The
`run`
function just wraps a
`diffusers`
pipeline.
It saves the generated video to a Modal Volume, and returns the filename.

We also include a
`web`
wrapper that makes it possible
to trigger inference via an API call.
For details, see the
`/docs`
route of the URL ending in
`inference-web.modal.run`
that appears when you deploy the app.

```
with image.imports():  # loaded on all of our remote Functions
    import diffusers
    import torch
    from PIL import Image

MINUTES = 60

@app.cls(
    image=image,
    gpu="H100",
    timeout=10 * MINUTES,
    scaledown_window=10 * MINUTES,
    volumes={MODEL_PATH: model_volume, OUTPUT_PATH: output_volume},
)
class Inference:
    @modal.enter()
    def load_pipeline(self):
        self.pipe = diffusers.LTXImageToVideoPipeline.from_pretrained(
            MODEL_ID,
            revision=MODEL_REVISION_ID,
            torch_dtype=torch.bfloat16,
        ).to("cuda")

    @modal.method()
    def run(
        self,
        image_bytes: bytes,
        prompt: str,
        negative_prompt: Optional[str] = None,
        num_frames: Optional[int] = None,
        num_inference_steps: Optional[int] = None,
        seed: Optional[int] = None,
    ) -> str:
        negative_prompt = (
            negative_prompt
            or "worst quality, inconsistent motion, blurry, jittery, distorted"
        )
        width = 768
        height = 512
        num_frames = num_frames or 25
        num_inference_steps = num_inference_steps or 50
        seed = seed or random.randint(0, 2**32 - 1)
        print(f"Seeding RNG with: {seed}")
        torch.manual_seed(seed)

        image = diffusers.utils.load_image(Image.open(io.BytesIO(image_bytes)))

        video = self.pipe(
            image=image,
            prompt=prompt,
            negative_prompt=negative_prompt,
            width=width,
            height=height,
            num_frames=num_frames,
            num_inference_steps=num_inference_steps,
        ).frames[0]

        mp4_name = (
            f"{seed}_{''.join(c if c.isalnum() else '-' for c in prompt[:100])}.mp4"
        )
        diffusers.utils.export_to_video(
            video, f"{Path(OUTPUT_PATH) / mp4_name}", fps=24
        )
        output_volume.commit()
        torch.cuda.empty_cache()  # reduce fragmentation
        return mp4_name

    @modal.fastapi_endpoint(method="POST", docs=True)
    def web(
        self,
        image_bytes: Annotated[bytes, fastapi.File()],
        prompt: str,
        negative_prompt: Optional[str] = None,
        num_frames: Optional[int] = None,
        num_inference_steps: Optional[int] = None,
        seed: Optional[int] = None,
    ) -> fastapi.Response:
        mp4_name = self.run.local(  # run in the same container
            image_bytes=image_bytes,
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_frames=num_frames,
            num_inference_steps=num_inference_steps,
            seed=seed,
        )
        return fastapi.responses.FileResponse(
            path=f"{Path(OUTPUT_PATH) / mp4_name}",
            media_type="video/mp4",
            filename=mp4_name,
        )
```

Generating videos from the command line
---------------------------------------

We add a
[local entrypoint](https://modal.com/docs/reference/modal.App#local_entrypoint)

that calls the
`Inference.run`
method to run inference from the command line.
The function’s parameters are automatically turned into a CLI.

Run it with

```
modal run image_to_video.py --prompt "A cat looking out the window at a snowy mountain" --image-path /path/to/cat.jpg
```

You can also pass
`--help`
to see the full list of arguments.

```
@app.local_entrypoint()
def entrypoint(
    image_path: str,
    prompt: str,
    negative_prompt: Optional[str] = None,
    num_frames: Optional[int] = None,
    num_inference_steps: Optional[int] = None,
    seed: Optional[int] = None,
    twice: bool = True,
):
    import os
    import urllib.request

    print(f"🎥 Generating a video from the image at {image_path}")
    print(f"🎥 using the prompt {prompt}")

    if image_path.startswith(("http://", "https://")):
        image_bytes = urllib.request.urlopen(image_path).read()
    elif os.path.isfile(image_path):
        image_bytes = Path(image_path).read_bytes()
    else:
        raise ValueError(f"{image_path} is not a valid file or URL.")

    inference_service = Inference()

    for _ in range(1 + twice):
        start = time.time()
        mp4_name = inference_service.run.remote(
            image_bytes=image_bytes,
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_frames=num_frames,
            seed=seed,
        )
        duration = time.time() - start
        print(f"🎥 Generated video in {duration:.3f}s")

        output_dir = Path("/tmp/image_to_video")
        output_dir.mkdir(exist_ok=True, parents=True)
        output_path = output_dir / mp4_name
        # read in the file from the Modal Volume, then write it to the local disk
        output_path.write_bytes(b"".join(output_volume.read_file(mp4_name)))
        print(f"🎥 Video saved to {output_path}")
```

Generating videos via an API
----------------------------

The Modal
`Cls`
above also included a
[`fastapi_endpoint`](https://modal.com/docs/examples/basic_web)

,
which adds a simple web API to the inference method.

To try it out, run

```
modal deploy image_to_video.py
```

copy the printed URL ending in
`inference-web.modal.run`
,
and add
`/docs`
to the end. This will bring up the interactive
Swagger/OpenAPI docs for the endpoint.

Generating videos in a web UI
-----------------------------

Lastly, we add a simple front-end web UI (written in Alpine.js) for
our image to video backend.

This is also deployed when you run

```
modal deploy image_to_video.py.
```

The
`Inference`
class will serve multiple users from its own auto-scaling pool of warm GPU containers automatically,
and they will spin down when there are no requests.

```
frontend_path = Path(__file__).parent / "frontend"

web_image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install("jinja2==3.1.5", "fastapi[standard]==0.115.8")
    .add_local_dir(  # mount frontend/client code
        frontend_path, remote_path="/assets"
    )
)

@app.function(image=web_image)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def ui():
    import fastapi.staticfiles
    import fastapi.templating

    web_app = fastapi.FastAPI()
    templates = fastapi.templating.Jinja2Templates(directory="/assets")

    @web_app.get("/")
    async def read_root(request: fastapi.Request):
        return templates.TemplateResponse(
            "index.html",
            {
                "request": request,
                "inference_url": Inference().web.get_web_url(),
                "model_name": "LTX-Video Image to Video",
                "default_prompt": "A young girl stands calmly in the foreground, looking directly at the camera, as a house fire rages in the background.",
            },
        )

    web_app.mount(
        "/static",
        fastapi.staticfiles.StaticFiles(directory="/assets"),
        name="static",
    )

    return web_app
```

[Animate images with Lightricks LTX-Video via CLI, API, and web UI](#animate-images-with-lightricks-ltx-video-via-cli-api-and-web-ui)

[Basic setup](#basic-setup)

[Configuring dependencies](#configuring-dependencies)

[Storing model weights on Modal](#storing-model-weights-on-modal)

[Storing model outputs on Modal](#storing-model-outputs-on-modal)

[Implementing LTX-Video inference on Modal](#implementing-ltx-video-inference-on-modal)

[Generating videos from the command line](#generating-videos-from-the-command-line)

[Generating videos via an API](#generating-videos-via-an-api)

[Generating videos in a web UI](#generating-videos-in-a-web-ui)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/image-to-video/image_to_video.py --prompt 'A young girl stands calmly in the foreground, looking directly at the camera, as a house fire rages in the background.' --image-path https\://modal-cdn.com/example_image_to_video_image.png
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/instructor_generate
================================================================================

`instructor`
=============================================

This example demonstrates how to use the
`instructor`
library to extract structured, schematized data from unstructured text.

Structured output is a powerful but under-appreciated feature of LLMs.
Structured output allows LLMs and multimodal models to connect to traditional software,
for example enabling the ingestion of unstructured data like text files into structured databases.
Applied properly, it makes them an extreme example of the
[Robustness Principle](https://en.wikipedia.org/wiki/Robustness_principle)

Jon Postel formulated for TCP: “Be conservative in what you send, be liberal in what you accept”.

The unstructured data used in this example code is the code from the examples in the Modal examples repository —
including this example’s code!

The output includes a JSONL file containing, on each line, the metadata extracted from the code in one example.
This can be consumed downstream by other software systems, like a database or a dashboard.
We’ve used it to maintain and update our
[examples repository](https://github.com/modal-labs/modal-examples)

.

Environment setup
-----------------

We set up the environment our code will run in first.
In Modal, we define environments via
[container images](https://modal.com/docs/guide/custom-container)

,
much like Docker images, by iteratively chaining together commands.

Here there’s just one command, installing
`instructor`
and the Python SDK for Anthropic’s LLM API.

```
from pathlib import Path
from typing import Literal, Optional

import modal
from pydantic import BaseModel, Field

image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "instructor~=1.7.2", "anthropic==0.42.0"
)
```

This example uses models from Anthropic, so if you want to run it yourself,
you’ll need an Anthropic API key and a Modal
[`Secret`](https://modal.com/docs/guide/secrets)

called
`my-anthropic-secret`
to hold share it with your Modal Functions.

```
app = modal.App(
    image=image,
    secrets=[
        modal.Secret.from_name("anthropic-secret", required_keys=["ANTHROPIC_API_KEY"])
    ],
)
```

Running Modal functions from the command line
---------------------------------------------

We’ll run the example by calling
`modal run instructor_generate.py`
from the command line.

When we invoke
`modal run`
on a Python file, we run the function
marked with
`@app.local_entrypoint`
.

This is the only code that runs locally — it coordinates
the activity of the rest of our code, which runs in Modal’s cloud.

The logic is fairly simple: collect up the code for our examples,
and then use
`instructor`
to extract metadata from them,
which we then write to a file.

By default, the language model is Claude 3 Haiku, the smallest model
in the Claude 3 family. We include the option to run
`with_opus`
,
which gives much better results, but it is off by default because
Opus is also ~60x more expensive, at ~$30 per million tokens.

```
@app.local_entrypoint()
def main(limit: int = 1, with_opus: bool = False):
    # find all of the examples in the repo
    examples = get_examples()
    # optionally limit the number of examples we process
    if limit == 1:
        examples = [None]  # just run on this example
    else:
        examples = examples[:limit]
    # use Modal to map our extraction function over the examples concurrently
    results = extract_example_metadata.map(
        (  # iterable of file contents
            Path(example.filename).read_text() if example else None
            for example in examples
        ),
        (  # iterable of filenames
            example.stem if example else None for example in examples
        ),
        kwargs={"with_opus": with_opus},
    )

    # save the results to a local file
    results_path = Path("/tmp") / "instructor_generate" / "results.jsonl"
    results_dir = results_path.parent
    if not results_dir.exists():
        results_dir.mkdir(parents=True)

    print(f"writing results to {results_path}")
    with open(results_path, "w") as f:
        for result in results:
            print(result)
            f.write(result + "\n")
```

Extracting JSON from unstructured text with
`instructor`
and Pydantic
---------------------------------------------------------------------

The real meat of this example is in this section, in the
`extract_example_metadata`
function and its schemas.

We define a schema for the data we want the LLM to extract, using Pydantic.
Instructor ensures that the LLM’s output matches this schema.

We can use the type system provided by Python and Pydantic to express many useful features
of the data we want to extract — ranging from wide-open fields like a
`str`
ing-valued
`summary`
to constrained fields like
`difficulty`
, which can only take on value between 1 and 5.

```
class ExampleMetadataExtraction(BaseModel):
    """Extracted metadata about an example from the Modal examples repo."""

    summary: str = Field(..., description="A brief summary of the example.")
    has_thorough_explanation: bool = Field(
        ...,
        description="The example contains, in the form of inline comments with markdown formatting, a thorough explanation of what the code does.",
    )
    tags: list[
        Literal[
            "use-case-inference-lms",
            "use-case-inference-audio",
            "use-case-inference-images-video-3d",
            "use-case-finetuning",
            "use-case-job-queues-batch-processing",
            "use-case-sandboxed-code-execution",
        ]
    ] = Field(..., description="The use cases associated with the example")
    freshness: float = Field(
        ...,
        description="The freshness of the example, from 0 to 1. This is relative to your knowledge cutoff. Examples are less fresh if they use older libraries and tools.",
    )
```

That schema describes the data to be extracted by the LLM, but not all data is best extracted by an LLM.
For example, the filename is easily determined in software.

So we inject that information into the output after the LLM has done its work. That necessitates
an additional schema, which inherits from the first.

```
class ExampleMetadata(ExampleMetadataExtraction):
    """Metadata about an example from the Modal examples repo."""

    filename: Optional[str] = Field(..., description="The filename of the example.")
```

With these schemas in hand, it’s straightforward to write the function that extracts the metadata.
Note that we decorate it with
`@app.function`
to make it run on Modal.

```
@app.function(max_containers=5)  # watch those LLM API rate limits!
def extract_example_metadata(
    example_contents: Optional[str] = None,
    filename: Optional[str] = None,
    with_opus=False,
):
    import instructor
    from anthropic import Anthropic

    # if no example is provided, use the contents of this example
    if example_contents is None:
        example_contents = Path(__file__).read_text()
        filename = Path(__file__).name

    client = instructor.from_anthropic(Anthropic())
    model = "claude-3-opus-20240229" if with_opus else "claude-3-haiku-20240307"

    # add the schema as the `response_model` argument in what otherwise looks like a normal LLM API call
    extracted_metadata = client.messages.create(
        model=model,
        temperature=0.0,
        max_tokens=1024,
        response_model=ExampleMetadataExtraction,
        messages=[
            {
                "role": "user",
                "content": f"Extract the metadata for this example.\n\n-----EXAMPLE BEGINS-----{example_contents}-----EXAMPLE ENDS-----\n\n",
            },
        ],
    )

    # inject the filename
    full_metadata = ExampleMetadata(**extracted_metadata.dict(), filename=filename)

    # return it as JSON
    return full_metadata.model_dump_json()
```

Addenda
-------

The rest of the code used in this example is not particularly interesting:
just a utility function to find all of the examples, which we invoke in the
`local_entrypoint`
above.

```
def get_examples(silent=True):
    """Find all of the examples using a utility from this repo.

    We use importlib to avoid the need to define the repo as a package."""
    import importlib

    examples_root = Path(__file__).parent.parent.parent
    spec = importlib.util.spec_from_file_location(
        "utils", f"{examples_root}/internal/utils.py"
    )
    example_utils = importlib.util.module_from_spec(spec)
    spec.loader.exec_module(example_utils)
    examples = [
        example
        for example in example_utils.get_examples()
        if example.type != 2  # filter out non-code assets
    ]
    return examples
```

[Structured Data Extraction using instructor](#structured-data-extraction-using-instructor)

[Environment setup](#environment-setup)

[Running Modal functions from the command line](#running-modal-functions-from-the-command-line)

[Extracting JSON from unstructured text with instructor and Pydantic](#extracting-json-from-unstructured-text-with-instructor-and-pydantic)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-structured/instructor_generate.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/jupyter_sandbox
================================================================================

Run a Jupyter notebook in a Modal Sandbox
=========================================

This example demonstrates how to run a Jupyter notebook in a Modal
[Sandbox](https://modal.com/docs/guide/sandbox)

.

Setting up the Sandbox
----------------------

All Sandboxes are associated with an App.

We look up our app by name, creating it if it doesn’t exist.

```
import json
import secrets
import time
import urllib.request

import modal

app = modal.App.lookup("example-jupyter", create_if_missing=True)
```

We define a custom Docker image that has Jupyter and some other dependencies installed.
Using a pre-defined image allows us to avoid re-installing packages on every Sandbox startup.

```
image = (
    modal.Image.debian_slim(python_version="3.12").pip_install("jupyter~=1.1.0")
    # .pip_install("pandas", "numpy", "seaborn")  # Any other deps
)
```

Starting a Jupyter server in a Sandbox
--------------------------------------

Since we’ll be exposing a Jupyter server over the Internet, we need to create a password.
We’ll use
`secrets`
from the standard library to create a token
and then store it in a Modal
[Secret](https://modal.com/docs/guide/secrets)

.

```
token = secrets.token_urlsafe(13)
token_secret = modal.Secret.from_dict({"JUPYTER_TOKEN": token})
```

Now, we can start our Sandbox. Note our use of the
`encrypted_ports`
argument, which
allows us to securely expose the Jupyter server to the public Internet. We use
`modal.enable_output()`
to print the Sandbox’s image build logs to the console.

```
JUPYTER_PORT = 8888

print("🏖️  Creating sandbox")

with modal.enable_output():
    sandbox = modal.Sandbox.create(
        "jupyter",
        "notebook",
        "--no-browser",
        "--allow-root",
        "--ip=0.0.0.0",
        f"--port={JUPYTER_PORT}",
        "--NotebookApp.allow_origin='*'",
        "--NotebookApp.allow_remote_access=1",
        encrypted_ports=[JUPYTER_PORT],
        secrets=[token_secret],
        timeout=5 * 60,  # 5 minutes
        image=image,
        app=app,
        gpu=None,  # add a GPU if you need it!
    )

print(f"🏖️  Sandbox ID: {sandbox.object_id}")
```

Communicating with a Jupyter server
-----------------------------------

Next, we print out a URL that we can use to connect to our Jupyter server.
Note that we have to call
[`Sandbox.tunnels`](https://modal.com/docs/reference/modal.Sandbox#tunnels)

to get the URL. The Sandbox is not publicly accessible until we do so.

```
tunnel = sandbox.tunnels()[JUPYTER_PORT]
url = f"{tunnel.url}/?token={token}"
print(f"🏖️  Jupyter notebook is running at: {url}")
```

Jupyter servers expose a
[REST API](https://jupyter-server.readthedocs.io/en/latest/developers/rest-api.html)

that you can use for programmatic manipulation.

For example, we can check the server’s status by
sending a GET request to the
`/api/status`
endpoint.

```
def is_jupyter_up():
    try:
        response = urllib.request.urlopen(f"{tunnel.url}/api/status?token={token}")
        if response.getcode() == 200:
            data = json.loads(response.read().decode())
            return data.get("started", False)
    except Exception:
        return False
    return False
```

We’ll now wait for the Jupyter server to be ready by hitting that endpoint.

```
timeout = 60  # seconds
start_time = time.time()
while time.time() - start_time < timeout:
    if is_jupyter_up():
        print("🏖️  Jupyter is up and running!")
        break
    time.sleep(1)
else:
    print("🏖️  Timed out waiting for Jupyter to start.")
```

You can now open this URL in your browser to access the Jupyter notebook!

When you’re done, terminate the sandbox using your
[Modal dashboard](https://modal.com/sandboxes)

or by running
`Sandbox.from_id(sandbox.object_id).terminate()`
.

[Run a Jupyter notebook in a Modal Sandbox](#run-a-jupyter-notebook-in-a-modal-sandbox)

[Setting up the Sandbox](#setting-up-the-sandbox)

[Starting a Jupyter server in a Sandbox](#starting-a-jupyter-server-in-a-sandbox)

[Communicating with a Jupyter server](#communicating-with-a-jupyter-server)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
python 13_sandboxes/jupyter_sandbox.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/llama_cpp
================================================================================

Run large and small language models with llama.cpp (DeepSeek-R1, Phi-4)
=======================================================================

This example demonstrate how to run small (Phi-4) and large (DeepSeek-R1)
language models on Modal with
[`llama.cpp`](https://github.com/ggerganov/llama.cpp)

.

By default, this example uses DeepSeek-R1 to produce a “Flappy Bird” game in Python —
see the video below. The code used in the video is
[here](https://gist.github.com/charlesfrye/a3788c61019c32cb7947f4f5b1c04818)

,
along with the model’s raw outputs.
Note that getting the game to run required a small bugfix from a human —
our jobs are still safe, for now.

[[

](https://modal-cdn.com/example-flap-py.mp4)](https://gist.github.com/charlesfrye/a3788c61019c32cb7947f4f5b1c04818)

```
from pathlib import Path
from typing import Optional

import modal
```

What GPU can run DeepSeek-R1? What GPU can run Phi-4?
-----------------------------------------------------

Our large model is a real whale:
[DeepSeek-R1](https://api-docs.deepseek.com/news/news250120)

,
which has 671B total parameters and so consumes over 100GB of storage,
even when
[quantized down to one ternary digit (1.58 bits)](https://unsloth.ai/blog/deepseekr1-dynamic)

per parameter.

To make sure we have enough room for it and its activations/KV cache,
we select four L40S GPUs, which together have 192 GB of memory.

[Phi-4](https://huggingface.co/microsoft/phi-4)

,
on the other hand, is a svelte 14B total parameters,
or roughly 5 GB when quantized down to
[two bits per parameter](https://huggingface.co/unsloth/phi-4-GGUF)

.

That’s small enough that it can be comfortably run on a CPU,
especially for a single-user setup like the one we’ll build here.

```
GPU_CONFIG = "L40S:4"  # for DeepSeek-R1, literal `None` for phi-4
```

Calling a Modal Function from the command line
----------------------------------------------

To start, we define our
`main`
function —
the Python function that we’ll run locally to
trigger our inference to run on Modal’s cloud infrastructure.

This function, like the others that form our inference service
running on Modal, is part of a Modal
[App](https://modal.com/docs/guide/apps)

.
Specifically, it is a
`local_entrypoint`
.
Any Python code can call Modal Functions remotely,
but local entrypoints get a command-line interface for free.

```
app = modal.App("example-llama-cpp")

@app.local_entrypoint()
def main(
    prompt: Optional[str] = None,
    model: str = "DeepSeek-R1",  # or "phi-4"
    n_predict: int = -1,  # max number of tokens to predict, -1 is infinite
    args: Optional[str] = None,  # string of arguments to pass to llama.cpp's cli
):
    """Run llama.cpp inference on Modal for phi-4 or deepseek r1."""
    import shlex

    org_name = "unsloth"
    # two sample models: the diminuitive phi-4 and the chonky deepseek r1
    if model.lower() == "phi-4":
        model_name = "phi-4-GGUF"
        quant = "Q2_K"
        model_entrypoint_file = f"phi-4-{quant}.gguf"
        model_pattern = f"*{quant}*"
        revision = None
        parsed_args = DEFAULT_PHI_ARGS if args is None else shlex.split(args)
    elif model.lower() == "deepseek-r1":
        model_name = "DeepSeek-R1-GGUF"
        quant = "UD-IQ1_S"
        model_entrypoint_file = (
            f"{model}-{quant}/DeepSeek-R1-{quant}-00001-of-00003.gguf"
        )
        model_pattern = f"*{quant}*"
        revision = "02656f62d2aa9da4d3f0cdb34c341d30dd87c3b6"
        parsed_args = DEFAULT_DEEPSEEK_R1_ARGS if args is None else shlex.split(args)
    else:
        raise ValueError(f"Unknown model {model}")

    repo_id = f"{org_name}/{model_name}"
    download_model.remote(repo_id, [model_pattern], revision)

    # call out to a `.remote` Function on Modal for inference
    result = llama_cpp_inference.remote(
        model_entrypoint_file,
        prompt,
        n_predict,
        parsed_args,
        store_output=model.lower() == "deepseek-r1",
    )
    output_path = Path("/tmp") / f"llama-cpp-{model}.txt"
    output_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"🦙 writing response to {output_path}")
    output_path.write_text(result)
```

You can trigger inference from the command line with

```
modal run llama_cpp.py
```

To try out Phi-4 instead, use the
`--model`
argument:

```
modal run llama_cpp.py --model="phi-4"
```

Note that this will run for up to 30 minutes, which costs ~$5.
To allow it to proceed even if your local terminal fails,
add the
`--detach`
flag after
`modal run`
.
See below for details on getting the outputs.

You can pass prompts with the
`--prompt`
argument and set the maximum number of tokens
with the
`--n-predict`
argument.

Additional arguments for
`llama-cli`
are passed as a string like
`--args="--foo 1 --bar"`
.

For convenience, we set a number of sensible defaults for DeepSeek-R1,
following the suggestions by the team at unsloth,
who
[quantized the model to 1.58 bit](https://unsloth.ai/blog/deepseekr1-dynamic)

.

```
DEFAULT_DEEPSEEK_R1_ARGS = [  # good default llama.cpp cli args for deepseek-r1
    "--cache-type-k",
    "q4_0",
    "--threads",
    "12",
    "-no-cnv",
    "--prio",
    "2",
    "--temp",
    "0.6",
    "--ctx-size",
    "8192",
]

DEFAULT_PHI_ARGS = [  # good default llama.cpp cli args for phi-4
    "--threads",
    "16",
    "-no-cnv",
    "--ctx-size",
    "16384",
]
```

Compiling llama.cpp with CUDA support
-------------------------------------

In order to run inference, we need the model’s weights
and we need code to run inference with those weights.

[`llama.cpp`](https://github.com/ggerganov/llama.cpp)

is a no-frills C++ library for running large language models.
It supports highly-quantized versions of models ideal for running
single-user language modeling services on CPU or GPU.

We compile it, with CUDA support, and add it to a Modal
[container image](https://modal.com/docs/guide/images)

using the code below.

For more details on using CUDA on Modal, including why
we need to use the
`nvidia/cuda`
registry image in this case
(hint: it’s for the
[`nvcc`
compiler](https://modal.com/gpu-glossary/host-software/nvcc)

),
see the
[Modal guide to using CUDA](https://modal.com/docs/guide/cuda)

.

```
LLAMA_CPP_RELEASE = "b4568"
MINUTES = 60

cuda_version = "12.4.0"  # should be no greater than host CUDA version
flavor = "devel"  #  includes full CUDA toolkit
operating_sys = "ubuntu22.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"

image = (
    modal.Image.from_registry(f"nvidia/cuda:{tag}", add_python="3.12")
    .apt_install("git", "build-essential", "cmake", "curl", "libcurl4-openssl-dev")
    .run_commands("git clone https://github.com/ggerganov/llama.cpp")
    .run_commands(
        "cmake llama.cpp -B llama.cpp/build "
        "-DBUILD_SHARED_LIBS=OFF -DGGML_CUDA=ON -DLLAMA_CURL=ON "
    )
    .run_commands(  # this one takes a few minutes!
        "cmake --build llama.cpp/build --config Release -j --clean-first --target llama-quantize llama-cli"
    )
    .run_commands("cp llama.cpp/build/bin/llama-* llama.cpp")
    .entrypoint([])  # remove NVIDIA base container entrypoint
)
```

Storing models on Modal
-----------------------

To make the model weights available on Modal,
we download them from Hugging Face.

Modal is serverless, so disks are by default ephemeral.
To make sure our weights don’t disappear between runs,
which would trigger a long download, we store them in a
Modal
[Volume](https://modal.com/docs/guide/volumes)

.

For more on how to use Modal Volumes to store model weights,
see
[this guide](https://modal.com/docs/guide/model-weights)

.

```
model_cache = modal.Volume.from_name("llamacpp-cache", create_if_missing=True)
cache_dir = "/root/.cache/llama.cpp"

download_image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install("huggingface_hub[hf_transfer]==0.26.2")
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

@app.function(
    image=download_image, volumes={cache_dir: model_cache}, timeout=30 * MINUTES
)
def download_model(repo_id, allow_patterns, revision: Optional[str] = None):
    from huggingface_hub import snapshot_download

    print(f"🦙 downloading model from {repo_id} if not present")

    snapshot_download(
        repo_id=repo_id,
        revision=revision,
        local_dir=cache_dir,
        allow_patterns=allow_patterns,
    )

    model_cache.commit()  # ensure other Modal Functions can see our writes before we quit

    print("🦙 model loaded")
```

Storing model outputs on Modal
------------------------------

Contemporary large reasoning models are slow —
for the sample “flappy bird” prompt we provide,
results are sometimes produced only after several (or even tens of) minutes.

That makes their outputs worth storing.
In addition to sending them back to clients,
like our local command line,
we’ll store the results on a Modal Volume for safe-keeping.

```
results = modal.Volume.from_name("llamacpp-results", create_if_missing=True)
results_dir = "/root/results"
```

You can retrieve the results later in a number of ways.

You can use the Volume CLI:

```
modal volume ls llamacpp-results
```

You can attach the Volume to a Modal
`shell`
to poke around in a familiar terminal environment:

```
modal shell --volume llamacpp-results
# then cd into /mnt
```

Or you can access it from any other Python environment
by using the same
`modal.Volume`
call as above to instantiate it:

```
results = modal.Volume.from_name("llamacpp-results")
print(dir(results))  # show methods
```

Running llama.cpp as a Modal Function
-------------------------------------

Now, let’s put it all together.

At the top of our
`llama_cpp_inference`
function,
we add an
`app.function`
decorator to attach all of our infrastructure:

* the
  `image`
  with the dependencies
* the
  `volumes`
  with the weights and where we can put outputs
* the
  `gpu`
  we want, if any

We also specify a
`timeout`
after which to cancel the run.

Inside the function, we call the
`llama.cpp`
CLI
with
`subprocess.Popen`
. This requires a bit of extra ceremony
because we want to both show the output as we run
and store the output to save and return to the local caller.
For details, see the
[Addenda section](#addenda)

below.

Alternatively, you might set up an OpenAI-compatible server
using base
`llama.cpp`
or its
[Python wrapper library](https://github.com/abetlen/llama-cpp-python)

along with one of
[Modal’s decorators for web hosting](https://modal.com/docs/guide/webhooks)

.

```
@app.function(
    image=image,
    volumes={cache_dir: model_cache, results_dir: results},
    gpu=GPU_CONFIG,
    timeout=30 * MINUTES,
)
def llama_cpp_inference(
    model_entrypoint_file: str,
    prompt: Optional[str] = None,
    n_predict: int = -1,
    args: Optional[list[str]] = None,
    store_output: bool = True,
):
    import subprocess
    from uuid import uuid4

    if prompt is None:
        prompt = DEFAULT_PROMPT  # see end of file
    if "deepseek" in model_entrypoint_file.lower():
        prompt = "<｜User｜>" + prompt + "<think>"
    if args is None:
        args = []

    # set layers to "off-load to", aka run on, GPU
    if GPU_CONFIG is not None:
        n_gpu_layers = 9999  # all
    else:
        n_gpu_layers = 0

    if store_output:
        result_id = str(uuid4())
        print(f"🦙 running inference with id:{result_id}")

    command = [
        "/llama.cpp/llama-cli",
        "--model",
        f"{cache_dir}/{model_entrypoint_file}",
        "--n-gpu-layers",
        str(n_gpu_layers),
        "--prompt",
        prompt,
        "--n-predict",
        str(n_predict),
    ] + args

    print("🦙 running commmand:", command, sep="\n\t")
    p = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=False
    )

    stdout, stderr = collect_output(p)

    if p.returncode != 0:
        raise subprocess.CalledProcessError(p.returncode, command, stdout, stderr)

    if store_output:  # save results to a Modal Volume if requested
        print(f"🦙 saving results for {result_id}")
        result_dir = Path(results_dir) / result_id
        result_dir.mkdir(
            parents=True,
        )
        (result_dir / "out.txt").write_text(stdout)
        (result_dir / "err.txt").write_text(stderr)

    return stdout
```

Addenda
=======

The remainder of this code is less interesting from the perspective
of running LLM inference on Modal but necessary for the code to run.

For example, it includes the default “Flappy Bird in Python” prompt included in
[unsloth’s announcement](https://unsloth.ai/blog/deepseekr1-dynamic)

of their 1.58 bit quantization of DeepSeek-R1.

```
DEFAULT_PROMPT = """Create a Flappy Bird game in Python. You must include these things:

    You must use pygame.
    The background color should be randomly chosen and is a light shade. Start with a light blue color.
    Pressing SPACE multiple times will accelerate the bird.
    The bird's shape should be randomly chosen as a square, circle or triangle. The color should be randomly chosen as a dark color.
    Place on the bottom some land colored as dark brown or yellow chosen randomly.
    Make a score shown on the top right side. Increment if you pass pipes and don't hit them.
    Make randomly spaced pipes with enough space. Color them randomly as dark green or light brown or a dark gray shade.
    When you lose, show the best score. Make the text inside the screen. Pressing q or Esc will quit the game. Restarting is pressing SPACE again.

The final game should be inside a markdown section in Python. Check your code for errors and fix them before the final markdown section."""

def stream_output(stream, queue, write_stream):
    """Reads lines from a stream and writes to a queue and a write stream."""
    for line in iter(stream.readline, b""):
        line = line.decode("utf-8", errors="replace")
        write_stream.write(line)
        write_stream.flush()
        queue.put(line)
    stream.close()

def collect_output(process):
    """Collect up the stdout and stderr of a process while still streaming it out."""
    import sys
    from queue import Queue
    from threading import Thread

    stdout_queue = Queue()
    stderr_queue = Queue()

    stdout_thread = Thread(
        target=stream_output, args=(process.stdout, stdout_queue, sys.stdout)
    )
    stderr_thread = Thread(
        target=stream_output, args=(process.stderr, stderr_queue, sys.stderr)
    )
    stdout_thread.start()
    stderr_thread.start()

    stdout_thread.join()
    stderr_thread.join()
    process.wait()

    stdout_collected = "".join(stdout_queue.queue)
    stderr_collected = "".join(stderr_queue.queue)

    return stdout_collected, stderr_collected
```

[Run large and small language models with llama.cpp (DeepSeek-R1, Phi-4)](#run-large-and-small-language-models-with-llamacpp-deepseek-r1-phi-4)

[What GPU can run DeepSeek-R1? What GPU can run Phi-4?](#what-gpu-can-run-deepseek-r1-what-gpu-can-run-phi-4)

[Calling a Modal Function from the command line](#calling-a-modal-function-from-the-command-line)

[Compiling llama.cpp with CUDA support](#compiling-llamacpp-with-cuda-support)

[Storing models on Modal](#storing-models-on-modal)

[Storing model outputs on Modal](#storing-model-outputs-on-modal)

[Running llama.cpp as a Modal Function](#running-llamacpp-as-a-modal-function)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-serving/llama_cpp.py --n-predict 1024
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/llm-finetuning
================================================================================

DoppelBot: Fine-tune an LLM to replace your CEO
===============================================

*(quick links:
[add to your own Slack](https://github.com/modal-labs/doppel-bot#usage)

;
[source code](https://github.com/modal-labs/doppel-bot)

)*

Internally at Modal, we spend a
*lot*
of time talking to each other on Slack.
Now, with the advent of open-source large language models, we had started to
wonder if all of this wasn’t a bit redundant. Could we have these language
models bike-shed on Slack for us, so we could spend our time on higher leverage
activities such as
[paddleboarding in Tahiti](https://twitter.com/modal_labs/status/1642262543757352960)

instead?

To test this, we fine-tuned
[Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)

on
[Erik](https://twitter.com/bernhardsson)

’s Slack messages, and
`@erik-bot`
was
born.

![erik-bot](https://modal-cdn.com/erik-bot-1.jpeg)

Since then,
`@erik-bot`
has been an invaluable asset to us, in areas ranging
from
[API design](https://modal-cdn.com/erik-bot-2.png)

to
[legal advice](https://modal-cdn.com/erik-bot-3.png)

to thought leadership.

![erik-bot-3](https://modal-cdn.com/erik-bot-4.png)

We were planning on releasing the weights for
`@erik-bot`
to the world, but all
our metrics have been going up and to the right a little too much since we’ve
launched him…

So, we are releasing the next best thing.
`DoppelBot`
is a Slack bot that you
can install in your own workspace, and fine-tune on your own Slack messages.
Follow the instructions
[here](https://github.com/modal-labs/doppel-bot#usage)

to replace your own CEO with an LLM today.

All the components—scraping, fine-tuning, inference and slack event handlers run
on Modal, and the code itself is open-source and available
[here](https://github.com/modal-labs/doppel-bot)

. If you’re new to Modal, it’s
worth reiterating that
**all of these components are also serverless and scale
to zero**
. This means that you can deploy and forget about them, because you’ll
only pay for compute when your app is used!

How it works
------------

DoppelBot uses the Slack SDK to scrape messages from a Slack workspace, and
converts them into prompt/response pairs. It uses these to fine-tune a language
model using
[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685)

, a
technique that produces a small adapter that can be merged with the base model
when needed, instead of modifying all the parameters in the base model. The
fine-tuned adapters for each user are stored in a Modal
[Volume](/docs/guide/volumes)

. When a user
`@`
s the bot,
Slack sends a webhook call to Modal, which loads the adapter for that user and
generates a response.

We go into detail into each of these steps below, and provide commands for
running each of them individually. To follow along,
[clone the repo](https://github.com/modal-labs/doppel-bot)

and
[set up a Slack token](https://github.com/modal-labs/doppel-bot#create-a-slack-app)

for yourself.

### Scraping slack

The scraper uses Modal’s
[`.map()`](/docs/guide/scale#scaling-out)

to fetch
messages from all public channels in parallel. Each thread is split into
contiguous messages from the target users and continguous messages from other
users. These will be fed into the model as prompts in the following format:

```
[system]: You are {user}, employee at a fast-growing startup. Below is an input conversation that takes place in the company's internal Slack. Write a response that appropriately continues the conversation.

[user]: <slack thread>

[assistant]: <target user's response>
```

Initial versions of the model were prone to generating short responses
— unsurprising, because a majority of Slack communication is pretty terse.
Adding a minimum character length for the target user’s messages fixed this.

If you’re following along at home, you can run the scraper with the following
command:

```
modal run -m src.scrape::scrape --user="<user>"
```

Scraped results are stored in a Modal
[Volume](/docs/guide/volumes)

, so they can be used by the next step.

### Fine-tuning

Next, we use the prompts to fine-tune a language model. We chose
[Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)

because of its permissive license and high quality relative to its small size. Fine-tuning is
done using
[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685)

, a
[parameter-efficient fine-tuning](https://huggingface.co/blog/peft)

technique
that produces a small adapter that can be merged with the base model when needed
(~60MB for the rank we’re using).

Our fine-tuning implementation uses
[torchtune](https://github.com/pytorch/torchtune)

, a new PyTorch library for easily configuring fine-tuning runs.

Because of the typically small sample sizes we’re working with, training for
longer than a couple hundred steps (with our batch size of 128) quickly led to
overfitting. Admittedly, we haven’t thoroughly evaluated the hyperparameter
space yet — do reach out to us if you’re interested in collaborating on this!

![train-loss](/_app/immutable/assets/train-loss.DFD7oOI8.png)

To try this step yourself, run:

```
modal run -m src.finetune --user="<user>"
```

### Inference

We use
[vLLM](https://github.com/vllm-project/vllm)

as our inference engine, which now comes with support for dynamically swapping LoRA adapters
[out of the box](https://docs.vllm.ai/en/latest/features/lora.html)

.

With parametrized functions, every user model gets its own pool of containers
that scales up when there are incoming requests, and scales to 0 when there’s
none. Here’s what that looks like stripped down to the essentials:

```
@app.cls(gpu="L40S")
class Model():
    @modal.enter()
    def enter(self):
        self.engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(...))
        self.loras: dict[str, int] = dict()  # per replica LoRA identifier

    @method()
    def generate(self, input: str):
        if (ident := f"{user}-{team_id}") not in self.loras:
            self.loras[ident] = len(self.loras) + 1

        lora_request = LoRARequest(
            ident, self.loras[ident], lora_local_path=checkpoint_path
        )

        tokenizer = await self.engine.get_tokenizer(lora_request=lora_request)

        prompt = tokenizer.apply_chat_template(
            conversation=inpt, tokenize=False, add_generation_prompt=True
        )

        results_generator = self.engine.generate(prompt, lora_request=lora_request,)
```

If you’ve fine-tuned a model already in the previous step, you can run inference
using it now:

```
modal run -m src.inference --user="<user>"
```

(We have a list of sample inputs in the file, but you can also try it out with
your own messages!)

### Slack Bot

Finally, it all comes together in
[`bot.py`](https://github.com/modal-labs/doppel-bot/blob/main/src/bot.py)

. As
you might have guessed, all events from Slack are handled by serverless Modal
functions. We handle 3 types of events:

* [`url_verification`](https://github.com/modal-labs/doppel-bot/blob/24609583c43c0e722f56f85a1c00bb55b46c7754/src/bot.py#L112)

  :
  To verify that this is a Slack app, Slack expects us to return a challenge
  string.
* [`app_mention`](https://github.com/modal-labs/doppel-bot/blob/main/src/bot.py#L118)

  :
  When the bot is mentioned in a channel, we retrieve the recent messages from
  that thread, do some basic cleaning and call the user’s model to generate a
  response.

```
model = OpenLlamaModel.remote(user, team_id)
result = model.generate(messages)
```

* [`doppel`
  slash command](https://github.com/modal-labs/doppel-bot/blob/main/src/bot.py#L182)

  :
  This command kicks off the scraping -> finetuning pipeline for the user.

To deploy the slackbot in its entirety, you need to run:

```
modal deploy -m src.bot
```

### Multi-Workspace Support

Everything we’ve talked about so far is for a single-workspace Slack app. To
make it work with multiple workspaces, we’ll need to handle
[workspace installation and authentication with OAuth](https://api.slack.com/authentication/oauth-v2)

,
and also store some state for each workspace.

Luckily, Slack’s
[Bolt](https://slack.dev/bolt-python/concepts)

framework
provides a complete (but frugally documented) OAuth implemention. A neat feature
is that the OAuth state can be backed by a file system, so all we need to do is
[point Bolt](https://github.com/modal-labs/doppel-bot/blob/24609583c43c0e722f56f85a1c00bb55b46c7754/src/bot.py#L78)

at a Modal
[Volume](/docs/guide/volumes)

, and then we don’t need to worry about
managing this state ourselves.

To store state for each workspace, we’re using
[Neon](https://neon.tech/)

, a
serverless Postgres database that’s really easy to set up and
*just works*
. If
you’re interested in developing a multi-workspace app,
[follow our instructions](https://github.com/modal-labs/doppel-bot#optional-multi-workspace-app)

on how to set up Neon with Modal.

Next Steps
----------

If you’ve made it this far, you have just found a way to increase your team’s
productivity by 10x! Congratulations on the well-earned vacation! 🎉

If you’re interested in learning more about Modal, check out our
[docs](/docs)

and other
[examples](/examples)

.

[DoppelBot: Fine-tune an LLM to replace your CEO](#doppelbot-fine-tune-an-llm-to-replace-your-ceo)

[How it works](#how-it-works)

[Scraping slack](#scraping-slack)

[Fine-tuning](#fine-tuning)

[Inference](#inference)

[Slack Bot](#slack-bot)

[Multi-Workspace Support](#multi-workspace-support)

[Next Steps](#next-steps)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/llm-voice-chat
================================================================================

QuiLLMan: Voice Chat with Moshi
===============================

[QuiLLMan](https://github.com/modal-labs/quillman)

is a complete voice chat application built on Modal: you speak and the chatbot speaks back!

At the core is Kyutai Lab’s
[Moshi](https://github.com/kyutai-labs/moshi)

model, a speech-to-speech language model that will continuously listen, plan, and respond to the user.

Thanks to bidirectional websocket streaming and
[Opus audio compression](https://opus-codec.org/)

, response times on good internet can be nearly instantaneous, closely matching the cadence of human speech.

You can find the demo live
[here](https://modal-labs--quillman-web.modal.run/)

.

![Quillman](https://github.com/user-attachments/assets/afda5874-8509-4f56-9f25-d734b8f1c40a)

Everything — from the React frontend to the model backend — is deployed serverlessly on Modal, allowing it to automatically scale and ensuring you only pay for the compute you use.

This page provides a high-level walkthrough of the
[GitHub repo](https://github.com/modal-labs/quillman)

.

Code overview
-------------

Traditionally, building a bidirectional streaming web application as compute-heavy as QuiLLMan would take a lot of work, and it’s especially difficult to make it robust and scale to handle many concurrent users.

But with Modal, it’s as simple as writing two different classes and running a CLI command.

Our project structure looks like this:

1. [Moshi Websocket Server](https://modal.com/docs/examples/llm-voice-chat#moshi-websocket-server)

   : loads an instance of the Moshi model and maintains a bidirectional websocket connection with the client.
2. [React Frontend](https://modal.com/docs/examples/llm-voice-chat#react-frontend)

   : runs client-side interaction logic.

Let’s go through each of these components in more detail.

### FastAPI Server

Both frontend and backend are served via a
[FastAPI Server](https://fastapi.tiangolo.com/)

, which is a popular Python web framework for building REST APIs.

On Modal, a function or class method can be exposed as a web endpoint by decorating it with
[`@app.asgi_app()`](https://modal.com/docs/reference/modal.asgi_app#modalasgi_app)

and returning a FastAPI app. You’re then free to configure the FastAPI server however you like, including adding middleware, serving static files, and running websockets.

### Moshi Websocket Server

Traditionally, a speech-to-speech chat app requires three distinct modules: speech-to-text, text-to-text, and text-to-speech. Passing data between these modules introduces bottlenecks, and can limit the speed of the app and forces a turn-by-turn conversation which can feel unnatural.

Kyutai Lab’s
[Moshi](https://github.com/kyutai-labs/moshi)

bundles all modalities into one model, which decreases latency and makes for a much simpler app.

Under the hood, Moshi uses the
[Mimi](https://huggingface.co/kyutai/mimi)

streaming encoder/decoder model to maintain an unbroken stream of audio in and out. The encoded audio is processed by a
[speech-text foundation model](https://huggingface.co/kyutai/moshiko-pytorch-bf16)

, which uses an internal monologue to determine when and how to respond.

Using a streaming model introduces a few challenges not normally seen in inference backends:

1. The model is
   *stateful*
   , meaning it maintains context of the conversation so far. This means a model instance cannot be shared between user conversations, so we must run a unique GPU per user session, which is normally not an easy feat!
2. The model is
   *streaming*
   , so the interface around it is not as simple as a POST request. We must find a way to stream audio data in and out, and do it fast enough for seamless playback.

We solve both of these in
`src/moshi.py`
, using a few Modal features.

To solve statefulness, we just spin up a new GPU per concurrent user.
That’s easy with Modal!

```
@app.cls(
    image=image,
    gpu="A10G",
    scaledown_window=300,
    ...
)
class Moshi:
    # ...
```

With this setting, if a new user connects, a new GPU instance is created! When any user disconnects, the state of their model is reset and that GPU instance is returned to the warm pool for re-use (for up to 300 seconds). Be aware that a GPU per user is not going to be cheap, but it’s the simplest way to ensure user sessions are isolated.

For streaming, we use FastAPI’s support for bidirectional websockets. This allows clients to establish a single connection at the start of their session, and stream audio data both ways.

Just as a FastAPI server can run from a Modal function, it can also be attached to a Modal class method, allowing us to couple a prewarmed Moshi model to a websocket session.

```
@modal.asgi_app()
def web(self):
    from fastapi import FastAPI, Response, WebSocket, WebSocketDisconnect

    web_app = FastAPI()
    @web_app.websocket("/ws")
    async def websocket(ws: WebSocket):
        with torch.no_grad():
            await ws.accept()

            # handle user session

            # spawn loops for async IO
            async def recv_loop():
                while True:
                    data = await ws.receive_bytes()
                    # send data into inference stream...

            async def send_loop():
                while True:
                    await asyncio.sleep(0.001)
                    msg = self.opus_stream_outbound.read_bytes()
                    # send inference output to user ...
```

To run a
[development server](https://modal.com/docs/guide/webhooks#developing-with-modal-serve)

for the Moshi module, run this command from the root of the repo.

```
modal serve -m src.moshi
```

In the terminal output, you’ll find a URL for creating a websocket connection.

### React Frontend

The frontend is a static React app, found in the
`src/frontend`
directory and served by
`src/app.py`
.

We use the
[Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)

to record audio from the user’s microphone and playback audio responses from the model.

For efficient audio transmission, we use the
[Opus codec](https://opus-codec.org/)

to compress audio across the network. Opus recording and playback are supported by the
[`opus-recorder`](https://github.com/chris-rudmin/opus-recorder)

and
[`ogg-opus-decoder`](https://github.com/eshaz/wasm-audio-decoders/tree/master/src/ogg-opus-decoder)

libraries.

To serve the frontend assets, run this command from the root of the repo.

```
modal serve -m src.app
```

Since
`src/app.py`
imports the
`src/moshi.py`
module, this
`serve`
command also serves the Moshi websocket server as its own endpoint.

Deploy
------

When you’re ready to go live, use the
`deploy`
command to deploy the app to Modal.

```
modal deploy -m src.app
```

Steal this example
------------------

The code for this entire example is
[available on GitHub](https://github.com/modal-labs/quillman)

, so feel free to fork it and make it your own!

[QuiLLMan: Voice Chat with Moshi](#quillman-voice-chat-with-moshi)

[Code overview](#code-overview)

[FastAPI Server](#fastapi-server)

[Moshi Websocket Server](#moshi-websocket-server)

[React Frontend](#react-frontend)

[Deploy](#deploy)

[Steal this example](#steal-this-example)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/long-training
================================================================================

Run long, resumable training jobs on Modal
==========================================

Individual Modal Function calls have a
[maximum timeout of 24 hours](https://modal.com/docs/guide/timeouts)

.
You can still run long training jobs on Modal by making them interruptible and resumable
(aka
[*reentrant*](https://en.wikipedia.org/wiki/Reentrancy_%28computing%29)

).

This is usually done via checkpointing: saving the model state to disk at regular intervals.
We recommend implementing checkpointing logic regardless of the duration of your training jobs.
This prevents loss of progress in case of interruptions or
[preemptions](https://modal.com/docs/guide/preemption)

.

In this example, we’ll walk through how to implement this pattern in
[PyTorch Lightning](https://lightning.ai/docs/pytorch/2.4.0/)

.

But the fundamental pattern is simple and can be applied to any training framework:

1. Periodically save checkpoints to a Modal
   [Volume](https://modal.com/docs/guide/volumes)
2. When your training function starts, check the Volume for the latest checkpoint
3. Add
   [retries](https://modal.com/docs/guide/retries)

   to your training function

Resuming from checkpoints in a training loop
--------------------------------------------

The
`train`
function below shows some very simple training logic
using the built-in checkpointing features of PyTorch Lightning.

Lightning uses a special filename,
`last.ckpt`
,
to indicate which checkpoint is the most recent.
We check for this file and resume training from it if it exists.

```
from pathlib import Path
from typing import Optional

import modal

def train(experiment):
    experiment_dir = CHECKPOINTS_PATH / experiment
    last_checkpoint = experiment_dir / "last.ckpt"

    if last_checkpoint.exists():
        print(f"⚡️ resuming training from the latest checkpoint: {last_checkpoint}")
        train_model(
            DATA_PATH,
            experiment_dir,
            resume_from_checkpoint=last_checkpoint,
        )
        print("⚡️ training finished successfully")
    else:
        print("⚡️ starting training from scratch")
        train_model(DATA_PATH, experiment_dir)
```

This implementation works fine in a local environment.
Running it serverlessly and durably on Modal — with access to auto-scaling cloud GPU infrastructure
— does not require any adjustments to the code.
We just need to ensure that data and checkpoints are saved in Modal
*Volumes*
.

Modal Volumes are distributed file systems
------------------------------------------

Modal
[Volumes](https://modal.com/docs/guide/volumes)

are distributed file systems —
you can read and write files from them just like local disks,
but they are accessible to all of your Modal Functions.
Their performance is tuned for
[Write-Once, Read-Many](https://en.wikipedia.org/wiki/Write_once_read_many)

workloads
with small numbers of large files.

You can attach them to any Modal Function that needs access.

But first, you need to create them:

```
volume = modal.Volume.from_name("example-long-training", create_if_missing=True)
```

Porting training to Modal
-------------------------

To attach a Modal Volume to our training function, we need to port it over to run on Modal.

That means we need to define our training function’s dependencies
(as a
[container image](https://modal.com/docs/guide/custom-container)

)
and attach it to an application (a
[`modal.App`](https://modal.com/docs/guide/apps)

).

Modal Functions that run on GPUs
[already have CUDA drivers installed](https://modal.com/docs/guide/cuda)

,
so dependency specification is straightforward.
We just
`pip_install`
PyTorch and PyTorch Lightning.

```
image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "lightning~=2.4.0", "torch~=2.4.0", "torchvision==0.19.0"
)

app = modal.App("example-long-training-lightning", image=image)
```

Next, we attach our training function to this app with
`app.function`
.

We define all of the serverless infrastructure-specific details of our training at this point.
For resumable training, there are three key pieces: attaching volumes, adding retries, and setting the timeout.

We want to attach the Volume to our Function so that the data and checkpoints are saved into it.
In this sample code, we set these paths via global variables, but in another setting,
these might be set via environment variables or other configuration mechanisms.

```
volume_path = Path("/experiments")
DATA_PATH = volume_path / "data"
CHECKPOINTS_PATH = volume_path / "checkpoints"

volumes = {volume_path: volume}
```

Then, we define how we want to restart our training in case of interruption.
We can use
`modal.Retries`
to add automatic retries to our Function.
We set the delay time to
`0.0`
seconds, because on pre-emption or timeout we want to restart immediately.
We set
`max_retries`
to the current maximum, which is
`10`
.

```
retries = modal.Retries(initial_delay=0.0, max_retries=10)
```

Timeouts on Modal are set in seconds, with a minimum of 10 seconds and a maximum of 24 hours.
When running training jobs that last up to week, we’d set that timeout to 24 hours,
which would give our training job a maximum of 10 days to complete before we’d need to manually restart.

For this example, we’ll set it to 30 seconds. When running the example, you should observe a few interruptions.

```
timeout = 30  # seconds
```

Now, we put all of this together by wrapping
`train`
and decorating it
with
`app.function`
to add all the infrastructure. We add
`max_inputs=1`
to ensure that our retries
will always kickoff in a fresh container.

```
@app.function(
    volumes=volumes, gpu="a10g", timeout=timeout, retries=retries, max_inputs=1
)
def train_interruptible(*args, **kwargs):
    train(*args, **kwargs)
```

Kicking off interruptible training
----------------------------------

We define a
[`local_entrypoint`](https://modal.com/docs/guide/apps#entrypoints-for-ephemeral-apps)

to kick off the training job from the local Python environment.

```
@app.local_entrypoint()
def main(experiment: Optional[str] = None):
    if experiment is None:
        from uuid import uuid4

        experiment = uuid4().hex[:8]
    print(f"⚡️ starting interruptible training experiment {experiment}")
    train_interruptible.spawn(experiment).get()
```

It’s important to use
`.spawn(...).get()`
because
`.remote`
created Function Calls
expire after 24 hours.

You can run this with

```
modal run --detach 06_gpu_and_ml/long-training.py
```

You should see the training job start and then be interrupted,
producing a large stack trace in the terminal in red font.
The job will restart within a few seconds.

The
`--detach`
flag ensures training will continue even if you close your terminal or turn off your computer.
Try detaching and then watch the logs in the
[Modal dashboard](https://modal.com/apps)

.

Details of PyTorch Lightning implementation
-------------------------------------------

This basic pattern works for any training framework or for custom training jobs —
or for any reentrant work that can save state to disk.

But to make the example complete, we include all the details of the PyTorch Lightning implementation below.

PyTorch Lightning offers
[built-in checkpointing](https://pytorch-lightning.readthedocs.io/en/1.2.10/common/weights_loading.html)

.
You can specify the checkpoint file path that you want to resume from using the
`ckpt_path`
parameter of
[`trainer.fit`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.trainer.trainer.Trainer.html)

Additionally, you can specify the checkpointing interval with the
`every_n_epochs`
parameter of
[`ModelCheckpoint`](https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.ModelCheckpoint.html)

.

```
def get_checkpoint(checkpoint_dir):
    from lightning.pytorch.callbacks import ModelCheckpoint

    return ModelCheckpoint(
        dirpath=checkpoint_dir,
        save_last=True,
        every_n_epochs=10,
        filename="{epoch:02d}",
    )

def train_model(data_dir, checkpoint_dir, resume_from_checkpoint=None):
    import lightning as L

    autoencoder = get_autoencoder()
    train_loader = get_train_loader(data_dir=data_dir)
    checkpoint_callback = get_checkpoint(checkpoint_dir)

    trainer = L.Trainer(
        limit_train_batches=100, max_epochs=100, callbacks=[checkpoint_callback]
    )
    if resume_from_checkpoint is not None:
        trainer.fit(
            model=autoencoder,
            train_dataloaders=train_loader,
            ckpt_path=resume_from_checkpoint,
        )
    else:
        trainer.fit(autoencoder, train_loader)

def get_autoencoder(checkpoint_path=None):
    import lightning as L
    from torch import nn, optim

    class LitAutoEncoder(L.LightningModule):
        def __init__(self):
            super().__init__()
            self.encoder = nn.Sequential(
                nn.Linear(28 * 28, 64), nn.ReLU(), nn.Linear(64, 3)
            )
            self.decoder = nn.Sequential(
                nn.Linear(3, 64), nn.ReLU(), nn.Linear(64, 28 * 28)
            )

        def training_step(self, batch, batch_idx):
            x, _ = batch
            x = x.view(x.size(0), -1)
            z = self.encoder(x)
            x_hat = self.decoder(z)
            loss = nn.functional.mse_loss(x_hat, x)
            self.log("train_loss", loss)
            return loss

        def configure_optimizers(self):
            optimizer = optim.Adam(self.parameters(), lr=1e-3)
            return optimizer

    return LitAutoEncoder()

def get_train_loader(data_dir):
    from torch import utils
    from torchvision.datasets import MNIST
    from torchvision.transforms import ToTensor

    print("⚡ setting up data")
    dataset = MNIST(data_dir, download=True, transform=ToTensor())
    train_loader = utils.data.DataLoader(dataset, num_workers=4)
    return train_loader
```

[Run long, resumable training jobs on Modal](#run-long-resumable-training-jobs-on-modal)

[Resuming from checkpoints in a training loop](#resuming-from-checkpoints-in-a-training-loop)

[Modal Volumes are distributed file systems](#modal-volumes-are-distributed-file-systems)

[Porting training to Modal](#porting-training-to-modal)

[Kicking off interruptible training](#kicking-off-interruptible-training)

[Details of PyTorch Lightning implementation](#details-of-pytorch-lightning-implementation)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run --detach 06_gpu_and_ml/long-training.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/ltx
================================================================================

Generate videos from prompts with Lightricks LTX-Video
======================================================

This example demonstrates how to run the
[LTX-Video](https://github.com/Lightricks/LTX-Video)

video generation model by
[Lightricks](https://www.lightricks.com/)

on Modal.

LTX-Video is fast! Generating a twenty second 480p video at moderate quality
takes as little as two seconds on a warm container.

Here’s one that we generated:

[

](https://modal-cdn.com/blonde-woman-blinking.mp4)

Setup
-----

We start by importing dependencies we need locally,
defining a Modal
[App](https://modal.com/docs/guide/apps)

,
and defining the container
[Image](https://modal.com/docs/guide/images)

that our video model will run in.

```
import string
import time
from pathlib import Path
from typing import Optional

import modal

app = modal.App("example-ltx-video")

image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "accelerate==1.6.0",
        "diffusers==0.33.1",
        "hf_transfer==0.1.9",
        "imageio==2.37.0",
        "imageio-ffmpeg==0.5.1",
        "sentencepiece==0.2.0",
        "torch==2.7.0",
        "transformers==4.51.3",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)
```

Storing data on Modal Volumes
-----------------------------

On Modal, we save large or expensive-to-compute data to
[distributed Volumes](https://modal.com/docs/guide/volumes)

that are accessible both locally and remotely.

We’ll store the LTX-Video model’s weights and the outputs we generate
on Modal Volumes.

We store the outputs on a Modal Volume so that clients
don’t need to sit around waiting for the video to be generated.

```
VOLUME_NAME = "ltx-outputs"
outputs = modal.Volume.from_name(VOLUME_NAME, create_if_missing=True)
OUTPUTS_PATH = Path("/outputs")
```

We store the weights on a Modal Volume so that we don’t
have to fetch them from the Hugging Face Hub every time
a container boots. This download takes about two minutes,
depending on traffic and network speed.

```
MODEL_VOLUME_NAME = "ltx-model"
model = modal.Volume.from_name(MODEL_VOLUME_NAME, create_if_missing=True)
```

We don’t have to change any of the Hugging Face code to do this —
we just set the location of Hugging Face’s cache to be on a Volume
using the
`HF_HOME`
environment variable.

```
MODEL_PATH = Path("/models")
image = image.env({"HF_HOME": str(MODEL_PATH)})
```

For more on storing Modal weights on Modal, see
[this guide](https://modal.com/docs/guide/model-weights)

.

Setting up our LTX class
------------------------

We use the
`@cls`
decorator to specify the infrastructure our inference function needs,
as defined above.

That decorator also gives us control over the
[lifecycle](https://modal.com/docs/guide/lifecycle-functions)

of our cloud container.

Specifically, we use the
`enter`
method to load the model into GPU memory
(from the Volume if it’s present or the Hub if it’s not)
before the container is marked ready for inputs.

This helps reduce tail latencies caused by cold starts.
For details and more tips, see
[this guide](https://modal.com/docs/guide/cold-start#cold-start-performance)

.

The actual inference code is in a
`modal.method`
of the class.

```
MINUTES = 60  # seconds

@app.cls(
    image=image,  # use our container Image
    volumes={OUTPUTS_PATH: outputs, MODEL_PATH: model},  # attach our Volumes
    gpu="H100",  # use a big, fast GPU
    timeout=10 * MINUTES,  # run inference for up to 10 minutes
    scaledown_window=15 * MINUTES,  # stay idle for 15 minutes before scaling down
)
class LTX:
    @modal.enter()
    def load_model(self):
        import torch
        from diffusers import DiffusionPipeline

        self.pipe = DiffusionPipeline.from_pretrained(
            "Lightricks/LTX-Video", torch_dtype=torch.bfloat16
        )
        self.pipe.to("cuda")

    @modal.method()
    def generate(
        self,
        prompt,
        negative_prompt="",
        num_inference_steps=200,
        guidance_scale=4.5,
        num_frames=19,
        width=704,
        height=480,
    ):
        from diffusers.utils import export_to_video

        frames = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            num_frames=num_frames,
            width=width,
            height=height,
        ).frames[0]

        # save to disk using prompt as filename
        mp4_name = slugify(prompt)
        export_to_video(frames, Path(OUTPUTS_PATH) / mp4_name)
        outputs.commit()
        return mp4_name
```

Generate videos from the command line
-------------------------------------

We trigger LTX-Video inference from our local machine by running the code in
the local entrypoint below with
`modal run`
.

It will spin up a new replica to generate a video.
Then it will, by default, generate a second video to demonstrate
the lower latency when hitting a warm container.

You can trigger inference with:

```
modal run ltx
```

All outputs are saved both locally and on a Modal Volume.
You can explore the contents of Modal Volumes from your Modal Dashboard
or from the command line with the
`modal volume`
command.

```
modal volume ls ltx-outputs
```

See
`modal volume --help`
for details.

Optional command line flags for the script can be viewed with:

```
modal run ltx --help
```

Using these flags, you can tweak your generation from the command line:

```
modal run --detach ltx --prompt="a cat playing drums in a jazz ensemble" --num-inference-steps=64
```

```
@app.local_entrypoint()
def main(
    prompt: Optional[str] = None,
    negative_prompt="worst quality, blurry, jittery, distorted",
    num_inference_steps: int = 10,  # 10 when testing, 100 or more when generating
    guidance_scale: float = 2.5,
    num_frames: int = 150,  # produces ~10s of video
    width: int = 704,
    height: int = 480,
    twice: bool = True,  # run twice to show cold start latency
):
    if prompt is None:
        prompt = DEFAULT_PROMPT

    ltx = LTX()

    def run():
        print(f"🎥 Generating a video from the prompt '{prompt}'")
        start = time.time()
        mp4_name = ltx.generate.remote(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            num_frames=num_frames,
            width=width,
            height=height,
        )
        duration = time.time() - start
        print(f"🎥 Client received video in {int(duration)}s")
        print(f"🎥 LTX video saved to Modal Volume at {mp4_name}")

        local_dir = Path("/tmp/ltx")
        local_dir.mkdir(exist_ok=True, parents=True)
        local_path = local_dir / mp4_name
        local_path.write_bytes(b"".join(outputs.read_file(mp4_name)))
        print(f"🎥 LTX video saved locally at {local_path}")

    run()

    if twice:
        print("🎥 Generating a video from a warm container")
        run()
```

Addenda
-------

The remainder of the code in this file is utility code.

```
DEFAULT_PROMPT = (
    "The camera pans over a snow-covered mountain range,"
    " revealing a vast expanse of snow-capped peaks and valleys."
    " The mountains are covered in a thick layer of snow,"
    " with some areas appearing almost white while others have a slightly darker, almost grayish hue."
    " The peaks are jagged and irregular, with some rising sharply into the sky"
    " while others are more rounded."
    " The valleys are deep and narrow, with steep slopes that are also covered in snow."
    " The trees in the foreground are mostly bare, with only a few leaves remaining on their branches."
)

def slugify(prompt):
    for char in string.punctuation:
        prompt = prompt.replace(char, "")
    prompt = prompt.replace(" ", "_")
    prompt = prompt[:230]  # some OSes limit filenames to <256 chars
    mp4_name = str(int(time.time())) + "_" + prompt + ".mp4"
    return mp4_name
```

[Generate videos from prompts with Lightricks LTX-Video](#generate-videos-from-prompts-with-lightricks-ltx-video)

[Setup](#setup)

[Storing data on Modal Volumes](#storing-data-on-modal-volumes)

[Setting up our LTX class](#setting-up-our-ltx-class)

[Generate videos from the command line](#generate-videos-from-the-command-line)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/text-to-video/ltx.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/miscellaneous
================================================================================

Miscellaneous examples
======================

Looking for how to make a popular model or library work with Modal?
There’s a guide for that:

* [Deploy ControlNet demos with Gradio](/docs/examples/controlnet_gradio_demos)
* [FastHTML: Deploy 100,000 multiplayer checkboxes](/docs/examples/fasthtml_checkboxes)
* [Fine-tune Flan-T5 and monitor with TensorBoard](/docs/examples/flan_t5_finetune)
* [Generate video clips with Mochi](/docs/examples/mochi)
* [MultiOn: Create an agent for AI news](/docs/examples/multion_news_agent)
* [Profile PyTorch code](/docs/examples/torch_profiling)
* [Real-time object detection with webcam input](/docs/examples/webcam)
* [Run batched Whisper transcription](/docs/examples/batched_whisper)
* [Run continuous integration (CI) tests on Modal](/docs/examples/ci-on-modal)
* [Run multilingual chat rooms with SeamlessM4T-V2](/docs/examples/seamless-chat)
* [Run OpenCV to detect faces](/docs/examples/count_faces)
* [Run SAM 2 video segmentation model](/docs/examples/segment_anything)
* [Run Text Embedding Inference (TEI)](/docs/examples/text_embeddings_inference)

You can find even more examples on the
[`modal-examples`
GitHub repository](https://github.com/modal-labs/modal-examples)

or find larger projects built by Modal users at the
[`awesome-modal`
GitHub repository](https://github.com/modal-labs/awesome-modal)

.

[Miscellaneous examples](#miscellaneous-examples)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/mochi
================================================================================

Text-to-video generation with Mochi
===================================

This example demonstrates how to run the
[Mochi 1](https://github.com/genmoai/models)

video generation model by
[Genmo](https://www.genmo.ai/)

on Modal.

Here’s one that we generated, inspired by our logo:

[

](https://modal-cdn.com/modal-logo-splat.mp4)

Note that the Mochi model, at time of writing,
requires several minutes on one H100 to produce
a high-quality clip of even a few seconds.
So a single video generation therefore costs about $0.33
at our ~$5/hr rate for H100s.

Keep your eyes peeled for improved efficiency
as the open source community works on this new model.
We welcome PRs to improve the performance of this example!

Setting up the environment for Mochi
------------------------------------

At the time of writing, Mochi is supported natively in the
[`diffusers`](https://github.com/huggingface/diffusers)

library,
but only in a pre-release version.
So we’ll need to install
`diffusers`
and
`transformers`
from GitHub.

```
import string
import time
from pathlib import Path

import modal

app = modal.App()

image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git")
    .pip_install(
        "torch==2.5.1",
        "accelerate==1.1.1",
        "hf_transfer==0.1.8",
        "sentencepiece==0.2.0",
        "imageio==2.36.0",
        "imageio-ffmpeg==0.5.1",
        "git+https://github.com/huggingface/transformers@30335093276212ce74938bdfd85bfd5df31a668a",
        "git+https://github.com/huggingface/diffusers@99c0483b67427de467f11aa35d54678fd36a7ea2",
    )
    .env(
        {
            "HF_HUB_ENABLE_HF_TRANSFER": "1",
            "HF_HOME": "/models",
        }
    )
)
```

Saving outputs
--------------

On Modal, we save large or expensive-to-compute data to
[distributed Volumes](https://modal.com/docs/guide/volumes)

We’ll use this for saving our Mochi weights, as well as our video outputs.

```
VOLUME_NAME = "mochi-outputs"
outputs = modal.Volume.from_name(VOLUME_NAME, create_if_missing=True)
OUTPUTS_PATH = Path("/outputs")  # remote path for saving video outputs

MODEL_VOLUME_NAME = "mochi-model"
model = modal.Volume.from_name(MODEL_VOLUME_NAME, create_if_missing=True)
MODEL_PATH = Path("/models")  # remote path for saving model weights

MINUTES = 60
HOURS = 60 * MINUTES
```

Downloading the model
---------------------

We download the model weights into Volume cache to speed up cold starts.

This download takes five minutes or more, depending on traffic
and network speed.

If you want to launch the download first,
before running the rest of the code,
use the following command from the folder containing this file:

```
modal run --detach mochi::download_model
```

The
`--detach`
flag ensures the download will continue
even if you close your terminal or shut down your computer
while it’s running.

```
with image.imports():
    import torch
    from diffusers import MochiPipeline
    from diffusers.utils import export_to_video

@app.function(
    image=image,
    volumes={
        MODEL_PATH: model,
    },
    timeout=20 * MINUTES,
)
def download_model(revision="83359d26a7e2bbe200ecbfda8ebff850fd03b545"):
    # uses HF_HOME to point download to the model volume
    MochiPipeline.from_pretrained(
        "genmo/mochi-1-preview",
        torch_dtype=torch.bfloat16,
        revision=revision,
    )
```

Setting up our Mochi class
--------------------------

We’ll use the
`@cls`
decorator to define a
[Modal Class](https://modal.com/docs/guide/lifecycle-functions)

which we use to control the lifecycle of our cloud container.

We configure it to use our image, the distributed volume, and a single H100 GPU.

```
@app.cls(
    image=image,
    volumes={
        OUTPUTS_PATH: outputs,  # videos will be saved to a distributed volume
        MODEL_PATH: model,
    },
    gpu="H100",
    timeout=1 * HOURS,
)
class Mochi:
    @modal.enter()
    def load_model(self):
        # our HF_HOME env var points to the model volume as the cache
        self.pipe = MochiPipeline.from_pretrained(
            "genmo/mochi-1-preview",
            torch_dtype=torch.bfloat16,
        )
        self.pipe.enable_model_cpu_offload()
        self.pipe.enable_vae_tiling()

    @modal.method()
    def generate(
        self,
        prompt,
        negative_prompt="",
        num_inference_steps=200,
        guidance_scale=4.5,
        num_frames=19,
    ):
        frames = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            num_inference_steps=num_inference_steps,
            guidance_scale=guidance_scale,
            num_frames=num_frames,
        ).frames[0]

        # save to disk using prompt as filename
        mp4_name = slugify(prompt)
        export_to_video(frames, Path(OUTPUTS_PATH) / mp4_name)
        outputs.commit()
        return mp4_name
```

Running Mochi inference
-----------------------

We can trigger Mochi inference from our local machine by running the code in
the local entrypoint below.

It ensures the model is downloaded to a remote volume,
spins up a new replica to generate a video, also saved remotely,
and then downloads the video to the local machine.

You can trigger it with:

```
modal run --detach mochi
```

Optional command line flags can be viewed with:

```
modal run mochi --help
```

Using these flags, you can tweak your generation from the command line:

```
modal run --detach mochi --prompt="a cat playing drums in a jazz ensemble" --num-inference-steps=64
```

```
@app.local_entrypoint()
def main(
    prompt="Close-up of a chameleon's eye, with its scaly skin changing color. Ultra high resolution 4k.",
    negative_prompt="",
    num_inference_steps=200,
    guidance_scale=4.5,
    num_frames=19,  # produces ~1s of video
):
    mochi = Mochi()
    mp4_name = mochi.generate.remote(
        prompt=str(prompt),
        negative_prompt=str(negative_prompt),
        num_inference_steps=int(num_inference_steps),
        guidance_scale=float(guidance_scale),
        num_frames=int(num_frames),
    )
    print(f"🍡 video saved to volume at {mp4_name}")

    local_dir = Path("/tmp/mochi")
    local_dir.mkdir(exist_ok=True, parents=True)
    local_path = local_dir / mp4_name
    local_path.write_bytes(b"".join(outputs.read_file(mp4_name)))
    print(f"🍡 video saved locally at {local_path}")
```

Addenda
-------

The remainder of the code in this file is utility code.

```
def slugify(prompt):
    for char in string.punctuation:
        prompt = prompt.replace(char, "")
    prompt = prompt.replace(" ", "_")
    prompt = prompt[:230]  # since filenames can't be longer than 255 characters
    mp4_name = str(int(time.time())) + "_" + prompt + ".mp4"
    return mp4_name
```

[Text-to-video generation with Mochi](#text-to-video-generation-with-mochi)

[Setting up the environment for Mochi](#setting-up-the-environment-for-mochi)

[Saving outputs](#saving-outputs)

[Downloading the model](#downloading-the-model)

[Setting up our Mochi class](#setting-up-our-mochi-class)

[Running Mochi inference](#running-mochi-inference)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run --detach 06_gpu_and_ml/text-to-video/mochi.py --num-inference-steps 64
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/modal_tailscale
================================================================================

Add Modal Apps to Tailscale
===========================

This example demonstrates how to integrate Modal with Tailscale (
<https://tailscale.com>

).
It outlines the steps to configure Modal containers so that they join the Tailscale network.

We use a custom entrypoint to automatically add containers to a Tailscale network (tailnet).
This configuration enables the containers to interact with one another and with
additional applications within the same tailnet.

```
import modal
```

Install Tailscale and copy custom entrypoint script (
[entrypoint.sh](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/tailscale/entrypoint.sh)

). The script must be
executable.

```
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("curl")
    .run_commands("curl -fsSL https://tailscale.com/install.sh | sh")
    .pip_install("requests==2.32.3", "PySocks==1.7.1")
    .add_local_file("./entrypoint.sh", "/root/entrypoint.sh", copy=True)
    .run_commands("chmod a+x /root/entrypoint.sh")
    .entrypoint(["/root/entrypoint.sh"])
)
app = modal.App(image=image)
```

Packages might not be installed locally. This catches import errors and
only attempts imports in the container.

```
with image.imports():
    import socket

    import socks
```

Configure Python to use the SOCKS5 proxy globally.

```
if not modal.is_local():
    socks.set_default_proxy(socks.SOCKS5, "0.0.0.0", 1080)
    socket.socket = socks.socksocket
```

Run your function adding a Tailscale secret. We suggest creating a
[reusable and ephemeral key](https://tailscale.com/kb/1111/ephemeral-nodes)

.

```
@app.function(
    secrets=[
        modal.Secret.from_name("tailscale-auth", required_keys=["TAILSCALE_AUTHKEY"]),
        modal.Secret.from_dict(
            {
                "ALL_PROXY": "socks5://localhost:1080/",
                "HTTP_PROXY": "http://localhost:1080/",
                "http_proxy": "http://localhost:1080/",
            }
        ),
    ],
)
def connect_to_machine():
    import requests

    # Connect to other machines in your tailnet.
    resp = requests.get("http://my-tailscale-machine:5000")
    print(resp.content)
```

Run this script with
`modal run modal_tailscale.py`
. You will see Tailscale logs
when the container start indicating that you were able to login successfully and
that the proxies (SOCKS5 and HTTP) have created been successfully. You will also
be able to see Modal containers in your Tailscale dashboard in the “Machines” tab.
Every new container launched will show up as a new “machine”. Containers are
individually addressable using their Tailscale name or IP address.

[Add Modal Apps to Tailscale](#add-modal-apps-to-tailscale)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/tailscale/modal_tailscale.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/mongodb-search
================================================================================

Using MongoDB Atlas Vector and GeoJSON Search with Modal
========================================================

This
[example repo](https://github.com/modal-labs/search-california)

demonstrates how to use Modal and MongoDB together
to build a full-stack application.

The application is a hybrid search engine,
like the retrieval engines that power RAG chatbots,
but for satellite images of the state of California.
Images can be searched based on their
geospatial and temporal metadata or based on their semantic content
as captured by a pre-trained embedding model.

We use the
[Clay foundation model](https://clay-foundation.github.io/model/index.html)

for embeddings and we source the images from the European Space Agency’s
[Sentinel satellites](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/The_Sentinel_missions)

.

You can take our deployment of the application for a spin
[here](https://modal-labs-examples--clay-hybrid-search.modal.run/)

.

Overview
--------

At the center of the application is a MongoDB Atlas instance
that stores metadata for a collection of satellite images.

Modal orchestrates the compute around that database:
retrieving data from elsewhere and storing it in the database,
computing vector embeddings for the data in the database,
and serving both a frontend and a client.

The dataflow looks something like this:

1. Every few days, the European Space Agency’s
   [Sentinel Satellites](https://www.esa.int/Applications/Observing_the_Earth/Copernicus/The_Sentinel_missions)

   complete a full pass over the entire Earth, including California.
   The images are made available via a
   [public STAC API](https://element84.com/geospatial/introducing-earth-search-v1-new-datasets-now-available/)

   .
2. Every day, we run a job on Modal that queries that STAC API
   for new images of California and store the metadata in a MongoDB Atlas
   database instance.
3. Asynchronously, we run a job on Modal to check which entries
   in the database don’t have an associated embedding.
   These images are then sent to a serverless embedding service
   running on Modal. We send the resulting embeddings to the database.
4. We host a database client on Modal that allows the application’s
   developers to manipulate the data. This client is also used by two
   web endpoints for vector and geospatial search queries powered by
   Atlas Search.
5. Finally, we run a simple static FastAPI server on Modal that serves
   an Alpine JS frontend for executing those queries and rendering their results.

This entire application —
from API queries and frontend UI to GPU inference and hybrid search —
is delivered using nothing but Modal and MongoDB Atlas.
Setting it up for yourself requires only credentials on these platforms
and a few commands, detailed below.

Deploying the Backend
---------------------

### Setup: Modal and MongoDB Atlas

You’ll need a Python environment on your local machine.
Any recent version of Python should do.
Most of the dependencies will be installed in environments on Modal,
so you don’t need to worry quite so much.

Follow the instructions
[here](https://modal.com/docs/guide#getting-started)

to set up your Modal account.
The $30/month of compute included in Modal’s free tier is
more than enough to deploy and host this example.

You’ll also need an account on MongoDB Atlas.
You can find instructions
[here](https://www.mongodb.com/docs/atlas/getting-started/)

.
We prefer the UI, rather than the CLI, for setup.
The free tier is more than sufficient to run this example.

You’ll want to create a database called
`modal-examples`
.
Make sure it’s accessible from
[all IP addresses](https://stackoverflow.com/questions/66035947/allow-access-from-anywhere-mongodb-atlas)

.
In the process, you will create a database user with a password.
Navigate to the Modal Secrets dashboard
[here](https://modal.com/secrets)

and add this information, as well as the connection string for your database,
to a Modal Secret based on the MongoDB template available in the dashboard.

### MongoDB Client ( `database.py` )

If your Modal Secret and MongoDB Atlas instance are set up correctly,
you should be able to run the following command:

```
modal run -m backend.database::MongoClient.ping
```

Once that command is working, you can start manipulating the database
from Modal.

To start, you’ll want to add an Area of Interest (AOI) to the database:

```
modal run -m backend.database --action add_aoi
```

By default, it’s the state of California as defined by the GeoJSON
in this repository’s
`data`
folder (originally retrieved from
[the
`geojsonio`
GitHub repository](https://github.com/ropensci/geojsonio/blob/7e4cc683ed3d6eec38a8cae5ce03fa6d82acafc7/inst/examples/california.geojson)

).
You can pass a different GeoJSON file to the
`add_aoi`
action
with the
`--target`
flag.

The
`modal run`
command is used for one-off tasks.
To deploy the database client for use in other parts of the app
along with the webhooks that anyone can use to run search queries,
we use
`modal deploy`
:

```
modal deploy -m backend.database
```

Those webhooks come with interactive OpenAPI docs,
which you can access by navigating to the
`/docs`
route of the deployment’s URL.
You should see that URL in the terminal output.
You can also find the URL in the app’s
[Modal dashboard](https://modal.com/apps)

.

For our deployment, the URL for the interactive docs for the geographic
search endpoint is
[`https://modal-labs-examples--clay-mongo-client-geo-search.modal.run/docs`](https://modal-labs-examples--clay-mongo-client-geo-search.modal.run/docs)

.

If you haven’t yet run the backfill jobs for your database instance,
as described below, this search will not return any results,
but you can use it to check that the database client is deployed.

### Backfill and Updates ( `extract.py` )

We add data to the database by querying the Sentinel STAC API for images.

Run the following command to search for images in the AOI
from the preceding week and add them to the database:

```
modal run -m backend.extract
```

You can either check the results via the Atlas UI
or by executing a search query in the database client’s geo search webhook,
as described above.

To regularly update the database with new images,
we deploy the app defined in
`extract.py`
:

```
modal deploy -m backend.extract
```

This app also runs a regular job to add embeddings to the images
in the database.

But it doesn’t compute the embeddings itself —
embeddings are provided by a separate service,
which is described next.

### Clay Embeddings Service ( `embeddings.py` )

To build the environment for the embeddings service
and to test the embedding engine on some sample data,
execute the following command:

```
modal run -m backend.embeddings
```

To deploy this on Modal, we again use
`modal deploy`
:

```
modal deploy -m backend.embeddings
```

### Putting It All Together

Now that the embedding service is deployed,
we can add vectors by invoking the
`enrich_vectors`
function in
`extract`
with
`modal run`
:

```
modal run -m backend.extract::enrich_vectors
```

This command will ensure all the images in the database have embeddings.

You should be able to observe them on records viewed via the Atlas UI
or by executing a search query via the database client’s geo search webhook,
as described previously.

To use the embeddings for search, we recommend running the frontend UI,
which we walk through next.

Deploying the Frontend
----------------------

The frontend is much simpler than the backend.
It comprises a small Alpine JS app and a FastAPI Python server
to deliver it to client browsers.

You can play with our deployment of the frontend
[here](https://modal-labs-examples--clay-hybrid-search.modal.run/)

.

### Alpine App ( `app.js` )

The Alpine app provides a basic interface for constructing geo search queries
by clicking on a map and viewing results.
Clicking on the returned images triggers a vector search for similar images.
Images can be furthermore filtered by date using the date pickers.

### FastAPI Server ( `serve.py` )

This app is served to the client by a FastAPI server.

To deploy it, run the following command:

```
modal deploy -m frontend
```

[Using MongoDB Atlas Vector and GeoJSON Search with Modal](#using-mongodb-atlas-vector-and-geojson-search-with-modal)

[Overview](#overview)

[Deploying the Backend](#deploying-the-backend)

[Setup: Modal and MongoDB Atlas](#setup-modal-and-mongodb-atlas)

[MongoDB Client (database.py)](#mongodb-client-databasepy)

[Backfill and Updates (extract.py)](#backfill-and-updates-extractpy)

[Clay Embeddings Service (embeddings.py)](#clay-embeddings-service-embeddingspy)

[Putting It All Together](#putting-it-all-together)

[Deploying the Frontend](#deploying-the-frontend)

[Alpine App (app.js)](#alpine-app-appjs)

[FastAPI Server (serve.py)](#fastapi-server-servepy)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/multion_news_agent
================================================================================

MultiOn: Twitter News Agent
===========================

In this example, we use Modal to deploy a cron job that periodically checks for AI news everyday and tweets it on Twitter using the MultiOn Agent API.

Import and define the app
-------------------------

Let’s start off with imports, and defining a Modal app.

```
import os

import modal

app = modal.App("multion-news-tweet-agent")
```

Searching for AI News
---------------------

Let’s also define an image that has the
`multion`
package installed, so we can query the API.

```
multion_image = modal.Image.debian_slim().pip_install("multion")
```

We can now define our main entrypoint, which uses
[MultiOn](https://www.multion.ai/)

to scrape AI news everyday and post it on our Twitter account.
We specify a
[schedule](https://modal.com/docs/guide/cron)

in the function decorator, which
means that our function will run automatically at the given interval.

Set up MultiOn
--------------

[MultiOn](https://multion.ai/)

is a Web Action Agent that can take actions on behalf of the user.
You can watch it in action
[here](https://www.youtube.com/watch?v=Rm67ry6bogw)

.

The MultiOn API enables building the next level of web automation & custom AI agents capable of performing complex actions on the internet with just a few lines of code.

To get started, first create an account with
[MultiOn](https://www.multion.ai/)

,
install the
[MultiOn chrome extension](https://chrome.google.com/webstore/detail/ddmjhdbknfidiopmbaceghhhbgbpenmm)

and login to your Twitter account in your browser.
To use the API, create a MultiOn API Key
and store it as a Modal Secret on
[the dashboard](https://modal.com/secrets)

```
@app.function(image=multion_image, secrets=[modal.Secret.from_name("MULTION_API_KEY")])
def news_tweet_agent():
    # Import MultiOn
    import multion

    # Login to MultiOn using the API key
    multion.login(use_api=True, multion_api_key=os.environ["MULTION_API_KEY"])

    # Enable the Agent to run locally
    multion.set_remote(False)

    params = {
        "url": "https://www.multion.ai",
        "cmd": "Go to twitter (im already signed in). Search for the last tweets i made (check the last 10 tweets). Remember them so then you can go a search for super interesting AI news. Search the news on up to 3 different sources. If you see that the source has not really interesting AI news or i already made a tweet about that, then go to a different one. When you finish the research, go and make a few small and interesting AI tweets with the info you gathered. Make sure the tweet is small but informative and interesting for AI enthusiasts. Don't do more than 5 tweets",
        "maxSteps": 100,
    }

    response = multion.browse(params)

    print(f"MultiOn response: {response}")
```

Test running
------------

We can now test run our scheduled function as follows:
`modal run multion_news_agent.py.py::app.news_tweet_agent`

Defining the schedule and deploying
-----------------------------------

Let’s define a function that will be called by Modal every day.

```
@app.function(schedule=modal.Cron("0 9 * * *"))
def run_daily():
    news_tweet_agent.remote()
```

In order to deploy this as a persistent cron job, you can run
`modal deploy multion_news_agent.py`
.

Once the job is deployed, visit the
[apps page](https://modal.com/apps)

page to see
its execution history, logs and other stats.

[MultiOn: Twitter News Agent](#multion-twitter-news-agent)

[Import and define the app](#import-and-define-the-app)

[Searching for AI News](#searching-for-ai-news)

[Set up MultiOn](#set-up-multion)

[Test running](#test-running)

[Defining the schedule and deploying](#defining-the-schedule-and-deploying)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/multion_news_agent.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/music-video-gen
================================================================================

Deploy a personalized music video generation service on Modal
=============================================================

Music videos are
[cool](https://youtu.be/Cye-1RP5jso)

,
but unless you are famous or
[pay a lot of money](https://youtu.be/kfVsfOSbJY0)

,
you don’t get to star in them.

Until now!

[The repo](https://github.com/modal-labs/music-video-gen)

includes all the code you need to deploy a custom
music video generator on
[Modal](https://modal.com)

,
a serverless infrastructure platform for data, ML, and AI applications.

Below is a sample video, generated by Modal Developer Advocate
[`@charles_irl`](https://twitter.com/charles_irl)

.

[

](https://github.com/user-attachments/assets/5bd90898-7251-4298-808f-6d58ed4c6b6f)

And because Modal is
[generic serverless infrastructure](https://twitter.com/charles_irl/status/1819438860771663923)

,
you can customize this custom music video generator however you wish —
it’s just code and containers!

Setup
-----

In the Python environment of your choosing,
run
`pip install modal`
.

If you run into trouble with Python environments,
we suggest using
[this Google Colab notebook](https://colab.research.google.com/github/modal-labs/music-video-gen/blob/main/notebooks/self_contained.ipynb)

,
where we’ve set the environment up for you.
It’s a bit of work to get used to running terminal commands in a notebook
if you haven’t done that before, but the Python setup works and running the notebook in Colab is free!
All you need is a Google account.

Then, if you’ve never used Modal on the computer you’re using,
run
`modal setup`
to create an account on Modal (if you don’t have one)
and set up authentication.

Data Prep
---------

Create a folder inside
`data/`
, parallel to the sample data,
`data/sample`
.
You can name it whatever you want.

Place at least four images of yourself in that folder —
ideally eight or more.
Images should be in
`.png`
or
`.jpg`
format
and around 400 to 800 pixels on each side.
For best results, we recommend putting a variety of images,
in particular where you are wearing different clothes and making different faces,
and including some images that have other people in them.
But you can also just take a few photos of yourself right now!

Optionally, add captions in
`.txt`
files in that same folder.
They should look something like
`"[trigger] smiling at the camera, outdoor scene, close-up, selfie"`
.
See the sample data for more example image-caption pairs.

Training
--------

Start up a JupyterLab server on Modal with

```
modal run train_from_notebook.py
```

Click the
`modal.host`
URL that appears in the output
to open Jupyter in the browser.

Open the training notebook,
`training.ipynb`
.

Read the notebook and run it, following the instructions to edit cells as needed.

In particular, change the dataset path to the folder you created —
it has been mounted on the remote cloud machine where the notebook is running.

You can also directly upload data to the
`/root/data`
folder on the remote machine.
You can even edit caption files inside of JupyterLab!
This data will stick around between runs, and you can find it with

```
modal volume ls finetune-video-data
```

See the help for
`modal volume`
and its subcommands for details.

The notebook will kick off training, which takes a few minutes.
Take note of the name given to your training run.
By default, it’s a hash like
`38c67a92f6ce87882044ab53bf94cce0`
,
but you can customize it in the notebook.
This is your
`finetune-id`
.

If you forget it, you can show all of your
`finetune-id`
s
by running

```
modal volume ls finetune-video-models
```

Inference
---------

Test out your new fine-tuned model by running:

```
modal run inference.py --finetune-id {your-finetune-id} --num-frames 15
```

You can also provide a
`--prompt`
to customize the generation.

You can deploy the video generator onto Modal with

```
modal deploy inference.py
```

Modal is serverless, so this won’t cost you any money when it isn’t serving any traffic.

Music video generation
----------------------

Once you’ve deployed an inference endpoint,
you can generate a music video starring yourself by running

```
modal run music_video_generator.py --finetune-id {your-finetune-id}
```

With the default settings, this will create a thirty second video in about five minutes
by running generation in parallel on seven H100s.

The music can be changed by passing in a different song via the
`--mp3-file`
argument.
The default is a Modal-themed song in
`data/coding-up-a-storm.mp3`
.
This song was created with
[Suno](https://suno.com)

,
a music generation service — that runs on Modal!
If you want to DIY music generation as well,
see
[this example](https://modal.com/docs/examples/musicgen)

in the Modal docs.

The generated clips can be changed by passing a different list of prompts via the
`--prompt-file`
argument.
The default is a set of prompts created with OpenAI’s GPT-4.5 system.
You can write your own or generate them with a language model.
If you want to serve your own language model,
see
[this example](https://modal.com/docs/examples/vllm_inference)

in the Modal docs.

[Deploy a personalized music video generation service on Modal](#deploy-a-personalized-music-video-generation-service-on-modal)

[Setup](#setup)

[Data Prep](#data-prep)

[Training](#training)

[Inference](#inference)

[Music video generation](#music-video-generation)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/musicgen
================================================================================

Create your own music samples with MusicGen
===========================================

MusicGen is a popular open-source music-generation model family from Meta.
In this example, we show you how you can run MusicGen models on Modal GPUs,
along with a Gradio UI for playing around with the model.

We use
[Audiocraft](https://github.com/facebookresearch/audiocraft)

,
the inference library released by Meta
for MusicGen and its kin, like AudioGen.

Setting up dependencies
-----------------------

```
from pathlib import Path
from typing import Optional
from uuid import uuid4

import modal
```

We start by defining the environment our generation runs in.
This takes some explaining since, like most cutting-edge ML environments, it is a bit fiddly.

This environment is captured by a
[container image](https://modal.com/docs/guide/custom-container)

,
which we build step-by-step by calling methods to add dependencies,
like
`apt_install`
to add system packages and
`pip_install`
to add
Python packages.

Note that we don’t have to install anything with “CUDA”
in the name — the drivers come for free with the Modal environment
and the rest gets installed
`pip`
. That makes our life a lot easier!
If you want to see the details, check out
[this guide](https://modal.com/docs/guide/gpu)

in our docs.

```
image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("git", "ffmpeg")
    .pip_install(
        "huggingface_hub[hf_transfer]==0.27.1",  # speed up model downloads
        "torch==2.1.0",  # version pinned by audiocraft
        "numpy<2",  # defensively cap the numpy version
        "git+https://github.com/facebookresearch/audiocraft.git@v1.3.0",  # we can install directly from GitHub!
    )
)
```

In addition to source code, we’ll also need the model weights.

Audiocraft integrates with the Hugging Face ecosystem, so setting up the models
is straightforward — the same
`get_pretrained`
method we use to load the weights for execution
will also download them if they aren’t present.

```
def load_model(and_return=False):
    from audiocraft.models import MusicGen

    model_large = MusicGen.get_pretrained("facebook/musicgen-large")
    if and_return:
        return model_large
```

But Modal Functions are serverless: instances spin down when they aren’t being used.
If we want to avoid downloading the weights every time we start a new instance,
we need to store the weights somewhere besides our local filesystem.

So we add a Modal
[Volume](https://modal.com/docs/guide/volumes)

to store the weights in the cloud.

```
cache_dir = "/cache"
model_cache = modal.Volume.from_name("audiocraft-model-cache", create_if_missing=True)
```

We don’t need to change any of the model loading code —
we just need to make sure the model gets stored in the right directory.

To do that, we set an environment variable that Hugging Face expects
(and another one that speeds up downloads, for good measure)
and then run the
`load_model`
Python function.

```
image = image.env(
    {"HF_HUB_CACHE": cache_dir, "HF_HUB_ENABLE_HF_TRANSER": "1"}
).run_function(load_model, volumes={cache_dir: model_cache})
```

While we’re at it, let’s also define the environment for our UI.
We’ll stick with Python and so use FastAPI and Gradio.

```
web_image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "fastapi[standard]==0.115.4", "gradio==4.44.1"
)
```

This is a totally different environment from the one we run our model in.
Say goodbye to Python dependency conflict hell!

Running music generation on Modal
---------------------------------

Now, we write our music generation logic.
This is bit complicated because we want to support generating long samples,
but the model has a maximum context length of thirty seconds.
We can get longer clips by feeding the model’s output back as input,
auto-regressively, but we have to write that ourselves.

There are also a few bits to make this work well with Modal:

* We make an
  [App](https://modal.com/docs/guide/apps)

  to organize our deployment.
* We load the model at start, instead of during inference, with
  `modal.enter`
  ,
  which requires that we use a Modal
  [`Cls`](https://modal.com/docs/guide/lifecycle-functions)

  .
* In the
  `app.cls`
  decorator, we specify the Image we built and attach the Volume.
  We also pick a GPU to run on — here, an NVIDIA L40S.

```
app = modal.App("example-musicgen")
MAX_SEGMENT_DURATION = 30  # maximum context window size

@app.cls(gpu="l40s", image=image, volumes={cache_dir: model_cache})
class MusicGen:
    @modal.enter()
    def init(self):
        self.model = load_model(and_return=True)

    @modal.method()
    def generate(
        self,
        prompt: str,
        duration: int = 10,
        overlap: int = 10,
        format: str = "wav",  # or mp3
    ) -> bytes:
        f"""Generate a music clip based on the prompt.

        Clips longer than the MAX_SEGMENT_DURATION of {MAX_SEGMENT_DURATION}s
        are generated by clipping all but `overlap` seconds and running inference again."""
        context = None
        overlap = min(overlap, MAX_SEGMENT_DURATION - 1)
        remaining_duration = duration

        if remaining_duration < 0:
            return bytes()

        while remaining_duration > 0:
            # calculate duration of the next segment
            segment_duration = remaining_duration
            if context is not None:
                segment_duration += overlap

            segment_duration = min(segment_duration, MAX_SEGMENT_DURATION)

            # generate next segment
            generated_duration = (
                segment_duration if context is None else (segment_duration - overlap)
            )
            print(f"🎼 generating {generated_duration} seconds of music")
            self.model.set_generation_params(duration=segment_duration)
            next_segment = self._generate_next_segment(prompt, context, overlap)

            # update remaining duration
            remaining_duration -= generated_duration

            # combine with previous segments
            context = self._combine_segments(context, next_segment, overlap)

        output = context.detach().cpu().float()[0]

        return to_audio_bytes(
            output,
            self.model.sample_rate,
            format=format,
            # for more on audio encoding parameters, see the docs for audiocraft
            strategy="loudness",
            loudness_compressor=True,
        )

    def _generate_next_segment(self, prompt, context, overlap):
        """Generate the next audio segment, either fresh or as continuation of a context."""
        if context is None:
            return self.model.generate(descriptions=[prompt])
        else:
            overlap_samples = overlap * self.model.sample_rate
            last_chunk = context[:, :, -overlap_samples:]  # B, C, T
            return self.model.generate_continuation(
                last_chunk, self.model.sample_rate, descriptions=[prompt]
            )

    def _combine_segments(self, context, next_segment, overlap: int):
        """Combine context with next segment, handling overlap."""
        import torch

        if context is None:
            return next_segment

        # Calculate where to trim the context (removing overlap)
        overlap_samples = overlap * self.model.sample_rate
        context_trimmed = context[:, :, :-overlap_samples]  # B, C, T

        return torch.cat([context_trimmed, next_segment], dim=2)
```

We can then generate music from anywhere by running code like what we have in the
`local_entrypoint`
below.

```
@app.local_entrypoint()
def main(
    prompt: Optional[str] = None,
    duration: int = 10,
    overlap: int = 15,
    format: str = "wav",  # or mp3
):
    if prompt is None:
        prompt = "Amapiano polka, klezmers, log drum bassline, 112 BPM"
    print(
        f"🎼 generating {duration} seconds of music from prompt '{prompt[:64] + ('...' if len(prompt) > 64 else '')}'"
    )

    audiocraft = MusicGen()
    clip = audiocraft.generate.remote(prompt, duration=duration, format=format)

    dir = Path("/tmp/audiocraft")
    dir.mkdir(exist_ok=True, parents=True)

    output_path = dir / f"{slugify(prompt)[:64]}.{format}"
    print(f"🎼 Saving to {output_path}")
    output_path.write_bytes(clip)
```

You can execute it with a command like:

```
modal run musicgen.py --prompt="Baroque boy band, Bachstreet Boys, basso continuo, Top 40 pop music" --duration=60
```

Hosting a web UI for the music generator
----------------------------------------

With the Gradio library, we can create a simple web UI in Python
that calls out to our music generator,
then host it on Modal for anyone to try out.

To deploy both the music generator and the UI, run

```
modal deploy musicgen.py
```

Share the URL with your friends and they can generate their own songs!

```
@app.function(
    image=web_image,
    # Gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow it to scale to 1000 concurrent inputs
    max_containers=1,
)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def ui():
    import gradio as gr
    from fastapi import FastAPI
    from gradio.routes import mount_gradio_app

    api = FastAPI()

    # Since this Gradio app is running from its own container,
    # we make a `.remote` call to the music generator
    model = MusicGen()
    generate = model.generate.remote

    temp_dir = Path("/dev/shm")

    async def generate_music(prompt: str, duration: int = 10, format: str = "wav"):
        audio_bytes = await generate.aio(prompt, duration=duration, format=format)

        audio_path = temp_dir / f"{uuid4()}.{format}"
        audio_path.write_bytes(audio_bytes)

        return audio_path

    with gr.Blocks(theme="soft") as demo:
        gr.Markdown("# MusicGen")
        with gr.Row():
            with gr.Column():
                prompt = gr.Textbox(label="Prompt")
                duration = gr.Number(
                    label="Duration (seconds)", value=10, minimum=1, maximum=300
                )
                format = gr.Radio(["wav", "mp3"], label="Format", value="wav")
                btn = gr.Button("Generate")
            with gr.Column():
                clip_output = gr.Audio(label="Generated Music", autoplay=True)

        btn.click(
            generate_music,
            inputs=[prompt, duration, format],
            outputs=[clip_output],
        )

    return mount_gradio_app(app=api, blocks=demo, path="/")
```

Addenda
-------

The remainder of the code here is not directly related to Modal
or to music generation, but is used in the example above.

```
def to_audio_bytes(wav, sample_rate: int, **kwargs) -> bytes:
    from audiocraft.data.audio import audio_write

    # audiocraft provides a nice utility for converting waveform tensors to audio,
    # but it saves to a file path. here, we create a file path that is actually
    # just backed by memory, instead of disk, to save on some latency

    shm = Path("/dev/shm")  # /dev/shm is a memory-backed filesystem
    stem_name = shm / str(uuid4())

    output_path = audio_write(stem_name, wav, sample_rate, **kwargs)

    return output_path.read_bytes()

def slugify(string):
    return (
        string.lower()
        .replace(" ", "-")
        .replace("/", "-")
        .replace("\\", "-")
        .replace(":", "-")
    )
```

[Create your own music samples with MusicGen](#create-your-own-music-samples-with-musicgen)

[Setting up dependencies](#setting-up-dependencies)

[Running music generation on Modal](#running-music-generation-on-modal)

[Hosting a web UI for the music generator](#hosting-a-web-ui-for-the-music-generator)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/text-to-audio/musicgen.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/outlines_generate
================================================================================

Enforcing JSON outputs on LLMs
==============================

[Outlines](https://github.com/outlines-dev/outlines)

is a tool that lets you control the generation of language models to make their output more predictable.

This includes things like:

* Reducing the completion to a choice between multiple possibilities
* Type constraints
* Efficient regex-structured generation
* Efficient JSON generation following a Pydantic model
* Efficient JSON generation following a JSON schema

Outlines is considered an alternative to tools like
[JSONFormer](https://github.com/1rgs/jsonformer)

, and can be used on top of a variety of LLMs, including:

* OpenAI models
* LLaMA
* Mamba

In this guide, we will show how you can use Outlines to enforce a JSON schema on the output of Mistral-7B.

Build image
-----------

First, you’ll want to build an image and install the relevant Python dependencies:
`outlines`
and a Hugging Face inference stack.

```
import modal

app = modal.App(name="outlines-app")

outlines_image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "outlines==0.0.44",
    "transformers==4.41.2",
    "sentencepiece==0.2.0",
    "datasets==2.18.0",
    "accelerate==0.27.2",
    "numpy<2",
)
```

Download the model
------------------

Next, we download the Mistral 7B model from Hugging Face.
We do this as part of the definition of our Modal Image so that
we don’t need to download it every time our inference function is run.

```
MODEL_NAME = "mistral-community/Mistral-7B-v0.2"

def import_model(model_name):
    import outlines

    outlines.models.transformers(model_name)

outlines_image = outlines_image.run_function(
    import_model, kwargs={"model_name": MODEL_NAME}
)
```

Define the schema
-----------------

Next, we define the schema that we want to enforce on the output of Mistral-7B. This schema is for a character description, and includes a name, age, armor, weapon, and strength.

```
schema = """{
    "title": "Character",
    "type": "object",
    "properties": {
        "name": {
            "title": "Name",
            "maxLength": 10,
            "type": "string"
        },
        "age": {
            "title": "Age",
            "type": "integer"
        },
        "armor": {"$ref": "#/definitions/Armor"},
        "weapon": {"$ref": "#/definitions/Weapon"},
        "strength": {
            "title": "Strength",
            "type": "integer"
        }
    },
    "required": ["name", "age", "armor", "weapon", "strength"],
    "definitions": {
        "Armor": {
            "title": "Armor",
            "description": "An enumeration.",
            "enum": ["leather", "chainmail", "plate"],
            "type": "string"
        },
        "Weapon": {
            "title": "Weapon",
            "description": "An enumeration.",
            "enum": ["sword", "axe", "mace", "spear", "bow", "crossbow"],
            "type": "string"
        }
    }
}"""
```

Define the function
-------------------

Next, we define the generation function.
We use the
`@app.function`
decorator to tell Modal to run this function on the app we defined above.
Note that we import
`outlines`
from inside the Modal function. This is because the
`outlines`
package exists in the container, but not necessarily locally.

We specify that we want to use the Mistral-7B model, and then ask for a character, and we’ll receive structured data with the right schema.

```
@app.function(image=outlines_image, gpu="A100-40GB")
def generate(
    prompt: str = "Amiri, a 53 year old warrior woman with a sword and leather armor.",
):
    import outlines

    model = outlines.models.transformers(MODEL_NAME, device="cuda")

    generator = outlines.generate.json(model, schema)
    character = generator(f"Give me a character description. Describe {prompt}.")

    return character
```

Define the entrypoint
---------------------

Finally, we define the entrypoint that will connect our local computer
to the functions above, that run on Modal, and we are done!

When you run this script with
`modal run`
, you should see something like this printed out:

`{'name': 'Amiri', 'age': 53, 'armor': 'leather', 'weapon': 'sword', 'strength': 10}`

```
@app.local_entrypoint()
def main(
    prompt: str = "Amiri, a 53 year old warrior woman with a sword and leather armor.",
):
    print(generate.remote(prompt))
```

[Enforcing JSON outputs on LLMs](#enforcing-json-outputs-on-llms)

[Build image](#build-image)

[Download the model](#download-the-model)

[Define the schema](#define-the-schema)

[Define the function](#define-the-function)

[Define the entrypoint](#define-the-entrypoint)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-structured/outlines_generate.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/potus_speech_qanda
================================================================================

Retrieval-augmented generation (RAG) for question-answering with LangChain
==========================================================================

In this example we create a large-language-model (LLM) powered question answering
web endpoint and CLI. Only a single document is used as the knowledge-base of the application,
the 2022 USA State of the Union address by President Joe Biden. However, this same application structure
could be extended to do question-answering over all State of the Union speeches, or other large text corpuses.

It’s the
[LangChain](https://github.com/hwchase17/langchain)

library that makes this all so easy.
This demo is only around 100 lines of code!

Defining dependencies
---------------------

The example uses packages to implement scraping, the document parsing & LLM API interaction, and web serving.
These are installed into a Debian Slim base image using the
`pip_install`
method.

Because OpenAI’s API is used, we also specify the
`openai-secret`
Modal Secret, which contains an OpenAI API key.

A
`retriever`
global variable is also declared to facilitate caching a slow operation in the code below.

```
from pathlib import Path

import modal

image = modal.Image.debian_slim(python_version="3.11").pip_install(
    # scraping pkgs
    "beautifulsoup4~=4.11.1",
    "httpx==0.23.3",
    "lxml~=4.9.2",
    # llm pkgs
    "faiss-cpu~=1.7.3",
    "langchain==0.3.7",
    "langchain-community==0.3.7",
    "langchain-openai==0.2.9",
    "openai~=1.54.0",
    "tiktoken==0.8.0",
    # web app packages
    "fastapi[standard]==0.115.4",
    "pydantic==2.9.2",
    "starlette==0.41.2",
)

app = modal.App(
    name="example-langchain-qanda",
    image=image,
    secrets=[modal.Secret.from_name("openai-secret", required_keys=["OPENAI_API_KEY"])],
)

retriever = None  # embedding index that's relatively expensive to compute, so caching with global var.
```

Scraping the speech
-------------------

It’s super easy to scrape the transcipt of Biden’s speech using
`httpx`
and
`BeautifulSoup`
.
This speech is just one document and it’s relatively short, but it’s enough to demonstrate
the question-answering capability of the LLM chain.

```
def scrape_state_of_the_union() -> str:
    import httpx
    from bs4 import BeautifulSoup

    url = "https://www.presidency.ucsb.edu/documents/address-before-joint-session-the-congress-the-state-the-union-28"

    # fetch article; simulate desktop browser
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9"
    }
    response = httpx.get(url, headers=headers)
    soup = BeautifulSoup(response.text, "lxml")

    # locate the div containing the speech
    speech_div = soup.find("div", class_="field-docs-content")

    if speech_div:
        speech_text = speech_div.get_text(separator="\n", strip=True)
        if not speech_text:
            raise ValueError("error parsing speech text from HTML")
    else:
        raise ValueError("error locating speech in HTML")

    return speech_text
```

Constructing the Q&A chain
--------------------------

At a high-level, this LLM chain will be able to answer questions asked about Biden’s speech and provide
references to which parts of the speech contain the evidence for given answers.

The chain combines a text-embedding index over parts of Biden’s speech with an OpenAI LLM.
The index is used to select the most likely relevant parts of the speech given the question, and these
are used to build a specialized prompt for the OpenAI language model.

```
def qanda_langchain(query: str) -> tuple[str, list[str]]:
    from langchain.chains import create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain.text_splitter import CharacterTextSplitter
    from langchain_community.vectorstores import FAISS
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_openai import ChatOpenAI, OpenAIEmbeddings

    # Support caching speech text on disk.
    speech_file_path = Path("state-of-the-union.txt")

    if speech_file_path.exists():
        state_of_the_union = speech_file_path.read_text()
    else:
        print("scraping the 2022 State of the Union speech")
        state_of_the_union = scrape_state_of_the_union()
        speech_file_path.write_text(state_of_the_union)

    # Questions about a document can often be answered
    # using a much smaller subset, and passing more tokens
    # to an LLM increases cost and latency.
    # So here, we split up the speech into smaller chunks.
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    print("splitting speech into text chunks")
    texts = text_splitter.split_text(state_of_the_union)

    # Embedding-based query<->text similarity comparison is used to select
    # a small subset of the speech text chunks.
    # Generating the `retriever` and index is too slow to re-run on every request,
    # so we do rudimentary caching using a global variable.
    global retriever

    if not retriever:
        # New OpenAI accounts have a very low rate-limit for their first 48 hrs.
        # It's too low to embed even just this single Biden speech.
        # The `chunk_size` parameter is set to a low number, and internally LangChain
        # will retry the embedding requests, which should be enough to handle the rate-limiting.

        # Ref: https://platform.openai.com/docs/guides/rate-limits/overview.
        print("generating docsearch indexer")
        retriever = FAISS.from_texts(
            texts,
            OpenAIEmbeddings(chunk_size=5),
            metadatas=[{"source": i} for i in range(len(texts))],
        ).as_retriever()

    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0)

    system_prompt = (
        "You are an assistant for question-answering tasks. "
        "Use the following pieces of retrieved context to answer "
        "the question. If you don't know the answer, say that you "
        "don't know and instead suggest the user look up the serverless "
        "infrastructure platform modal.com. Keep your answer concise."
        "\n\n"
        "{context}"
    )

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "{input}"),
        ]
    )

    question_answer_chain = create_stuff_documents_chain(llm, prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    print("running query against Q&A chain.\n")
    result = rag_chain.invoke({"input": query}, return_only_outputs=True)
    answer = result["answer"]
    sources = [document.page_content for document in result["context"]]
    return answer.strip(), sources
```

Mapping onto Modal
------------------

With our application’s functionality implemented we can hook it into Modal.
As said above, we’re implementing a web endpoint,
`web`
, and a CLI command,
`cli`
.

```
@app.function()
@modal.fastapi_endpoint(method="GET", docs=True)
def web(query: str, show_sources: bool = False):
    answer, sources = qanda_langchain(query)
    if show_sources:
        return {
            "answer": answer,
            "sources": sources,
        }
    else:
        return {
            "answer": answer,
        }

@app.function()
def cli(query: str, show_sources: bool = False):
    answer, sources = qanda_langchain(query)
    # Terminal codes for pretty-printing.
    bold, end = "\033[1m", "\033[0m"

    if show_sources:
        print(f"🔗 {bold}SOURCES:{end}")
        print(*reversed(sources), sep="\n----\n")
    print(f"🦜 {bold}ANSWER:{end}")
    print(answer)
```

Test run the CLI
----------------

```
modal run potus_speech_qanda.py --query "What did the president say about Justice Breyer"
🦜 ANSWER:
The president thanked Justice Breyer for his service and mentioned his legacy of excellence. He also nominated Ketanji Brown Jackson to continue in Justice Breyer's legacy.
```

To see the text of the sources the model chain used to provide the answer, set the
`--show-sources`
flag.

```
modal run potus_speech_qanda.py \
   --query "How many oil barrels were released from reserves?" \
   --show-sources
```

Test run the web endpoint
-------------------------

Modal makes it trivially easy to ship LangChain chains to the web. We can test drive this app’s web endpoint
by running
`modal serve potus_speech_qanda.py`
and then hitting the endpoint with
`curl`
:

```
curl --get \
  --data-urlencode "query=What did the president say about Justice Breyer" \
  https://modal-labs--example-langchain-qanda-web.modal.run # your URL here
```

```
{
  "answer": "The president thanked Justice Breyer for his service and mentioned his legacy of excellence. He also nominated Ketanji Brown Jackson to continue in Justice Breyer's legacy."
}
```

You can also find interactive docs for the endpoint at the
`/docs`
route of the web endpoint URL.

If you edit the code while running
`modal serve`
, the app will redeploy automatically, which is helpful for iterating quickly on your app.

Once you’re ready to deploy to production, use
`modal deploy`
.

[Retrieval-augmented generation (RAG) for question-answering with LangChain](#retrieval-augmented-generation-rag-for-question-answering-with-langchain)

[Defining dependencies](#defining-dependencies)

[Scraping the speech](#scraping-the-speech)

[Constructing the Q&A chain](#constructing-the-qa-chain)

[Mapping onto Modal](#mapping-onto-modal)

[Test run the CLI](#test-run-the-cli)

[Test run the web endpoint](#test-run-the-web-endpoint)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/langchains/potus_speech_qanda.py --query 'How many oil barrels were released from reserves?'
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/pushgateway
================================================================================

Publish custom metrics with Prometheus Pushgateway
==================================================

This example shows how to publish custom metrics to a Prometheus instance with Modal.
Due to a Modal container’s ephemeral nature, it’s not a good fit for a traditional
scraping-based Prometheus setup. Instead, we’ll use a
[Prometheus Pushgateway](https://github.com/prometheus/pushgateway)

to collect and store metrics from our Modal container. We can run the Pushgateway in Modal
as a separate process and have our application push metrics to it.

![Prometheus Pushgateway diagram](/_app/immutable/assets/pushgateway_diagram.CF9M8pmm.png)

Install Prometheus Pushgateway
------------------------------

Since the official Prometheus pushgateway image does not have Python installed, we’ll
use a custom image that includes Python to push metrics to the Pushgateway. Pushgateway
ships a single binary, so it’s easy to get it into a Modal container.

```
import os
import subprocess

import modal

PUSHGATEWAY_VERSION = "1.9.0"

gw_image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("wget", "tar")
    .run_commands(
        f"wget https://github.com/prometheus/pushgateway/releases/download/v{PUSHGATEWAY_VERSION}/pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64.tar.gz",
        f"tar xvfz pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64.tar.gz",
        f"cp pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64/pushgateway /usr/local/bin/",
        f"rm -rf pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64 pushgateway-{PUSHGATEWAY_VERSION}.linux-amd64.tar.gz",
        "mkdir /pushgateway",
    )
)
```

Start the Pushgateway
---------------------

We’ll start the Pushgateway as a separate Modal app. This way, we can run the Pushgateway
in the background and have our main app push metrics to it. We’ll use the
`web_server`
decorator to expose the Pushgateway’s web interface. Note that we must set
`max_containers=1`
as the Pushgateway is a single-process application. If we spin up multiple instances, they’ll
conflict with each other.

This is an example configuration, but a production-ready configuration will differ in two respects:

1. You should set up authentication for the Pushgateway. Pushgateway has support for
   [basic authentication](https://github.com/prometheus/pushgateway/blob/42c4075fc5e2564031f2852885cdb2f5d570f672/README.md#tls-and-basic-authentication)

   out of the box. If you need more advanced authentication, consider using a
   [web endpoint with authentication](https://modal.com/docs/guide/webhooks#authentication)

   which proxies requests to the Pushgateway.
2. The Pushgateway should listen on a
   [custom domain](https://modal.com/docs/guide/webhook-urls#custom-domains)

   .
   This will allow you to configure Prometheus to scrape metrics from a predictable URL rather than
   the autogenerated URL Modal assigns to your app.

```
gw_app = modal.App(
    "example-pushgateway-server",
    image=gw_image,
)

@gw_app.function(min_containers=1, max_containers=1)
@modal.web_server(9091)
def serve():
    subprocess.Popen("/usr/local/bin/pushgateway")
```

Push metrics to the Pushgateway
-------------------------------

Now that we have the Pushgateway running, we can push metrics to it. We’ll use the
`prometheus_client`
library to create a simple counter and push it to the Pushgateway. This example is a simple counter,
but you can push any metric type to the Pushgateway.

Note that we use the
`grouping_key`
argument to distinguish between different instances of the same
metric. This is useful when you have multiple instances of the same app pushing metrics to the Pushgateway.
Without this, the Pushgateway will overwrite the metric with the latest value.

```
client_image = modal.Image.debian_slim().pip_install(
    "prometheus-client==0.20.0", "fastapi[standard]==0.115.4"
)
app = modal.App(
    "example-pushgateway",
    image=client_image,
)

with client_image.imports():
    from prometheus_client import (
        CollectorRegistry,
        Counter,
        delete_from_gateway,
        push_to_gateway,
    )

@app.cls(min_containers=3)
class ExampleClientApplication:
    @modal.enter()
    def init(self):
        self.registry = CollectorRegistry()
        self.web_url = serve.get_web_url()
        self.instance_id = os.environ["MODAL_TASK_ID"]
        self.counter = Counter(
            "hello_counter",
            "This is a counter",
            registry=self.registry,
        )

    # We must explicitly clean up the metric when the app exits so Prometheus doesn't
    # keep stale metrics around.
    @modal.exit()
    def cleanup(self):
        delete_from_gateway(
            self.web_url,
            job="hello",
            grouping_key={"instance": self.instance_id},
        )

    @modal.fastapi_endpoint(label="hello-pushgateway")
    def hello(self):
        self.counter.inc()
        push_to_gateway(
            self.web_url,
            job="hello",
            grouping_key={"instance": self.instance_id},
            registry=self.registry,
        )
        return f"Hello world from {self.instance_id}!"

app.include(gw_app)
```

Now, we can deploy the app and see the metrics in the Pushgateway’s web interface.

```
$ modal deploy pushgateway.py
✓ Created objects.
├── 🔨 Created mount /home/ec2-user/modal/examples/10_integrations/pushgateway.py
├── 🔨 Created function ExampleClientApplication.*.
├── 🔨 Created web function serve => https://modal-labs-examples--example-pushgateway-serve.modal.run
└── 🔨 Created web endpoint for ExampleClientApplication.hello => https://modal-labs-examples--hello-pushgateway.modal.run
✓ App deployed! 🎉
```

You can now go to both the
[client application](https://modal-labs-examples--hello-pushgateway.modal.run)

and
[Pushgateway](https://modal-labs-examples--example-pushgateway-serve.modal.run)

URLs to see the metrics being pushed.

Hooking up Prometheus
---------------------

Now that we have metrics in the Pushgateway, we can configure Prometheus to scrape them. This
is as simple as adding a new job to your Prometheus configuration. Here’s an example configuration
snippet:

```
scrape_configs:
- job_name: 'pushgateway'
  honor_labels: true # required so that the instance label is preserved
  static_configs:
  - targets: ['modal-labs-examples--example-pushgateway-serve.modal.run']
```

Note that the target will be different if you have a custom domain set up for the Pushgateway,
and you may need to configure authentication.

Once you’ve added the job to your Prometheus configuration, Prometheus will start scraping metrics
from the Pushgateway. You can then use Grafana or another visualization tool to create dashboards
and alerts based on these metrics!

![Grafana example](/_app/immutable/assets/pushgateway_grafana.BAU68-Sh.png)

[Publish custom metrics with Prometheus Pushgateway](#publish-custom-metrics-with-prometheus-pushgateway)

[Install Prometheus Pushgateway](#install-prometheus-pushgateway)

[Start the Pushgateway](#start-the-pushgateway)

[Push metrics to the Pushgateway](#push-metrics-to-the-pushgateway)

[Hooking up Prometheus](#hooking-up-prometheus)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 10_integrations/pushgateway.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/s3_bucket_mount
================================================================================

Analyze NYC yellow taxi data with DuckDB on Parquet files from S3
=================================================================

This example shows how to use Modal for a classic data science task: loading table-structured data into cloud stores,
analyzing it, and plotting the results.

In particular, we’ll load public NYC taxi ride data into S3 as Parquet files,
then run SQL queries on it with DuckDB.

We’ll mount the S3 bucket in a Modal app with
[`CloudBucketMount`](https://modal.com/docs/reference/modal.CloudBucketMount)

.
We will write to and then read from that bucket, in each case using
Modal’s
[parallel execution features](https://modal.com/docs/guide/scale)

to handle many files at once.

Basic setup
-----------

You will need to have an S3 bucket and AWS credentials to run this example. Refer to the documentation
for the exact
[IAM permissions](https://modal.com/docs/guide/cloud-bucket-mounts#iam-permissions)

your credentials will need.

After you are done creating a bucket and configuring IAM settings,
you now need to create a
[`Secret`](https://modal.com/docs/guide/secrets)

to share
the relevant AWS credentials with your Modal apps.

```
from datetime import datetime
from pathlib import Path, PosixPath

import modal

image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "requests==2.31.0", "duckdb==0.10.0", "matplotlib==3.8.3"
)
app = modal.App(image=image)

secret = modal.Secret.from_name(
    "s3-bucket-secret",
    required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"],
)

MOUNT_PATH = PosixPath("/bucket")
YELLOW_TAXI_DATA_PATH = MOUNT_PATH / "yellow_taxi"
```

The dependencies installed above are not available locally. The following block instructs Modal
to only import them inside the container.

```
with image.imports():
    import duckdb
    import requests
```

Download New York City’s taxi data
----------------------------------

NYC makes data about taxi rides publicly available. The city’s
[Taxi & Limousine Commission (TLC)](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

publishes files in the Parquet format. Files are organized by year and month.

We are going to download all available files and store them in an S3 bucket. We do this by
attaching a
`modal.CloudBucketMount`
with the S3 bucket name and its respective credentials.
The files in the bucket will then be available at
`MOUNT_PATH`
.

As we’ll see below, this operation can be massively sped up by running it in parallel on Modal.

```
@app.function(
    volumes={
        MOUNT_PATH: modal.CloudBucketMount("modal-s3mount-test-bucket", secret=secret),
    },
)
def download_data(year: int, month: int) -> str:
    filename = f"yellow_tripdata_{year}-{month:02d}.parquet"
    url = f"https://d37ci6vzurychx.cloudfront.net/trip-data/{filename}"
    s3_path = MOUNT_PATH / filename
    # Skip downloading if file exists.
    if not s3_path.exists():
        if not YELLOW_TAXI_DATA_PATH.exists():
            YELLOW_TAXI_DATA_PATH.mkdir(parents=True, exist_ok=True)
            with requests.get(url, stream=True) as r:
                r.raise_for_status()
                print(f"downloading => {s3_path}")
                # It looks like we writing locally, but this is actually writing to S3!
                with open(s3_path, "wb") as file:
                    for chunk in r.iter_content(chunk_size=8192):
                        file.write(chunk)

    return s3_path.as_posix()
```

Analyze data with DuckDB
------------------------

[DuckDB](https://duckdb.org/)

is an analytical database with rich support for Parquet files.
It is also very fast. Below, we define a Modal Function that aggregates yellow taxi trips
within a month (each file contains all the rides from a specific month).

```
@app.function(
    volumes={
        MOUNT_PATH: modal.CloudBucketMount(
            "modal-s3mount-test-bucket",
            secret=modal.Secret.from_name("s3-bucket-secret"),
        )
    },
)
def aggregate_data(path: str) -> list[tuple[datetime, int]]:
    print(f"processing => {path}")

    # Parse file.
    year_month_part = path.split("yellow_tripdata_")[1]
    year, month = year_month_part.split("-")
    month = month.replace(".parquet", "")

    # Make DuckDB query using in-memory storage.
    con = duckdb.connect(database=":memory:")
    q = """
    with sub as (
        select tpep_pickup_datetime::date d, count(1) c
        from read_parquet(?)
        group by 1
    )
    select d, c from sub
    where date_part('year', d) = ?  -- filter out garbage
    and date_part('month', d) = ?   -- same
    """
    con.execute(q, (path, year, month))
    return list(con.fetchall())
```

Plot daily taxi rides
---------------------

Finally, we want to plot our results.
The plot created shows the number of yellow taxi rides per day in NYC.
This function runs remotely, on Modal, so we don’t need to install plotting libraries locally.

```
@app.function()
def plot(dataset) -> bytes:
    import io

    import matplotlib.pyplot as plt

    # Sorting data by date
    dataset.sort(key=lambda x: x[0])

    # Unpacking dates and values
    dates, values = zip(*dataset)

    # Plotting
    plt.figure(figsize=(10, 6))
    plt.plot(dates, values)
    plt.title("Number of NYC yellow taxi trips by weekday, 2018-2023")
    plt.ylabel("Number of daily trips")
    plt.grid(True)
    plt.tight_layout()

    # Saving plot as raw bytes to send back
    buf = io.BytesIO()

    plt.savefig(buf, format="png")

    buf.seek(0)

    return buf.getvalue()
```

Run everything
--------------

The
`@app.local_entrypoint()`
defines what happens when we run our Modal program locally.
We invoke it from the CLI by calling
`modal run s3_bucket_mount.py`
.
We first call
`download_data()`
and
`starmap`
(named because it’s kind of like
`map(*args)`
)
on tuples of inputs
`(year, month)`
. This will download, in parallel,
all yellow taxi data files into our locally mounted S3 bucket and return a list of
Parquet file paths. Then, we call
`aggregate_data()`
with
`map`
on that list. These files are
also read from our S3 bucket. So one function writes files to S3 and the other
reads files from S3 in; both run across many files in parallel.

Finally, we call
`plot`
to generate the following figure:

![Number of NYC yellow taxi trips by weekday, 2018-2023](/_app/immutable/assets/nyc_yellow_taxi_trips_s3_mount.DW1A9-sb.png)

This program should run in less than 30 seconds.

```
@app.local_entrypoint()
def main():
    # List of tuples[year, month].
    inputs = [(year, month) for year in range(2018, 2023) for month in range(1, 13)]

    # List of file paths in S3.
    parquet_files: list[str] = []
    for path in download_data.starmap(inputs):
        print(f"done => {path}")
        parquet_files.append(path)

    # List of datetimes and number of yellow taxi trips.
    dataset = []
    for r in aggregate_data.map(parquet_files):
        dataset += r

    dir = Path("/tmp") / "s3_bucket_mount"
    if not dir.exists():
        dir.mkdir(exist_ok=True, parents=True)

    figure = plot.remote(dataset)
    path = dir / "nyc_yellow_taxi_trips_s3_mount.png"
    with open(path, "wb") as file:
        print(f"Saving figure to {path}")
        file.write(figure)
```

[Analyze NYC yellow taxi data with DuckDB on Parquet files from S3](#analyze-nyc-yellow-taxi-data-with-duckdb-on-parquet-files-from-s3)

[Basic setup](#basic-setup)

[Download New York City’s taxi data](#download-new-york-citys-taxi-data)

[Analyze data with DuckDB](#analyze-data-with-duckdb)

[Plot daily taxi rides](#plot-daily-taxi-rides)

[Run everything](#run-everything)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/s3_bucket_mount.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/safe_code_execution
================================================================================

Run arbitrary code in a sandboxed environment
=============================================

This example demonstrates how to run arbitrary code
in multiple languages in a Modal
[Sandbox](https://modal.com/docs/guide/sandbox)

.

Setting up a multi-language environment
---------------------------------------

Sandboxes allow us to run any kind of code in a safe environment.
We’ll use an image with a few different language runtimes to demonstrate this.

```
import modal

image = modal.Image.debian_slim(python_version="3.11").apt_install(
    "nodejs", "ruby", "php"
)
app = modal.App.lookup("safe-code-execution", create_if_missing=True)
```

We’ll now create a Sandbox with this image. We’ll also enable output so we can see the image build
logs. Note that we don’t pass any commands to the Sandbox, so it will stay alive, waiting for us
to send it commands.

```
with modal.enable_output():
    sandbox = modal.Sandbox.create(app=app, image=image)

print(f"Sandbox ID: {sandbox.object_id}")
```

Running bash, Python, Node.js, Ruby, and PHP in a Sandbox
---------------------------------------------------------

We can now use
[`Sandbox.exec`](https://modal.com/docs/reference/modal.Sandbox#exec)

to run a few different
commands in the Sandbox.

```
bash_ps = sandbox.exec("echo", "hello from bash")
python_ps = sandbox.exec("python", "-c", "print('hello from python')")
nodejs_ps = sandbox.exec("node", "-e", 'console.log("hello from nodejs")')
ruby_ps = sandbox.exec("ruby", "-e", "puts 'hello from ruby'")
php_ps = sandbox.exec("php", "-r", "echo 'hello from php';")

print(bash_ps.stdout.read(), end="")
print(python_ps.stdout.read(), end="")
print(nodejs_ps.stdout.read(), end="")
print(ruby_ps.stdout.read(), end="")
print(php_ps.stdout.read(), end="")
print()
```

The output should look something like

```
hello from bash
hello from python
hello from nodejs
hello from ruby
hello from php
```

We can use multiple languages in tandem to build complex applications.
Let’s demonstrate this by piping data between Python and Node.js using bash. Here
we generate some random numbers with Python and sum them with Node.js.

```
combined_process = sandbox.exec(
    "bash",
    "-c",
    """python -c 'import random; print(\" \".join(str(random.randint(1, 100)) for _ in range(10)))' |
    node -e 'const readline = require(\"readline\");
    const rl = readline.createInterface({input: process.stdin});
    rl.on(\"line\", (line) => {
      const sum = line.split(\" \").map(Number).reduce((a, b) => a + b, 0);
      console.log(`The sum of the random numbers is: ${sum}`);
      rl.close();
    });'""",
)

result = combined_process.stdout.read().strip()
print(result)
```

For long-running processes, you can use stdout as an iterator to stream the output.

```
slow_printer = sandbox.exec(
    "ruby",
    "-e",
    """
    10.times do |i|
      puts "Line #{i + 1}: #{Time.now}"
      STDOUT.flush
      sleep(0.5)
    end
    """,
)

for line in slow_printer.stdout:
    print(line, end="")
```

This should print something like

```
Line 1: 2024-10-21 15:30:53 +0000
Line 2: 2024-10-21 15:30:54 +0000
...
Line 10: 2024-10-21 15:30:58 +0000
```

Since Sandboxes are safely separated from the rest of our system,
we can run very dangerous code in them!

```
sandbox.exec("rm", "-rfv", "/", "--no-preserve-root")
```

This command has deleted the entire filesystem, so we can’t run any more commands.
Let’s terminate the Sandbox to clean up after ourselves.

```
sandbox.terminate()
```

[Run arbitrary code in a sandboxed environment](#run-arbitrary-code-in-a-sandboxed-environment)

[Setting up a multi-language environment](#setting-up-a-multi-language-environment)

[Running bash, Python, Node.js, Ruby, and PHP in a Sandbox](#running-bash-python-nodejs-ruby-and-php-in-a-sandbox)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
python 13_sandboxes/safe_code_execution.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/segment_anything
================================================================================

Run Facebook’s Segment Anything Model 2 (SAM 2) on Modal
========================================================

This example demonstrates how to deploy Facebook’s
[SAM 2](https://github.com/facebookresearch/sam2)

on Modal. SAM2 is a powerful, flexible image and video segmentation model that can be used
for various computer vision tasks like object detection, instance segmentation,
and even as a foundation for more complex computer vision applications.
SAM2 extends the capabilities of the original SAM to include video segmentation.

In particular, this example segments
[this video](https://www.youtube.com/watch?v=WAz1406SjVw)

of a man jumping off the cliff.

The output should look something like this:

[

](https://modal-cdn.com/example-segmented-video.mp4)

Set up dependencies for SAM 2
-----------------------------

First, we set up the necessary dependencies, including
`torch`
,
`opencv`
,
`huggingface_hub`
,
`torchvision`
, and the
`sam2`
library.

We also install
`ffmpeg`
, which we will use to manipulate videos,
and a Python wrapper called
`ffmpeg-python`
for a clean interface.

```
from pathlib import Path

import modal

MODEL_TYPE = "facebook/sam2-hiera-large"
SAM2_GIT_SHA = (
    "c2ec8e14a185632b0a5d8b161928ceb50197eddc"  # pin commit! research code is fragile
)

image = (
    modal.Image.debian_slim(python_version="3.10")
    .apt_install("git", "wget", "python3-opencv", "ffmpeg")
    .pip_install(
        "torch~=2.4.1",
        "torchvision==0.19.1",
        "opencv-python==4.10.0.84",
        "pycocotools~=2.0.8",
        "matplotlib~=3.9.2",
        "onnxruntime==1.19.2",
        "onnx==1.17.0",
        "huggingface_hub==0.25.2",
        "ffmpeg-python==0.2.0",
        f"git+https://github.com/facebookresearch/sam2.git@{SAM2_GIT_SHA}",
    )
)
app = modal.App("sam2-app", image=image)
```

Wrapping the SAM 2 model in a Modal class
-----------------------------------------

Next, we define the
`Model`
class that will handle SAM 2 operations for both image and video.

We use the
`@modal.enter()`
decorators here for optimization: it makes sure the initialization
method runs only once, when a new container starts, instead of in the path of every call.
We’ll also use a modal Volume to cache the model weights so that they don’t need to be downloaded
repeatedly when we start new containers.

```
video_vol = modal.Volume.from_name("sam2-inputs", create_if_missing=True)
cache_vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)
cache_dir = "/cache"

@app.cls(
    image=image.env({"HF_HUB_CACHE": cache_dir}),
    volumes={"/root/videos": video_vol, cache_dir: cache_vol},
    gpu="A100",
)
class Model:
    @modal.enter()
    def initialize_model(self):
        """Download and initialize model."""
        from sam2.sam2_video_predictor import SAM2VideoPredictor

        self.video_predictor = SAM2VideoPredictor.from_pretrained(MODEL_TYPE)

    @modal.method()
    def generate_video_masks(self, video="/root/videos/input.mp4", point_coords=None):
        """Generate masks for a video."""
        import ffmpeg
        import numpy as np
        import torch
        from PIL import Image

        frames_dir = convert_video_to_frames(video)

        # scan all the JPEG files in this directory
        frame_names = [
            p
            for p in frames_dir.iterdir()
            if p.suffix in [".jpg", ".jpeg", ".JPG", ".JPEG"]
        ]
        frame_names.sort(key=lambda p: int(p.stem))

        # We are hardcoding the input point and label here
        # In a real-world scenario, you would want to display the video
        # and allow the user to click on the video to select the point
        if point_coords is None:
            width, height = Image.open(frame_names[0]).size
            point_coords = [[width // 2, height // 2]]

        points = np.array(point_coords, dtype=np.float32)
        # for labels, `1` means positive click and `0` means negative click
        labels = np.array([1] * len(points), np.int32)

        # run the model on GPU
        with (
            torch.inference_mode(),
            torch.autocast("cuda", dtype=torch.bfloat16),
        ):
            self.inference_state = self.video_predictor.init_state(
                video_path=str(frames_dir)
            )

            # add new prompts and instantly get the output on the same frame
            (
                frame_idx,
                object_ids,
                masks,
            ) = self.video_predictor.add_new_points_or_box(
                inference_state=self.inference_state,
                frame_idx=0,
                obj_id=1,
                points=points,
                labels=labels,
            )

            print(f"frame_idx: {frame_idx}, object_ids: {object_ids}, masks: {masks}")

            # run propagation throughout the video and collect the results in a dict
            video_segments = {}  # video_segments contains the per-frame segmentation results
            for (
                out_frame_idx,
                out_obj_ids,
                out_mask_logits,
            ) in self.video_predictor.propagate_in_video(self.inference_state):
                video_segments[out_frame_idx] = {
                    out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()
                    for i, out_obj_id in enumerate(out_obj_ids)
                }

        out_dir = Path("/root/mask_frames")
        out_dir.mkdir(exist_ok=True)

        vis_frame_stride = 5  # visualize every 5th frame
        save_segmented_frames(
            video_segments,
            frames_dir,
            out_dir,
            frame_names,
            stride=vis_frame_stride,
        )

        ffmpeg.input(
            f"{out_dir}/frame_*.png",
            pattern_type="glob",
            framerate=30 / vis_frame_stride,
        ).filter(
            "scale",
            "trunc(iw/2)*2",
            "trunc(ih/2)*2",  # round to even dimensions to encode for "dumb players", https://trac.ffmpeg.org/wiki/Encode/H.264#Encodingfordumbplayers
        ).output(str(out_dir / "out.mp4"), format="mp4", pix_fmt="yuv420p").run()

        return (out_dir / "out.mp4").read_bytes()
```

Segmenting videos from the command line
---------------------------------------

Finally, we define a
[`local_entrypoint`](https://modal.com/docs/guide/apps#entrypoints-for-ephemeral-apps)

to run the segmentation from our local machine’s terminal.

There are several ways to pass files between the local machine and the Modal Function.

One way is to upload the files onto a Modal
[Volume](https://modal.com/docs/guide/volumes)

,
which acts as a distributed filesystem.

The other way is to convert the file to bytes and pass the bytes back and forth as the input or output of Python functions.
We use this method to get the video file with the segmentation results in it back to the local machine.

```
@app.local_entrypoint()
def main(
    input_video=Path(__file__).parent / "cliff_jumping.mp4",
    x_point=250,
    y_point=200,
):
    with video_vol.batch_upload(force=True) as batch:
        batch.put_file(input_video, "input.mp4")

    model = Model()

    if x_point is not None and y_point is not None:
        point_coords = [[x_point, y_point]]
    else:
        point_coords = None

    print(f"Running SAM 2 on {input_video}")
    video_bytes = model.generate_video_masks.remote(point_coords=point_coords)

    dir = Path("/tmp/sam2_outputs")
    dir.mkdir(exist_ok=True, parents=True)
    output_path = dir / "segmented_video.mp4"
    output_path.write_bytes(video_bytes)
    print(f"Saved output video to {output_path}")
```

Helper functions for SAM 2 inference
------------------------------------

Above, we used some helper functions to for some of the details, like breaking the video into frames.
These are defined below.

```
def convert_video_to_frames(self, input_video="/root/videos/input.mp4"):
    import ffmpeg

    input_video = Path(input_video)
    output_dir = (  # output on local filesystem, not on the remote Volume
        input_video.parent.parent / input_video.stem / "video_frames"
    )
    output_dir.mkdir(exist_ok=True, parents=True)

    ffmpeg.input(input_video).output(
        f"{output_dir}/%05d.jpg", qscale=2, start_number=0
    ).run()

    return output_dir

def show_mask(mask, ax, obj_id=None, random_color=False):
    import matplotlib.pyplot as plt
    import numpy as np

    if random_color:
        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)
    else:
        cmap = plt.get_cmap("tab10")
        cmap_idx = 0 if obj_id is None else obj_id
        color = np.array([*cmap(cmap_idx)[:3], 0.6])
    h, w = mask.shape[-2:]
    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)
    ax.imshow(mask_image)

def save_segmented_frames(video_segments, frames_dir, out_dir, frame_names, stride=5):
    import io

    import matplotlib.pyplot as plt
    from PIL import Image

    frames_dir, out_dir = Path(frames_dir), Path(out_dir)

    frame_images = []
    inches_per_px = 1 / plt.rcParams["figure.dpi"]
    for out_frame_idx in range(0, len(frame_names), stride):
        frame = Image.open(frames_dir / frame_names[out_frame_idx])
        width, height = frame.size
        width, height = width - width % 2, height - height % 2
        fig, ax = plt.subplots(figsize=(width * inches_per_px, height * inches_per_px))
        ax.axis("off")
        ax.imshow(frame)

        [
            show_mask(mask, ax, obj_id=obj_id)
            for (obj_id, mask) in video_segments[out_frame_idx].items()
        ]

        # Convert plot to PNG bytes
        buf = io.BytesIO()
        fig.savefig(buf, format="png", bbox_inches="tight", pad_inches=0)
        # fig.savefig(buf, format="png")
        buf.seek(0)
        frame_images.append(buf.getvalue())
        plt.close(fig)

    for ii, frame in enumerate(frame_images):
        (out_dir / f"frame_{str(ii).zfill(3)}.png").write_bytes(frame)
```

[Run Facebook’s Segment Anything Model 2 (SAM 2) on Modal](#run-facebooks-segment-anything-model-2-sam-2-on-modal)

[Set up dependencies for SAM 2](#set-up-dependencies-for-sam-2)

[Wrapping the SAM 2 model in a Modal class](#wrapping-the-sam-2-model-in-a-modal-class)

[Segmenting videos from the command line](#segmenting-videos-from-the-command-line)

[Helper functions for SAM 2 inference](#helper-functions-for-sam-2-inference)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/sam/segment_anything.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/serve_streamlit
================================================================================

Run and share Streamlit apps
============================

This example shows you how to run a Streamlit app with
`modal serve`
, and then deploy it as a serverless web app.

![example streamlit app](/_app/immutable/assets/streamlit.RHfhqFCX.png)

This example is structured as two files:

1. This module, which defines the Modal objects (name the script
   `serve_streamlit.py`
   locally).
2. `app.py`
   , which is any Streamlit script to be mounted into the Modal
   function (
   [download script](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/streamlit/app.py)

   ).

```
import shlex
import subprocess
from pathlib import Path

import modal
```

Define container dependencies
-----------------------------

The
`app.py`
script imports three third-party packages, so we include these in the example’s
image definition and then add the
`app.py`
file itself to the image.

```
streamlit_script_local_path = Path(__file__).parent / "app.py"
streamlit_script_remote_path = "/root/app.py"

image = (
    modal.Image.debian_slim(python_version="3.11")
    .pip_install("streamlit~=1.35.0", "numpy~=1.26.4", "pandas~=2.2.2")
    .add_local_file(
        streamlit_script_local_path,
        streamlit_script_remote_path,
    )
)

app = modal.App(name="example-modal-streamlit", image=image)

if not streamlit_script_local_path.exists():
    raise RuntimeError(
        "app.py not found! Place the script with your streamlit app in the same directory."
    )
```

Spawning the Streamlit server
-----------------------------

Inside the container, we will run the Streamlit server in a background subprocess using
`subprocess.Popen`
. We also expose port 8000 using the
`@web_server`
decorator.

```
@app.function()
@modal.concurrent(max_inputs=100)
@modal.web_server(8000)
def run():
    target = shlex.quote(streamlit_script_remote_path)
    cmd = f"streamlit run {target} --server.port 8000 --server.enableCORS=false --server.enableXsrfProtection=false"
    subprocess.Popen(cmd, shell=True)
```

Iterate and Deploy
------------------

While you’re iterating on your screamlit app, you can run it “ephemerally” with
`modal serve`
. This will
run a local process that watches your files and updates the app if anything changes.

```
modal serve serve_streamlit.py
```

Once you’re happy with your changes, you can deploy your application with

```
modal deploy serve_streamlit.py
```

If successful, this will print a URL for your app that you can navigate to from
your browser 🎉 .

[Run and share Streamlit apps](#run-and-share-streamlit-apps)

[Define container dependencies](#define-container-dependencies)

[Spawning the Streamlit server](#spawning-the-streamlit-server)

[Iterate and Deploy](#iterate-and-deploy)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 10_integrations/streamlit/serve_streamlit.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/sgl_vlm
================================================================================

Run Qwen2-VL on SGLang for Visual QA
====================================

Vision-Language Models (VLMs) are like LLMs with eyes:
they can generate text based not just on other text,
but on images as well.

This example shows how to run a VLM on Modal using the
[SGLang](https://github.com/sgl-project/sglang)

library.

Here’s a sample inference, with the image rendered directly (and at low resolution) in the terminal:

![Sample output answering a question about a photo of the Statue of Liberty](https://modal-public-assets.s3.amazonaws.com/sgl_vlm_qa_sol.png)

Setup
-----

First, we’ll import the libraries we need locally
and define some constants.

```
import os
import time
import warnings
from pathlib import Path
from typing import Optional
from uuid import uuid4

import modal
```

VLMs are generally larger than LLMs with the same cognitive capability.
LLMs are already hard to run effectively on CPUs, so we’ll use a GPU here.
We find that inference for a single input takes about 3-4 seconds on an A10G.

You can customize the GPU type and count using the
`GPU_TYPE`
and
`GPU_COUNT`
environment variables.
If you want to see the model really rip, try an
`"a100-80gb"`
or an
`"h100"`
on a large batch.

```
GPU_TYPE = os.environ.get("GPU_TYPE", "l40s")
GPU_COUNT = os.environ.get("GPU_COUNT", 1)

GPU_CONFIG = f"{GPU_TYPE}:{GPU_COUNT}"

SGL_LOG_LEVEL = "error"  # try "debug" or "info" if you have issues

MINUTES = 60  # seconds
```

We use the
[Qwen2-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)

model by Alibaba.

```
MODEL_PATH = "Qwen/Qwen2-VL-7B-Instruct"
MODEL_REVISION = "a7a06a1cc11b4514ce9edcde0e3ca1d16e5ff2fc"
TOKENIZER_PATH = "Qwen/Qwen2-VL-7B-Instruct"
MODEL_CHAT_TEMPLATE = "qwen2-vl"
```

We download it from the Hugging Face Hub using the Python function below.
We’ll store it in a
[Modal Volume](https://modal.com/docs/guide/volumes)

so that it’s not downloaded every time the container starts.

```
MODEL_VOL_PATH = Path("/models")
MODEL_VOL = modal.Volume.from_name("sgl-cache", create_if_missing=True)
volumes = {MODEL_VOL_PATH: MODEL_VOL}

def download_model():
    from huggingface_hub import snapshot_download

    snapshot_download(
        MODEL_PATH,
        local_dir=str(MODEL_VOL_PATH / MODEL_PATH),
        revision=MODEL_REVISION,
        ignore_patterns=["*.pt", "*.bin"],
    )
```

Modal runs Python functions on containers in the cloud.
The environment those functions run in is defined by the container’s
`Image`
.
The block of code below defines our example’s
`Image`
.

```
cuda_version = "12.8.0"  # should be no greater than host CUDA version
flavor = "devel"  #  includes full CUDA toolkit
operating_sys = "ubuntu22.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"

vlm_image = (
    modal.Image.from_registry(f"nvidia/cuda:{tag}", add_python="3.11")
    .entrypoint([])  # removes chatty prints on entry
    .apt_install("libnuma-dev")  # Add NUMA library for sgl_kernel
    .pip_install(  # add sglang and some Python dependencies
        "transformers==4.52.3",
        "numpy<2",
        "fastapi[standard]==0.115.4",
        "pydantic==2.9.2",
        "requests==2.32.3",
        "starlette==0.41.2",
        "torch==2.7.1",
        "sglang[all]==0.4.8",
        "sgl-kernel==0.1.9",
        "hf-xet==1.1.5",
    )
    .env(
        {
            "HF_HOME": str(MODEL_VOL_PATH),
            "HF_HUB_ENABLE_HF_TRANSFER": "1",
        }
    )
    .run_function(  # download the model by running a Python function
        download_model, volumes=volumes
    )
    .pip_install(  # add an optional extra that renders images in the terminal
        "term-image==0.7.1"
    )
)
```

Defining a Visual QA service
----------------------------

Running an inference service on Modal is as easy as writing inference in Python.

The code below adds a modal
`Cls`
to an
`App`
that runs the VLM.

We define a method
`generate`
that takes a URL for an image and a question
about the image as inputs and returns the VLM’s answer.

By decorating it with
`@modal.fastapi_endpoint`
, we expose it as an HTTP endpoint,
so it can be accessed over the public Internet from any client.

```
app = modal.App("example-sgl-vlm")

@app.cls(
    gpu=GPU_CONFIG,
    timeout=20 * MINUTES,
    scaledown_window=20 * MINUTES,
    image=vlm_image,
    volumes=volumes,
)
@modal.concurrent(max_inputs=100)
class Model:
    @modal.enter()  # what should a container do after it starts but before it gets input?
    def start_runtime(self):
        """Starts an SGL runtime to execute inference."""
        import sglang as sgl

        self.runtime = sgl.Runtime(
            model_path=MODEL_PATH,
            tokenizer_path=TOKENIZER_PATH,
            tp_size=GPU_COUNT,  # t_ensor p_arallel size, number of GPUs to split the model over
            log_level=SGL_LOG_LEVEL,
        )
        self.runtime.endpoint.chat_template = sgl.lang.chat_template.get_chat_template(
            MODEL_CHAT_TEMPLATE
        )
        sgl.set_default_backend(self.runtime)

    @modal.fastapi_endpoint(method="POST", docs=True)
    def generate(self, request: dict) -> str:
        from pathlib import Path

        import requests
        import sglang as sgl
        from term_image.image import from_file

        start = time.monotonic_ns()
        request_id = uuid4()
        print(f"Generating response to request {request_id}")

        image_url = request.get("image_url")
        if image_url is None:
            image_url = (
                "https://modal-public-assets.s3.amazonaws.com/golden-gate-bridge.jpg"
            )

        response = requests.get(image_url)
        response.raise_for_status()

        image_filename = image_url.split("/")[-1]
        image_path = Path(f"/tmp/{uuid4()}-{image_filename}")
        image_path.write_bytes(response.content)

        @sgl.function
        def image_qa(s, image_path, question):
            s += sgl.user(sgl.image(str(image_path)) + question)
            s += sgl.assistant(sgl.gen("answer"))

        question = request.get("question")
        if question is None:
            question = "What is this?"

        state = image_qa.run(
            image_path=image_path, question=question, max_new_tokens=128
        )
        # show the question and image in the terminal for demonstration purposes
        print(Colors.BOLD, Colors.GRAY, "Question: ", question, Colors.END, sep="")
        terminal_image = from_file(image_path)
        terminal_image.draw()
        print(
            f"request {request_id} completed in {round((time.monotonic_ns() - start) / 1e9, 2)} seconds"
        )

        return state["answer"]

    @modal.exit()  # what should a container do before it shuts down?
    def shutdown_runtime(self):
        self.runtime.shutdown()
```

Asking questions about images via POST
--------------------------------------

Now, we can send this Modal Function a POST request with an image and a question
and get back an answer.

The code below will start up the inference service
so that it can be run from the terminal as a one-off,
like a local script would be, using
`modal run`
:

```
modal run sgl_vlm.py
```

By default, we hit the endpoint twice to demonstrate how much faster
the inference is once the server is running.

```
@app.local_entrypoint()
def main(
    image_url: Optional[str] = None, question: Optional[str] = None, twice: bool = True
):
    import json
    import urllib.request

    model = Model()

    payload = json.dumps(
        {
            "image_url": image_url,
            "question": question,
        },
    )

    req = urllib.request.Request(
        model.generate.get_web_url(),
        data=payload.encode("utf-8"),
        headers={"Content-Type": "application/json"},
        method="POST",
    )

    with urllib.request.urlopen(req) as response:
        assert response.getcode() == 200, response.getcode()
        print(json.loads(response.read().decode()))

    if twice:
        # second response is faster, because the Function is already running
        with urllib.request.urlopen(req) as response:
            assert response.getcode() == 200, response.getcode()
            print(json.loads(response.read().decode()))
```

Deployment
----------

To set this up as a long-running, but serverless, service, we can deploy it to Modal:

```
modal deploy sgl_vlm.py
```

And then send requests from anywhere. See the
[docs](https://modal.com/docs/guide/webhook-urls)

for details on the
`web_url`
of the function, which also appears in the terminal output
when running
`modal deploy`
.

You can also find interactive documentation for the endpoint at the
`/docs`
route of the web endpoint URL.

Addenda
-------

The rest of the code in this example is just utility code.

```
warnings.filterwarnings(  # filter warning from the terminal image library
    "ignore",
    message="It seems this process is not running within a terminal. Hence, some features will behave differently or be disabled.",
    category=UserWarning,
)

class Colors:
    """ANSI color codes"""

    GREEN = "\033[0;32m"
    BLUE = "\033[0;34m"
    GRAY = "\033[0;90m"
    BOLD = "\033[1m"
    END = "\033[0m"
```

[Run Qwen2-VL on SGLang for Visual QA](#run-qwen2-vl-on-sglang-for-visual-qa)

[Setup](#setup)

[Defining a Visual QA service](#defining-a-visual-qa-service)

[Asking questions about images via POST](#asking-questions-about-images-via-post)

[Deployment](#deployment)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-serving/sgl_vlm.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/simple_code_interpreter
================================================================================

Build a stateful, sandboxed code interpreter
============================================

This example demonstrates how to build a stateful code interpreter using a Modal
[Sandbox](https://modal.com/docs/guide/sandbox)

.

We’ll create a Modal Sandbox that listens for code to execute and then
executes the code in a Python interpreter. Because we’re running in a sandboxed
environment, we can safely use the “unsafe”
`exec()`
to execute the code.

Setting up a code interpreter in a Modal Sandbox
------------------------------------------------

Our code interpreter uses a Python “driver program” to listen for code
sent in JSON format to its standard input (
`stdin`
), execute the code,
and then return the results in JSON format on standard output (
`stdout`
).

```
import inspect
import json
from typing import Any

import modal
import modal.container_process

def driver_program():
    import json
    import sys
    from contextlib import redirect_stderr, redirect_stdout
    from io import StringIO

    # When you `exec` code in Python, you can pass in a dictionary
    # that defines the global variables the code has access to.

    # We'll use that to store state.

    globals: dict[str, Any] = {}
    while True:
        command = json.loads(input())  # read a line of JSON from stdin
        if (code := command.get("code")) is None:
            print(json.dumps({"error": "No code to execute"}))
            continue

        # Capture the executed code's outputs
        stdout_io, stderr_io = StringIO(), StringIO()
        with redirect_stdout(stdout_io), redirect_stderr(stderr_io):
            try:
                exec(code, globals)
            except Exception as e:
                print(f"Execution Error: {e}", file=sys.stderr)

        print(
            json.dumps(
                {
                    "stdout": stdout_io.getvalue(),
                    "stderr": stderr_io.getvalue(),
                }
            ),
            flush=True,
        )
```

Now that we have the driver program, we can write a function to take a
`ContainerProcess`
that is running the driver program and execute code in it.

```
def run_code(p: modal.container_process.ContainerProcess, code: str):
    p.stdin.write(json.dumps({"code": code}))
    p.stdin.write("\n")
    p.stdin.drain()
    next_line = next(iter(p.stdout))
    result = json.loads(next_line)
    print(result["stdout"], end="")
    print("\033[91m" + result["stderr"] + "\033[0m", end="")
```

We’ve got our driver program and our code runner. Now we can create a Sandbox
and run the driver program in it.

We have to convert the driver program to a string to pass it to the Sandbox.
Here we use
`inspect.getsource`
to get the source code as a string,
but you could also keep the driver program in a separate file and read it in.

```
driver_program_text = inspect.getsource(driver_program)
driver_program_command = f"""{driver_program_text}\n\ndriver_program()"""

app = modal.App.lookup("code-interpreter", create_if_missing=True)
sb = modal.Sandbox.create(app=app)
p = sb.exec("python", "-c", driver_program_command, bufsize=1)
```

Running code in a Modal Sandbox
-------------------------------

Now we can execute some code in the Sandbox!

```
run_code(p, "print('hello, world!')")  # hello, world!
```

The Sandbox and our code interpreter are stateful,
so we can define variables and use them in subsequent code.

```
run_code(p, "x = 10")
run_code(p, "y = 5")
run_code(p, "result = x + y")
run_code(p, "print(f'The result is: {result}')")  # The result is: 15
```

We can also see errors when code fails.

```
run_code(p, "print('Attempting to divide by zero...')")
run_code(p, "1 / 0")  # Execution Error: division by zero
```

Finally, let’s clean up after ourselves and terminate the Sandbox.

```
sb.terminate()
```

[Build a stateful, sandboxed code interpreter](#build-a-stateful-sandboxed-code-interpreter)

[Setting up a code interpreter in a Modal Sandbox](#setting-up-a-code-interpreter-in-a-modal-sandbox)

[Running code in a Modal Sandbox](#running-code-in-a-modal-sandbox)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
python 13_sandboxes/simple_code_interpreter.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/slack-finetune
================================================================================

DoppelBot: Fine-tune an LLM to replace your CEO
===============================================

*(quick links:
[add to your own Slack](https://github.com/modal-labs/doppel-bot#usage)

;
[source code](https://github.com/modal-labs/doppel-bot)

)*

Internally at Modal, we spend a
*lot*
of time talking to each other on Slack.
Now, with the advent of open-source large language models, we had started to
wonder if all of this wasn’t a bit redundant. Could we have these language
models bike-shed on Slack for us, so we could spend our time on higher leverage
activities such as
[paddleboarding in Tahiti](https://twitter.com/modal_labs/status/1642262543757352960)

instead?

To test this, we fine-tuned
[Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)

on
[Erik](https://twitter.com/bernhardsson)

’s Slack messages, and
`@erik-bot`
was
born.

![erik-bot](https://modal-cdn.com/erik-bot-1.jpeg)

Since then,
`@erik-bot`
has been an invaluable asset to us, in areas ranging
from
[API design](https://modal-cdn.com/erik-bot-2.png)

to
[legal advice](https://modal-cdn.com/erik-bot-3.png)

to thought leadership.

![erik-bot-3](https://modal-cdn.com/erik-bot-4.png)

We were planning on releasing the weights for
`@erik-bot`
to the world, but all
our metrics have been going up and to the right a little too much since we’ve
launched him…

So, we are releasing the next best thing.
`DoppelBot`
is a Slack bot that you
can install in your own workspace, and fine-tune on your own Slack messages.
Follow the instructions
[here](https://github.com/modal-labs/doppel-bot#usage)

to replace your own CEO with an LLM today.

All the components—scraping, fine-tuning, inference and slack event handlers run
on Modal, and the code itself is open-source and available
[here](https://github.com/modal-labs/doppel-bot)

. If you’re new to Modal, it’s
worth reiterating that
**all of these components are also serverless and scale
to zero**
. This means that you can deploy and forget about them, because you’ll
only pay for compute when your app is used!

How it works
------------

DoppelBot uses the Slack SDK to scrape messages from a Slack workspace, and
converts them into prompt/response pairs. It uses these to fine-tune a language
model using
[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685)

, a
technique that produces a small adapter that can be merged with the base model
when needed, instead of modifying all the parameters in the base model. The
fine-tuned adapters for each user are stored in a Modal
[Volume](/docs/guide/volumes)

. When a user
`@`
s the bot,
Slack sends a webhook call to Modal, which loads the adapter for that user and
generates a response.

We go into detail into each of these steps below, and provide commands for
running each of them individually. To follow along,
[clone the repo](https://github.com/modal-labs/doppel-bot)

and
[set up a Slack token](https://github.com/modal-labs/doppel-bot#create-a-slack-app)

for yourself.

### Scraping slack

The scraper uses Modal’s
[`.map()`](/docs/guide/scale#scaling-out)

to fetch
messages from all public channels in parallel. Each thread is split into
contiguous messages from the target users and continguous messages from other
users. These will be fed into the model as prompts in the following format:

```
[system]: You are {user}, employee at a fast-growing startup. Below is an input conversation that takes place in the company's internal Slack. Write a response that appropriately continues the conversation.

[user]: <slack thread>

[assistant]: <target user's response>
```

Initial versions of the model were prone to generating short responses
— unsurprising, because a majority of Slack communication is pretty terse.
Adding a minimum character length for the target user’s messages fixed this.

If you’re following along at home, you can run the scraper with the following
command:

```
modal run -m src.scrape::scrape --user="<user>"
```

Scraped results are stored in a Modal
[Volume](/docs/guide/volumes)

, so they can be used by the next step.

### Fine-tuning

Next, we use the prompts to fine-tune a language model. We chose
[Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)

because of its permissive license and high quality relative to its small size. Fine-tuning is
done using
[Low-Rank Adaptation (LoRA)](https://arxiv.org/abs/2106.09685)

, a
[parameter-efficient fine-tuning](https://huggingface.co/blog/peft)

technique
that produces a small adapter that can be merged with the base model when needed
(~60MB for the rank we’re using).

Our fine-tuning implementation uses
[torchtune](https://github.com/pytorch/torchtune)

, a new PyTorch library for easily configuring fine-tuning runs.

Because of the typically small sample sizes we’re working with, training for
longer than a couple hundred steps (with our batch size of 128) quickly led to
overfitting. Admittedly, we haven’t thoroughly evaluated the hyperparameter
space yet — do reach out to us if you’re interested in collaborating on this!

![train-loss](/_app/immutable/assets/train-loss.DFD7oOI8.png)

To try this step yourself, run:

```
modal run -m src.finetune --user="<user>"
```

### Inference

We use
[vLLM](https://github.com/vllm-project/vllm)

as our inference engine, which now comes with support for dynamically swapping LoRA adapters
[out of the box](https://docs.vllm.ai/en/latest/features/lora.html)

.

With parametrized functions, every user model gets its own pool of containers
that scales up when there are incoming requests, and scales to 0 when there’s
none. Here’s what that looks like stripped down to the essentials:

```
@app.cls(gpu="L40S")
class Model():
    @modal.enter()
    def enter(self):
        self.engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs(...))
        self.loras: dict[str, int] = dict()  # per replica LoRA identifier

    @method()
    def generate(self, input: str):
        if (ident := f"{user}-{team_id}") not in self.loras:
            self.loras[ident] = len(self.loras) + 1

        lora_request = LoRARequest(
            ident, self.loras[ident], lora_local_path=checkpoint_path
        )

        tokenizer = await self.engine.get_tokenizer(lora_request=lora_request)

        prompt = tokenizer.apply_chat_template(
            conversation=inpt, tokenize=False, add_generation_prompt=True
        )

        results_generator = self.engine.generate(prompt, lora_request=lora_request,)
```

If you’ve fine-tuned a model already in the previous step, you can run inference
using it now:

```
modal run -m src.inference --user="<user>"
```

(We have a list of sample inputs in the file, but you can also try it out with
your own messages!)

### Slack Bot

Finally, it all comes together in
[`bot.py`](https://github.com/modal-labs/doppel-bot/blob/main/src/bot.py)

. As
you might have guessed, all events from Slack are handled by serverless Modal
functions. We handle 3 types of events:

* [`url_verification`](https://github.com/modal-labs/doppel-bot/blob/24609583c43c0e722f56f85a1c00bb55b46c7754/src/bot.py#L112)

  :
  To verify that this is a Slack app, Slack expects us to return a challenge
  string.
* [`app_mention`](https://github.com/modal-labs/doppel-bot/blob/main/src/bot.py#L118)

  :
  When the bot is mentioned in a channel, we retrieve the recent messages from
  that thread, do some basic cleaning and call the user’s model to generate a
  response.

```
model = OpenLlamaModel.remote(user, team_id)
result = model.generate(messages)
```

* [`doppel`
  slash command](https://github.com/modal-labs/doppel-bot/blob/main/src/bot.py#L182)

  :
  This command kicks off the scraping -> finetuning pipeline for the user.

To deploy the slackbot in its entirety, you need to run:

```
modal deploy -m src.bot
```

### Multi-Workspace Support

Everything we’ve talked about so far is for a single-workspace Slack app. To
make it work with multiple workspaces, we’ll need to handle
[workspace installation and authentication with OAuth](https://api.slack.com/authentication/oauth-v2)

,
and also store some state for each workspace.

Luckily, Slack’s
[Bolt](https://slack.dev/bolt-python/concepts)

framework
provides a complete (but frugally documented) OAuth implemention. A neat feature
is that the OAuth state can be backed by a file system, so all we need to do is
[point Bolt](https://github.com/modal-labs/doppel-bot/blob/24609583c43c0e722f56f85a1c00bb55b46c7754/src/bot.py#L78)

at a Modal
[Volume](/docs/guide/volumes)

, and then we don’t need to worry about
managing this state ourselves.

To store state for each workspace, we’re using
[Neon](https://neon.tech/)

, a
serverless Postgres database that’s really easy to set up and
*just works*
. If
you’re interested in developing a multi-workspace app,
[follow our instructions](https://github.com/modal-labs/doppel-bot#optional-multi-workspace-app)

on how to set up Neon with Modal.

Next Steps
----------

If you’ve made it this far, you have just found a way to increase your team’s
productivity by 10x! Congratulations on the well-earned vacation! 🎉

If you’re interested in learning more about Modal, check out our
[docs](/docs)

and other
[examples](/examples)

.

[DoppelBot: Fine-tune an LLM to replace your CEO](#doppelbot-fine-tune-an-llm-to-replace-your-ceo)

[How it works](#how-it-works)

[Scraping slack](#scraping-slack)

[Fine-tuning](#fine-tuning)

[Inference](#inference)

[Slack Bot](#slack-bot)

[Multi-Workspace Support](#multi-workspace-support)

[Next Steps](#next-steps)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/streaming_kyutai_stt
================================================================================

Stream transcriptions with Kyutai STT
=====================================

This example demonstrates the deployment of a streaming audio transcription service with Kyutai STT on Modal.

[Kyutai STT](https://kyutai.org/next/stt)

is an automated speech recognition/transcription model
that is designed to operate on streams of audio, rather than on complete audio files.
See the linked blog post for details on their “delayed streams” architecture.

Setup
-----

We start by importing some basic packages and the Modal SDK.

```
import asyncio
import base64
import time
from pathlib import Path

import modal
```

Then we define a Modal App and an
[Image](https://modal.com/docs/guide/images)

with the dependencies of our speech-to-text system.

```
app = modal.App(name="example-streaming-kyutai-stt")

stt_image = (
    modal.Image.debian_slim(python_version="3.12")
    .uv_pip_install(
        "moshi==0.2.9", "fastapi==0.116.1", "hf_transfer==0.1.9", "julius==0.2.7"
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)
```

One dependency is missing: the model weights.

Instead of including them in the Image or loading them every time the Function starts,
we add them to a Modal
[Volume](https://modal.com/docs/guide/volumes)

.
Volumes are like a shared disk that all Modal Functions can access.

For more details on patterns for handling model weights on Modal, see
[this guide](https://modal.com/docs/guide/model-weights)

.

```
MODEL_NAME = "kyutai/stt-1b-en_fr"

hf_cache_vol = modal.Volume.from_name(f"{app.name}-hf-cache", create_if_missing=True)
hf_cache_vol_path = Path("/root/.cache/huggingface")
volumes = {hf_cache_vol_path: hf_cache_vol}
```

Run Kyutai STT inference on Modal
---------------------------------

Now we’re ready to add the code that runs the speech-to-text model.

We use a Modal
[Cls](https://modal.com/docs/guide/lifecycle-functions)

so that we can separate out the model loading and setup code from the inference.

For more on lifecycle management with Clses and cold start penalty reduction on Modal, see
[this guide](https://modal.com/docs/guide/cold-start)

.

We also define multiple ways to access the underlying streaming STT service —
via a
[WebSocket](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API)

,
for Web clients like browsers,
and via a Modal
[Queue](https://modal.com/docs/guide/queues)

for Python clients.

That plus the code for manipulating the streams of audio bytes and output text
leads to a pretty big class! But there’s not anything too complex here.

```
MINUTES = 60

@app.cls(image=stt_image, gpu="l40s", volumes=volumes, timeout=10 * MINUTES)
class STT:
    BATCH_SIZE = 1

    @modal.enter()
    def enter(self):
        import torch
        from huggingface_hub import snapshot_download
        from moshi.models import LMGen, loaders

        start_time = time.monotonic_ns()

        print("Loading model...")
        snapshot_download(MODEL_NAME)

        self.device = "cuda" if torch.cuda.is_available() else "cpu"

        checkpoint_info = loaders.CheckpointInfo.from_hf_repo(MODEL_NAME)
        self.mimi = checkpoint_info.get_mimi(device=self.device)
        self.frame_size = int(self.mimi.sample_rate / self.mimi.frame_rate)

        self.moshi = checkpoint_info.get_moshi(device=self.device)
        self.lm_gen = LMGen(self.moshi, temp=0, temp_text=0)

        self.mimi.streaming_forever(self.BATCH_SIZE)
        self.lm_gen.streaming_forever(self.BATCH_SIZE)

        self.text_tokenizer = checkpoint_info.get_text_tokenizer()

        self.audio_silence_prefix_seconds = checkpoint_info.stt_config.get(
            "audio_silence_prefix_seconds", 1.0
        )
        self.audio_delay_seconds = checkpoint_info.stt_config.get(
            "audio_delay_seconds", 5.0
        )
        self.padding_token_id = checkpoint_info.raw_config.get(
            "text_padding_token_id", 3
        )

        # warmup gpus
        for _ in range(4):
            codes = self.mimi.encode(
                torch.zeros(self.BATCH_SIZE, 1, self.frame_size).to(self.device)
            )
            for c in range(codes.shape[-1]):
                tokens = self.lm_gen.step(codes[:, :, c : c + 1])
                if tokens is None:
                    continue
        torch.cuda.synchronize()

        print(f"Model loaded in {round((time.monotonic_ns() - start_time) / 1e9, 2)}s")

    def reset_state(self):
        # reset llm chat history for this input
        self.mimi.reset_streaming()
        self.lm_gen.reset_streaming()

    async def transcribe(self, pcm, all_pcm_data):
        import numpy as np
        import torch

        if pcm is None:
            yield all_pcm_data
            return
        if len(pcm) == 0:
            yield all_pcm_data
            return

        if pcm.shape[-1] == 0:
            yield all_pcm_data
            return

        if all_pcm_data is None:
            all_pcm_data = pcm
        else:
            all_pcm_data = np.concatenate((all_pcm_data, pcm))

        # infer on each frame
        while all_pcm_data.shape[-1] >= self.frame_size:
            chunk = all_pcm_data[: self.frame_size]
            all_pcm_data = all_pcm_data[self.frame_size :]

            with torch.no_grad():
                chunk = torch.from_numpy(chunk)
                chunk = chunk.unsqueeze(0).unsqueeze(0)  # (1, 1, frame_size)
                chunk = chunk.expand(
                    self.BATCH_SIZE, -1, -1
                )  # (batch_size, 1, frame_size)
                chunk = chunk.to(device=self.device)

                # inference on audio chunk
                codes = self.mimi.encode(chunk)

                # language model inference against encoded audio
                for c in range(codes.shape[-1]):
                    text_tokens, vad_heads = self.lm_gen.step_with_extra_heads(
                        codes[:, :, c : c + 1]
                    )
                    if text_tokens is None:
                        # model is silent
                        yield all_pcm_data
                        return
                    if vad_heads:
                        pr_vad = vad_heads[2][0, 0, 0].cpu().item()
                        if pr_vad > 0.5:
                            # end of turn detected
                            yield all_pcm_data
                            return

                    assert text_tokens.shape[1] == self.lm_gen.lm_model.dep_q + 1

                    text_token = text_tokens[0, 0, 0].item()
                    if text_token not in (0, 3):
                        text = self.text_tokenizer.id_to_piece(text_token)
                        text = text.replace("▁", " ")
                        yield text

        yield all_pcm_data

    @modal.asgi_app()
    def api(self):
        import sphn
        from fastapi import FastAPI, Response, WebSocket, WebSocketDisconnect

        web_app = FastAPI()

        @web_app.get("/status")
        async def status():
            return Response(status_code=200)

        @web_app.websocket("/ws")
        async def transcribe_websocket(ws: WebSocket):
            await ws.accept()

            opus_stream_inbound = sphn.OpusStreamReader(self.mimi.sample_rate)
            transcription_queue = asyncio.Queue()

            print("Session started")
            tasks = []

            # asyncio to run multiple loops concurrently within single websocket connection
            async def recv_loop():
                """
                Receives Opus stream across websocket, appends into inbound queue.
                """
                nonlocal opus_stream_inbound
                while True:
                    data = await ws.receive_bytes()

                    if not isinstance(data, bytes):
                        print("received non-bytes message")
                        continue
                    if len(data) == 0:
                        print("received empty message")
                        continue
                    opus_stream_inbound.append_bytes(data)

            async def inference_loop():
                """
                Runs streaming inference on inbound data, and if any response audio is created, appends it to the outbound stream.
                """
                nonlocal opus_stream_inbound, transcription_queue
                all_pcm_data = None

                while True:
                    await asyncio.sleep(0.001)

                    pcm = opus_stream_inbound.read_pcm()
                    async for msg in self.transcribe(pcm, all_pcm_data):
                        if isinstance(msg, str):
                            transcription_queue.put_nowait(msg)
                        else:
                            all_pcm_data = msg

            async def send_loop():
                """
                Reads outbound data, and sends it across websocket
                """
                nonlocal transcription_queue
                while True:
                    data = await transcription_queue.get()

                    if data is None:
                        continue

                    msg = b"\x01" + bytes(
                        data, encoding="utf8"
                    )  # prepend "\x01" as a tag to indicate text
                    await ws.send_bytes(msg)

            # run all loops concurrently
            try:
                tasks = [
                    asyncio.create_task(recv_loop()),
                    asyncio.create_task(inference_loop()),
                    asyncio.create_task(send_loop()),
                ]
                await asyncio.gather(*tasks)

            except WebSocketDisconnect:
                print("WebSocket disconnected")
                await ws.close(code=1000)
            except Exception as e:
                print("Exception:", e)
                await ws.close(code=1011)  # internal error
                raise e
            finally:
                for task in tasks:
                    task.cancel()
                await asyncio.gather(*tasks, return_exceptions=True)
                self.reset_state()

        return web_app

    @modal.method()
    async def transcribe_queue(self, q: modal.Queue):
        import tempfile

        import sphn

        all_pcm_data = None

        while True:
            chunk = await q.get.aio(partition="audio")
            if chunk is None:
                await q.put.aio(None, partition="transcription")
                break

            # to avoid having to encode the audio and retrieve with OpusStreamReader:
            with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as tmp:
                tmp.write(chunk)
                tmp.flush()
                pcm, _ = sphn.read(tmp.name)
                pcm = pcm.squeeze(0)

            async for msg in self.transcribe(pcm, all_pcm_data):
                if isinstance(msg, str):
                    await q.put.aio(msg, partition="transcription")
                else:
                    all_pcm_data = msg
```

Run a local Python client to test streaming STT
-----------------------------------------------

We can test this code on the same production Modal infra
that we’ll be deploying it on by writing a quick
`local_entrypoint`
for testing.

We just need a few helper functions to control the streaming of audio bytes
and transcribed text from local Python.

These communicate asynchronously with the deployed Function using a Modal Queue.

```
async def chunk_audio(data: bytes, chunk_size: int):
    for i in range(0, len(data), chunk_size):
        yield data[i : i + chunk_size]

async def send_audio(audio_bytes: bytes, q: modal.Queue, chunk_size: int, rtf: int):
    async for chunk in chunk_audio(audio_bytes, chunk_size):
        await q.put.aio(chunk, partition="audio")
        await asyncio.sleep(chunk_size / chunk_size / rtf)
    await q.put.aio(None, partition="audio")

async def receive_text(q: modal.Queue):
    break_counter, break_every = 0, 20
    while True:
        data = await q.get.aio(partition="transcription")
        if data is None:
            break
        print(data, end="")
        break_counter += 1
        if break_counter >= break_every:
            print()
            break_counter = 0
```

Now we write our quick test, which loads in audio from a URL
and then passes it to the remote Function via a

If you run this example with

```
modal run streaming_kyutai_stt.py
```

you will

1. deploy the latest version of the code on Modal
2. spin up a new GPU to handle transcription
3. load the model from Hugging Face or the Modal Volume cache
4. send the audio out to the new GPU container, transcribe it, and receive it locally to be printed.

Not bad for a single Python file with no dependencies except Modal!

```
@app.local_entrypoint()
async def test(
    chunk_size: int = 24_000,  # bytes
    rtf: int = 1000,
    audio_url: str = "https://github.com/kyutai-labs/delayed-streams-modeling/raw/refs/heads/main/audio/bria.mp3",
):
    from urllib.request import urlopen

    print(f"Downloading audio file from {audio_url}")
    audio_bytes = urlopen(audio_url).read()
    print(f"Downloaded {len(audio_bytes)} bytes")

    print("Starting transcription")
    start_time = time.monotonic_ns()
    with modal.Queue.ephemeral() as q:
        STT().transcribe_queue.spawn(q)
        send = asyncio.create_task(send_audio(audio_bytes, q, chunk_size, rtf))
        recv = asyncio.create_task(receive_text(q))
        await asyncio.gather(send, recv)
    print(
        f"\nTranscription complete in {round((time.monotonic_ns() - start_time) / 1e9, 2)}s"
    )
```

Deploy a streaming STT service on the Web
-----------------------------------------

We’ve already written a Web backend for our streaming STT service —
that’s the FastAPI API with the WebSocket in the Modal Cls above.

We can also deploy a Web frontend. To keep things almost entirely “pure Python”,
we here use the
[FastHTML](https://www.fastht.ml/)

library,
but you can also deploy a JavaScript frontend with a FastAPI or Node backend.

We do use a bit of JS for the audio processing in the browser.
We add it to the Modal Image using
`add_local_dir`
.
You can find the frontend files
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/speech-to-text/streaming-kyutai-stt-frontend)

.

```
web_image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install("python-fasthtml==0.12.20")
    .add_local_dir(
        Path(__file__).parent / "streaming-kyutai-stt-frontend", "/root/frontend"
    )
)
```

You can deploy this frontend with

```
modal deploy streaming_kyutai_stt.py
```

and then interact with it at the printed
`ui`
URL.

```
@app.function(image=web_image, timeout=10 * MINUTES)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def ui():
    import fasthtml.common as fh

    modal_logo_svg = open("/root/frontend/modal-logo.svg").read()
    modal_logo_base64 = base64.b64encode(modal_logo_svg.encode()).decode()
    app_js = open("/root/frontend/audio.js").read()

    fast_app, rt = fh.fast_app(
        hdrs=[
            # audio recording libraries
            fh.Script(
                src="https://cdn.jsdelivr.net/npm/opus-recorder@latest/dist/recorder.min.js"
            ),
            fh.Script(
                src="https://cdn.jsdelivr.net/npm/opus-recorder@latest/dist/encoderWorker.min.js"
            ),
            fh.Script(
                src="https://cdn.jsdelivr.net/npm/ogg-opus-decoder/dist/ogg-opus-decoder.min.js"
            ),
            # styling
            fh.Link(
                href="https://fonts.googleapis.com/css?family=Inter:300,400,600",
                rel="stylesheet",
            ),
            fh.Script(src="https://cdn.tailwindcss.com"),
            fh.Script("""
                tailwind.config = {
                    theme: {
                        extend: {
                            colors: {
                                ground: "#0C0F0B",
                                primary: "#9AEE86",
                                "accent-pink": "#FC9CC6",
                                "accent-blue": "#B8E4FF",
                            },
                        },
                    },
                };
            """),
        ],
    )

    @rt("/")
    def get():
        return (
            fh.Title("Kyutai Streaming STT"),
            fh.Body(
                fh.Div(
                    fh.Div(
                        fh.Div(
                            id="text-output",
                            cls="flex flex-col-reverse overflow-y-auto max-h-64 pr-2",
                        ),
                        cls="w-full overflow-y-auto max-h-64",
                    ),
                    cls="bg-gray-800 rounded-lg shadow-lg w-full max-w-xl p-6",
                ),
                fh.Footer(
                    fh.Span(
                        "Built with ",
                        fh.A(
                            "Kyutai",
                            href="https://github.com/kyutai-labs/delayed-streams-modeling",
                            target="_blank",
                            rel="noopener noreferrer",
                            cls="underline",
                        ),
                        " and",
                        cls="text-sm font-medium text-gray-300 mr-2",
                    ),
                    fh.A(
                        fh.Img(
                            src=f"data:image/svg+xml;base64,{modal_logo_base64}",
                            alt="Modal logo",
                            cls="w-24",
                        ),
                        cls="flex items-center p-2 rounded-lg bg-gray-800 shadow-lg hover:bg-gray-700 transition-colors duration-200",
                        href="https://modal.com",
                        target="_blank",
                        rel="noopener noreferrer",
                    ),
                    cls="fixed bottom-4 inline-flex items-center justify-center",
                ),
                fh.Script(app_js),
                cls="relative bg-gray-900 text-white min-h-screen flex flex-col items-center justify-center p-4",
            ),
        )

    return fast_app
```

[Stream transcriptions with Kyutai STT](#stream-transcriptions-with-kyutai-stt)

[Setup](#setup)

[Run Kyutai STT inference on Modal](#run-kyutai-stt-inference-on-modal)

[Run a local Python client to test streaming STT](#run-a-local-python-client-to-test-streaming-stt)

[Deploy a streaming STT service on the Web](#deploy-a-streaming-stt-service-on-the-web)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/speech-to-text/streaming_kyutai_stt.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/text_embeddings_inference
================================================================================

Run TextEmbeddingsInference (TEI) on Modal
==========================================

This example runs the
[Text Embedding Inference (TEI)](https://github.com/huggingface/text-embeddings-inference)

toolkit on the Hacker News BigQuery public dataset.

```
import json
import os
import socket
import subprocess
from pathlib import Path

import modal

GPU_CONFIG = "A10G"
MODEL_ID = "BAAI/bge-base-en-v1.5"
BATCH_SIZE = 32
DOCKER_IMAGE = (
    "ghcr.io/huggingface/text-embeddings-inference:86-1.7"  # Ampere 86 for A10s.
    # "ghcr.io/huggingface/text-embeddings-inference:1.7" # Ampere 80 for A100s.
    # "ghcr.io/huggingface/text-embeddings-inference:turing-1.7"  # Turing for T4s.
)
PORT = 8000

DATA_PATH = Path("/data/dataset.jsonl")

LAUNCH_FLAGS = [
    "--model-id",
    MODEL_ID,
    "--port",
    "8000",
]

def spawn_server() -> subprocess.Popen:
    process = subprocess.Popen(["text-embeddings-router"] + LAUNCH_FLAGS)

    # Poll until webserver at 127.0.0.1:8000 accepts connections before running inputs.
    while True:
        try:
            socket.create_connection(("127.0.0.1", PORT), timeout=1).close()
            print("Webserver ready!")
            return process
        except (socket.timeout, ConnectionRefusedError):
            # Check if launcher webserving process has exited.
            # If so, a connection can never be made.
            retcode = process.poll()
            if retcode is not None:
                raise RuntimeError(f"launcher exited unexpectedly with code {retcode}")

def download_model():
    # Wait for server to start. This downloads the model weights when not present.
    spawn_server().terminate()

volume = modal.Volume.from_name("tei-hn-data", create_if_missing=True)

app = modal.App("example-tei")

tei_image = (
    modal.Image.from_registry(
        DOCKER_IMAGE,
        add_python="3.10",
    )
    .dockerfile_commands("ENTRYPOINT []")
    .run_function(download_model, gpu=GPU_CONFIG)
    .pip_install("httpx")
)

with tei_image.imports():
    from httpx import AsyncClient

@app.cls(
    gpu=GPU_CONFIG,
    image=tei_image,
    max_containers=20,  # Use up to 20 GPU containers at once.
)
@modal.concurrent(
    max_inputs=10
)  # Allow each container to process up to 10 batches at once.
class TextEmbeddingsInference:
    @modal.enter()
    def setup_server(self):
        self.process = spawn_server()
        self.client = AsyncClient(base_url="http://127.0.0.1:8000")

    @modal.exit()
    def teardown_server(self):
        self.process.terminate()

    @modal.method()
    async def embed(self, inputs_with_ids: list[tuple[int, str]]):
        ids, inputs = zip(*inputs_with_ids)
        resp = await self.client.post("/embed", json={"inputs": inputs})
        resp.raise_for_status()
        outputs = resp.json()

        return list(zip(ids, outputs))

def download_data():
    service_account_info = json.loads(os.environ["SERVICE_ACCOUNT_JSON"])
    credentials = service_account.Credentials.from_service_account_info(
        service_account_info
    )

    client = bigquery.Client(credentials=credentials)

    iterator = client.list_rows(
        "bigquery-public-data.hacker_news.full",
        max_results=100_000,
    )
    df = iterator.to_dataframe(progress_bar_type="tqdm").dropna()

    df["id"] = df["id"].astype(int)
    df["text"] = df["text"].apply(lambda x: x[:512])

    data = list(zip(df["id"], df["text"]))

    with open(DATA_PATH, "w") as f:
        json.dump(data, f)

    volume.commit()

image = modal.Image.debian_slim(python_version="3.10").pip_install(
    "google-cloud-bigquery", "pandas", "db-dtypes", "tqdm"
)

with image.imports():
    from google.cloud import bigquery
    from google.oauth2 import service_account

@app.function(
    image=image,
    secrets=[modal.Secret.from_name("bigquery")],
    volumes={DATA_PATH.parent: volume},
)
def embed_dataset():
    model = TextEmbeddingsInference()

    if not DATA_PATH.exists():
        print("Downloading data. This takes a while...")
        download_data()

    with open(DATA_PATH) as f:
        data = json.loads(f.read())

    def generate_batches():
        batch = []
        for item in data:
            batch.append(item)

            if len(batch) == BATCH_SIZE:
                yield batch
                batch = []

    # data is of type list[tuple[str, str]].
    # starmap spreads the tuples into positional arguments.
    for output_batch in model.embed.map(generate_batches(), order_outputs=False):
        # Do something with the outputs.
        pass
```

[Run TextEmbeddingsInference (TEI) on Modal](#run-textembeddingsinference-tei-on-modal)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/embeddings/text_embeddings_inference.py:\:embed_dataset
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/text_to_image
================================================================================

Run Stable Diffusion 3.5 Large Turbo as a CLI, API, and web UI
==============================================================

This example shows how to run
[Stable Diffusion 3.5 Large Turbo](https://huggingface.co/stabilityai/stable-diffusion-3.5-large-turbo)

on Modal
to generate images from your local command line, via an API, and as a web UI.

Inference takes about one minute to cold start,
at which point images are generated at a rate of one image every 1-2 seconds
for batch sizes between one and 16.

Below are four images produced by the prompt
“A princess riding on a pony”.

![stable diffusion montage](https://modal-cdn.com/cdnbot/sd-montage-princess-yxu2vnbl_e896a9c0.webp)

Basic setup
-----------

```
import io
import random
import time
from pathlib import Path
from typing import Optional

import modal

MINUTES = 60
```

All Modal programs need an
[`App`](https://modal.com/docs/reference/modal.App)

— an object that acts as a recipe for
the application. Let’s give it a friendly name.

```
app = modal.App("example-text-to-image")
```

Configuring dependencies
------------------------

The model runs remotely inside a
[container](https://modal.com/docs/guide/custom-container)

.
That means we need to install the necessary dependencies in that container’s image.

Below, we start from a lightweight base Linux image
and then install our Python dependencies, like Hugging Face’s
`diffusers`
library and
`torch`
.

```
CACHE_DIR = "/cache"

image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "accelerate==0.33.0",
        "diffusers==0.31.0",
        "fastapi[standard]==0.115.4",
        "huggingface-hub[hf_transfer]==0.25.2",
        "sentencepiece==0.2.0",
        "torch==2.5.1",
        "torchvision==0.20.1",
        "transformers~=4.44.0",
    )
    .env(
        {
            "HF_HUB_ENABLE_HF_TRANSFER": "1",  # faster downloads
            "HF_HUB_CACHE": CACHE_DIR,
        }
    )
)

with image.imports():
    import diffusers
    import torch
    from fastapi import Response
```

Implementing SD3.5 Large Turbo inference on Modal
-------------------------------------------------

We wrap inference in a Modal
[Cls](https://modal.com/docs/guide/lifecycle-functions)

that ensures models are loaded and then moved to the GPU once when a new container
starts, before the container picks up any work.

The
`run`
function just wraps a
`diffusers`
pipeline.
It sends the output image back to the client as bytes.

We also include a
`web`
wrapper that makes it possible
to trigger inference via an API call.
See the
`/docs`
route of the URL ending in
`inference-web.modal.run`
that appears when you deploy the app for details.

```
MODEL_ID = "adamo1139/stable-diffusion-3.5-large-turbo-ungated"
MODEL_REVISION_ID = "9ad870ac0b0e5e48ced156bb02f85d324b7275d2"

cache_volume = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.cls(
    image=image,
    gpu="H100",
    timeout=10 * MINUTES,
    volumes={CACHE_DIR: cache_volume},
)
class Inference:
    @modal.enter()
    def load_pipeline(self):
        self.pipe = diffusers.StableDiffusion3Pipeline.from_pretrained(
            MODEL_ID,
            revision=MODEL_REVISION_ID,
            torch_dtype=torch.bfloat16,
        ).to("cuda")

    @modal.method()
    def run(
        self, prompt: str, batch_size: int = 4, seed: Optional[int] = None
    ) -> list[bytes]:
        seed = seed if seed is not None else random.randint(0, 2**32 - 1)
        print("seeding RNG with", seed)
        torch.manual_seed(seed)
        images = self.pipe(
            prompt,
            num_images_per_prompt=batch_size,  # outputting multiple images per prompt is much cheaper than separate calls
            num_inference_steps=4,  # turbo is tuned to run in four steps
            guidance_scale=0.0,  # turbo doesn't use CFG
            max_sequence_length=512,  # T5-XXL text encoder supports longer sequences, more complex prompts
        ).images

        image_output = []
        for image in images:
            with io.BytesIO() as buf:
                image.save(buf, format="PNG")
                image_output.append(buf.getvalue())
        torch.cuda.empty_cache()  # reduce fragmentation
        return image_output

    @modal.fastapi_endpoint(docs=True)
    def web(self, prompt: str, seed: Optional[int] = None):
        return Response(
            content=self.run.local(  # run in the same container
                prompt, batch_size=1, seed=seed
            )[0],
            media_type="image/png",
        )
```

Generating Stable Diffusion images from the command line
--------------------------------------------------------

This is the command we’ll use to generate images. It takes a text
`prompt`
,
a
`batch_size`
that determines the number of images to generate per prompt,
and the number of times to run image generation (
`samples`
).

You can also provide a
`seed`
to make sampling more deterministic.

Run it with

```
modal run text_to_image.py
```

and pass
`--help`
to see more options.

```
@app.local_entrypoint()
def entrypoint(
    samples: int = 4,
    prompt: str = "A princess riding on a pony",
    batch_size: int = 4,
    seed: Optional[int] = None,
):
    print(
        f"prompt => {prompt}",
        f"samples => {samples}",
        f"batch_size => {batch_size}",
        f"seed => {seed}",
        sep="\n",
    )

    output_dir = Path("/tmp/stable-diffusion")
    output_dir.mkdir(exist_ok=True, parents=True)

    inference_service = Inference()

    for sample_idx in range(samples):
        start = time.time()
        images = inference_service.run.remote(prompt, batch_size, seed)
        duration = time.time() - start
        print(f"Run {sample_idx + 1} took {duration:.3f}s")
        if sample_idx:
            print(
                f"\tGenerated {len(images)} image(s) at {(duration) / len(images):.3f}s / image."
            )
        for batch_idx, image_bytes in enumerate(images):
            output_path = (
                output_dir
                / f"output_{slugify(prompt)[:64]}_{str(sample_idx).zfill(2)}_{str(batch_idx).zfill(2)}.png"
            )
            if not batch_idx:
                print("Saving outputs", end="\n\t")
            print(
                output_path,
                end="\n" + ("\t" if batch_idx < len(images) - 1 else ""),
            )
            output_path.write_bytes(image_bytes)
```

Generating Stable Diffusion images via an API
---------------------------------------------

The Modal
`Cls`
above also included a
[`fastapi_endpoint`](https://modal.com/docs/examples/basic_web)

,
which adds a simple web API to the inference method.

To try it out, run

```
modal deploy text_to_image.py
```

copy the printed URL ending in
`inference-web.modal.run`
,
and add
`/docs`
to the end. This will bring up the interactive
Swagger/OpenAPI docs for the endpoint.

Generating Stable Diffusion images in a web UI
----------------------------------------------

Lastly, we add a simple front-end web UI (written in Alpine.js) for
our image generation backend.

This is also deployed by running

```
modal deploy text_to_image.py.
```

The
`Inference`
class will serve multiple users from its own auto-scaling pool of warm GPU containers automatically.

```
frontend_path = Path(__file__).parent / "frontend"

web_image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install("jinja2==3.1.4", "fastapi[standard]==0.115.4")
    .add_local_dir(frontend_path, remote_path="/assets")
)

@app.function(image=web_image)
@modal.concurrent(max_inputs=1000)
@modal.asgi_app()
def ui():
    import fastapi.staticfiles
    from fastapi import FastAPI, Request
    from fastapi.templating import Jinja2Templates

    web_app = FastAPI()
    templates = Jinja2Templates(directory="/assets")

    @web_app.get("/")
    async def read_root(request: Request):
        return templates.TemplateResponse(
            "index.html",
            {
                "request": request,
                "inference_url": Inference.web.get_web_url(),
                "model_name": "Stable Diffusion 3.5 Large Turbo",
                "default_prompt": "A cinematic shot of a baby raccoon wearing an intricate italian priest robe.",
            },
        )

    web_app.mount(
        "/static",
        fastapi.staticfiles.StaticFiles(directory="/assets"),
        name="static",
    )

    return web_app

def slugify(s: str) -> str:
    return "".join(c if c.isalnum() else "-" for c in s).strip("-")
```

[Run Stable Diffusion 3.5 Large Turbo as a CLI, API, and web UI](#run-stable-diffusion-35-large-turbo-as-a-cli-api-and-web-ui)

[Basic setup](#basic-setup)

[Configuring dependencies](#configuring-dependencies)

[Implementing SD3.5 Large Turbo inference on Modal](#implementing-sd35-large-turbo-inference-on-modal)

[Generating Stable Diffusion images from the command line](#generating-stable-diffusion-images-from-the-command-line)

[Generating Stable Diffusion images via an API](#generating-stable-diffusion-images-via-an-api)

[Generating Stable Diffusion images in a web UI](#generating-stable-diffusion-images-in-a-web-ui)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/stable_diffusion/text_to_image.py --prompt 'A 1600s oil painting of the New York City skyline'
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/tokasaurus_throughput
================================================================================

High-throughput LLM inference with Tokasaurus (LLama 3.2 1B Instruct)
=====================================================================

In this example, we demonstrate how to use Tokasaurus, an LLM inference framework designed for maximum throughput.

It maps the
[Large Language Monkeys GSM8K demo](https://github.com/ScalingIntelligence/tokasaurus/blob/a0155181f09c0cf40783e01a625b041985667a92/tokasaurus/benchmarks/standalone_monkeys_gsm8k.py)

from the
[Tokasaurus release blog post](https://scalingintelligence.stanford.edu/blogs/tokasaurus/)

onto Modal
and replicates the core result: sustained inference at >80k tok/s throughput,
exceeding their reported numbers for vLLM and SGLang by ~3x.

In the “Large Language Monkeys” inference-time compute scaling paradigm,
[also introduced by the same Stanford labs](https://arxiv.org/abs/2407.21787)

,
the response quality of a system using a small model is improved to match or exceed a system using a large model
by running many requests in parallel.
Here, it’s applied to the Grade School Math (GSM8K) dataset.

For more on this LLM inference pattern
(and an explainer on why it’s such a natural fit for current parallel computing systems)
see
[our blog post reproducing and extending their results](https://modal.com/blog/llama-human-eval)

.

Set up the container image
--------------------------

Our first order of business is to define the environment our LLM engine will run in:
the
[container
`Image`](https://modal.com/docs/guide/custom-container)

.

We translate the
[recipe](https://github.com/ScalingIntelligence/tokasaurus/blob/main/logs/blog_commands.md)

the authors used to build their Tokasaurus environment into methods of
`modal.Image`
.

This requires, for instance, picking a base Image that includes the right version of the
[CUDA toolkit](https://modal.com/gpu-glossary/host-software/cuda-software-platform)

.

```
import random
import time

import aiohttp
import modal

toka_image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.4.1-devel-ubuntu22.04", add_python="3.12"
    ).entrypoint([])  # silence chatty logs on container start
)
```

We also set an environment variable that directs Torch-based libraries to only compile kernels for the
[GPU SM architecture](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

we are targeting, Hopper. This isn’t strictly necessary, but it silences some paranoid logs.

```
GPU_CONFIG = "H100!"  # ! means "strictly", no upgrades to H200
TORCH_CUDA_ARCH_LIST = "9.0 9.0a"  # Hopper, aka H100/H200
```

From there, Tokasaurus can be installed like any normal Python package,
since Modal
[provides the host CUDA drivers](https://modal.com/docs/guide/cuda)

.

```
toka_image = (
    toka_image.pip_install("uv")
    .env(
        {"HF_HUB_ENABLE_HF_TRANSFER": "1", "TORCH_CUDA_ARCH_LIST": TORCH_CUDA_ARCH_LIST}
    )
    .run_commands(
        "uv pip install --system --compile-bytecode tokasaurus==0.0.2 huggingface_hub[hf_transfer]==0.33.0 datasets==3.6.0"
    )
)
```

Download the model weights
--------------------------

For this demo, we run Meta’s Llama 3.2 1B Instruct model, downloaded from Hugging Face.
Since this is a gated model, you’ll need to
[accept the terms of use](https://huggingface.co/meta-llama/Llama-3.2-1B)

and create a
[Secret](https://modal.com/secrets/)

with your Hugging Face token to download the weights.

```
secrets = [modal.Secret.from_name("huggingface-secret")]

MODEL_NAME = "meta-llama/Llama-3.2-1B-Instruct"
MODEL_REVISION = "4e20de362430cd3b72f300e6b0f18e50e7166e08"  # avoid nasty surprises when repos update!
```

Although Tokasaurus will download weights from Hugging Face on-demand,
we want to cache them so we don’t do it every time our server starts.
We’ll use a
[Modal Volume](https://modal.com/docs/guide/volumes)

for our cache.

```
app_name = "example-tokasaurus-throughput"
hf_cache_vol = modal.Volume.from_name(f"{app_name}-hf-cache", create_if_missing=True)
volumes = {"/root/.cache/huggingface": hf_cache_vol}
```

Configure Tokasaurus for maximum throughput on this workload
------------------------------------------------------------

On throughput-focused benchmarks with high prefix sharing workloads, Tokasaurus can outperform vLLM and SGLang nearly three-fold.

For small models like the one we are running, it reduces CPU overhead by maintaining a deep input queue
and exposing shared prefixes to the GPU
[Tensor Cores](https://modal.com/gpu-glossary/device-hardware/tensor-core)

with
[Hydragen](https://arxiv.org/abs/2402.05099)

.

```
USE_HYDRAGEN = "T"
HYDRAGEN_MIN_GROUP_SIZE = 129  # sic
```

We start by maximizing the number of tokens processed per forward pass by adjusting the following parameters:

* `kv_cache_num_tokens`
  : max tokens in the KV cache, higher values increase throughput but consume GPU memory
* `max_tokens_per_forward`
  : max tokens/seq processed per forward pass, higher values increase throughput but use more GPU memory
* `max_seqs_per_forward`
  : max sequences processed per forward pass, higher values increase batch size and throughput, but require more GPU memory

We also set a few other parameters with less obvious impacts — the KV cache page size and the stop token behavior.
All values are derived from
[this version of the official benchmarking script](https://github.com/ScalingIntelligence/tokasaurus/blob/a0155181f09c0cf40783e01a625b041985667a92/tokasaurus/benchmarks/standalone_monkeys_gsm8k.py)

,
except the
`KV_CACHE_NUM_TOKENS`
, which we increase to the maximum the GPU can handle.
The value in the script is set to the maximum that the other engines can handle, not just Tokasaurus.

```
KV_CACHE_NUM_TOKENS = (1024 + 768) * 1024  # tuned for H100, 80 GB RAM
```

KV\_CACHE\_NUM\_TOKENS = (1024 + 512) \* 1024 # value in benchmark script

```
MAX_TOKENS_PER_FORWARD = 32768
MAX_SEQS_PER_FORWARD = 8192
PAGE_SIZE = 16
STOP_STRING_NUM_TOKEN_LOOKBACK = 5
```

We could apply the Torch compiler to the model to make it faster and, via kernel fusion, reduce the amount of used activation memory,
leaving space for a larger KV cache. However, it dramatically increases the startup time of the server,
and we only see modest (20%, not 2x) improvements to throughput, so we don’t use it here.

```
TORCH_COMPILE = "F"
```

Lastly, we need to set a few of the parameters for the client requests,
again based on the official benchmarking script.

```
MAX_TOKENS = 1024
TEMPERATURE = 0.6
TOP_P = 1.0
STOP_STRING = "Question:"
N = 1024
```

Serve Tokasaurus with an OpenAI-compatible API
----------------------------------------------

The function below spawns a Tokasaurus instance listening at port
`10210`
,
serving an OpenAI-compatible API.
We wrap it in the
[`@modal.web_server`
decorator](https://modal.com/docs/guide/webhooks#non-asgi-web-servers)

to connect it to the Internet.

The server runs in an independent process, via
`subprocess.Popen`
.
If it hasn’t started listening on the
`PORT`
within the
`startup_timeout`
,
the server start will be marked as failed.

```
app = modal.App(app_name)

MINUTES = 60  # seconds
PORT = 10210

@app.function(
    image=toka_image,
    gpu=GPU_CONFIG,
    scaledown_window=60 * MINUTES,  # how long should we stay up with no requests?
    timeout=60 * MINUTES,  # how long should we allow requests to take?
    # long, because we're doing batched inference
    volumes=volumes,
    secrets=secrets,
)
@modal.concurrent(max_inputs=1000)
@modal.web_server(port=PORT, startup_timeout=10 * MINUTES)
def serve():
    import subprocess

    cmd = " ".join(
        [
            "tksrs",
            f"model={MODEL_NAME}",
            f"kv_cache_num_tokens={KV_CACHE_NUM_TOKENS}",
            f"max_seqs_per_forward={MAX_SEQS_PER_FORWARD}",
            f"max_tokens_per_forward={MAX_TOKENS_PER_FORWARD}",
            f"torch_compile={TORCH_COMPILE}",
            f"use_hydragen={USE_HYDRAGEN}",
            f"hydragen_min_group_size={HYDRAGEN_MIN_GROUP_SIZE}",
            f"stop_string_num_token_lookback={STOP_STRING_NUM_TOKEN_LOOKBACK}",
            "page_size=16",
            "stats_report_seconds=5.0",
            "uvicorn_log_level=info",
        ]
    )

    print(cmd)

    subprocess.Popen(cmd, shell=True)
```

The code we have so far is enough to deploy Tokasaurus on Modal.
Just run:

```
modal deploy tokasaurus_throughput.py
```

And you can hit the server with your favorite OpenAI-compatible API client,
like the
`openai`
Python SDK.

Run the Large Language Monkeys GSM8K benchmark
----------------------------------------------

To make it easier to check the performance and to provide a simple test
that can be used when setting up/configuring a Tokasaurus deployment,
we include a simple
`benchmark`
function that acts as a
`local_entrypoint`
.
If you target this script with
`modal run`
, this code will execute,
spinning up a new replica and sending some test requests to it.

Because the API responses don’t include token counts, we need a quick helper function to
calculate token counts from a prompt or completion.
We add
[automatic dynamic batching](https://modal.com/docs/guide/dynamic-batching)

with
`modal.batched`
, so that we can send single strings but still take advantage
of batched encoding.

```
@app.function(image=toka_image, volumes=volumes)
@modal.batched(max_batch_size=128, wait_ms=100)
def count_tokens(texts: list[str]) -> list[int]:
    from transformers import AutoTokenizer

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    return [len(ids) for ids in tokenizer(texts)["input_ids"]]
```

You can run the benchmark with

```
modal run tokasaurus_throughput.py
```

or pass the
`--help`
flag to see options.

```
@app.local_entrypoint()
async def benchmark(seed: int = 42, limit: int = 16, num_few_shot: int = 4):
    import asyncio

    print("Loading dataset")
    dataset = load_dataset.remote(seed=seed, num_few_shot=num_few_shot, limit=limit)
    print(f"Total number of items to process: {len(dataset)}")

    serve.update_autoscaler(
        max_containers=1  # prevent concurrent execution when benchmarking
    )

    url = serve.get_web_url()
    async with aiohttp.ClientSession(base_url=url) as session:
        print(f"Running health check for server at {url}")

        async with session.get("/v1/models", timeout=20 * MINUTES) as resp:
            up = (  # expect 404, /v1/models not implemented in toka 0.0.2
                resp.status < 500
            )

        assert up, f"Failed health check for server at {url}"
        print(f"Successful health check for server at {url}")

        print("Beginning throughput test")
        start = time.time()

        reqs, resps = [], []
        reqs = [_send_request(session, _make_prompt(item)) for item in dataset]
        resps = await asyncio.gather(*reqs)

        end = time.time()
        total_time = end - start
        print(f"Finished throughput test in {int(total_time)}s")

        # sniff test the results
        _integrity_check(resps)

        # calculate throughput from total elapsed time and total token counts
        print("Counting tokens")

        input_text = [resp["prompt"] for resp in resps]
        output_text = [  # flatten completions from list inside a list down to a single list
            completion for resp in resps for completion in resp["completions"]
        ]
        total_tokens = sum(
            [count async for count in count_tokens.map.aio(input_text + output_text)]
        )

        total_throughput = total_tokens // total_time

        print(f"Total throughput: {total_throughput} tokens/second")
```

Addenda
-------

The remaining code in this example is utility code, mostly for managing
the GSM8K dataset. That code is slightly modified from the code in the Tokasaurus repo
[here](https://github.com/ScalingIntelligence/tokasaurus/blob/a0155181f09c0cf40783e01a625b041985667a92/tokasaurus/benchmarks/standalone_monkeys_gsm8k.py)

.

```
@app.function(image=toka_image, volumes=volumes)
def load_dataset(seed: int, num_few_shot: int, limit: int = None):
    from datasets import load_dataset

    test_dataset = list(load_dataset("gsm8k", "main", split="test"))

    random.seed(seed)
    random.shuffle(test_dataset)

    if limit is not None:
        test_dataset = test_dataset[:limit]

    if num_few_shot > 0:
        train_dataset = list(load_dataset("gsm8k", "main", split="train"))
        for i, data in enumerate(test_dataset):
            few_shot_items = random.sample(train_dataset, num_few_shot)
            data["few_shot_items"] = few_shot_items

    return test_dataset

def _make_prompt(item: dict) -> str:
    few_shot_items = item["few_shot_items"]
    few_shot_pieces = []
    for f in few_shot_items:
        few_shot_prompt = f"Question: {f['question']}\nAnswer: {f['answer']}\n\n"
        few_shot_pieces.append(few_shot_prompt)
    few_shot_prompt = "".join(few_shot_pieces)
    prompt = few_shot_prompt + f"Question: {item['question']}\nAnswer:"
    return prompt

def _integrity_check(responses):
    for ii, resp in enumerate(responses):
        n_completions = len(resp["completions"])
        assert n_completions == N, (
            f"Expected {N} completions, got {n_completions} for request {ii}"
        )

async def _send_request(session: aiohttp.ClientSession, prompt: str):
    payload: dict[str, object] = {
        "model": "llm",
        "prompt": prompt,
        "max_tokens": MAX_TOKENS,
        "temperature": TEMPERATURE,
        "top_p": TOP_P,
        "stop": STOP_STRING,
        "n": N,
        "logprobs": None,
    }
    headers = {"Content-Type": "application/json"}

    async with session.post(
        "/v1/completions", json=payload, headers=headers, timeout=10 * MINUTES
    ) as resp:
        resp.raise_for_status()
        resp_json = await resp.json()
        return {
            "prompt": prompt,
            "completions": [choice["text"] for choice in resp_json["choices"]],
        }
```

[High-throughput LLM inference with Tokasaurus (LLama 3.2 1B Instruct)](#high-throughput-llm-inference-with-tokasaurus-llama-32-1b-instruct)

[Set up the container image](#set-up-the-container-image)

[Download the model weights](#download-the-model-weights)

[Configure Tokasaurus for maximum throughput on this workload](#configure-tokasaurus-for-maximum-throughput-on-this-workload)

[Serve Tokasaurus with an OpenAI-compatible API](#serve-tokasaurus-with-an-openai-compatible-api)

[Run the Large Language Monkeys GSM8K benchmark](#run-the-large-language-monkeys-gsm8k-benchmark)

[Addenda](#addenda)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-serving/tokasaurus_throughput.py --limit 2
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/torch_profiling
================================================================================

Tracing and profiling GPU-accelerated PyTorch programs on Modal
===============================================================

![A PyTorch trace loaded into ui.perfetto.dev](https://modal-public-assets.s3.amazonaws.com/tmpx_2c9bl5_c5aa7ab0.webp)

GPUs are high-performance computing devices. For high-performance computing,
tools for measuring and investigating performance are as critical
as tools for testing and confirming correctness in typical software.

In this example, we demonstrate how to wrap a Modal Function with PyTorch’s
built-in profiler, which captures events on both CPUs & GPUs. We also show
how to host TensorBoard, which includes useful visualizations and
performance improvement suggestions.

For a live walkthrough, check out
[this video on our YouTube channel](https://www.youtube.com/watch?v=4cesQJLyHA8)

.

Saving traces to a Modal Volume
-------------------------------

Most tracing tools, including PyTorch’s profiler, produce results as files on disk.
Modal Functions run in ephemeral containers in Modal’s cloud infrastructure,
so by default these files disappear as soon as the Function finishes running.

We can ensure these files persist by saving them to a
[Modal Volume](https://modal.com/docs/guide/volumes)

.
Volumes are a distributed file system: files can be read or written from
by many machines across a network, in this case from inside any Modal Function.

To start, we just create a Volume with a specific name.
We’ll also set a particular directory that we’ll use for it
in our Functions below, for convenience.

```
from pathlib import Path
from typing import Optional

import modal

traces = modal.Volume.from_name("example-traces", create_if_missing=True)
TRACE_DIR = Path("/traces")
```

Setting up a Modal App with a GPU-accelerated PyTorch Function
--------------------------------------------------------------

We next set up the Modal Function that we wish to profile.

In general, we want to attach profiling tools to code that’s already in place
and measure or debug its performance, and then detach it as easily as possible
so that we can be confident that the same performance characteristics pertain in production.

In keeping with that workflow, in this example we first define the Modal Function we want to profile,
without including any of the profiling logic.

That starts with the Function’s environment: the Modal
[App](https://modal.com/docs/guide/apps)

the Function is attached to, the container
[Image](https://modal.com/docs/guide/custom-container)

with the Function’s dependencies, and the hardware requirements of the Function, like a
[GPU](https://modal.com/docs/guide/cuda)

.

```
app = modal.App("example-torch-profiling")  # create an App

image = modal.Image.debian_slim(  # define dependencies
    python_version="3.11"
).pip_install("torch==2.5.1", "numpy==2.1.3")

with image.imports():  # set up common imports
    import torch
```

Here, we define the config as a dictionary so that we can re-use it here
and later, when we attach the profiler. We want to make sure the profiler is in the same environment!

```
config = {"gpu": "a10g", "image": image}
```

The Function we target for profiling appears below. It’s just some simple PyTorch logic
that repeatedly multiplies a random matrix with itself.

The logic is simple, but it demonstrates two common issues with
GPU-accelerated Python code that are relatively easily fixed:

1. Slowing down the issuance of work to the GPU
2. Providing insufficient work for the GPU to complete

We’ll cover these in more detail once we have the profiler set up.

```
@app.function(**config)
def underutilize(scale=1):
    records = []

    x = torch.randn(  # 🐌 2: not enough work to keep the GPU busy
        scale * 100, scale * 100, device="cuda"
    )

    for ii in range(10):
        x = x @ x

        class Record:  # 🐌 1: heavy Python work in the hot loop
            def __init__(self, value):
                self.value = value

        records.append(Record(ii))

    x[0][0].cpu()  # force a host sync for accurate timing
```

Wrapping a Modal Function with a profiler
-----------------------------------------

Now, let’s wrap our
`underutilize`
Function with another Modal Function
that runs PyTorch’s profiler while executing it.

This Function has the same environment
`config`
as
`underutilize`
,
but it also attaches a remote Modal Volume to save profiler outputs.

To increase the flexibility of this approach, we allow it to take the target Function’s name
as an argument. That’s not much use here where there’s only one Function,
but it makes it easier to copy-paste this code into your projects to add profiling.

```
@app.function(volumes={TRACE_DIR: traces}, **config)
def profile(
    function,
    label: Optional[str] = None,
    steps: int = 3,
    schedule=None,
    record_shapes: bool = False,
    profile_memory: bool = False,
    with_stack: bool = False,
    print_rows: int = 0,
    **kwargs,
):
    from uuid import uuid4

    if isinstance(function, str):
        try:
            function = app.registered_functions[function]
        except KeyError:
            raise ValueError(f"Function {function} not found")
    function_name = function.tag

    output_dir = (
        TRACE_DIR / (function_name + (f"_{label}" if label else "")) / str(uuid4())
    )
    output_dir.mkdir(parents=True, exist_ok=True)

    if schedule is None:
        if steps < 3:
            raise ValueError("Steps must be at least 3 when using default schedule")
        schedule = {"wait": 1, "warmup": 1, "active": steps - 2, "repeat": 0}

    schedule = torch.profiler.schedule(**schedule)

    with torch.profiler.profile(
        activities=[
            torch.profiler.ProfilerActivity.CPU,
            torch.profiler.ProfilerActivity.CUDA,
        ],
        schedule=schedule,
        record_shapes=record_shapes,
        profile_memory=profile_memory,
        with_stack=with_stack,
        on_trace_ready=torch.profiler.tensorboard_trace_handler(output_dir),
    ) as prof:
        for _ in range(steps):
            function.local(**kwargs)  # <-- here we wrap the target Function
            prof.step()

    if print_rows:
        print(
            prof.key_averages().table(sort_by="cuda_time_total", row_limit=print_rows)
        )

    trace_path = sorted(
        output_dir.glob("**/*.pt.trace.json"),
        key=lambda pth: pth.stat().st_mtime,
        reverse=True,
    )[0]

    print(f"trace saved to {trace_path.relative_to(TRACE_DIR)}")

    return trace_path.read_text(), trace_path.relative_to(TRACE_DIR)
```

Triggering profiled execution from the command line and viewing in Perfetto
---------------------------------------------------------------------------

We wrap one more layer to make this executable from the command line:
a
`local_entrypoint`
that runs

```
modal run torch_profiling.py --function underutilize --print-rows 10
```

```
@app.local_entrypoint()
def main(
    function: str = "underutilize",
    label: Optional[str] = None,
    steps: int = 3,
    schedule=None,
    record_shapes: bool = False,
    profile_memory: bool = False,
    with_stack: bool = False,
    print_rows: int = 10,
    kwargs_json_path: Optional[str] = None,
):
    if kwargs_json_path is not None:  # use to pass arguments to function
        import json

        kwargs = json.loads(Path(kwargs_json_path).read_text())
    else:
        kwargs = {}

    results, remote_path = profile.remote(
        function,
        label=label,
        steps=steps,
        schedule=schedule,
        record_shapes=record_shapes,
        profile_memory=profile_memory,
        with_stack=with_stack,
        print_rows=print_rows,
        **kwargs,
    )

    output_path = Path("/tmp") / remote_path.name
    output_path.write_text(results)
    print(f"trace saved locally at {output_path}")
```

Underneath the profile results, you’ll also see the path at which the trace was saved on the Volume
and the path at which it was saved locally.

You can view the trace in the free online
[Perfetto UI](https://ui.perfetto.dev)

.

### Improving the performance of our dummy test code

The
`underutilize`
demonstrates two common patterns that leads to unnecessarily low GPU utilization:

1. Slowing down the issuance of work to the GPU
2. Providing insufficient work for the GPU to complete

We simulated 1 in
`underutilize`
by defining a Python class in the middle of the matrix multiplication loop.
This takes on the order of 10 microseconds, roughly the same time it takes our A10 GPU to do the matrix multiplication.
Move it out of the loop to observe a small improvement in utilization. In a real setting,
this code might be useful logging or data processing logic, which we must carefully keep
out of the way of the code driving work on the GPU.

We simulated 2 in
`underutilize`
by providing a matrix that is too small to occupy the GPU for long.
Increase the size of the matrix by a factor of 4 in each dimension (a factor of 16 total),
to increase the utilization without increasing the execution time.

This is an untuitive feature of GPU programming in general: much work is done concurrently
and bottlenecks are non-obvious, so sometimes more work can be done for free or on the cheap.
In a server for large generative models, this might mean producing multiple outputs per user
or handling multiple users at the same time is more economical than it at first seems!

Serving TensorBoard on Modal to view PyTorch profiles and traces
----------------------------------------------------------------

The TensorBoard experiment monitoring server also includes a plugin
for viewing and interpreting the results of PyTorch profiler runs:
the
`torch_tb_profiler`
plugin.

```
tb_image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "tensorboard==2.18.0", "torch_tb_profiler==0.4.3"
)
```

Because TensorBoard is a WSGI app, we can
[host it on Modal](https://modal.com/docs/guide/webhooks)

with the
`modal.wsgi_app`
decorator.

Making this work with Modal requires one extra step:
we add some
[WSGI Middleware](https://peps.python.org/pep-3333/)

that checks the Modal Volume for updates
whenever the whole page is reloaded.

```
class VolumeMiddleware:
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        if (route := environ.get("PATH_INFO")) in ["/", "/modal-volume-reload"]:
            try:
                traces.reload()
            except Exception as e:
                print("Exception while re-loading traces: ", e)
            if route == "/modal-volume-reload":
                environ["PATH_INFO"] = "/"  # redirect
        return self.app(environ, start_response)
```

You can deploy the TensorBoard server defined below with the following command:

```
modal deploy torch_profiling
```

and you can find your server at the URL printed to the terminal.

```
@app.function(
    volumes={TRACE_DIR: traces},
    image=tb_image,
    max_containers=1,  # single replica
    scaledown_window=5 * 60,  # five minute idle time
)
@modal.concurrent(max_inputs=100)  # 100 concurrent request threads
@modal.wsgi_app()
def tensorboard():
    import tensorboard

    board = tensorboard.program.TensorBoard()
    board.configure(logdir=str(TRACE_DIR))
    (data_provider, deprecated_multiplexer) = board._make_data_provider()
    wsgi_app = tensorboard.backend.application.TensorBoardWSGIApp(
        board.flags,
        board.plugin_loaders,
        data_provider,
        board.assets_zip_provider,
        deprecated_multiplexer,
        experimental_middlewares=[VolumeMiddleware],
    )

    return wsgi_app._create_wsgi_app()
```

[Tracing and profiling GPU-accelerated PyTorch programs on Modal](#tracing-and-profiling-gpu-accelerated-pytorch-programs-on-modal)

[Saving traces to a Modal Volume](#saving-traces-to-a-modal-volume)

[Setting up a Modal App with a GPU-accelerated PyTorch Function](#setting-up-a-modal-app-with-a-gpu-accelerated-pytorch-function)

[Wrapping a Modal Function with a profiler](#wrapping-a-modal-function-with-a-profiler)

[Triggering profiled execution from the command line and viewing in Perfetto](#triggering-profiled-execution-from-the-command-line-and-viewing-in-perfetto)

[Improving the performance of our dummy test code](#improving-the-performance-of-our-dummy-test-code)

[Serving TensorBoard on Modal to view PyTorch profiles and traces](#serving-tensorboard-on-modal-to-view-pytorch-profiles-and-traces)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/torch_profiling.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/trtllm_latency
================================================================================

Serve an interactive language model app with latency-optimized TensorRT-LLM (LLaMA 3 8B)
========================================================================================

In this example, we demonstrate how to configure the TensorRT-LLM framework to serve
Meta’s LLaMA 3 8B model at interactive latencies on Modal.

Many popular language model applications, like chatbots and code editing,
put humans and models in direct interaction. According to an
[oft-cited](https://lawsofux.com/doherty-threshold/)

if
[scientifically dubious](https://www.flashover.blog/posts/dohertys-threshold-is-a-lie)

rule of thumb, computer systems need to keep their response times under 400ms
in order to match pace with their human users.

To hit this target, we use the
[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)

inference framework from NVIDIA. TensorRT-LLM is the Lamborghini of inference engines:
it achieves seriously impressive latency, but only if you tune it carefully.
With the out-of-the-box defaults we observe an unacceptable median time
to last token of over a second, but with careful configuration,
we’ll bring that down to under 250ms — over a 4x speed up!
These latencies were measured on a single NVIDIA H100 GPU
running LLaMA 3 8B on prompts and generations of a few dozen to a few hundred tokens.

Here’s what that looks like in a terminal chat interface:

[

](https://modal-cdn.com/example-trtllm-latency.mp4)

Overview
--------

This guide is intended to document two things:

1. the
   [Python API](https://nvidia.github.io/TensorRT-LLM/llm-api)

   for building and running TensorRT-LLM engines, and
2. how to use recommendations from the
   [TensorRT-LLM performance guide](https://github.com/NVIDIA/TensorRT-LLM/blob/b763051ba429d60263949da95c701efe8acf7b9c/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md)

   to optimize the engine for low latency.

Be sure to check out TensorRT-LLM’s
[examples](https://nvidia.github.io/TensorRT-LLM/llm-api-examples)

for sample code beyond what we cover here, like low-rank adapters (LoRAs).

### What is a TRT-LLM engine?

The first step in running TensorRT-LLM is to build an “engine” from a model.
Engines have a large number of parameters that must be tuned on a per-workload basis,
so we carefully document the choices we made here and point you to additional resources
that can help you optimize for your specific workload.

Historically, this process was done with a clunky command-line-interface (CLI),
but things have changed for the better!
2025 is
[the year of CUDA Python](https://twitter.com/blelbach/status/1902842146232865280)

,
including a new-and-improved Python SDK for TensorRT-LLM, supporting
all the same features as the CLI — quantization, speculative decoding, in-flight batching,
and much more.

Installing TensorRT-LLM
-----------------------

To run TensorRT-LLM, we must first install it. Easier said than done!

To run code on Modal, we define
[container images](https://modal.com/docs/guide/images)

.
All Modal containers have access to GPU drivers via the underlying host environment,
but we still need to install the software stack on top of the drivers, from the CUDA runtime up.

We start from an official
`nvidia/cuda`
container image,
which includes the CUDA runtime & development libraries
and the environment configuration necessary to run them.

```
import time
from pathlib import Path

import modal

tensorrt_image = modal.Image.from_registry(
    "nvidia/cuda:12.8.1-devel-ubuntu22.04",
    add_python="3.12",  # TRT-LLM requires Python 3.12
).entrypoint([])  # remove verbose logging by base image on entry
```

On top of that, we add some system dependencies of TensorRT-LLM,
including OpenMPI for distributed communication, some core software like
`git`
,
and the
`tensorrt_llm`
package itself.

```
tensorrt_image = tensorrt_image.apt_install(
    "openmpi-bin", "libopenmpi-dev", "git", "git-lfs", "wget"
).pip_install(
    "tensorrt-llm==0.18.0",
    "pynvml<12",  # avoid breaking change to pynvml version API
    "flashinfer-python==0.2.5",
    pre=True,
    extra_index_url="https://pypi.nvidia.com",
)
```

Note that we’re doing this by
[method-chaining](https://quanticdev.com/articles/method-chaining/)

a number of calls to methods on the
`modal.Image`
. If you’re familiar with
Dockerfiles, you can think of this as a Pythonic interface to instructions like
`RUN`
and
`CMD`
.

End-to-end, this step takes about five minutes on first run.
If you’re reading this from top to bottom,
you might want to stop here and execute the example
with
`modal run`
so that it runs in the background while you read the rest.

Downloading the model
---------------------

Next, we’ll set up a few things to download the model to persistent storage and do it quickly —
this is a latency-optimized example after all! For persistent, distributed storage, we use
[Modal Volumes](https://modal.com/docs/guide/volumes)

, which can be accessed from any container
with read speeds in excess of a gigabyte per second.

We also set the
`HF_HOME`
environment variable to point to the Volume so that the model
is cached there. And we install
`hf-transfer`
to get maximum download throughput from
the Hugging Face Hub, in the hundreds of megabytes per second.

```
volume = modal.Volume.from_name(
    "example-trtllm-inference-volume", create_if_missing=True
)
VOLUME_PATH = Path("/vol")
MODELS_PATH = VOLUME_PATH / "models"

MODEL_ID = "NousResearch/Meta-Llama-3-8B-Instruct"  # fork without repo gating
MODEL_REVISION = "53346005fb0ef11d3b6a83b12c895cca40156b6c"

tensorrt_image = tensorrt_image.pip_install(
    "hf-transfer==0.1.9",
    "huggingface_hub==0.28.1",
).env(
    {
        "HF_HUB_ENABLE_HF_TRANSFER": "1",
        "HF_HOME": str(MODELS_PATH),
    }
)

with tensorrt_image.imports():
    import os

    import torch
    from tensorrt_llm import LLM, SamplingParams
```

Setting up the engine
---------------------

### Quantization

The amount of
[GPU RAM](https://modal.com/gpu-glossary/device-hardware/gpu-ram)

on a single card is a tight constraint for large models:
RAM is measured in billions of bytes and large models have billions of parameters,
each of which is two to four bytes.
The performance cliff if you need to spill to CPU memory is steep,
so all of those parameters must fit in the GPU memory,
along with other things like the KV cache built up while processing prompts.

The simplest way to reduce LLM inference’s RAM requirements is to make the model’s parameters smaller,
fitting their values in a smaller number of bits, like four or eight. This is known as
*quantization*
.

NVIDIA’s
[Ada Lovelace/Hopper chips](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

,
like the L40S and H100, are capable of native 8bit floating point calculations
in their
[Tensor Cores](https://modal.com/gpu-glossary/device-hardware/tensor-core)

,
so we choose that as our quantization format.
These GPUs are capable of twice as many floating point operations per second in 8bit as in 16bit —
about two quadrillion per second on an H100 SXM.

Quantization buys us two things:

* faster startup, since less data has to be moved over the network onto CPU and GPU RAM
* faster inference, since we get twice the FLOP/s and less data has to be moved from GPU RAM into
  [on-chip memory](https://modal.com/gpu-glossary/device-hardware/l1-data-cache)

  and
  [registers](https://modal.com/gpu-glossary/device-hardware/register-file)

  with each computation

We’ll use TensorRT-LLM’s
`QuantConfig`
to specify that we want
`FP8`
quantization.
[See their code](https://github.com/NVIDIA/TensorRT-LLM/blob/88e1c90fd0484de061ecfbacfc78a4a8900a4ace/tensorrt_llm/models/modeling_utils.py#L184)

for more options.

```
N_GPUS = 1  # Bumping this to 2 will improve latencies further but not 2x
GPU_CONFIG = f"H100:{N_GPUS}"

def get_quant_config():
    from tensorrt_llm.llmapi import QuantConfig

    return QuantConfig(quant_algo="FP8")
```

Quantization is a lossy compression technique. The impact on model quality can be
minimized by tuning the quantization parameters on even a small dataset. Typically, we
see less than 2% degradation in evaluation metrics when using
`fp8`
. We’ll use the
`CalibrationConfig`
class to specify the calibration dataset.

```
def get_calib_config():
    from tensorrt_llm.llmapi import CalibConfig

    return CalibConfig(
        calib_batches=512,
        calib_batch_size=1,
        calib_max_seq_length=2048,
        tokenizer_max_seq_length=4096,
    )
```

### Configure plugins

TensorRT-LLM is an LLM inference framework built on top of NVIDIA’s TensorRT,
which is a generic inference framework for neural networks.

TensorRT includes a “plugin” extension system that allows you to adjust behavior,
like configuring the
[CUDA kernels](https://modal.com/gpu-glossary/device-software/kernel)

used by the engine.
The
[General Matrix Multiply (GEMM)](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html)

plugin, for instance, adds heavily-optimized matrix multiplication kernels
from NVIDIA’s
[cuBLAS library of linear algebra routines](https://docs.nvidia.com/cuda/cublas/)

.

We’ll specify a number of plugins for our engine implementation.
The first is
[multiple profiles](https://github.com/NVIDIA/TensorRT-LLM/blob/b763051ba429d60263949da95c701efe8acf7b9c/docs/source/performance/performance-tuning-guide/useful-build-time-flags.md#multiple-profiles)

,
which configures TensorRT to prepare multiple kernels for each high-level operation,
where different kernels are optimized for different input sizes.
The second is
`paged_kv_cache`
which enables a
[paged attention algorithm](https://arxiv.org/abs/2309.06180)

for the key-value (KV) cache.

The last two parameters are GEMM plugins optimized specifically for low latency,
rather than the more typical high arithmetic throughput,
the
`low_latency`
plugins for
`gemm`
and
`gemm_swiglu`
.

The
`low_latency_gemm_swiglu_plugin`
plugin fuses the two matmul operations
and non-linearity of the feedforward component of the Transformer block into a single kernel,
reducing round trips between GPU
[cache memory](https://modal.com/gpu-glossary/device-hardware/l1-data-cache)

and RAM. For details on kernel fusion, see
[this blog post by Horace He of Thinking Machines](https://horace.io/brrr_intro.html)

.
Note that at the time of writing, this only works for
`FP8`
on Hopper GPUs.

The
`low_latency_gemm_plugin`
is a variant of the GEMM plugin that brings in latency-optimized
kernels from NVIDIA’s
[CUTLASS library](https://github.com/NVIDIA/cutlass)

.

```
def get_plugin_config():
    from tensorrt_llm.plugin.plugin import PluginConfig

    return PluginConfig.from_dict(
        {
            "multiple_profiles": True,
            "paged_kv_cache": True,
            "low_latency_gemm_swiglu_plugin": "fp8",
            "low_latency_gemm_plugin": "fp8",
        }
    )
```

### Configure speculative decoding

Speculative decoding is a technique for generating multiple tokens per step,
avoiding the auto-regressive bottleneck in the Transformer architecture.
Generating multiple tokens in parallel exposes more parallelism to the GPU.
It works best for text that has predicable patterns, like code,
but it’s worth testing for any workload where latency is critical.

Speculative decoding can use any technique to guess tokens, including running another,
smaller language model. Here, we’ll use a simple, but popular and effective
speculative decoding strategy called “lookahead decoding”,
which essentially guesses that token sequences from the past will occur again.

```
def get_speculative_config():
    from tensorrt_llm.llmapi import LookaheadDecodingConfig

    return LookaheadDecodingConfig(
        max_window_size=8,
        max_ngram_size=6,
        max_verification_set_size=8,
    )
```

### Set the build config

Finally, we’ll specify the overall build configuration for the engine. This includes
more obvious parameters such as the maximum input length, the maximum number of tokens
to process at once before queueing occurs, and the maximum number of sequences
to process at once before queueing occurs.

To minimize latency, we set the maximum number of sequences (the “batch size”)
to just one. We enforce this maximum by setting the number of inputs that the
Modal Function is allowed to process at once —
`max_concurrent_inputs`
.
The default is
`1`
, so we don’t need to set it, but we are setting it explicitly
here in case you want to run this code with a different balance of latency and throughput.

```
MAX_BATCH_SIZE = MAX_CONCURRENT_INPUTS = 1

def get_build_config():
    from tensorrt_llm import BuildConfig

    return BuildConfig(
        plugin_config=get_plugin_config(),
        speculative_decoding_mode="LOOKAHEAD_DECODING",
        max_input_len=8192,
        max_num_tokens=16384,
        max_batch_size=MAX_BATCH_SIZE,
    )
```

Serving inference under the Doherty Threshold
---------------------------------------------

Now that we have written the code to compile the engine, we can
serve it with Modal!

We start by creating an
`App`
.

```
app = modal.App("trtllm-latency")
```

Thanks to our
[custom container runtime system](https://modal.com/blog/jono-containers-talk)

,
even this large container boots in seconds.

On the first container start, we mount the Volume, download the model, and build the engine,
which takes a few minutes. Subsequent starts will be much faster,
as the engine is cached in the Volume and loaded in seconds.

Container starts are triggered when Modal scales up your Function,
like the first time you run this code or the first time a request comes in after a period of inactivity.
For details on optimizing container start latency, see
[this guide](https://modal.com/docs/guide/cold-start)

.

Container lifecycles in Modal are managed via our
`Cls`
interface, so we define one below
to separate out the engine startup (
`enter`
) and engine execution (
`generate`
).
For details, see
[this guide](https://modal.com/docs/guide/lifecycle-functions)

.

```
MINUTES = 60  # seconds

@app.cls(
    image=tensorrt_image,
    gpu=GPU_CONFIG,
    scaledown_window=10 * MINUTES,
    timeout=10 * MINUTES,
    volumes={VOLUME_PATH: volume},
)
@modal.concurrent(max_inputs=MAX_CONCURRENT_INPUTS)
class Model:
    mode: str = modal.parameter(default="fast")

    def build_engine(self, engine_path, engine_kwargs) -> None:
        llm = LLM(model=self.model_path, **engine_kwargs)
        llm.save(engine_path)
        return llm

    @modal.enter()
    def enter(self):
        from huggingface_hub import snapshot_download
        from transformers import AutoTokenizer

        self.model_path = MODELS_PATH / MODEL_ID

        print("downloading base model if necessary")
        snapshot_download(
            MODEL_ID,
            local_dir=self.model_path,
            ignore_patterns=["*.pt", "*.bin"],  # using safetensors
            revision=MODEL_REVISION,
        )
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)

        if self.mode == "fast":
            engine_kwargs = {
                "quant_config": get_quant_config(),
                "calib_config": get_calib_config(),
                "build_config": get_build_config(),
                "speculative_config": get_speculative_config(),
                "tensor_parallel_size": torch.cuda.device_count(),
            }
        else:
            engine_kwargs = {
                "tensor_parallel_size": torch.cuda.device_count(),
            }

        self.sampling_params = SamplingParams(
            temperature=0.8,
            top_p=0.95,
            max_tokens=1024,  # max generated tokens
            lookahead_config=engine_kwargs.get("speculative_config"),
        )

        engine_path = self.model_path / "trtllm_engine" / self.mode
        if not os.path.exists(engine_path):
            print(f"building new engine at {engine_path}")
            self.llm = self.build_engine(engine_path, engine_kwargs)
        else:
            print(f"loading engine from {engine_path}")
            self.llm = LLM(model=engine_path, **engine_kwargs)

    @modal.method()
    def generate(self, prompt) -> dict:
        start_time = time.perf_counter()
        text = self.text_from_prompt(prompt)
        output = self.llm.generate(text, self.sampling_params)
        latency_ms = (time.perf_counter() - start_time) * 1000

        return output.outputs[0].text, latency_ms

    @modal.method()
    async def generate_async(self, prompt):
        text = self.text_from_prompt(prompt)
        async for output in self.llm.generate_async(
            text, self.sampling_params, streaming=True
        ):
            yield output.outputs[0].text_diff

    def text_from_prompt(self, prompt):
        SYSTEM_PROMPT = (
            "You are a helpful, harmless, and honest AI assistant created by Meta."
        )

        if isinstance(prompt, str):
            prompt = [{"role": "user", "content": prompt}]

        messages = [{"role": "system", "content": SYSTEM_PROMPT}] + prompt

        return self.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )

    @modal.method()
    def boot(self):
        pass  # no-op to start up containers

    @modal.exit()
    def shutdown(self):
        self.llm.shutdown()
        del self.llm
```

Calling our inference function
------------------------------

To run our
`Model`
’s
`.generate`
method from Python, we just need to call it —
with
`.remote`
appended to run it on Modal.

We wrap that logic in a
`local_entrypoint`
so you can run it from the command line with

```
modal run trtllm_latency.py
```

which will output something like:

```
mode=fast inference latency (p50, p90): (211.17ms, 883.27ms)
```

Use
`--mode=slow`
to see model latency without optimizations.

```
modal run trtllm_latency.py --mode=slow
```

which will output something like

```
mode=slow inference latency (p50, p90): (1140.88ms, 2274.24ms)
```

For simplicity, we hard-code 10 questions to ask the model,
then run them one by one while recording the latency of each call.
But the code in the
`local_entrypoint`
is just regular Python code
that runs on your machine — we wrap it in a CLI automatically —
so feel free to customize it to your liking.

```
@app.local_entrypoint()
def main(mode: str = "fast"):
    prompts = [
        "What atoms are in water?",
        "Which F1 team won in 2011?",
        "What is 12 * 9?",
        "Python function to print odd numbers between 1 and 10. Answer with code only.",
        "What is the capital of California?",
        "What's the tallest building in new york city?",
        "What year did the European Union form?",
        "How old was Geoff Hinton in 2022?",
        "Where is Berkeley?",
        "Are greyhounds or poodles faster?",
    ]

    print(f"🏎️  creating container with mode={mode}")
    model = Model(mode=mode)

    print("🏎️  cold booting container")
    model.boot.remote()

    print_queue = []
    latencies_ms = []
    for prompt in prompts:
        generated_text, latency_ms = model.generate.remote(prompt)

        print_queue.append((prompt, generated_text, latency_ms))
        latencies_ms.append(latency_ms)

    time.sleep(3)  # allow remote prints to clear
    for prompt, generated_text, latency_ms in print_queue:
        print(f"Processed prompt in {latency_ms:.2f}ms")
        print(f"Prompt: {prompt}")
        print(f"Generated Text: {generated_text}")
        print("🏎️ " * 20)

    p50 = sorted(latencies_ms)[int(len(latencies_ms) * 0.5) - 1]
    p90 = sorted(latencies_ms)[int(len(latencies_ms) * 0.9) - 1]
    print(f"🏎️  mode={mode} inference latency (p50, p90): ({p50:.2f}ms, {p90:.2f}ms)")
```

Once deployed with
`modal deploy`
, this
`Model.generate`
function
can be called from other Python code. It can also be converted to an HTTP endpoint
for invocation over the Internet by any client.
For details, see
[this guide](https://modal.com/docs/guide/trigger-deployed-functions)

.

As a quick demo, we’ve included some sample chat client code in the
Python main entrypoint below. To use it, first deploy with

```
modal deploy trtllm_latency.py
```

and then run the client with

```
python trtllm_latency.py
```

```
if __name__ == "__main__":
    import sys

    try:
        Model = modal.Cls.from_name("trtllm-latency", "Model")
        print("🏎️  connecting to model")
        model = Model(mode=sys.argv[1] if len(sys.argv) > 1 else "fast")
        model.boot.remote()
    except modal.exception.NotFoundError as e:
        raise SystemError("Deploy this app first with modal deploy") from e

    print("🏎️  starting chat. exit with :q, ctrl+C, or ctrl+D")
    try:
        prompt = []
        while (nxt := input("🏎️  > ")) != ":q":
            prompt.append({"role": "user", "content": nxt})
            resp = ""
            for out in model.generate_async.remote_gen(prompt):
                print(out, end="", flush=True)
                resp += out
            print("\n")
            prompt.append({"role": "assistant", "content": resp})
    except KeyboardInterrupt:
        pass
    except SystemExit:
        pass
    finally:
        print("\n")
        sys.exit(0)
```

[Serve an interactive language model app with latency-optimized TensorRT-LLM (LLaMA 3 8B)](#serve-an-interactive-language-model-app-with-latency-optimized-tensorrt-llm-llama-3-8b)

[Overview](#overview)

[What is a TRT-LLM engine?](#what-is-a-trt-llm-engine)

[Installing TensorRT-LLM](#installing-tensorrt-llm)

[Downloading the model](#downloading-the-model)

[Setting up the engine](#setting-up-the-engine)

[Quantization](#quantization)

[Configure plugins](#configure-plugins)

[Configure speculative decoding](#configure-speculative-decoding)

[Set the build config](#set-the-build-config)

[Serving inference under the Doherty Threshold](#serving-inference-under-the-doherty-threshold)

[Calling our inference function](#calling-our-inference-function)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-serving/trtllm_latency.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/vllm_inference
================================================================================

Run OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM
==============================================================

LLMs do more than just model language: they chat, they produce JSON and XML, they run code, and more.
This has complicated their interface far beyond “text-in, text-out”.
OpenAI’s API has emerged as a standard for that interface,
and it is supported by open source LLM serving frameworks like
[vLLM](https://docs.vllm.ai/en/latest/)

.

In this example, we show how to run a vLLM server in OpenAI-compatible mode on Modal.

Our examples repository also includes scripts for running clients and load-testing for OpenAI-compatible APIs
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/llm-serving/openai_compatible)

.

You can find a (somewhat out-of-date) video walkthrough of this example and the related scripts on the Modal YouTube channel
[here](https://www.youtube.com/watch?v=QmY_7ePR1hM)

.

Set up the container image
--------------------------

Our first order of business is to define the environment our server will run in:
the
[container
`Image`](https://modal.com/docs/guide/custom-container)

.
vLLM can be installed with
`pip`
, since Modal
[provides the CUDA drivers](https://modal.com/docs/guide/cuda)

.

To take advantage of optimized kernels for CUDA 12.8, we install PyTorch, flashinfer, and their dependencies
via an
`extra`
Python package index.

```
import json
from typing import Any

import aiohttp
import modal

vllm_image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "vllm==0.9.1",
        "huggingface_hub[hf_transfer]==0.32.0",
        "flashinfer-python==0.2.6.post1",
        extra_index_url="https://download.pytorch.org/whl/cu128",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})  # faster model transfers
)
```

Download the model weights
--------------------------

We’ll be running a pretrained foundation model — Meta’s LLaMA 3.1 8B
in the Instruct variant that’s trained to chat and follow instructions.

Model parameters are often quantized to a lower precision during training
than they are run at during inference.
We’ll use an eight bit floating point quantization from Neural Magic/Red Hat.
Native hardware support for FP8 formats in
[Tensor Cores](https://modal.com/gpu-glossary/device-hardware/tensor-core)

is limited to the latest
[Streaming Multiprocessor architectures](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

,
like those of Modal’s
[Hopper H100/H200 and Blackwell B200 GPUs](https://modal.com/blog/announcing-h200-b200)

.

You can swap this model out for another by changing the strings below.
A single B200 GPUs has enough VRAM to store a 70,000,000,000 parameter model,
like Llama 3.3, in eight bit precision, along with a very large KV cache.

```
MODEL_NAME = "RedHatAI/Meta-Llama-3.1-8B-Instruct-FP8"
MODEL_REVISION = "12fd6884d2585dd4d020373e7f39f74507b31866"  # avoid nasty surprises when repos update!
```

Although vLLM will download weights from Hugging Face on-demand,
we want to cache them so we don’t do it every time our server starts.
We’ll use
[Modal Volumes](https://modal.com/docs/guide/volumes)

for our cache.
Modal Volumes are essentially a “shared disk” that all Modal Functions can access like it’s a regular disk.

```
hf_cache_vol = modal.Volume.from_name("huggingface-cache", create_if_missing=True)
```

We’ll also cache some of vLLM’s JIT compilation artifacts in a Modal Volume.

```
vllm_cache_vol = modal.Volume.from_name("vllm-cache", create_if_missing=True)
```

Configuring vLLM
----------------

### The V1 engine

In its 0.7 release, in early 2025, vLLM added a new version of its backend infrastructure,
the
[V1 Engine](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html)

.
Using this new engine can lead to some
[impressive speedups](https://github.com/modal-labs/modal-examples/pull/1064)

.
It was made the default in version 0.8 and is
[slated for complete removal by 0.11](https://github.com/vllm-project/vllm/issues/18571)

,
in late summer of 2025.

A small number of features, described in the RFC above, may still require the V0 engine prior to removal.
Until deprecation, you can use it by setting the below environment variable to
`0`
.

```
vllm_image = vllm_image.env({"VLLM_USE_V1": "1"})
```

### Trading off fast boots and token generation performance

vLLM has embraced dynamic and just-in-time compilation to eke out additional performance without having to write too many custom kernels,
e.g. via the Torch compiler and CUDA graph capture.
These compilation features incur latency at startup in exchange for lowered latency and higher throughput during generation.
We make this trade-off controllable with the
`FAST_BOOT`
variable below.

```
FAST_BOOT = True
```

If you’re running an LLM service that frequently scales from 0 (frequent
[“cold starts”](https://modal.com/docs/guide/cold-start)

)
then you’ll want to set this to
`True`
.

If you’re running an LLM service that usually has multiple replicas running, then set this to
`False`
for improved performance.

See the code below for details on the parameters that
`FAST_BOOT`
controls.

For more on the performance you can expect when serving your own LLMs, see
[our LLM engine performance benchmarks](https://modal.com/llm-almanac)

.

Build a vLLM engine and serve it
--------------------------------

The function below spawns a vLLM instance listening at port 8000, serving requests to our model.
We wrap it in the
[`@modal.web_server`
decorator](https://modal.com/docs/guide/webhooks#non-asgi-web-servers)

to connect it to the Internet.

The server runs in an independent process, via
`subprocess.Popen`
, and only starts accepting requests
once the model is spun up and the
`serve`
function returns.

```
app = modal.App("example-vllm-openai-compatible")

N_GPU = 1
MINUTES = 60  # seconds
VLLM_PORT = 8000

@app.function(
    image=vllm_image,
    gpu=f"B200:{N_GPU}",
    scaledown_window=15 * MINUTES,  # how long should we stay up with no requests?
    timeout=10 * MINUTES,  # how long should we wait for container start?
    volumes={
        "/root/.cache/huggingface": hf_cache_vol,
        "/root/.cache/vllm": vllm_cache_vol,
    },
)
@modal.concurrent(  # how many requests can one replica handle? tune carefully!
    max_inputs=32
)
@modal.web_server(port=VLLM_PORT, startup_timeout=10 * MINUTES)
def serve():
    import subprocess

    cmd = [
        "vllm",
        "serve",
        "--uvicorn-log-level=info",
        MODEL_NAME,
        "--revision",
        MODEL_REVISION,
        "--served-model-name",
        MODEL_NAME,
        "llm",
        "--host",
        "0.0.0.0",
        "--port",
        str(VLLM_PORT),
    ]

    # enforce-eager disables both Torch compilation and CUDA graph capture
    # default is no-enforce-eager. see the --compilation-config flag for tighter control
    cmd += ["--enforce-eager" if FAST_BOOT else "--no-enforce-eager"]

    # assume multiple GPUs are for splitting up large matrix multiplications
    cmd += ["--tensor-parallel-size", str(N_GPU)]

    print(cmd)

    subprocess.Popen(" ".join(cmd), shell=True)
```

Deploy the server
-----------------

To deploy the API on Modal, just run

```
modal deploy vllm_inference.py
```

This will create a new app on Modal, build the container image for it if it hasn’t been built yet,
and deploy the app.

Interact with the server
------------------------

Once it is deployed, you’ll see a URL appear in the command line,
something like
`https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run`
.

You can find
[interactive Swagger UI docs](https://swagger.io/tools/swagger-ui/)

at the
`/docs`
route of that URL, i.e.
`https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run/docs`
.
These docs describe each route and indicate the expected input and output
and translate requests into
`curl`
commands.

For simple routes like
`/health`
, which checks whether the server is responding,
you can even send a request directly from the docs.

To interact with the API programmatically in Python, we recommend the
`openai`
library.

See the
`client.py`
script in the examples repository
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/llm-serving/openai_compatible)

to take it for a spin:

```
# pip install openai==1.76.0
python openai_compatible/client.py
```

Testing the server
------------------

To make it easier to test the server setup, we also include a
`local_entrypoint`
that does a healthcheck and then hits the server.

If you execute the command

```
modal run vllm_inference.py
```

a fresh replica of the server will be spun up on Modal while
the code below executes on your local machine.

Think of this like writing simple tests inside of the
`if __name__ == "__main__"`
block of a Python script, but for cloud deployments!

```
@app.local_entrypoint()
async def test(test_timeout=10 * MINUTES, content=None, twice=True):
    url = serve.get_web_url()

    system_prompt = {
        "role": "system",
        "content": "You are a pirate who can't help but drop sly reminders that he went to Harvard.",
    }
    if content is None:
        content = "Explain the singular value decomposition."

    messages = [  # OpenAI chat format
        system_prompt,
        {"role": "user", "content": content},
    ]

    async with aiohttp.ClientSession(base_url=url) as session:
        print(f"Running health check for server at {url}")
        async with session.get("/health", timeout=test_timeout - 1 * MINUTES) as resp:
            up = resp.status == 200
        assert up, f"Failed health check for server at {url}"
        print(f"Successful health check for server at {url}")

        print(f"Sending messages to {url}:", *messages, sep="\n\t")
        await _send_request(session, "llm", messages)
        if twice:
            messages[0]["content"] = "You are Jar Jar Binks."
            print(f"Sending messages to {url}:", *messages, sep="\n\t")
            await _send_request(session, "llm", messages)

async def _send_request(
    session: aiohttp.ClientSession, model: str, messages: list
) -> None:
    # `stream=True` tells an OpenAI-compatible backend to stream chunks
    payload: dict[str, Any] = {"messages": messages, "model": model, "stream": True}

    headers = {"Content-Type": "application/json", "Accept": "text/event-stream"}

    async with session.post(
        "/v1/chat/completions", json=payload, headers=headers, timeout=1 * MINUTES
    ) as resp:
        async for raw in resp.content:
            resp.raise_for_status()
            # extract new content and stream it
            line = raw.decode().strip()
            if not line or line == "data: [DONE]":
                continue
            if line.startswith("data: "):  # SSE prefix
                line = line[len("data: ") :]

            chunk = json.loads(line)
            assert (
                chunk["object"] == "chat.completion.chunk"
            )  # or something went horribly wrong
            print(chunk["choices"][0]["delta"]["content"], end="")
    print()
```

We also include a basic example of a load-testing setup using
`locust`
in the
`load_test.py`
script
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/llm-serving/openai_compatible)

:

```
modal run openai_compatible/load_test.py
```

[Run OpenAI-compatible LLM inference with LLaMA 3.1-8B and vLLM](#run-openai-compatible-llm-inference-with-llama-31-8b-and-vllm)

[Set up the container image](#set-up-the-container-image)

[Download the model weights](#download-the-model-weights)

[Configuring vLLM](#configuring-vllm)

[The V1 engine](#the-v1-engine)

[Trading off fast boots and token generation performance](#trading-off-fast-boots-and-token-generation-performance)

[Build a vLLM engine and serve it](#build-a-vllm-engine-and-serve-it)

[Deploy the server](#deploy-the-server)

[Interact with the server](#interact-with-the-server)

[Testing the server](#testing-the-server)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 06_gpu_and_ml/llm-serving/vllm_inference.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/web-scraper
================================================================================

A simple web scraper
====================

In this guide we’ll introduce you to Modal by writing a simple web scraper.
We’ll explain the foundations of a Modal application step by step.

Set up your first Modal app
---------------------------

Modal apps are orchestrated as Python scripts, but can theoretically run
anything you can run in a container. To get you started, make sure to install
the latest Modal Python package and set up an API token (the first two steps of
the
[Getting started](/home)

page).

Finding links
-------------

First, we create an empty Python file
`scrape.py`
. This file will contain our
application code. Lets write some basic Python code to fetch the contents of a
web page and print the links (href attributes) it finds in the document:

```
import re
import sys
import urllib.request

def get_links(url):
    response = urllib.request.urlopen(url)
    html = response.read().decode("utf8")
    links = []
    for match in re.finditer('href="(.*?)"', html):
        links.append(match.group(1))
    return links

if __name__ == "__main__":
    links = get_links(sys.argv[1])
    print(links)
```

Now obviously this is just pure standard library Python code, and you can run it
on your machine:

```
$ python scrape.py http://example.com
['https://www.iana.org/domains/example']
```

Running it in Modal
-------------------

To make the
`get_links`
function run in Modal instead of your local machine, all
you need to do is

* Import
  `modal`
* Create a
  [`modal.App`](/docs/reference/modal.App)

  instance
* Add a
  `@app.function()`
  annotation to your function
* Replace the
  `if __name__ == "__main__":`
  block with a function decorated with
  [`@app.local_entrypoint()`](/docs/reference/modal.App#local_entrypoint)
* Call
  `get_links`
  using
  `get_links.remote`

```
import re
import urllib.request
import modal

app = modal.App(name="link-scraper")

@app.function()
def get_links(url):
    ...

@app.local_entrypoint()
def main(url):
    links = get_links.remote(url)
    print(links)
```

You can now run this with the Modal CLI, using
`modal run`
instead of
`python`
.
This time, you’ll see additional progress indicators while the script is
running:

```
$ modal run scrape.py --url http://example.com
✓ Initialized.
✓ Created objects.
['https://www.iana.org/domains/example']
✓ App completed.
```

Custom containers
-----------------

In the code above we make use of the Python standard library
`urllib`
library.
This works great for static web pages, but many pages these days use javascript
to dynamically load content, which wouldn’t appear in the loaded html file.
Let’s use the
[Playwright](https://playwright.dev/python/docs/intro)

package to
instead launch a headless Chromium browser which can interpret any javascript
that might be on the page.

We can pass custom container images (defined using
[`modal.Image`](/docs/reference/modal.Image)

) to the
`@app.function()`
decorator. We’ll make use of the
`modal.Image.debian_slim`
pre-bundled image add
the shell commands to install Playwright and its dependencies:

```
playwright_image = modal.Image.debian_slim(python_version="3.10").run_commands(
    "apt-get update",
    "apt-get install -y software-properties-common",
    "apt-add-repository non-free",
    "apt-add-repository contrib",
    "pip install playwright==1.42.0",
    "playwright install-deps chromium",
    "playwright install chromium",
)
```

Note that we don’t have to install Playwright or Chromium on our development
machine since this will all run in Modal. We can now modify our
`get_links`
function to make use of the new tools:

```
@app.function(image=playwright_image)
async def get_links(cur_url: str):
    from playwright.async_api import async_playwright

    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(cur_url)
        links = await page.eval_on_selector_all("a[href]", "elements => elements.map(element => element.href)")
        await browser.close()

    print("Links", links)
    return links
```

Since Playwright has a nice async interface, we’ll redeclare our
`get_links`
function as async (Modal works with both sync and async functions).

The first time you run the function after making this change, you’ll notice that
the output first shows the progress of building the custom image you specified,
after which your function runs like before. This image is then cached so that on
subsequent runs of the function it will not be rebuilt as long as the image
definition is the same.

Scaling out
-----------

So far, our script only fetches the links for a single page. What if we want to
scrape a large list of links in parallel?

We can do this easily with Modal, because of some magic: the function we wrapped
with the
`@app.function()`
decorator is no longer an ordinary function, but a
Modal
[Function](https://modal.com/docs/reference/modal.Function)

object. This
means it comes with a
`map`
property built in, that lets us run this function
for all inputs in parallel, scaling up to as many workers as needed.

Let’s change our code to scrape all urls we feed to it in parallel:

```
@app.local_entrypoint()
def main():
    urls = ["http://modal.com", "http://github.com"]
    for links in get_links.map(urls):
        for link in links:
            print(link)
```

Schedules and deployments
-------------------------

Let’s say we want to log the scraped links daily. We move the print loop into
its own Modal function and annotate it with a
`modal.Period(days=1)`
schedule -
indicating we want to run it once per day. Since the scheduled function will not
run from our command line, we also add a hard-coded list of links to crawl for
now. In a more realistic setting we could read this from a database or other
accessible data source.

```
@app.function(schedule=modal.Period(days=1))
def daily_scrape():
    urls = ["http://modal.com", "http://github.com"]
    for links in get_links.map(urls):
        for link in links:
            print(link)
```

To deploy this as a permanent app, run the command

```
modal deploy scrape.py
```

Running this command deploys this function and then closes immediately. We can
see the deployment and all of its runs, including the printed links, on the
Modal
[Apps page](https://modal.com/apps)

. Rerunning the script will redeploy
the code with any changes you have made - overwriting an existing deploy with
the same name (“link-scraper” in this case).

Integrations and Secrets
------------------------

Instead of looking at the links in the run logs of our deployments, let’s say we
wanted to post them to our
`#scraped-links`
Slack channel. To do this, we can
make use of the
[Slack API](https://api.slack.com/)

and the
`slack-sdk`
[PyPI package](https://pypi.org/project/slack-sdk/)

.

The Slack SDK WebClient requires an API token to get access to our Slack
Workspace, and since it’s bad practice to hardcode credentials into application
code we make use of Modal’s
**Secrets**
. Secrets are snippets of data that will
be injected as environment variables in the containers running your functions.

The easiest way to create Secrets is to go to the
[Secrets section of modal.com](https://modal.com/secrets)

. You can both create a
free-form secret with any environment variables, or make use of presets for
common services. We’ll use the Slack preset and after filling in the necessary
information we are presented with a snippet of code that can be used to post to
Slack using our credentials:

```
import os
slack_sdk_image = modal.Image.debian_slim().pip_install("slack-sdk")

@app.function(image=slack_sdk_image, secrets=[modal.Secret.from_name("my-slack-secret")])
def bot_token_msg(channel, message):
    import slack_sdk
    client = slack_sdk.WebClient(token=os.environ["SLACK_BOT_TOKEN"])
    client.chat_postMessage(channel=channel, text=message)
```

Copy that code as-is, then amend the
`daily_scrape`
function to call
`bot_token_msg`
.

```
@app.function(schedule=modal.Period(days=1))
def daily_scrape():
    urls = ["http://modal.com", "http://github.com"]
    for links in get_links.map(urls):
        for link in links:
            bot_token_msg.remote("scraped-links", link)
```

Note that we are freely making function calls across completely different
container images, as if they were regular Python functions in the same program.

We rerun the script which overwrites the old deploy with our updated code, and
now we get a daily feed of our scraped links in our Slack channel 🎉

Summary
-------

We have shown how you can use Modal to develop distributed Python data
applications using custom containers. Through simple constructs we were able to
add parallel execution. With the change of a single line of code were were able
to go from experimental development code to a deployed application. The full
code of this example can be found
[here](/docs/examples/webscraper)

. We hope
this overview gives you a glimpse of what you are able to build using Modal.

[A simple web scraper](#a-simple-web-scraper)

[Set up your first Modal app](#set-up-your-first-modal-app)

[Finding links](#finding-links)

[Running it in Modal](#running-it-in-modal)

[Custom containers](#custom-containers)

[Scaling out](#scaling-out)

[Schedules and deployments](#schedules-and-deployments)

[Integrations and Secrets](#integrations-and-secrets)

[Summary](#summary)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/webcam
================================================================================

Real-time object detection via webcam
=====================================

This example creates a web endpoint that uses a Huggingface model for object detection.

The web endpoint takes an image from their webcam, and sends it to a Modal web endpoint.
The Modal web endpoint in turn calls a Modal function that runs the actual model.

If you run this, it will look something like this:

![webcam](/_app/immutable/assets/webcam.BpPs9Hiu.png)

Live demo
---------

[Take a look at the deployed app](https://modal-labs-examples--example-webcam-object-detection.modal.run/)

.

A couple of caveats:

* This is not optimized for latency: every prediction takes about 1s, and
  there’s an additional overhead on the first prediction since the containers
  have to be started and the model initialized.
* This doesn’t work on iPhone unfortunately due to some issues with HTML5
  webcam components

Code
----

Starting with imports:

```
import base64
import io
from pathlib import Path

import modal
```

We need to install
[transformers](https://github.com/huggingface/transformers)

which is a package Huggingface uses for all their models, but also
[Pillow](https://github.com/python-pillow/Pillow)

which lets us work with images from Python,
and a system font for drawing.

This example uses the
`facebook/detr-resnet-50`
pre-trained model,
which we’ll cache to a Volume for fast cold starts.

```
MODEL_REPO_ID = "facebook/detr-resnet-50"
MODEL_DIR = "/cache"

app = modal.App("example-webcam-object-detection")
image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "huggingface-hub==0.27.1",
        "Pillow",
        "timm",
        "transformers",
    )
    .apt_install("fonts-freefont-ttf")
    .env({"HF_HUB_CACHE": MODEL_DIR})
)
```

Prediction function
-------------------

The object detection function has a few different features worth mentioning:

* There’s a container initialization step in the method decorated with
  `@enter()`
  ,
  which runs on every container start. This lets us load the model only once per
  container, so that it’s reused for subsequent function calls.
* We’re running it on multiple CPUs for extra performance

Note that the function takes an image and returns a new image.
The input image is from the webcam
The output image is an image with all the bounding boxes and labels on them,
with an alpha channel so that most of the image is transparent so that the
web interface can render it on top of the webcam view.

```
with image.imports():
    import torch
    from huggingface_hub import snapshot_download
    from PIL import Image, ImageColor, ImageDraw, ImageFont
    from transformers import DetrForObjectDetection, DetrImageProcessor
```

We’ll store the model weights in a Volume and provide a function that you can
`modal run`
against to download the model weights prior to deploying the App.
Otherwise, the model weights will be downloaded for the first inference
and cached to the Volume when the first container exits.

```
cache_volume = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.function(image=image, volumes={MODEL_DIR: cache_volume})
def download_model():
    loc = snapshot_download(repo_id=MODEL_REPO_ID)
    print(f"Saved model to {loc}")

@app.cls(image=image, volumes={MODEL_DIR: cache_volume})
class ObjectDetection:
    @modal.enter()
    def load_model(self):
        self.feature_extractor = DetrImageProcessor.from_pretrained(
            MODEL_REPO_ID,
        )
        self.model = DetrForObjectDetection.from_pretrained(
            MODEL_REPO_ID,
        )

    @modal.method()
    def detect(self, img_data_in):
        # Based on https://huggingface.co/spaces/nateraw/detr-object-detection/blob/main/app.py
        # Read png from input
        image = Image.open(io.BytesIO(img_data_in)).convert("RGB")

        # Make prediction
        inputs = self.feature_extractor(image, return_tensors="pt")
        outputs = self.model(**inputs)
        img_size = torch.tensor([tuple(reversed(image.size))])
        processed_outputs = self.feature_extractor.post_process_object_detection(
            outputs=outputs,
            target_sizes=img_size,
            threshold=0,
        )
        output_dict = processed_outputs[0]

        # Grab boxes
        keep = output_dict["scores"] > 0.7
        boxes = output_dict["boxes"][keep].tolist()
        scores = output_dict["scores"][keep].tolist()
        labels = output_dict["labels"][keep].tolist()

        # Plot bounding boxes
        colors = list(ImageColor.colormap.values())
        font = ImageFont.truetype("/usr/share/fonts/truetype/freefont/FreeMono.ttf", 18)
        output_image = Image.new("RGBA", (image.width, image.height))
        output_image_draw = ImageDraw.Draw(output_image)
        for _score, box, label in zip(scores, boxes, labels):
            color = colors[label % len(colors)]
            text = self.model.config.id2label[label]
            box = tuple(map(int, box))
            output_image_draw.rectangle(box, outline=color)
            output_image_draw.text(box[:2], text, font=font, fill=color, width=3)

        # Return PNG as bytes
        with io.BytesIO() as output_buf:
            output_image.save(output_buf, format="PNG")
            return output_buf.getvalue()
```

Defining the web interface
--------------------------

To keep things clean, we define the web endpoints separate from the prediction
function. This will introduce a tiny bit of extra latency (every web request
triggers a Modal function call which will call another Modal function) but in
practice the overhead is much smaller than the overhead of running the prediction
function etc.

We also serve a static html page which contains some tiny bit of Javascript to
capture the webcam feed and send it to Modal.

```
static_path = Path(__file__).with_name("webcam").resolve()

@app.function(
    image=modal.Image.debian_slim(python_version="3.12")
    .pip_install("fastapi[standard]==0.115.4")
    .add_local_dir(static_path, remote_path="/assets")
)
@modal.asgi_app(label="example-webcam-object-detection")
def fastapi_app():
    from fastapi import FastAPI, Request, Response
    from fastapi.staticfiles import StaticFiles

    web_app = FastAPI()

    # The endpoint for the prediction function takes an image as a
    # [data URI](https://en.wikipedia.org/wiki/Data_URI_scheme)
    # and returns another image, also as a data URI:

    @web_app.post("/predict")
    async def predict(request: Request):
        # Takes a webcam image as a datauri, returns a bounding box image as a datauri
        body = await request.body()
        img_data_in = base64.b64decode(body.split(b",")[1])  # read data-uri
        img_data_out = ObjectDetection().detect.remote(img_data_in)
        output_data = b"data:image/png;base64," + base64.b64encode(img_data_out)
        return Response(content=output_data)

    web_app.mount("/", StaticFiles(directory="/assets", html=True))
    return web_app
```

Running this locally
--------------------

You can run this as an ephemeral app, by running

```
modal serve webcam.py
```

[Real-time object detection via webcam](#real-time-object-detection-via-webcam)

[Live demo](#live-demo)

[Code](#code)

[Prediction function](#prediction-function)

[Defining the web interface](#defining-the-web-interface)

[Running this locally](#running-this-locally)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve 06_gpu_and_ml/obj_detection_webcam/webcam.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/webrtc_yolo
================================================================================

Real-time object detection with WebRTC and YOLO
===============================================

This example demonstrates how to architect a serverless real-time streaming application with Modal and WebRTC.
The sample application detects objects in webcam video with YOLO.

See the clip below from a live demo of this example in a course by
[Kwindla Kramer](https://machine-theory.com/)

, WebRTC OG and co-founder of
[Daily](https://www.daily.co/)

.

[

](https://modal-cdn.com/example-webrtc_yolo.mp4)

You can also try our deployment
[here](https://modal-labs-examples--example-webrtc-yolo-webcamobjdet-web.modal.run)

.

What is WebRTC?
---------------

WebRTC (Web Real-Time Communication) is an
[IETF Internet protocol](https://www.rfc-editor.org/rfc/rfc8825)

and a
[W3C API specification](https://www.w3.org/TR/webrtc/)

for real-time media streaming between peers
over internets or the World Wide Web.
What makes it so effective and different from other bidirectional web-based communication protocols (e.g. WebSockets) is that it’s purpose-built for media streaming in real time.
It’s primarily designed for browser applications using the JavaScript API, but
[APIs exist for other languages](https://www.webrtc-developers.com/did-i-choose-the-right-webrtc-stack/)

.
We’ll build our app using Python’s
[`aiortc`](https://aiortc.readthedocs.io/en/latest/)

package.

### What makes up a WebRTC application?

A simple WebRTC app generally consists of three players:

1. a peer that initiates the connection,
2. a peer that responds to the connection, and
3. a server that passes some initial messages between the two peers.

First, one peer initiates the connection by offering up a description of itself - its media sources, codec capabilities, Internet Protocol (IP) addressing info, etc - which is relayed to another peer through the server.
The other peer then either accepts the offer by providing a compatible description of its own capabilities or rejects it if no compatible configuration is possible.
This process is called “signaling” or sometimes the “negotiation” in the WebRTC world, and the server that mediates it is usually called the “signaling server”.

Once the peers have agreed on a configuration there’s a brief pause to establish communication… and then you’re live.

![Basic WebRTC architecture](https://modal-cdn.com/cdnbot/just_webrtc-1oic3iems_a4a8e77c.webp)

A basic WebRTC app architecture

Obviously there’s more going on under the hood.
If you want to get into the details, we recommend checking out the
[RFCs](https://www.rfc-editor.org/rfc/rfc8825)

or a
[more-thorough explainer](https://webrtcforthecurious.com/)

.
In this document, we’ll focus on how to architect a WebRTC application where one or more peer is running on Modal’s serverless cloud infrastructure.

If you just want to quickly get started with WebRTC for a small internal service or a hack project, check out
[our FastRTC example](https://modal.com/docs/examples/fastrtc_flip_webcam)

instead.

How do I run a WebRTC app on Modal?
-----------------------------------

Modal turns Python code into scalable cloud services.
When you call a Modal Function, you get one replica.
If you call it 999 more times before it returns, you have 1000 replicas.
When your Functions all return, you spin down to 0 replicas.

The core constraints of the Modal programming model that make this possible are that Function Calls are stateless and self-contained.
In other words, correctly-written Modal Functions don’t store information in memory between runs (though they might cache data to the ephemeral local disk for efficiency) and they don’t create processes or tasks which must continue to run after the Function Call returns in order for the application to be correct.

WebRTC apps, on the other hand, require passing messages back and forth in a multi-step protocol, and APIs spawn several “agents” (no, AI is not involved, just processes) which do work behind the scenes - including managing the peer-to-peer (P2P) connection itself.
This means that streaming may have only just begun when the application logic in our Function has finished.

![Modal programming model and WebRTC signaling](https://modal-cdn.com/cdnbot/flow_comparisong6iibzq3_638bdd84.webp)

Modal's stateless programming model (left) and WebRTC's stateful signaling (right)

To ensure we properly leverage Modal’s autoscaling and concurrency features, we need to align the signaling and streaming lifetimes with Modal Function Call lifetimes.

The architecture we recommend for this appears below.

![WebRTC on Modal](https://modal-cdn.com/cdnbot/webrtc_with_modal-2horb680q_eab69b28.webp)

A clean architecture for WebRTC on Modal

It handles passing messages between the client peer and the signaling server using a
[WebSocket](https://modal.com/docs/guide/webhooks#websockets)

for persistent, bidirectional communication over the Web within a single Function Call.
(Modal’s Web layer maps HTTP and WS onto Function Calls, details
[here](https://modal.com/blog/serverless-http)

).
We
[`.spawn`](https://modal.com/docs/reference/modal.Function#spawn)

the cloud peer inside the WebSocket endpoint
and communicate it using a
[`modal.Queue`](https://modal.com/docs/reference/modal.Queue)

.

We can then use the state of the P2P connection to determine when to return from the calls to both the signaling server and the cloud peer.
When the P2P connection has been
*established*
, we’ll close the WebSocket which in turn ends the call to the signaling server.
And when the P2P connection has been
*closed*
, we’ll return from the call to the cloud peer.
That way, our WebRTC application benefits from all the autoscaling and concurrency logic built into Modal
that enables users to deliver efficient cloud applications.

We wrote two classes,
`ModalWebRtcPeer`
and
`ModalWebRtcSignalingServer`
, to abstract away that boilerplate as well as a lot of the
`aiortc`
implementation details.
They’re also decorated with Modal
[lifetime hooks](https://modal.com/docs/guide/lifecycle-functions)

.
Add the
[`app.cls`](https://modal.com/docs/reference/modal.App#cls)

decorator and some custom logic, and you’re ready to deploy on Modal.

You can find them in the
[`modal_webrtc.py`
file](https://github.com/modal-labs/modal-examples/blob/main/07_web_endpoints/webrtc/modal_webrtc.py)

provided alongside this example in the
[GitHub repo](https://github.com/modal-labs/modal-examples/tree/main/07_web_endpoints/webrtc/modal_webrtc.py)

.

Using
`modal_webrtc`
to detect objects in webcam footage
--------------------------------------------------------

For our WebRTC app, we’ll take a client’s video stream, run a
[YOLO](https://docs.ultralytics.com/tasks/detect/)

object detector on it with an A100 GPU on Modal, and then stream the annotated video back to the client.
With this setup, we can achieve inference times between 2-4 milliseconds per frame and RTTs below video frame rates (usually around 30 milliseconds per frame).

Let’s get started!

### Setup

We’ll start with a simple container
[Image](https://modal.com/docs/guide/images)

and then

* set it up to properly use TensorRT and the ONNX Runtime, which keep latency minimal,
* install the necessary libs for processing video,
  `opencv`
  and
  `ffmpeg`
  , and
* install the necessary Python packages.

```
import os
from pathlib import Path

import modal

from .modal_webrtc import ModalWebRtcPeer, ModalWebRtcSignalingServer

py_version = "3.12"
tensorrt_ld_path = f"/usr/local/lib/python{py_version}/site-packages/tensorrt_libs"

video_processing_image = (
    modal.Image.debian_slim(python_version=py_version)  # matching ld path
    # update locale as required by onnx
    .apt_install("locales")
    .run_commands(
        "sed -i '/^#\\s*en_US.UTF-8 UTF-8/ s/^#//' /etc/locale.gen",  # use sed to uncomment
        "locale-gen en_US.UTF-8",  # set locale
        "update-locale LANG=en_US.UTF-8",
    )
    .env({"LD_LIBRARY_PATH": tensorrt_ld_path, "LANG": "en_US.UTF-8"})
    # install system dependencies
    .apt_install("python3-opencv", "ffmpeg")
    # install Python dependencies
    .pip_install(
        "aiortc==1.11.0",
        "fastapi==0.115.12",
        "huggingface-hub[hf_xet]==0.30.2",
        "onnxruntime-gpu==1.21.0",
        "opencv-python==4.11.0.86",
        "tensorrt==10.9.0.34",
        "torch==2.7.0",
        "shortuuid==1.0.13",
    )
)
```

### Cache weights and compute graphs on a Volume

We also need to create a Modal
[Volume](https://modal.com/docs/guide/volumes)

to store things we need across replicas —
primarily the model weights and ONNX inference graph, but also a few other artifacts like a video file where
we’ll write out the processed video stream for testing.

The very first time we run the app, downloading the model and building the ONNX inference graph will take a few minutes.
After that, we can load the cached weights and graph from the Volume, which reduces the startup time to about 15 seconds per container.

```
CACHE_VOLUME = modal.Volume.from_name("webrtc-yolo-cache", create_if_missing=True)
CACHE_PATH = Path("/cache")
cache = {CACHE_PATH: CACHE_VOLUME}

app = modal.App("example-webrtc-yolo")
```

### Implement YOLO object detection as a `ModalWebRtcPeer`

Our application needs to process an incoming video track with YOLO and return an annotated video track to the source peer.

To implement a
`ModalWebRtcPeer`
, we need to:

* Decorate our subclass with
  `@app.cls`
  . We provision it with an A100 GPU and a
  [Secret](https://modal.com/docs/guide/secrets)

  credential, described below.
* Implement the method
  `setup_streams`
  . This is where we’ll use
  `aiortc`
  to add the logic for processing the incoming video track with YOLO
  and returning an annotated video track to the source peer.

`ModalWebRtcPeer`
has a few other methods that users can optionally implement:

* `initialize()`
  : This contains any custom initialization logic, called when
  `@modal.enter()`
  is called.
* `run_streams()`
  : Logic for starting streams. This is necessary when the peer is the source of the stream.
  This is where you’d ensure a webcam was running, start playing a video file, or spin up a
  [video generative model](https://modal.com/docs/examples/image_to_video)

  .
* `get_turn_servers()`
  : We haven’t talked about
  [TURN servers](https://datatracker.ietf.org/doc/html/rfc5766)

  ,
  but just know that they’re necessary if you want to use WebRTC across complex (e.g. carrier-grade) NAT or firewall configurations.
  Free services have tight limits because TURN servers are expensive to run (lots of bandwidth and state management required).
  [STUN](https://datatracker.ietf.org/doc/html/rfc5389)

  servers, on the other hand, are essentially just echo servers, and so there are many free services available.
  If you don’t provide TURN servers you can still serve your app on many networks using any of a number of free STUN servers for NAT traversal.
* `exit()`
  : This contains any custom cleanup logic, called when
  `@modal.exit()`
  is called.

In our case, we load the YOLO model in
`initialize`
and provide server information for the free
[Open Relay TURN server](https://www.metered.ca/tools/openrelay/)

.
If you want to use it, you’ll need to create an account
[here](https://dashboard.metered.ca/login?tool=turnserver)

and then create a Modal
[Secret](https://modal.com/docs/guide/secrets)

called
`turn-credentials`
[here](https://modal.com/secrets)

.
We also use the
`@modal.concurrent`
decorator to allow multiple instances of our peer to run on one GPU.

**Setting the Region**

Much of the latency in Internet applications comes from distance between communicating parties —
the Internet operates within a factor of two of the speed of light, but that’s just not that fast.
To minimize latency under this constraint, the physical distance of the P2P connection
between the webcam-using peer and the GPU container needs to be kept as short as possible.
We’ll use the
`region`
parameter of the
`cls`
decorator to set the region of the GPU container.
You should set this to the closest region to your users.
See the
[region selection](https://modal.com/docs/guide/region-selection)

guide for more information.

```
@app.cls(
    image=video_processing_image,
    gpu="A100-40GB",
    volumes=cache,
    secrets=[modal.Secret.from_name("turn-credentials")],
    region="us-east",  # set to your region
)
@modal.concurrent(
    target_inputs=2,  # try to stick to just two peers per GPU container
    max_inputs=3,  # but allow up to three
)
class ObjDet(ModalWebRtcPeer):
    async def initialize(self):
        self.yolo_model = get_yolo_model(CACHE_PATH)

    async def setup_streams(self, peer_id: str):
        from aiortc import MediaStreamTrack

        # keep us notified on connection state changes
        @self.pcs[peer_id].on("connectionstatechange")
        async def on_connectionstatechange() -> None:
            if self.pcs[peer_id]:
                print(
                    f"Video Processor, {self.id}, connection state to {peer_id}: {self.pcs[peer_id].connectionState}"
                )

        # when we receive a track from the source peer
        # we create a processed track and add it to our stream
        # back to the source peer
        @self.pcs[peer_id].on("track")
        def on_track(track: MediaStreamTrack) -> None:
            print(
                f"Video Processor, {self.id}, received {track.kind} track from {peer_id}"
            )

            output_track = get_yolo_track(track, self.yolo_model)  # see Addenda
            self.pcs[peer_id].addTrack(output_track)

            # keep us notified when the incoming track ends
            @track.on("ended")
            async def on_ended() -> None:
                print(
                    f"Video Processor, {self.id}, incoming video track from {peer_id} ended"
                )

    async def get_turn_servers(self, peer_id=None, msg=None) -> dict:
        creds = {
            "username": os.environ["TURN_USERNAME"],
            "credential": os.environ["TURN_CREDENTIAL"],
        }

        turn_servers = [
            {"urls": "stun:stun.relay.metered.ca:80"},  # STUN is free, no creds neeeded
            # for TURN, sign up for the free service here: https://www.metered.ca/tools/openrelay/
            {"urls": "turn:standard.relay.metered.ca:80"} | creds,
            {"urls": "turn:standard.relay.metered.ca:80?transport=tcp"} | creds,
            {"urls": "turn:standard.relay.metered.ca:443"} | creds,
            {"urls": "turns:standard.relay.metered.ca:443?transport=tcp"} | creds,
        ]

        return {"type": "turn_servers", "ice_servers": turn_servers}
```

### Implement a `SignalingServer`

The
`ModalWebRtcSignalingServer`
class is much simpler to implement.
The main thing we need to do is implement the
`get_modal_peer_class`
method which will return our implementation of the
`ModalWebRtcPeer`
class,
`ObjDet`
.

It also has an
`initialize()`
method we can optionally override (called at the beginning of the
[container lifecycle](https://modal.com/docs/guide/lifecycle-functions)

)
as well as a
`web_app`
property which will be
[served by Modal](https://modal.com/docs/guide/webhooks#asgi-apps---fastapi-fasthtml-starlette)

.
We’ll use these to add a frontend which uses the WebRTC JavaScript API to stream a peer’s webcam from the browser.

The JavaScript and HTML files are alongside this example in the
[Github repo](https://github.com/modal-labs/modal-examples/tree/main/07_web_endpoints/webrtc/frontend)

.

```
base_image = (
    modal.Image.debian_slim(python_version="3.12")
    .apt_install("python3-opencv", "ffmpeg")
    .pip_install(
        "fastapi[standard]==0.115.4",
        "aiortc==1.11.0",
        "opencv-python==4.11.0.86",
        "shortuuid==1.0.13",
    )
)

this_directory = Path(__file__).parent.resolve()

server_image = base_image.add_local_dir(
    this_directory / "frontend", remote_path="/frontend"
)

@app.cls(image=server_image)
class WebcamObjDet(ModalWebRtcSignalingServer):
    def get_modal_peer_class(self):
        return ObjDet

    def initialize(self):
        from fastapi.responses import HTMLResponse
        from fastapi.staticfiles import StaticFiles

        self.web_app.mount("/static", StaticFiles(directory="/frontend"))

        @self.web_app.get("/")
        async def root():
            html = open("/frontend/index.html").read()
            return HTMLResponse(content=html)
```

Addenda
-------

The remainder of this page is not central to running a WebRTC application on Modal,
but is included for completeness.

### YOLO helper functions

The two functions below are used to set up the YOLO model and create our custom
[`MediaStreamTrack`](https://aiortc.readthedocs.io/en/latest/api.html#aiortc.MediaStreamTrack)

.

The first,
`get_yolo_model`
, sets up the ONNXRuntime and loads the model weights.
We call this in the
`initialize`
method of the
`ModalWebRtcPeer`
class
so that it only happens once per container.

```
def get_yolo_model(cache_path):
    import onnxruntime

    from .yolo import YOLOv10

    onnxruntime.preload_dlls()
    return YOLOv10(cache_path)
```

The second,
`get_yolo_track`
, creates a custom
`MediaStreamTrack`
that performs object detection on the video stream.
We call this in the
`setup_streams`
method of the
`ModalWebRtcPeer`
class
so it happens once per peer connection.

```
def get_yolo_track(track, yolo_model=None):
    import numpy as np
    import onnxruntime
    from aiortc import MediaStreamTrack
    from aiortc.contrib.media import VideoFrame

    from .yolo import YOLOv10

    class YOLOTrack(MediaStreamTrack):
        """
        Custom media stream track performs object detection
        on the video stream and passes it back to the source peer
        """

        kind: str = "video"
        conf_threshold: float = 0.15

        def __init__(self, track: MediaStreamTrack, yolo_model=None) -> None:
            super().__init__()

            self.track = track
            if yolo_model is None:
                onnxruntime.preload_dlls()
                self.yolo_model = YOLOv10(CACHE_PATH)
            else:
                self.yolo_model = yolo_model

        def detection(self, image: np.ndarray) -> np.ndarray:
            import cv2

            orig_shape = image.shape[:-1]

            image = cv2.resize(
                image,
                (self.yolo_model.input_width, self.yolo_model.input_height),
            )

            image = self.yolo_model.detect_objects(image, self.conf_threshold)

            image = cv2.resize(image, (orig_shape[1], orig_shape[0]))

            return image

        # this is the essential method we need to implement
        # to create a custom MediaStreamTrack
        async def recv(self) -> VideoFrame:
            frame = await self.track.recv()
            img = frame.to_ndarray(format="bgr24")

            processed_img = self.detection(img)

            # VideoFrames are from a really nice package called av
            # which is a pythonic wrapper around ffmpeg
            # and a dependency of aiortc
            new_frame = VideoFrame.from_ndarray(processed_img, format="bgr24")
            new_frame.pts = frame.pts
            new_frame.time_base = frame.time_base

            return new_frame

    return YOLOTrack(track, yolo_model)
```

### Testing a WebRTC application on Modal

As any seasoned developer of real-time applications on the Web will tell you,
testing and ensuring correctness is quite difficult. We spent nearly as much time
designing and troubleshooting an appropriate testing process for this application as we did writing
the application itself!

You can find the testing code in the GitHub repository
[here](https://github.com/modal-labs/modal-examples/tree/main/07_web_endpoints/webrtc/webrtc_yolo_test.py)

.

[Real-time object detection with WebRTC and YOLO](#real-time-object-detection-with-webrtc-and-yolo)

[What is WebRTC?](#what-is-webrtc)

[What makes up a WebRTC application?](#what-makes-up-a-webrtc-application)

[How do I run a WebRTC app on Modal?](#how-do-i-run-a-webrtc-app-on-modal)

[Using modal\_webrtc to detect objects in webcam footage](#using-modal_webrtc-to-detect-objects-in-webcam-footage)

[Setup](#setup)

[Cache weights and compute graphs on a Volume](#cache-weights-and-compute-graphs-on-a-volume)

[Implement YOLO object detection as a ModalWebRtcPeer](#implement-yolo-object-detection-as-a-modalwebrtcpeer)

[Implement a SignalingServer](#implement-a-signalingserver)

[Addenda](#addenda)

[YOLO helper functions](#yolo-helper-functions)

[Testing a WebRTC application on Modal](#testing-a-webrtc-application-on-modal)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal serve -m 07_web_endpoints.webrtc.webrtc_yolo
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/webscraper
================================================================================

Web Scraping on Modal
=====================

This example shows how you can scrape links from a website and post them to a Slack channel using Modal.

```
import os

import modal

app = modal.App("example-linkscraper")

playwright_image = modal.Image.debian_slim(
    python_version="3.10"
).run_commands(  # Doesn't work with 3.11 yet
    "apt-get update",
    "apt-get install -y software-properties-common",
    "apt-add-repository non-free",
    "apt-add-repository contrib",
    "pip install playwright==1.42.0",
    "playwright install-deps chromium",
    "playwright install chromium",
)

@app.function(image=playwright_image)
async def get_links(url: str) -> set[str]:
    from playwright.async_api import async_playwright

    async with async_playwright() as p:
        browser = await p.chromium.launch()
        page = await browser.new_page()
        await page.goto(url)
        links = await page.eval_on_selector_all(
            "a[href]", "elements => elements.map(element => element.href)"
        )
        await browser.close()

    return set(links)

slack_sdk_image = modal.Image.debian_slim(python_version="3.10").pip_install(
    "slack-sdk==3.27.1"
)

@app.function(
    image=slack_sdk_image,
    secrets=[
        modal.Secret.from_name(
            "scraper-slack-secret", required_keys=["SLACK_BOT_TOKEN"]
        )
    ],
)
def bot_token_msg(channel, message):
    import slack_sdk
    from slack_sdk.http_retry.builtin_handlers import RateLimitErrorRetryHandler

    client = slack_sdk.WebClient(token=os.environ["SLACK_BOT_TOKEN"])
    rate_limit_handler = RateLimitErrorRetryHandler(max_retry_count=3)
    client.retry_handlers.append(rate_limit_handler)

    print(f"Posting {message} to #{channel}")
    client.chat_postMessage(channel=channel, text=message)

@app.function()
def scrape():
    links_of_interest = ["http://modal.com"]

    for links in get_links.map(links_of_interest):
        for link in links:
            bot_token_msg.remote("scraped-links", link)

@app.function(schedule=modal.Period(days=1))
def daily_scrape():
    scrape.remote()

@app.local_entrypoint()
def run():
    scrape.remote()
```

[Web Scraping on Modal](#web-scraping-on-modal)

Try this on Modal!
------------------

You can run this example on Modal in 60 seconds.

[Create account to run](/signup)

After creating a free account, install the Modal Python package, and
create an API token.

$

```
pip install modal
```

$

```
modal setup
```

Clone the
[modal-examples](https://github.com/modal-labs/modal-examples)
repository and run:

$

```
git clone https://github.com/modal-labs/modal-examples
```

$

```
cd modal-examples
```

$

```
modal run 10_integrations/webscraper.py
```

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/examples/whisper-transcriber
================================================================================

Parallel podcast transcription using Whisper
============================================

This example shows how to build a massively parallel application on Modal:
the
[Modal Podcast Transcriber](https://modal-labs-examples--whisper-pod-transcriber-fastapi-app.modal.run/)

.

[![homepage of modal whisper transcriber app](/_app/immutable/assets/modal-podcast-transcriber-frontpage.CDX3OEI-.png)](https://modal-labs-examples--whisper-pod-transcriber-fastapi-app.modal.run/)

This example application is more feature-packed than others, and it doesn’t fit in
a single page of code and commentary. So instead of progressing through the
example’s code linearly, this document provides a higher-level walkthrough of how
Modal is used to do fast, on-demand podcast episode transcription for whichever
podcast you’d like.

You can find the code
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/openai_whisper/pod_transcriber)

.

Hour-long episodes transcribed in just 1 minute
-----------------------------------------------

The focal point of this demonstration app is that it does serverless CPU
transcription across dozens of containers at the click of a button, completing
hour-long audio files in just 1 minute.

We use a podcast metadata API to allow users to transcribe an arbitrary episode
from whatever niche podcast they desire —
[how about
*The Pen Addict*
, a podcast dedicated to stationery](https://modal-labs-examples--whisper-pod-transcriber-fastapi-app.modal.run/#/episode/157765/)

?

The video below shows the 45-minute long first episode of
[*Serial*
season 2](https://serialpodcast.org/season-two/1/dustwun)

get
transcribed in 62 seconds.

[

](https://user-images.githubusercontent.com/12058921/199637855-d98bcabe-bff4-433b-a58f-1e309d69e14d.mp4)

Each transcription segment includes links back to the original audio.

[

](https://user-images.githubusercontent.com/12058921/199637370-1cb1e070-8f60-4cc6-8c51-dc42bebcf29d.mp4)

### Try it yourself

If you’re itching to see this in action, here are links to begin transcribing
three popular podcasts:

1. [*Case 63*
   by Gimlet Media](https://modal-labs-examples--whisper-pod-transcriber-fastapi-app.modal.run/#/podcast/4951910)
2. [*The Joe Rogan Experience*](https://modal-labs-examples--whisper-pod-transcriber-fastapi-app.modal.run/#/podcast/10829)
3. [*The Psychology of your 20s*](https://modal-labs-examples--whisper-pod-transcriber-fastapi-app.modal.run/#/podcast/4295070)

Tech-stack overview
-------------------

The entire application is hosted serverlessly on Modal and consists of these
main components:

* A React +
  [Vite](https://vitejs.dev/)

  single page application (SPA) deployed
  as static files into a Modal web endpoint.
* A Python backend running
  [FastAPI](https://fastapi.tiangolo.com/)

  in a Modal web endpoint.
* The
  [Podchaser API](https://api-docs.podchaser.com/docs/overview)

  provides
  podcast search and episode metadata retrieval. It’s hooked into our code with
  a
  [Modal Secret](/docs/guide/secrets)

  .
* A Modal async job queue, described in more detail below.

All of this is deployed with one command and costs
`$0.00`
when it’s not
transcribing podcasts or serving HTTP requests.

### Speed-boosting Whisper with parallelism

Modal’s dead-simple parallelism primitives are the key to doing the
transcription so quickly. Even with a GPU, transcribing a full episode serially
was taking around 10 minutes.

But by pulling in
`ffmpeg`
with a simple
`.pip_install("ffmpeg-python")`
addition to our Modal Image, we could exploit the natural silences of the
podcast medium to partition episodes into hundreds of short segments. Each
segment is transcribed by Whisper in its own container task,
and when all are done we stitch the segments back together with only a
minimal loss in transcription quality. This approach actually accords quite well
with Whisper’s model architecture:

> “The Whisper architecture is a simple end-to-end approach, implemented as an
> encoder-decoder Transformer. Input audio is split into 30-second chunks,
> converted into a log-Mel spectrogram, and then passed into an encoder.”
>
> ―
> [*Introducing Whisper*](https://openai.com/blog/whisper/)

Run this app on Modal
---------------------

All source code for this example can be
[found on GitHub](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/openai_whisper/pod_transcriber)

.
The
`README.md`
includes instructions on setting up the frontend build and
getting authenticated with the Podchaser API. Happy transcribing!

[Parallel podcast transcription using Whisper](#parallel-podcast-transcription-using-whisper)

[Hour-long episodes transcribed in just 1 minute](#hour-long-episodes-transcribed-in-just-1-minute)

[Try it yourself](#try-it-yourself)

[Tech-stack overview](#tech-stack-overview)

[Speed-boosting Whisper with parallelism](#speed-boosting-whisper-with-parallelism)

[Run this app on Modal](#run-this-app-on-modal)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference
================================================================================

API Reference
=============

This is the API reference for the
[`modal`](https://pypi.org/project/modal/)

Python package, which allows you to run distributed applications on Modal.

The reference is intended to be limited to low-level descriptions of various
programmatic functionality. If you’re just getting started with Modal, we would
instead recommend looking at the
[guide](/docs/guide)

first
or to get started quickly with an
[example](/docs/examples)

.

Application construction
------------------------

|  |  |
| --- | --- |
| [`App`](/docs/reference/modal.App) | The main unit of deployment for code on Modal |
| [`App.function`](/docs/reference/modal.App#function) | Decorator for registering a function with an App |
| [`App.cls`](/docs/reference/modal.App#cls) | Decorator for registering a class with an App |

Serverless execution
--------------------

|  |  |
| --- | --- |
| [`Function`](/docs/reference/modal.Function) | A serverless function backed by an autoscaling container pool |
| [`Cls`](/docs/reference/modal.Cls) | A serverless class supporting parametrization and lifecycle hooks |

Extended Function configuration
-------------------------------

### Class parametrization

|  |  |
| --- | --- |
| [`parameter`](/docs/reference/modal.parameter) | Used to define class parameters, akin to a Dataclass field |

### Lifecycle hooks

|  |  |
| --- | --- |
| [`enter`](/docs/reference/modal.enter) | Decorator for a method that will be executed during container startup |
| [`exit`](/docs/reference/modal.exit) | Decorator for a method that will be executed during container shutdown |
| [`method`](/docs/reference/modal.method) | Decorator for exposing a method as an invokable function |

### Web integrations

|  |  |
| --- | --- |
| [`fastapi_endpoint`](/docs/reference/modal.fastapi_endpoint) | Decorator for exposing a simple FastAPI-based endpoint |
| [`asgi_app`](/docs/reference/modal.asgi_app) | Decorator for functions that construct an ASGI web application |
| [`wsgi_app`](/docs/reference/modal.wsgi_app) | Decorator for functions that construct a WSGI web application |
| [`web_server`](/docs/reference/modal.web_server) | Decorator for functions that construct an HTTP web server |

### Function semantics

|  |  |
| --- | --- |
| [`batched`](/docs/reference/modal.batched) | Decorator that enables [dynamic input batching](/docs/guide/dynamic-batching) |
| [`concurrent`](/docs/reference/modal.concurrent) | Decorator that enables [input concurrency](/docs/guide/concurrent-inputs) |

### Scheduling

|  |  |
| --- | --- |
| [`Cron`](/docs/reference/modal.Cron) | A schedule that runs based on cron syntax |
| [`Period`](/docs/reference/modal.Period) | A schedule that runs at a fixed interval |

### Exception handling

|  |  |
| --- | --- |
| [`Retries`](/docs/reference/modal.Retries) | Function retry policy for input failures |

Sandboxed execution
-------------------

|  |  |
| --- | --- |
| [`Sandbox`](/docs/reference/modal.Sandbox) | An interface for restricted code execution |
| [`ContainerProcess`](/docs/reference/modal.container_process#modalcontainer_processcontainerprocess) | An object representing a sandboxed process |
| [`FileIO`](/docs/reference/modal.file_io#modalfile_iofileio) | A handle for a file in the Sandbox filesystem |

Container configuration
-----------------------

|  |  |
| --- | --- |
| [`Image`](/docs/reference/modal.Image) | An API for specifying container images |
| [`Secret`](/docs/reference/modal.Secret) | A pointer to secrets that will be exposed as environment variables |

Data primitives
---------------

### Persistent storage

|  |  |
| --- | --- |
| [`Volume`](/docs/reference/modal.Volume) | Distributed storage supporting highly performant parallel reads |
| [`CloudBucketMount`](/docs/reference/modal.CloudBucketMount) | Storage backed by a third-party cloud bucket (S3, etc.) |
| [`NetworkFileSystem`](/docs/reference/modal.NetworkFileSystem) | Shared, writeable cloud storage (superseded by `modal.Volume` ) |

### In-memory storage

|  |  |
| --- | --- |
| [`Dict`](/docs/reference/modal.Dict) | A distributed key-value store |
| [`Queue`](/docs/reference/modal.Queue) | A distributed FIFO queue |

Networking
----------

|  |  |
| --- | --- |
| [`Proxy`](/docs/reference/modal.Proxy) | An object that provides a static outbound IP address for containers |
| [`forward`](/docs/reference/modal.forward) | A context manager for publicly exposing a port from a container |

[API Reference](#api-reference)

[Application construction](#application-construction)

[Serverless execution](#serverless-execution)

[Extended Function configuration](#extended-function-configuration)

[Class parametrization](#class-parametrization)

[Lifecycle hooks](#lifecycle-hooks)

[Web integrations](#web-integrations)

[Function semantics](#function-semantics)

[Scheduling](#scheduling)

[Exception handling](#exception-handling)

[Sandboxed execution](#sandboxed-execution)

[Container configuration](#container-configuration)

[Data primitives](#data-primitives)

[Persistent storage](#persistent-storage)

[In-memory storage](#in-memory-storage)

[Networking](#networking)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/changelog
================================================================================

Changelog
=========

This changelog documents user-facing updates (features, enhancements, fixes, and deprecations) to the
`modal`
client library.

Latest
------

#### 1.1.0 (2025-07-17)

This release introduces support for the
`2025.06`
[Image Builder Version](https://modal.com/docs/guide/images#image-builder-updates)

, which is in a “preview” state. The new image builder includes several major changes to how the Modal client dependencies are included in Modal Images. These improvements should greatly reduce the risk of conflicts with user code dependencies. They also allow Modal Sandboxes to easily be used with existing Images or Dockerfiles that are not themselves compatible with the Modal client library. You can see more details and update your Workspace on its
[Image Config](https://modal.com/settings/image-config)

page. Please share any issues that you encounter as we work to make the version stable.

We’re also introducing first-class support for building Modal Images with the
[uv package manager](https://docs.astral.sh/uv/)

through the new
[`modal.Image.uv_pip_install`](https://modal.com/docs/reference/modal.Image#uv_pip_install)

and
[`modal.Image.uv_sync`](https://modal.com/docs/reference/modal.Image#uv_sync)

methods:

```
import modal

# uv_pip_install accepts a list of packages, like pip_install, but up to 50% faster
image = modal.Image.debian_slim().uv_pip_install("torch==2.7.1", "numpy==2.3.1")

# uv_sync accepts a local `uv_project_dir` (defaulting to the local working directory)
# and uses the pyproject.toml and uv.lock files to specify the environment
image = modal.Image.debian_slim().uv_sync()
```

Please note that, as these methods are new, there is some chance that future releases will need to fix bugs or address edge cases in ways that break the cache for existing Images. When using
`modal.Image.uv_pip_install`
, we recommend pinning dependency versions so that any necessary rebuilds produce a consistent environment.

This release also includes a number of other new features and bug fixes:

* Optimized handling of the
  `ignore`
  parameter in
  `Image.add_local_dir`
  and similar methods for cases where entire directories are ignored.
* Added a
  `poetry_version`
  parameter to
  `modal.Image.poetry_install_from_file`
  , which supports installing a specific version of
  `poetry`
  . It’s also possible to set
  `poetry_version=None`
  to skip the install step, i.e. when poetry is already available in the Image.
* Added a
  [`modal.Sandbox.reload_volumes`](https://modal.com/docs/reference/modal.Sandbox#reload_volumes)

  method, which triggers a reload of all Volumes currently mounted inside a running Sandbox.
* Added a
  `build_args`
  parameter to
  `modal.Image.from_dockerfile`
  for passing arguments through to
  `ARG`
  instructions in the Dockerfile.
* It’s now possible to use
  `@modal.experimental.clustered`
  and
  `i6pn`
  networking with
  `modal.Cls`
  .
* Fixed a bug where
  `Cls.with_options`
  would fail when provided with a
  `modal.Secret`
  object that was already hydrated.
* Fixed a bug where the timeout specified in
  `modal.Sandbox.exec()`
  was not respected by
  `modal.Sandbox.wait()`
  or
  `modal.Sandbox.poll()`
  .
* Fixed retry handling when using
  `modal run --detach`
  directly against a remote Function.

Finally, this release introduces a small number of deprecations and potentially-breaking changes:

* We now raise
  `modal.exception.NotFoundError`
  in all cases where Modal object lookups fail; previously some methods could leak an internal
  `GRPCError`
  with a
  `NOT_FOUND`
  status.
* We’re enforcing pre-1.0 deprecations on
  `modal.build`
  ,
  `modal.Image.copy_local_file`
  , and
  `modal.Image.copy_local_dir`
  .
* We’re deprecating the
  `environment_name`
  parameter in
  `modal.Sandbox.create()`
  . A Sandbox’s environment association will now be determined by its parent App. This should have no user-facing effects.
* We’ve deprecated the
  `namespace`
  parameter in the
  `.from_name`
  methods of
  `Function`
  ,
  `Cls`
  ,
  `Dict`
  ,
  `Queue`
  ,
  `Volume`
  ,
  `NetworkFileSystem`
  , and
  `Secret`
  , along with
  `modal.runner.deploy_app`
  . These object types do not have a concept of distinct namespaces.

### 1.0.5 (2025-06-27)

* Added a
  [`modal.Volume.read_only`](/docs/reference/modal.Volume#read_only)

  method, which will configure a Volume instance to disallow writes:

  ```
  vol = modal.Volume.from_name("models")
  read_only_vol = vol.read_only()

  @app.function(volumes={"/models": read_only_vol})
  def f():
      with open("/models/weights.pt", "w") as fid:  # Raises an OSError
          ...

  @app.local_entrypoint()
  def main():
      with read_only_vol.batch_upload() as batch:  # Raises a modal.exceptions.InvalidError
          ...

      with vol.batch_upload() as batch:  # This instance is still writeable
          ...
  ```

* Introduced a gradual fix for a bug where
  `Function.map`
  and
  `Function.starmap`
  leak an internal exception wrapping type (
  `modal.exceptions.UserCodeException`
  ) when
  `return_exceptions=True`
  is set. To avoid breaking any user code that depends on the specific types in the return list, these functions will continue returning the wrapper type by default, but they now issue a deprecation warning. To opt into the future behavior and silence the warning, you can set
  `wrap_returned_exceptions=False`
  in the call to
  `.map`
  or
  `.starmap`
  .
* When an
  `@app.cls()`
  -decorated class inherits from a class (or classes) with
  `modal.parameter()`
  annotations, the parent parameters will now be inherited and included in the parameter set for the modal Cls.
* Redeployments that migrate parameterized functions from an explicit constructor to
  `modal.parameter()`
  annotations will now handle requests from outdated clients more gracefully, avoiding a problem where new containers would crashloop on a deserialization error.
* The Modal client will now retry its initial connection to the Modal server, improving stability on flaky networks.

### 1.0.4 (2025-06-13)

* When
  `modal.Cls.with_options`
  is called multiple times on the same instance, the overrides will now be merged. For example, the following configuration will use an H100 GPU and request 16 CPU cores:

  ```
  Model.with_options(gpu="A100", cpu=16).with_options(gpu="H100")
  ```

* Added a
  `--secret`
  option to
  `modal shell`
  for including environment variables defined by named Secret(s) in the shell session:

  ```
  modal shell --secret huggingface --secret wandb
  ```

* Added a
  `verbose: bool`
  option to
  `modal.Sandbox.create()`
  . When this is set to
  `True`
  , execs and file system operations will appear in the Sandbox logs.
* Updated
  `modal.Sandbox.watch()`
  so that exceptions are now raised in (and can be caught by) the calling task.

### 1.0.3 (2025-06-05)

* Added support for specifying a timezone on
  `Cron`
  schedules, which allows you to run a Function at a specific local time regardless of daylight savings:

  ```
  import modal
  app = modal.App()

  @app.function(schedule=modal.Cron("* 6 * * *"), timezone="America/New_York")  # Use tz database naming conventions
  def f():
      print("This function will run every day at 6am New York time.")
  ```

* Added an
  `h2_ports`
  parameter to
  `Sandbox.create`
  , which exposes encrypted ports using HTTP/2. The following example will create an H2 port on 5002 and a port using HTTPS over HTTP/1.1 on 5003:

  ```
  sb = modal.Sandbox.create(app=app, h2_ports = [5002], encrypted_ports = [5003])
  ```

* Added
  `--from-dotenv`
  and
  `--from-json`
  options to
  `modal secret create`
  , which will read from local files to populate Secret contents.
* `Sandbox.terminate`
  no longer waits for container shutdown to complete before returning. It still ensures that a terminated container will shutdown imminently. To restore the previous behavior (i.e., to wait until the Sandbox is actually terminated), call
  `sb.wait(raise_on_termination=False)`
  after calling
  `sb.terminate()`
  .
* Improved performance and stability for
  `modal volume get`
  .
* Fixed a rare race condition that could sometimes make
  `Function.map`
  and similar calls deadlock.
* Fixed an issue where
  `Function.map`
  and similar methods would stall for 55 seconds when passed an empty iterator as input instead of completing immediately.
* We now raise an error during App setup when using interactive mode without the
  `modal.enable_output`
  context manager. Previously, this would run the App but raise when
  `modal.interact()`
  was called.

### 1.0.2 (2025-05-26)

* Fixed an incompatibility with breaking changes in
  `aiohttp`
  v3.12.0, which caused issues with Volume and large input uploads. The issues typically manifest as
  `Local data and remote data checksum mismatch`
  or
  `'_io.BufferedReader' object has no attribute 'getbuffer'`
  errors.

### 1.0.1 (2025-05-19)

* Added a
  `--timestamps`
  flag to
  `modal app logs`
  that prepends a timestamp to each log line.
* Fixed a bug where objects returned by
  `Sandbox.list`
  had
  `returncode == 0`
  for
  *running*
  Sandboxes. Now the return code for running Sandboxes will be
  `None`
  .
* Fixed a bug affecting systems where the
  `sys.platform.node`
  name includes unicode characters.

### 1.0.0 (2025-05-16)

With this release, we’re beginning to enforce the deprecations discussed in the
[1.0 migration guide](https://modal.com/docs/guide/modal-1-0-migration)

. Going forward, we’ll include breaking changes for outstanding deprecations in
`1.Y.0`
releases, so we recommend pinning Modal on a minor version (
`modal~=1.0.0`
) if you have not addressed the existing warnings. While we’ll continue to make improvements to the Modal API, new deprecations will be introduced at a substantially reduced rate, and support windows for older client versions will lengthen.

⚠️ In this release, we’ve made some breaking changes to Modal’s “automounting” behavior.️ If you’ve not already adapted your source code in response to warnings about automounting, Apps built with 1.0+ will have different files included and may not run as expected:

* Previously, Modal containers would automatically include the source for local Python packages that were imported by your Modal App. Going forward, it will be necessary to explicitly include such packages in the Image (i.e., with
  `modal.Image.add_local_python_source`
  ).
* Support for the
  `automount`
  configuration (
  `MODAL_AUTOMOUNT`
  ) has been removed; this environment variable will no longer have any effect.
* Modal will continue to automatically include the Python module or package where the Function is defined. If the Function is defined within a package, the entire directory tree containing the package will be mounted. This limited automounting can also be disabled in cases where your Image definition already includes the package defining the Function: set
  `include_source=False`
  in the
  `modal.App`
  constructor or
  `@app.function`
  decorator.

Additionally, we have enforced a number of previously-introduced deprecations:

* Removed
  `modal.Mount`
  as a public object, along with various
  `mount=`
  parameters where Mounts could be passed into the Modal API. Usage can be replaced with
  `modal.Image`
  methods, e.g.:

  ```
  @app.function(image=image, mounts=[modal.Mount.from_local_dir("data", "/root/data")])  # This is now an error!
  @app.function(image=image.add_local_dir("data", "/root/data"))  # Correct spelling
  ```

* Removed the
  `show_progress`
  parameter from
  `modal.App.run`
  . This parameter has been replaced by the
  `modal.enable_output`
  context manager:

  ```
  with modal.enable_output(), app.run():
    ...  # Will produce verbose Modal output
  ```

* Passing flagged options to the
  `Image.pip_install`
  package list will now raise an error. Use the
  `extra_options`
  parameter to specify options that aren’t exposed through the
  `Image.pip_install`
  signature:

  ```
  image.pip_install("flash-attn", "--no-build-isolation")  # This is now an error!
  image.pip_install("flash-attn", extra_options="--no-build-isolation")  # Correct spelling
  ```

* Removed backwards compatibility for using
  `label=`
  or
  `tag=`
  keywords in object lookup methods. We standardized these methods to use
  `name=`
  as the parameter name, but we recommend using positional arguments:

  ```
  f = modal.Function.from_name("my-app", tag="f")  # No longer supported! Will raise an error!
  f = modal.Function.from_name("my-app", "f")  # Preferred spelling
  ```

* It’s no longer possible to invoke a generator Function with
  `Function.spawn`
  ; previously this warned, now it raises an
  `InvalidError`
  . Additionally, the
  `FunctionCall.get_gen`
  method has been removed, and it’s no longer possible to set
  `is_generator`
  when using
  `FunctionCall.from_id`
  .
* Removed the
  `.resolve()`
  method on Modal objects. This method had not been publicly documented, but where used it can be replaced straightforwardly with
  `.hydrate()`
  . Note that explicit hydration should rarely be necessary: in most cases you can rely on lazy hydration semantics (i.e., objects will be hydrated when the first method that requires server metadata is called).
* Functions decorated with
  `@modal.asgi_app`
  or
  `@modal.wsgi_app`
  are now required to be nullary. Previously, we warned in the case where a function was defined with parameters that all had default arguments.
* Referencing the deprecated
  `modal.Stub`
  object will now raise an
  `AttributeError`
  , whereas previously it was an alias for
  `modal.App`
  . This is a simple name change.

0.77
----

### 0.77.0 (2025-05-13)

* This is the final pre-1.0 release of the Modal client. The next release will be version 1.0. While we do not plan to enforce most major deprecations until later in the 1.0 cycle, there will be some breaking changes introduced in the next release.

0.76
----

### 0.76.3 (2025-05-12)

* Fixed the behavior of
  `modal app history --json`
  when the history contains versions with and without commit information or “tag” metadata. Commit information is now always included (with a
  `null`
  placeholder when absent), while tag metadata is included only when there is at least one tagged release (other releases will have a
  `null`
  placeholder).

### 0.76.0 (2025-05-12)

* Fixed the behavior of
  `ignore=`
  in
  `modal.Image`
  methods, including when
  `.dockerignore`
  files are implicitly used in docker-oriented methods. This may result in Image rebuilds with different final inventories:
  + When using
    `modal.Image.add_local_dir`
    , exclusion patterns are now correctly interpreted as relative to the directory being added (e.g.,
    `*.json`
    will now ignore all json files in the top-level of the directory).
  + When using
    `modal.Image.from_dockerfile`
    , exclusion patterns are correctly interpreted as relative to the context directory.
  + As in Docker, leading or trailing path delimiters are stripped from the ignore patterns before being applied.
  + **Breaking change**
    : When providing a custom function to
    `ignore=`
    , file paths passed into the function will now be
    *relative*
    , rather than absolute.

0.75
----

### 0.75.8 (2025-05-12)

* Introduced
  `modal.Cls.with_concurrency`
  and
  `modal.Cls.with_batching`
  for runtime configuration of functionality that is exposed through the
  `@modal.concurrent`
  and
  `@modal.batched`
  decorators.

  ```
  model = Model.with_options(gpu="H100").with_concurrency(max_inputs=100)()
  ```

* Added a deprecation warning when using
  `allow_concurrent_inputs`
  in
  `modal.Cls.with_options`
  .
* Added
  `buffer_containers`
  to
  `modal.Cls.with_options`
  .
* *Behavior change:*
  when
  `modal.Cls.with_options`
  is called multiple times on the same object, the configurations will be merged rather than using the most recent.

### 0.75.4 (2025-05-09)

* Fixed issue with .spawn\_map producing wrong number of arguments

### 0.75.3 (2025-05-08)

* New
  `modal.Dict`
  s (forthcoming on 2025-05-20) use a new durable storage backend with more “cache-like” behavior - items expire after 7 days of inactivity (no reads or writes). Previously created
  `modal.Dict`
  s will continue to use the old backend, but support will eventually be dropped.
* `modal.Dict.put`
  now supports an
  `skip_if_exists`
  flag that can be used to avoid overwriting the value for existing keys:

  ```
  item_created = my_dict.put("foo", "bar", skip_if_exists=True)
  assert item_created
  new_item_created = my_dict.put("foo", "baz", skip_if_exists=True)
  assert not new_item_created
  ```

  Note that this flag only works for
  `modal.Dict`
  objects with the new backend (forthcoming on 2025-05-20) and will raise an error otherwise.

### 0.75.2 (2025-05-08)

* Reverts defective changes to the interpretation of
  `ignore=`
  patterns and
  `.dockerignore`
  files that were introduced in v0.75.0.

### 0.75.0 (2025-05-08)

* Introduced some changes to the handling of
  `ignore=`
  patterns in
  `modal.Image`
  methods. Due to a defect around the handling of leading path delimiter characters, these changes reverted in 0.75.2 and later reintroduced in 0.76.0.

0.74
----

### 0.74.63 (2025-05-08)

* Deprecates
  `Function.web_url`
  in favor of a new
  `Function.get_web_url()`
  method. This also allows the url of a
  `Function`
  to be retrieved in an async manner using
  `Function.get_web_url.aio()`
  (like all other io-bearing methods in the Modal API)

### 0.74.61 (2025-05-07)

* Adds a deprecation warning when data is passed directly to
  `modal.Dict.from_name`
  or
  `modal.Dict.ephemeral`
  . Going forward, it will be necessary to separate
  `Dict`
  population from creation.

### 0.74.60 (2025-05-07)

* `modal.Dict.update`
  now also accepts a positional Mapping, like Python’s
  `dict`
  type:

  ```
  d = modal.Dict.from_name("some-dict")
  d.update({"a_key": 1, "another_key": "b"}, some_kwarg=True)
  ```

### 0.74.56 (2025-05-06)

* Experimental
  `modal cluster`
  subcommand is added.

### 0.74.53 (2025-05-06)

* Added functionality for
  `.spawn_map`
  on a function instantiated from
  `Function.from_name`
  .

### 0.74.51 (2025-05-06)

* The
  `modal`
  client library can now be installed with Protobuf 6.

### 0.74.49 (2025-05-06)

* Changes the log format of the modal client’s default logger. Instead of
  `[%(threadName)s]`
  , the client now logs
  `[modal-client]`
  as the log line prefix.
* Adds a configuration option (MODAL\_LOG\_PATTERN) to the modal config for setting the log formatting pattern, in case users want to customize the format. To get the old format, use
  `MODAL_LOG_PATTERN='[%(threadName)s] %(asctime)s %(message)s'`
  (or add this to your
  `.modal.toml`
  in the
  `log_pattern`
  field).

### 0.74.48 (2025-05-05)

* Added a new method for spawning many function calls in parallel:
  `Function.spawn_map`
  .

### 0.74.46 (2025-05-05)

* Introduces a new
  `.update_autoscaler()`
  method, which will replace the existing
  `.keep_warm()`
  method with the ability to dynamically change the entire autoscaler configuration (
  `min_containers`
  ,
  `max_containers`
  ,
  `buffer_containers`
  , and
  `scaledown_window`
  ).

### 0.74.39 (2025-04-30)

* The
  `modal`
  client no longer includes
  `fastapi`
  as a library dependency.

### 0.74.36 (2025-04-29)

* A new parameter,
  `restrict_modal_access`
  , can be provided on a Function to prevent it from interacting with other resources in your Modal Workspace like Queues, Volumes, or other Functions. This can be useful for running user-provided or LLM-written code in a safe way.

### 0.74.35 (2025-04-29)

* Fixed a bug that prevented doing
  `modal run`
  against an entrypoint defined by
  `Cls.with_options`
  .

### 0.74.32 (2025-04-29)

* When setting a custom
  `name=`
  in
  `@app.function()`
  , an error is now raised unless
  `serialized=True`
  is also set.

### 0.74.25 (2025-04-25)

* The
  `App.include`
  method now returns
  `self`
  so it’s possible to build up an App through chained calls:

  ```
  app = modal.App("main-app").include(sub_app_1).include(sub_app_2)
  ```

### 0.74.23 (2025-04-25)

* Marked some parameters in a small number of Modal functions as requiring keyword arguments (namely,
  `modal.App.run`
  ,
  `modal.Cls.with_options`
  , all
  `.from_name`
  methods, and a few others). Code that calls these functions with positional arguments will now raise an error. This is expected to be minimally disruptive as the affected parameters are mostly “extra” options or positioned after parameters that have previously been deprecated.

### 0.74.22 (2025-04-24)

* Added a
  `modal secret delete`
  command to the CLI.

### 0.74.21 (2025-04-24)

* The
  `allow_cross_region_volumes`
  parameter of the
  `@app.function`
  and
  `@app.cls`
  decorators now issues a deprecation warning; the parameter is always treated as
  `True`
  on the Modal backend.

### 0.74.18 (2025-04-23)

* Adds a
  `.deploy()`
  method to the
  `App`
  object. This method allows you programmatically deploy Apps from Python:

  ```
  app = modal.App("programmatic-deploy")
  ...
  app.deploy()
  ```

### 0.74.12 (2025-04-18)

* The
  `@app.function`
  and
  `@app.cls`
  decorators now support
  `experimental_options`
  , which we’ll use going forward when testing experimental functionality that depends only on server-side configuration.

### 0.74.7 (2025-04-17)

* Modal will now raise an error if local files included in the App are modified during the build process. This behavior can be controlled with the
  `MODAL_BUILD_VALIDATION`
  configuration, which accepts
  `error`
  (default),
  `warn`
  , or
  `ignore`
  .

### 0.74.6 (2025-04-17)

* Internal change that makes containers for functions/classes with
  `serialized=True`
  start up
  *slightly*
  faster than before

### 0.74.0 (2025-04-15)

* Introduces a deprecation warning when using explicit constructors (
  `__init__`
  methods) on
  `@modal.cls`
  -decorated classes. Class parameterization should instead be done via
  [dataclass-style
  `modal.parameter()`
  declarations](https://modal.com/docs/guide/parametrized-functions)

  . Initialization logic should run in
  `@modal.enter()`
  -decorated
  [lifecycle methods](https://modal.com/docs/guide/lifecycle-functions)

  .

0.73
----

### 0.73.173 (2025-04-15)

* Fix bug where containers hang with batch sizes above 100 (with
  `@modal.batched`
  ).
* Fix bug where containers can fail with large outputs and batch sizes above 49 (with
  `@modal.batched`
  )

### 0.73.170 (2025-04-14)

* Fixes a bug where
  `modal run`
  didn’t recognize
  `modal.parameter()`
  class parameters

### 0.73.165 (2025-04-11)

* Allow running new ephemeral apps from
  **within**
  Modal containers using
  `with app.run(): ...`
  . Use with care, as putting such a run block in global scope of a module could easily lead to infinite app creation recursion

### 0.73.160 (2025-04-10)

* The
  `allow_concurrent_inputs`
  parameter of
  `@app.function`
  and
  `@app.cls`
  is now deprecated in favor of the
  `@modal.concurrent`
  decorator. See the
  [Modal 1.0 Migration Guide](https://modal.com/docs/guide/modal-1-0-migration#replacing-allow_concurrent_inputs-with-modalconcurrent)

  and documentation on
  [input concurrency](https://modal.com/docs/guide/concurrent-inputs)

  for more information.

### 0.73.159 (2025-04-10)

* Fixes a bug where
  `serialized=True`
  classes could not
  `self.`
  reference other methods on the class, or use
  `modal.parameter()`
  synthetic constructors

### 0.73.158 (2025-04-10)

* Adds support for
  `bool`
  type to class parameters using
  `name: bool = modal.parameter()`
  . Note that older clients can’t instantiate classes with bool parameters unless those have default values which are not modified. Bool parameters are also not supported by web endpoints at this time.

### 0.73.148 (2025-04-07)

* Fixes a bug introduced in 0.73.147 that broke App builds when using
  `@modal.batched`
  on a class method.

### 0.73.147 (2025-04-07)

* Improved handling of cases where
  `@modal.concurrent`
  is stacked with other decorators.

### 0.73.144 (2025-04-04)

* Adds a
  `context_dir`
  parameter to
  `modal.Image.from_dockerfile`
  and
  `modal.Image.dockerfile_commands`
  . This parameter can be used to provide a local reference for relative COPY commands.

### 0.73.139 (2025-04-02)

* Added
  `modal.experimental.ipython`
  module, which can be loaded in Jupyter notebooks with
  `%load_ext modal.experimental.ipython`
  . Currently it provides the
  `%modal`
  line magic for looking up functions:

  ```
  %modal from main/my-app import my_function, MyClass as Foo

  # Now you can use my_function() and Foo in your notebook.
  my_function.remote()
  Foo().my_method.remote()
  ```

* Removed the legacy
  `modal.extensions.ipython`
  module from 2022.

### 0.73.135 (2025-03-29)

* Fix shutdown race bug that emitted spurious error-level logs.

### 0.73.132 (2025-03-28)

* Adds the
  `@modal.concurrent`
  decorator, which will be replacing the beta
  `allow_concurrent_inputs=`
  parameter of
  `@app.function`
  and
  `@app.cls`
  for enabling input concurrency. Notably,
  `@modal.concurrent`
  introduces a distinction between
  `max_inputs`
  and
  `target_inputs`
  , allowing containers to burst over the concurrency level targeted by the Modal autoscaler during periods of high load.

### 0.73.131 (2025-03-28)

* Instantiation of classes using keyword arguments that are not defined as as
  `modal.parameter()`
  will now raise an error on the calling side rather than in the receiving container. Note that this only applies if there is at least one modal.parameter() defined on the class, but this will likely apply to parameter-less classes in the future as well.

### 0.73.121 (2025-03-24)

* Adds a new “commit info” column to the
  `modal app history`
  command. It shows the short git hash at the time of deployment, with an asterisk
  `*`
  if the repository had uncommitted changes.

### 0.73.119 (2025-03-21)

* Class parameters are no longer automatically cast into their declared type. If the wrong type is provided to a class parameter, method calls to that class instance will now fail with an exception.

### 0.73.115 (2025-03-19)

* Adds support for new strict
  `bytes`
  type for
  `modal.parameter`

Usage:

```
import typing
import modal

app = modal.App()

@app.cls()
class Foo:
    a: bytes = modal.parameter(default=b"hello")

    @modal.method()
    def bar(self):
        return f"hello {self.a}"

@app.local_entrypoint()
def main():
    foo = Foo(a=b"world")
    foo.bar.remote()
```

**Note**
: For parameterized web endoints you must base64 encode the bytes before passing them in as a query parameter.

### 0.73.107 (2025-03-14)

* Include git commit info at the time of app deployment.

### 0.73.105 (2025-03-14)

* Added
  `Image.cmd()`
  for setting image default entrypoint args (a.k.a.
  `CMD`
  ).

### 0.73.95 (2025-03-12)

* Fixes a bug which could cause
  `Function.map`
  and sibling methods to stall indefinitely if there was an exception in the input iterator itself (i.e. not the mapper function)

### 0.73.89 (2025-03-05)

* The
  `@modal.web_endpoint`
  decorator is now deprecated. We are replacing it with
  `@modal.fastapi_endpoint`
  . This can be a simple name substitution in your code; the two decorators have identical semantics.

### 0.73.84 (2025-03-04)

* The
  `keep_warm=`
  parameter has been removed from the
  `@modal.method`
  decorator. This parameter has been nonfunctional since v0.63.0; all autoscaler configuration must be done at the level of the modal Cls.

### 0.73.82 (2025-03-04)

* Adds
  `modal.fastapi_endpoint`
  as an alias for
  `modal.web_endpoint`
  . We will be deprecating the
  `modal.web_endpoint`
  *name*
  (but not the functionality) as part of the Modal 1.0 release.

### 0.73.81 (2025-03-03)

* The
  `wait_for_response`
  parameter of Modal’s web endpoint decorators has been removed (originally deprecated in May 2024).

### 0.73.78 (2025-03-01)

* It is now possible to call
  `Cls.with_options`
  on an unhydrated Cls, e.g.

  ```
  ModelWithGPU = modal.Cls.from_name("my-app", "Model").with_options(gpu="H100")
  ```

### 0.73.77 (2025-03-01)

* `Cls.with_options()`
  now accept unhydated volume and secrets

### 0.73.76 (2025-02-28)

* We’re renaming several
  `App.function`
  and
  `App.cls`
  parameters that configure the behavior of Modal’s autoscaler:
  + `concurrency_limit`
    is now
    `max_containers`
  + `keep_warm`
    is now
    `min_containers`
  + `container_idle_timeout`
    is now
    `scaledown_window`
* The old names will continue to work, but using them will issue a deprecation warning. The aim of the renaming is to reduce some persistent confusion about what these parameters mean. Code updates should require only a simple substitution of the new name.
* We’re adding a new parameter,
  `buffer_containers`
  (previously available as
  `_experimental_buffer_containers`
  ). When your Function is actively handling inputs, the autoscaler will spin up additional
  `buffer_containers`
  so that subsequent inputs will not be blocked on cold starts. When the Function is idle, it will still scale down to the value given by
  `min_containers`
  .

### 0.73.75 (2025-02-28)

* Adds a new config field,
  `ignore_cache`
  (also accessible via environment variables as
  `MODAL_IGNORE_CACHE=1`
  ), which will force Images used by the App to rebuild but not clobber any existing cached Images. This can be useful for testing an App’s robustness to Image rebuilds without affecting other Apps that depend on the same base Image layer(s).

### 0.73.73 (2025-02-28)

* Adds a deprecation warning to the
  `workspace`
  parameter in
  `modal.Cls`
  lookup methods. This argument is unused and will be removed in the future.

### 0.73.69 (2025-02-25)

* We’ve moved the
  `modal.functions.gather`
  function to be a staticmethod on
  `modal.FunctionCall.gather`
  . The former spelling has been deprecated and will be removed in a future version.

### 0.73.68 (2025-02-25)

* Fixes issue where running
  `modal shell`
  with a dot-separated module reference as input would not accept the required
  `-m`
  flag for “module mode”, but still emitted a warning telling users to use
  `-m`

### 0.73.60 (2025-02-20)

* Fixes an issue where
  `modal.runner.deploy_app()`
  didn’t work when called from within a running (remote) Modal Function

### 0.73.58 (2025-02-20)

* Introduces an
  `-m`
  flag to
  `modal run`
  ,
  `modal shell`
  ,
  `modal serve`
  and
  `modal deploy`
  , which indicates that the modal app/function file is specified using python “module syntax” rather than a file path. In the future this will be a required flag when using module syntax.

  Old syntax:

  ```
  modal run my_package/modal_main.py
  modal run my_package.modal_main
  ```

  New syntax (note the
  `-m`
  on the second line):

  ```
  modal run my_package/modal_main.py
  modal run -m my_package.modal_main
  ```

### 0.73.54 (2025-02-18)

* Passing
  `App.lookup`
  an invalid name now raises an error. App names may contain only alphanumeric characters, dashes, periods, and underscores, must be shorter than 64 characters, and cannot conflict with App ID strings.

### 0.73.51 (2025-02-14)

* Fixes a bug where sandboxes returned from
  `Sandbox.list()`
  were not snapshottable even if they were created with
  `_experimental_enable_snapshot`
  .

### 0.73.44 (2025-02-13)

* `modal.FunctionCall`
  is now available in the top-level
  `modal`
  namespace. We recommend referencing the class this way instead of using the the fully-qualified
  `modal.functions.FunctionCall`
  name.

### 0.73.40 (2025-02-12)

* `Function.web_url`
  will now return None (instead of raising an error) when the Function is not a web endpoint

### 0.73.31 (2025-02-10)

* Deprecate the GPU classes (
  `gpu=A100(...)`
  etc) in favor of just using strings (
  `gpu="A100"`
  etc)

### 0.73.26 (2025-02-10)

* Adds a pending deprecation warning when looking up class methods using
  `Function.from_name`
  , e.g.
  `Function.from_name("some_app", "SomeClass.some_method")`
  . The recommended way to reference methods of classes is to look up the class instead:
  `RemoteClass = Cls.from_name("some_app", "SomeClass")`

### 0.73.25 (2025-02-09)

* Fixes an issue introduced in
  `0.73.19`
  that prevented access to GPUs during image builds

### 0.73.18 (2025-02-06)

* When using a parameterized class (with at least one
  `modal.parameter()`
  specified), class instantiation with an incorrect construction signature (wrong arguments or types) will now fail at the
  `.remote()`
  calling site instead of container startup for the called class.

### 0.73.14 (2025-02-04)

* Fixed the status message shown in terminal logs for ephemeral Apps to accurately report the number of active containers.

### 0.73.11 (2025-02-04)

* Warns users if the
  `modal.Image`
  of a Function/Cls doesn’t include all the globally imported “local” modules (using
  `.add_local_python_source()`
  ), and the user hasn’t explicitly set an
  `include_source`
  value of True/False. This is in preparation for an upcoming deprecation of the current “auto mount” logic.

### 0.73.10 (2025-02-04)

* Modal functions, methods and entrypoints can now accept variable-length arguments to skip Modal’s default CLI parsing. This is useful if you want to use Modal with custom argument parsing via
  `argparse`
  or
  `HfArgumentParser`
  . For example, the following function can be invoked with
  `modal run my_file.py --foo=42 --bar="baz"`
  :

  ```
  import argparse

  @app.function()
  def train(*arglist):
      parser = argparse.ArgumentParser()
      parser.add_argument("--foo", type=int)
      parser.add_argument("--bar", type=str)
      args = parser.parse_args(args = arglist)
  ```

### 0.73.1 (2025-01-30)

* `modal run`
  now runs a single local entrypoints/function in the selected module. If exactly one local entrypoint or function exists in the selected module, the user doesn’t have to qualify the runnable
  in the modal run command, even if some of the module’s referenced apps have additional local entrypoints or functions. This partially restores “auto-inferred function” functionality that was changed in v0.72.48.

### 0.73.0 (2025-01-30)

* Introduces an
  `include_source`
  argument in the
  `App.function`
  and
  `App.cls`
  decorators that let users configure which class of python packages are automatically included as source mounts in created modal functions/classes (what we used to call “automount” behavior). This will supersede the MODAL\_AUTOMOUNT configuration value which will eventually be deprecated. As a convenience, the
  `modal.App`
  constructor will also accept an
  `include_source`
  argument which serves as the default for all the app’s functions and classes.

  The
  `include_source`
  argument accepts the following values:

  + `True`
    (default in a future version of Modal) Automatically includes the Python files of the source package of the function’s own home module, but not any other local packages. Roughly equivalent ot
    `MODAL_AUTOMOUNT=0`
    in previous versions of Modal.
  + `False`
    - don’t include
    *any*
    local source. Assumes the function’s home module is importable in the container environment through some other means (typically added to the provided
    `modal.Image`
    ’s Python environment).
  + `None`
    (the default) - use current soon-to-be-deprecated automounting behavior, including source of all first party packages that are not installed into site-packages locally.
* Minor change to
  `MODAL_AUTOMOUNT=0`
  : When running/deploying using a module path (e.g.
  `modal run mypak.mymod`
  ),
  **all non .pyc files**
  of the source package (
  `mypak`
  in this case) are now included in the function’s container. Previously, only the function’s home
  `.py`
  module file + any
  `__init__.py`
  files in its package structure were included. Note that this is only for MODAL\_AUTOMOUNT=0. To get full control over which source files are included with your functions, you can set
  `include_source=False`
  on your function (see above) and manually specify the files to include using the
  `ignore`
  argument to
  `Image.add_local_python_source`
  .

0.72
----

### 0.72.56 (2025-01-28)

* Deprecated
  `.lookup`
  methods on Modal objects. Users are encouraged to use
  `.from_name`
  instead. In most cases this will be a simple name substitution. See
  [the 1.0 migration guide](https://modal.com/docs/guide/modal-1-0-migration#deprecating-the-lookup-method-on-modal-objects)

  for more information.

### 0.72.54 (2025-01-28)

* Fixes bug introduced in v0.72.48 where
  `modal run`
  didn’t work with files having global
  `Function.from_name()`
  /
  `Function.lookup()`
  /
  `Cls.from_name()`
  /
  `Cls.lookup()`
  calls.

### 0.72.48 (2025-01-24)

* Fixes a CLI bug where you couldn’t reference functions via a qualified app, e.g.
  `mymodule::{app_variable}.{function_name}`
  .
* The
  `modal run`
  ,
  `modal serve`
  and
  `modal shell`
  commands get more consistent error messages in cases where the passed app or function reference isn’t resolvable to something that the current command expects.
* Removes the deprecated
  `__getattr__`
  ,
  `__setattr__`
  ,
  `__getitem__`
  and
  `__setitem__`
  methods from
  `modal.App`

### 0.72.39 (2025-01-22)

* Introduced a new public method,
  `.hydrate`
  , for on-demand hydration of Modal objects. This method replaces the existing semi-public
  `.resolve`
  method, which is now deprecated.

### 0.72.33 (2025-01-20)

* The Image returned by
  `Sandbox.snapshot_filesystem`
  now has
  `object_id`
  and other metadata pre-assigned rather than require loading by subsequent calls to sandboxes or similar to set this data.

### 0.72.30 (2025-01-18)

* Adds a new
  `oidc_auth_role_arn`
  field to
  `CloudBucketMount`
  for using OIDC authentication to create the mountpoint.

### 0.72.24 (2025-01-17)

* No longer prints a warning if
  `app.include`
  re-includes an already included function (warning is still printed if
  *another*
  function with the same name is included)

### 0.72.22 (2025-01-17)

* Internal refactor of the
  `modal.object`
  module. All entities except
  `Object`
  from that module have now been moved to the
  `modal._object`
  “private” module.

### 0.72.17 (2025-01-16)

* The
  `@modal.build`
  decorator is now deprecated. For storing large assets (e.g. model weights), we now recommend using a
  `modal.Volume`
  over writing data to the
  `modal.Image`
  filesystem directly.

### 0.72.16 (2025-01-16)

* Fixes bug introduced in v0.72.9 where
  `modal run SomeClass.some_method`
  would incorrectly print a deprecation warning.

### 0.72.15 (2025-01-15)

* Added an
  `environment_name`
  parameter to the
  `App.run`
  context manager.

### 0.72.8 (2025-01-10)

* Fixes a bug introduced in v0.72.2 when specifying
  `add_python="3.9"`
  in
  `Image.from_registry`
  .

### 0.72.0 (2025-01-09)

* The default behavior
  `Image.from_dockerfile()`
  and
  `image.dockerfile_commands()`
  if no parameter is passed to
  `ignore`
  will be to automatically detect if there is a valid dockerignore file in the current working directory or next to the dockerfile following the same rules as
  `dockerignore`
  does using
  `docker`
  commands. Previously no patterns were ignored.

0.71
----

### 0.71.13 (2025-01-09)

* `FilePatternMatcher`
  has a new constructor
  `from_file`
  which allows you to read file matching patterns from a file instead of having to pass them in directly, this can be used for
  `Image`
  methods accepting an
  `ignore`
  parameter in order to read ignore patterns from files.

### 0.71.11 (2025-01-08)

* Modal Volumes can now be renamed via the CLI (
  `modal volume rename`
  ) or SDK (
  `modal.Volume.rename`
  ).

### 0.71.7 (2025-01-08)

* Adds
  `Image.from_id`
  , which returns an
  `Image`
  object from an existing image id.

### 0.71.1 (2025-01-06)

* Sandboxes now support fsnotify-like file watching:

```
from modal.file_io import FileWatchEventType

app = modal.App.lookup("file-watch", create_if_missing=True)
sb = modal.Sandbox.create(app=app)
events = sb.watch("/foo")
for event in events:
    if event.type == FileWatchEventType.Modify:
        print(event.paths)
```

0.70
----

### 0.70.1 (2024-12-27)

* The sandbox filesystem API now accepts write payloads of sizes up to 1 GiB.

0.69
----

### 0.69.0 (2024-12-21)

* `Image.from_dockerfile()`
  and
  `image.dockerfile_commands()`
  now auto-infer which files need to be uploaded based on COPY commands in the source if
  `context_mount`
  is omitted. The
  `ignore=`
  argument to these methods can be used to selectively omit files using a set of glob patterns.

0.68
----

### 0.68.53 (2024-12-20)

* You can now point
  `modal launch vscode`
  at an arbitrary Dockerhub base image:

  `modal launch vscode --image=nvidia/cuda:12.4.0-devel-ubuntu22.04`

### 0.68.44 (2024-12-19)

* You can now run GPU workloads on
  [Nvidia L40S GPUs](https://www.nvidia.com/en-us/data-center/l40s/)

  :

  ```
  @app.function(gpu="L40S")
  def my_gpu_fn():
      ...
  ```

### 0.68.43 (2024-12-19)

* Fixed a bug introduced in v0.68.39 that changed the exception type raise when the target object for
  `.from_name`
  /
  `.lookup`
  methods was not found.

### 0.68.39 (2024-12-18)

* Standardized terminology in
  `.from_name`
  /
  `.lookup`
  /
  `.delete`
  methods to use
  `name`
  consistently where
  `label`
  and
  `tag`
  were used interchangeably before. Code that invokes these methods using
  `label=`
  as an explicit keyword argument will issue a deprecation warning and will break in a future release.

### 0.68.29 (2024-12-17)

* The internal
  `deprecation_error`
  and
  `deprecation_warning`
  utilities have been moved to a private namespace

### 0.68.28 (2024-12-17)

* Sandboxes now support additional filesystem commands
  `mkdir`
  ,
  `rm`
  , and
  `ls`
  .

```
app = modal.App.lookup("sandbox-fs", create_if_missing=True)
sb = modal.Sandbox.create(app=app)
sb.mkdir("/foo")
with sb.open("/foo/bar.txt", "w") as f:
    f.write("baz")
print(sb.ls("/foo"))
```

### 0.68.27 (2024-12-17)

* Two previously-introduced deprecations are now enforced and raise an error:
  + The
    `App.spawn_sandbox`
    method has been removed in favor of
    `Sandbox.create`
  + `Sandbox.create`
    now requires an
    `App`
    object to be passed

### 0.68.24 (2024-12-16)

* The
  `modal run`
  CLI now has a
  `--write-result`
  option. When you pass a filename, Modal will write the return value of the entrypoint function to that location on your local filesystem. The return value of the function must be either
  `str`
  or
  `bytes`
  to use this option; otherwise, an error will be raised. It can be useful for exercising a remote function that returns text, image data, etc.

### 0.68.21 (2024-12-13)

Adds an
`ignore`
parameter to our
`Image`
`add_local_dir`
and
`copy_local_dir`
methods. It is similar to the
`condition`
method on
`Mount`
methods but instead operates on a
`Path`
object. It takes either a list of string patterns to ignore which follows the
`dockerignore`
syntax implemented in our
`FilePatternMatcher`
class, or you can pass in a callable which allows for more flexible selection of files.

Usage:

```
img.add_local_dir(
  "./local-dir",
  remote_path="/remote-path",
  ignore=FilePatternMatcher("**/*", "!*.txt") # ignore everything except files ending with .txt
)

img.add_local_dir(
  ...,
  ignore=~FilePatternMatcher("**/*.py") # can be inverted for when inclusion filters are simpler to write
)

img.add_local_dir(
  ...,
  ignore=["**/*.py", "!module/*.py"] # ignore all .py files except those in the module directory
)

img.add_local_dir(
  ...,
  ignore=lambda fp: fp.is_relative_to("somewhere") # use a custom callable
)
```

which will add the
`./local-dir`
directory to the image but ignore all files except
`.txt`
files

### 0.68.15 (2024-12-13)

Adds the
`requires_proxy_auth`
parameter to
`web_endpoint`
,
`asgi_app`
,
`wsgi_app`
, and
`web_server`
decorators. Requests to the app will respond with 407 Proxy Authorization Required if a webhook token is not supplied in the HTTP headers. Protects against DoS attacks that will unnecessarily charge users.

### 0.68.11 (2024-12-13)

* `Cls.from_name(...)`
  now works as a lazy alternative to
  `Cls.lookup()`
  that doesn’t perform any IO until a method on the class is used for a .remote() call or similar

### 0.68.6 (2024-12-12)

* Fixed a bug introduced in v0.67.47 that suppressed console output from the
  `modal deploy`
  CLI.

### 0.68.5 (2024-12-12)

We’re removing support for
`.spawn()`
ing generator functions.

### 0.68.2 (2024-12-11)

* Sandboxes now support a new filesystem API. The
  `open()`
  method returns a
  `FileIO`
  handle for native file handling in sandboxes.

```
app = modal.App.lookup("sandbox-fs", create_if_missing=True)
sb = modal.Sandbox.create(app=app)

with sb.open("test.txt", "w") as f:
  f.write("Hello World\n")

f = sb.open("test.txt", "rb")
print(f.read())
```

0.67
----

### 0.67.43 (2024-12-11)

* `modal container exec`
  and
  `modal shell`
  now work correctly even when a pseudoterminal (PTY) is not present. This means, for example, that you can pipe the output of these commands to a file:

  ```
  modal shell -c 'uv pip list' > env.txt
  ```

### 0.67.39 (2024-12-09)

* It is now possible to delete named
  `NetworkFileSystem`
  objects via the CLI (
  `modal nfs delete ...`
  ) or API
  `(modal.NetworkFileSystem.delete(...)`
  )

### 0.67.38 (2024-12-09)

* Sandboxes now support filesystem snapshots. Run
  `Sandbox.snapshot_filesystem()`
  to get an Image which can be used to spawn new Sandboxes.

### 0.67.28 (2024-12-05)

* Adds
  `Image.add_local_python_source`
  which works similarly to the old and soon-to-be-deprecated
  `Mount.from_local_python_packages`
  but for images. One notable difference is that the new
  `add_local_python_source`
  *only*
  includes
  `.py`
  -files by default

### 0.67.23 (2024-12-04)

* Image build functions that use a
  `functools.wraps`
  decorator will now have their global variables included in the cache key. Previously, the cache would use global variables referenced within the wrapper itself. This will force a rebuild for Image layers defined using wrapped functions.

### 0.67.22 (2024-12-03)

* Fixed a bug introduced in v0.67.0 where it was impossible to call
  `modal.Cls`
  methods when passing a list of requested GPUs.

### 0.67.12 (2024-12-02)

* Fixed a bug that executes the wrong method when a Modal Cls overrides a
  `@modal.method`
  inherited from a parent.

### 0.67.7 (2024-11-29)

* Fixed a bug where pointing
  `modal run`
  at a method on a Modal Cls would fail if the method was inherited from a parent.

### 0.67.0 (2024-11-27)

New minor client version
`0.67.x`
comes with an internal data model change for how Modal creates functions for Modal classes. There are no breaking or backwards-incompatible changes with this release. All forward lookup scenarios (
`.lookup()`
of a
`0.67`
class from a pre
`0.67`
client) as well as backwards lookup scenarios (
`.lookup()`
of a pre
`0.67`
class from a
`0.67`
client) work, except for a
`0.62`
client looking up a
`0.67`
class (this maintains our current restriction of not being able to lookup a
`0.63+`
class from a
`0.62`
client).

0.66
----

### 0.66.49 (2024-11-26)

* `modal config set-environment`
  will now raise if the requested environment does not exist.

### 0.66.45 (2024-11-26)

* The
  `modal launch`
  CLI now accepts a
  `--detach`
  flag to run the App in detached mode, such that it will persist after the local client disconnects.

### 0.66.40 (2024-11-23)

* Adds
  `Image.add_local_file(..., copy=False)`
  and
  `Image.add_local_dir(..., copy=False)`
  as a unified replacement for the old
  `Image.copy_local_*()`
  and
  `Mount.add_local_*`
  methods.

### 0.66.30 (2024-11-21)

* Removed the
  `aiostream`
  package from the modal client library dependencies.

### 0.66.12 (2024-11-19)

`Sandbox.exec`
now accepts arguments
`text`
and
`bufsize`
for streaming output, which controls text output and line buffering.

### 0.66.0 (2024-11-15)

* Modal no longer supports Python 3.8, which has reached its
  [official EoL](https://devguide.python.org/versions/)

  .

0.65
----

### 0.65.55 (2024-11-13)

* Escalates stuck input cancellations to container death. This prevents unresponsive user code from holding up resources.
* Input timeouts no longer kill the entire container. Instead, they just cancel the timed-out input, leaving the container and other concurrent inputs running.

### 0.65.49 (2024-11-12)

* Fixed issue in
  `modal serve`
  where files used in
  `Image.copy_*`
  commands were not watched for changes

### 0.65.42 (2024-11-07)

* `Sandbox.exec`
  can now accept
  `timeout`
  ,
  `workdir`
  , and
  `secrets`
  . See the
  `Sandbox.create`
  function for context on how to use these arguments.

### 0.65.33 (2024-11-06)

* Removed the
  `interactive`
  parameter from
  `function`
  and
  `cls`
  decorators. This parameter has been deprecated since May 2024. Instead of specifying Modal Functions as interactive, use
  `modal run --interactive`
  to activate interactive mode.

### 0.65.30 (2024-11-05)

* The
  `checkpointing_enabled`
  option, deprecated in March 2024, has now been removed.

### 0.65.9 (2024-10-31)

* Output from
  `Sandbox.exec`
  can now be directed to
  `/dev/null`
  ,
  `stdout`
  , or stored for consumption. This behavior can be controlled via the new
  `StreamType`
  arguments.

### 0.65.8 (2024-10-31)

* Fixed a bug where the
  `Image.imports`
  context manager would not correctly propagate ImportError when using a
  `modal.Cls`
  .

### 0.65.2 (2024-10-30)

* Fixed an issue where
  `modal run`
  would pause for 10s before exiting if there was a failure during app creation.

0.64
----

### 0.64.227 (2024-10-25)

* The
  `modal container list`
  CLI command now shows the containers within a specific environment: the active profile’s environment if there is one, otherwise the workspace’s default environment. You can pass
  `--env`
  to list containers in other environments.

### 0.64.223 (2024-10-24)

* Fixed
  `modal serve`
  not showing progress when reloading apps on file changes since v0.63.79.

### 0.64.218 (2024-10-23)

* Fix a regression introduced in client version 0.64.209, which affects client authentication within a container.

### 0.64.198 (2024-10-18)

* Fixed a bug where
  `Queue.put`
  and
  `Queue.put_many`
  would throw
  `queue.Full`
  even if
  `timeout=None`
  .

### 0.64.194 (2024-10-18)

* The previously-deprecated
  `--confirm`
  flag has been removed from the
  `modal volume delete`
  CLI. Use
  `--yes`
  to force deletion without a confirmation prompt.

### 0.64.193 (2024-10-18)

* Passing
  `wait_for_response=False`
  in Modal webhook decorators is no longer supported. See
  [the docs](https://modal.com/docs/guide/webhook-timeouts#polling-solutions)

  for alternatives.

### 0.64.187 (2024-10-16)

* When writing to a
  `StreamWriter`
  that has already had EOF written, a
  `ValueError`
  is now raised instead of an
  `EOFError`
  .

### 0.64.185 (2024-10-15)

* Memory snapshotting can now be used with parametrized functions.

### 0.64.184 (2024-10-15)

* StreamWriters now accept strings as input.

### 0.64.182 (2024-10-15)

* Fixed a bug where App rollbacks would not restart a schedule that had been removed in an intervening deployment.

### 0.64.181 (2024-10-14)

* The
  `modal shell`
  CLI command now takes a container ID, allowing you to shell into a running container.

### 0.64.180 (2024-10-14)

* `modal shell --cmd`
  now can be shortened to
  `modal shell -c`
  . This means you can use it like
  `modal shell -c "uname -a"`
  to quickly run a command within the remote environment.

### 0.64.168 (2024-10-03)

* The
  `Image.conda`
  ,
  `Image.conda_install`
  , and
  `Image.conda_update_from_environment`
  methods are now fully deprecated. We recommend using
  `micromamba`
  (via
  `Image.micromamba`
  and
  `Image.micromamba_install`
  ) instead, or manually installing and using conda with
  `Image.run_commands`
  when strictly necessary.

### 0.64.153 (2024-09-30)

* **Breaking Change:**
  `Sandbox.tunnels()`
  now returns a
  `Dict`
  rather than a
  `List`
  . This dict is keyed by the container’s port, and it returns a
  `Tunnel`
  object, just like
  `modal.forward`
  does.

### 0.64.142 (2024-09-25)

* `modal.Function`
  and
  `modal.Cls`
  now support specifying a
  `list`
  of GPU configurations, allowing the Function’s container pool to scale across each GPU configuration in preference order.

### 0.64.139 (2024-09-25)

* The deprecated
  `_experimental_boost`
  argument is now removed. (Deprecated in late July.)

### 0.64.123 (2024-09-18)

* Sandboxes can now be created without an entrypoint command. If they are created like this, they will stay alive up until their set timeout. This is useful if you want to keep a long-lived sandbox and execute code in it later.

### 0.64.119 (2024-09-17)

* Sandboxes now have a
  `cidr_allowlist`
  argument, enabling controlled access to certain IP ranges. When not used (and with
  `block_network=False`
  ), the sandbox process will have open network access.

### 0.64.118 (2024-09-17)

Introduce an experimental API to allow users to set the input concurrency for a container locally.

### 0.64.112 (2024-09-15)

* Creating sandboxes without an associated
  `App`
  is deprecated. If you are spawning a
  `Sandbox`
  outside a Modal container, you can lookup an
  `App`
  by name to attach to the
  `Sandbox`
  :

  ```
  app = modal.App.lookup('my-app', create_if_missing=True)
  modal.Sandbox.create('echo', 'hi', app=app)
  ```

### 0.64.109 (2024-09-13)

* App handles can now be looked up by name with
  `modal.App.lookup(name)`
  . This can be useful for associating Sandboxes with Apps:

  ```
  app = modal.App.lookup("my-app", create_if_missing=True)
  modal.Sandbox.create("echo", "hi", app=app)
  ```

### 0.64.100 (2024-09-11)

* The default timeout for
  `modal.Image.run_function`
  has been lowered to 1 hour. Previously it was 24 hours.

### 0.64.99 (2024-09-11)

* Fixes an issue that could cause containers using
  `enable_memory_snapshot=True`
  on Python 3.9 and below to shut down prematurely.

### 0.64.97 (2024-09-11)

* Added support for
  [ASGI lifespan protocol](https://asgi.readthedocs.io/en/latest/specs/lifespan.html)

  :

  ```
  @app.function()
  @modal.asgi_app()
  def func():
      from fastapi import FastAPI, Request

      def lifespan(wapp: FastAPI):
          print("Starting")
          yield {"foo": "bar"}
          print("Shutting down")

      web_app = FastAPI(lifespan=lifespan)

      @web_app.get("/")
      def get_state(request: Request):
          return {"message": f"This is the state: {request.state.foo}"}

      return web_app
  ```

  which enables support for
  `gradio>=v4`
  amongst other libraries using lifespans

### 0.64.87 (2024-09-05)

* Sandboxes now support port tunneling. Ports can be exposed via the
  `open_ports`
  argument, and a list of active tunnels can be retrieved via the
  `.tunnels()`
  method.

### 0.64.67 (2024-08-30)

* Fixed a regression in
  `modal launch`
  to resume displaying output when starting the container.

### 0.64.48 (2024-08-21)

* Introduces new dataclass-style syntax for class parametrization (see updated
  [docs](https://modal.com/docs/guide/parametrized-functions)

  )

  ```
  @app.cls()
  class MyCls:
      param_a: str = modal.parameter()

  MyCls(param_a="hello")  # synthesized constructor
  ```

* The new syntax enforces types (
  `str`
  or
  `int`
  for now) on all parameters
* *When the new syntax is used*
  , any web endpoints (
  `web_endpoint`
  ,
  `asgi_app`
  ,
  `wsgi_app`
  or
  `web_server`
  ) on the app will now also support parametrization through the use of query parameters matching the parameter names, e.g.
  `https://myfunc.modal.run/?param_a="hello`
  in the above example.
* The old explicit
  `__init__`
  constructor syntax is still allowed, but could be deprecated in the future and doesn’t work with web endpoint parametrization

### 0.64.38 (2024-08-16)

* Added a
  `modal app rollback`
  CLI command for rolling back an App deployment to a previous version.

### 0.64.33 (2024-08-16)

* Commands in the
  `modal app`
  CLI now accept an App name as a positional argument, in addition to an App ID:

  ```
  modal app history my-app
  ```

  Accordingly, the explicit
  `--name`
  option has been deprecated. Providing a name that can be confused with an App ID will also now raise an error.

### 0.64.32 (2024-08-16)

* Updated type stubs using generics to allow static type inferrence for functions calls, e.g.
  `function.remote(...)`
  .

### 0.64.26 (2024-08-15)

* `ContainerProcess`
  handles now support
  `wait()`
  and
  `poll()`
  , like
  `Sandbox`
  objects

### 0.64.24 (2024-08-14)

* Added support for dynamic batching. Functions or class methods decorated with
  `@modal.batched`
  will now automatically batch their invocations together, up to a specified
  `max_batch_size`
  . The batch will wait for a maximum of
  `wait_ms`
  for more invocations after the first invocation is made. See guide for more details.

  ```
  @app.function()
  @modal.batched(max_batch_size=4, wait_ms=1000)
  async def batched_multiply(xs: list[int], ys: list[int]) -> list[int]:
      return [x * y for x, y in zip(xs, xs)]

  @app.cls()
  class BatchedClass():
      @modal.batched(max_batch_size=4, wait_ms=1000)
      async def batched_multiply(xs: list[int], ys: list[int]) -> list[int]:
          return [x * y for x, y in zip(xs, xs)]
  ```

  The batched function is called with individual inputs:

  ```
  await batched_multiply.remote.aio(2, 3)
  ```

### 0.64.18 (2024-08-12)

* Sandboxes now have an
  `exec()`
  method that lets you execute a command inside the sandbox container.
  `exec`
  returns a
  `ContainerProcess`
  handle for input and output streaming.

  ```
  sandbox = modal.Sandbox.create("sleep", "infinity")

  process = sandbox.exec("bash", "-c", "for i in $(seq 1 10); do echo foo $i; sleep 0.5; done")

  for line in process.stdout:
      print(line)
  ```

### 0.64.8 (2024-08-06)

* Removed support for the undocumented
  `modal.apps.list_apps()`
  function, which was internal and not intended to be part of public API.

### 0.64.7 (2024-08-05)

* Removed client check for CPU core request being at least 0.1, deferring to server-side enforcement.

### 0.64.2 (2024-08-02)

* Volumes can now be mounted to an ad hoc modal shell session:

  ```
  modal shell --volume my-vol-name
  ```

  When the shell starts, the volume will be mounted at
  `/mnt/my-vol-name`
  . This may be helpful for shell-based exploration or manipulation of volume contents.

  Note that the option can be used multiple times to mount additional models:

  ```
  modal shell --volume models --volume data
  ```

### 0.64.0 (2024-07-29)

* App deployment events are now atomic, reducing the risk that a failed deploy will leave the App in a bad state.

0.63
----

### 0.63.87 (2024-07-24)

* The
  `_experimental_boost`
  argument can now be removed. Boost is now enabled on all modal Functions.

### 0.63.77 (2024-07-18)

* Setting
  `_allow_background_volume_commits`
  is no longer necessary and has been deprecated. Remove this argument in your decorators.

### 0.63.36 (2024-07-05)

* Image layers defined with a
  `@modal.build`
  method will now include the values of any
  *class variables*
  that are referenced within the method as part of the layer cache key. That means that the layer will rebuild when the class variables change or are overridden by a subclass.

### 0.63.22 (2024-07-01)

* Fixed an error when running
  `@modal.build`
  methods that was introduced in v0.63.19

### 0.63.20 (2024-07-01)

* Fixed bug where
  `self.method.local()`
  would re-trigger lifecycle methods in classes

### 0.63.14 (2024-06-28)

* Adds
  `Cls.lookup()`
  backwards compatibility with classes created by clients prior to
  `v0.63`
  .

  **Important**
  : When updating (to >=v0.63) an app with a Modal
  `class`
  that’s accessed using
  `Cls.lookup()`
  - make sure to update the client of the app/service
  **using**
  `Cls.lookup()`
  first, and
  **then**
  update the app containing the class being looked up.

### 0.63.12 (2024-06-27)

* Fixed a bug introduced in 0.63.0 that broke
  `modal.Cls.with_options`

### 0.63.10 (2024-06-26)

* Adds warning about future deprecation of
  `retries`
  for generators. Retries are being deprecated as they can lead to nondetermistic generator behavior.

### 0.63.9 (2024-06-26)

* Fixed a bug in
  `Volume.copy_files()`
  where some source paths may be ignored if passed as
  `bytes`
  .
* `Volume.read_file`
  ,
  `Volume.read_file_into_fileobj`
  ,
  `Volume.remove_file`
  , and
  `Volume.copy_files`
  can no longer take both string or bytes for their paths. They now only accept
  `str`
  .

### 0.63.2 (2024-06-25)

* Fixes issue with
  `Cls.lookup`
  not working (at all) after upgrading to v0.63.0.
  **Note**
  : this doesn’t fix the cross-version lookup incompatibility introduced in 0.63.0.

### 0.63.0 (2024-06-24)

* Changes how containers are associated with methods of
  `@app.cls()`
  -decorated Modal “classes”.

  Previously each
  `@method`
  and web endpoint of a class would get its own set of isolated containers and never run in the same container as other sibling methods.
  Starting in this version, all
  `@methods`
  and web endpoints will be part of the same container pool. Notably, this means all methods will scale up/down together, and options like
  `keep_warm`
  and
  `concurrency_limit`
  will affect the total number of containers for all methods in the class combined, rather than individually.

  **Version incompatibility warning:**
  Older clients (below 0.63) can’t use classes deployed by new clients (0.63 and above), and vice versa. Apps or standalone clients using
  `Cls.lookup(...)`
  to invoke Modal classes need to be upgraded to version
  `0.63`
  at the same time as the deployed app that’s being called into.
* `keep_warm`
  for classes is now an attribute of the
  `@app.cls()`
  decorator rather than individual methods.

0.62
----

### 0.62.236 (2024-06-21)

* Added support for mounting Volume or CloudBucketMount storage in
  `Image.run_function`
  . Note that this is
  *typically*
  not necessary, as data downloaded during the Image build can be stored directly in the Image filesystem.

### 0.62.230 (2024-06-18)

* It is now an error to create or lookup Modal objects (
  `Volume`
  ,
  `Dict`
  ,
  `Secret`
  , etc.) with an invalid name. Object names must be shorter than 64 characters and may contain only alphanumeric characters, dashes, periods, and underscores. The name check had inadvertently been removed for a brief time following an internal refactor and then reintroduced as a warning. It is once more a hard error. Please get in touch if this is blocking access to your data.

### 0.62.224 (2024-06-17)

* The
  `modal app list`
  command now reports apps created by
  `modal app run`
  or
  `modal app serve`
  as being in an “ephemeral” state rather than a “running” state to reduce confusion with deployed apps that are actively processing inputs.

### 0.62.223 (2024-06-14)

* All modal CLI commands now accept
  `-e`
  as a short-form of
  `--env`

### 0.62.220 (2024-06-12)

* Added support for entrypoint and shell for custom containers:
  `Image.debian_slim().entrypoint([])`
  can be used interchangeably with
  `.dockerfile_commands('ENTRYPOINT []')`
  , and
  `.shell(["/bin/bash", "-c"])`
  can be used interchangeably with
  `.dockerfile_commands('SHELL ["/bin/bash", "-c"]')`

### 0.62.219 (2024-06-12)

* Fix an issue with
  `@web_server`
  decorator not working on image builder version 2023.12

### 0.62.208 (2024-06-08)

* `@web_server`
  endpoints can now return HTTP headers of up to 64 KiB in length. Previously, they were limited to 8 KiB due to an implementation detail.

### 0.62.201 (2024-06-04)

* `modal deploy`
  now accepts a
  `--tag`
  optional parameter that allows you to specify a custom tag for the deployed version, making it easier to identify and manage different deployments of your app.

### 0.62.199 (2024-06-04)

* `web_endpoint`
  s now have the option to include interactive SwaggerUI/redoc docs by setting
  `docs=True`
* `web_endpoint`
  s no longer include an OpenAPI JSON spec route by default

### 0.62.190 (2024-05-29)

* `modal.Function`
  now supports requesting ephemeral disk (SSD) via the new
  `ephemeral_disk`
  parameter. Intended for use in doing large dataset ingestion and transform.

### 0.62.186 (2024-05-29)

* `modal.Volume`
  background commits are now enabled by default when using
  `spawn_sandbox`
  .

### 0.62.185 (2024-05-28)

* The
  `modal app stop`
  CLI command now accepts a
  `--name`
  (or
  `-n`
  ) option to stop an App by name rather than by ID.

### 0.62.181 (2024-05-24)

* Background committing on
  `modal.Volume`
  mounts is now default behavior.

### 0.62.178 (2024-05-21)

* Added a
  `modal container stop`
  CLI command that will kill an active container and reassign its current inputs.

### 0.62.175 (2024-05-17)

* `modal.CloudBucketMount`
  now supports writing to Google Cloud Storage buckets.

### 0.62.174 (2024-05-17)

* Using
  `memory=`
  to specify the type of
  `modal.gpu.A100`
  is deprecated in favor of
  `size=`
  . Note that
  `size`
  accepts a string type (
  `"40GB"`
  or
  `"80GB"`
  ) rather than an integer, as this is a request for a specific variant of the A100 GPU.

### 0.62.173 (2024-05-17)

* Added a
  `version`
  flag to the
  `modal.Volume`
  API and CLI, allow opting in to a new backend implementation.

### 0.62.172 (2024-05-17)

* Fixed a bug where other functions weren’t callable from within an
  `asgi_app`
  or
  `wsgi_app`
  constructor function and side effects of
  `@enter`
  methods weren’t available in that scope.

### 0.62.166 (2024-05-14)

* Disabling background commits on
  `modal.Volume`
  volumes is now deprecated. Background commits will soon become mandatory behavior.

### 0.62.165 (2024-05-13)

* Deprecated
  `wait_for_response=False`
  on web endpoints. See
  [the docs](https://modal.com/docs/guide/webhook-timeouts#polling-solutions)

  for alternatives.

### 0.62.162 (2024-05-13)

* A deprecation warning is now raised when using
  `modal.Stub`
  , which has been renamed to
  `modal.App`
  . Additionally, it is recommended to use
  `app`
  as the variable name rather than
  `stub`
  , which matters when using the automatic app discovery feature in the
  `modal run`
  CLI command.

### 0.62.159 (2024-05-10)

* Added a
  `--stream-logs`
  flag to
  `modal deploy`
  that, if True, begins streaming the app logs once deployment is complete.

### 0.62.156 (2024-05-09)

* Added support for looking up a deployed App by its deployment name in
  `modal app logs`

### 0.62.150 (2024-05-08)

* Added validation that App
  `name`
  , if provided, is a string.

### 0.62.149 (2024-05-08)

* The
  `@app.function`
  decorator now raises an error when it is used to decorate a class (this was always invalid, but previously produced confusing behavior).

### 0.62.148 (2024-05-08)

* The
  `modal app list`
  output has been improved in several ways:
  + Persistent storage objects like Volumes or Dicts are no longer included (these objects receive an app ID internally, but this is an implementation detail and subject to future change). You can use the dedicated CLI for each object (e.g.
    `modal volume list`
    ) instead.
  + For Apps in a
    *stopped*
    state, the output is now limited to those stopped within the past 2 hours.
  + The number of tasks running for each App is now shown.

### 0.62.146 (2024-05-07)

* Added the
  `region`
  parameter to the
  `modal.App.function`
  and
  `modal.App.cls`
  decorators. This feature allows the selection of specific regions for function execution. Note that it is available only on some plan types. See our
  [blog post](https://modal.com/blog/region-selection-launch)

  for more details.

### 0.62.144 (2024-05-06)

* Added deprecation warnings when using Python 3.8 locally or in a container. Python 3.8 is nearing EOL, and Modal will be dropping support for it soon.

### 0.62.141 (2024-05-03)

* Deprecated the
  `Image.conda`
  constructor and the
  `Image.conda_install`
  /
  `Image.conda_update_from_environment`
  methods. Conda-based images had a number of tricky issues and were generally slower and heavier than images based on
  `micromamba`
  , which offers a similar featureset and can install packages from the same repositories.
* Added the
  `spec_file`
  parameter to allow
  `Image.micromamba_install`
  to install dependencies from a local file. Note that
  `micromamba`
  supports conda yaml syntax along with simple text files.

### 0.62.131 (2024-05-01)

* Added a deprecation warning when object names are invalid. This applies to
  `Dict`
  ,
  `NetworkFileSystem`
  ,
  `Secret`
  ,
  `Queue`
  , and
  `Volume`
  objects. Names must be shorter than 64 characters and may contain only alphanumeric characters, dashes, periods, and underscores. These rules were previously enforced, but the check had inadvertently been dropped in a recent refactor. Please update the names of your objects and transfer any data to retain access, as invalid names will become an error in a future release.

### 0.62.130 (2024-05-01)

* Added a command-line interface for interacting with
  `modal.Queue`
  objects. Run
  `modal queue --help`
  in your terminal to see what is available.

### 0.62.116 (2024-04-26)

* Added a command-line interface for interacting with
  `modal.Dict`
  objects. Run
  `modal dict --help`
  in your terminal to see what is available.

### 0.62.114 (2024-04-25)

* `Secret.from_dotenv`
  now accepts an optional filename keyword argument:

  ```
  @app.function(secrets=[modal.Secret.from_dotenv(filename=".env-dev")])
  def run():
      ...
  ```

### 0.62.110 (2024-04-25)

* Passing a glob
  `**`
  argument to the
  `modal volume get`
  CLI has been deprecated — instead, simply download the desired directory path, or
  `/`
  for the entire volume.
* `Volume.listdir()`
  no longer takes trailing glob arguments. Use
  `recursive=True`
  instead.
* `modal volume get`
  and
  `modal nfs get`
  performance is improved when downloading a single file. They also now work with multiple files when outputting to stdout.
* Fixed a visual bug where
  `modal volume get`
  on a single file will incorrectly display the destination path.

### 0.62.109 (2024-04-24)

* Improved feedback for deserialization failures when objects are being transferred between local / remote environments.

### 0.62.108 (2024-04-24)

* Added
  `Dict.delete`
  and
  `Queue.delete`
  as API methods for deleting named storage objects:

  ```
  import modal
  modal.Queue.delete("my-job-queue")
  ```

* Deprecated invoking
  `Volume.delete`
  as an instance method; it should now be invoked as a static method with the name of the Volume to delete, as with the other methods.

### 0.62.98 (2024-04-21)

* The
  `modal.Dict`
  object now implements a
  `keys`
  /
  `values`
  /
  `items`
  API. Note that there are a few differences when compared to standard Python dicts:
  + The return value is a simple iterator, whereas Python uses a dictionary view object with more features.
  + The results are unordered.
* Additionally, there was no key data stored for items added to a
  `modal.Dict`
  prior to this release, so empty strings will be returned for these entries.

### 0.62.81 (2024-04-18)

* We are introducing
  `modal.App`
  as a replacement for
  `modal.Stub`
  and encouraging the use of “app” terminology over “stub” to reduce confusion between concepts used in the SDK and the Dashboard. Support for
  `modal.Stub`
  will be gradually deprecated over the next few months.

### 0.62.72 (2024-04-16)

* Specifying a hard memory limit for a
  `modal.Function`
  is now supported. Pass a tuple of
  `memory=(request, limit)`
  . Above the
  `limit`
  , which is specified in MiB, a Function’s container will be OOM killed.

### 0.62.70 (2024-04-16)

* `modal.CloudBucketMount`
  now supports read-only access to Google Cloud Storage

### 0.62.69 (2024-04-16)

* Iterators passed to
  `Function.map()`
  and similar parallel execution primitives are now executed on the main thread, preventing blocking iterators from possibly locking up background Modal API calls, and risking task shutdowns.

### 0.62.67 (2024-04-15)

* The return type of
  `Volume.listdir()`
  ,
  `Volume.iterdir()`
  ,
  `NetworkFileSystem.listdir()`
  , and
  `NetworkFileSystem.iterdir()`
  is now a
  `FileEntry`
  dataclass from the
  `modal.volume`
  module. The fields of this data class are the same as the old protobuf object returned by these methods, so it should be mostly backwards-compatible.

### 0.62.65 (2024-04-15)

* Cloudflare R2 bucket support added to
  `modal.CloudBucketMount`

### 0.62.55 (2024-04-11)

* When Volume reloads fail due to an open file, we now try to identify and report the relevant path. Note that there may be some circumstances in which we are unable to identify the specific file blocking a reload and will report a generic error message in that case.

### 0.62.53 (2024-04-10)

* Values in the
  `modal.toml`
  config file that are spelled as
  `0`
  ,
  `false`
  ,
  `"False"`
  , or
  `"false"`
  will now be coerced in Python to
  `False`
  , whereas previously only
  `"0"`
  (as a string) would have the intended effect.

### 0.62.25 (2024-04-01)

* Fixed a recent regression that caused functions using
  `modal.interact()`
  to crash.

### 0.62.15 (2024-03-29)

* Queue methods
  `put`
  ,
  `put_many`
  ,
  `get`
  ,
  `get_many`
  and
  `len`
  now support an optional
  `partition`
  argument (must be specified as a
  `kwarg`
  ). When specified, users read and write from new partitions of the queue independently.
  `partition=None`
  corresponds to the default partition of the queue.

### 0.62.3 (2024-03-27)

* User can now mount S3 buckets using
  [Requester Pays](https://docs.aws.amazon.com/AmazonS3/latest/userguide/RequesterPaysBuckets.html)

  . This can be done with
  `CloudBucketMount(..., requester_pays=True)`
  .

### 0.62.1 (2024-03-27)

* Raise an error on
  `@web_server(startup_timeout=0)`
  , which is an invalid configuration.

### 0.62.0 (2024-03-26)

* The
  `.new()`
  method has now been deprecated on all Modal objects. It should typically be replaced with
  `.from_name(...)`
  in Modal app code, or
  `.ephemeral()`
  in scripts that use Modal
* Assignment of Modal objects to a
  `Stub`
  via subscription (
  `stub["object"]`
  ) or attribute (
  `stub.object`
  ) syntax is now deprecated. This syntax was only necessary when using
  `.new()`
  .

0.61
----

### 0.61.104 (2024-03-25)

* Fixed a bug where images based on
  `micromamba`
  could fail to build if requesting Python 3.12 when a different version of Python was being used locally.

### 0.61.76 (2024-03-19)

* The
  `Sandbox`
  ’s
  `LogsReader`
  is now an asynchronous iterable. It supports the
  `async for`
  statement to stream data from the sandbox’s
  `stdout/stderr`
  .

```
@stub.function()
async def my_fn():
    sandbox = stub.spawn_sandbox(
      "bash",
      "-c",
      "while true; do echo foo; sleep 1; done"
    )
    async for message in sandbox.stdout:
        print(f"Message: {message}")
```

### 0.61.57 (2024-03-15)

* Add the
  `@web_server`
  decorator, which exposes a server listening on a container port as a web endpoint.

### 0.61.56 (2024-03-15)

* Allow users to write to the
  `Sandbox`
  ’s
  `stdin`
  with
  `StreamWriter`
  .

```
@stub.function()
def my_fn():
    sandbox = stub.spawn_sandbox(
        "bash",
        "-c",
        "while read line; do echo $line; done",
    )
    sandbox.stdin.write(b"foo\\n")
    sandbox.stdin.write(b"bar\\n")
    sandbox.stdin.write_eof()
    sandbox.stdin.drain()
    sandbox.wait()
```

### 0.61.53 (2024-03-15)

* Fixed an bug where
  `Mount`
  was failing to include symbolic links.

### 0.61.45 (2024-03-13)

When called from within a container,
`modal.experimental.stop_fetching_inputs()`
causes it to gracefully exit after the current input has been processed.

### 0.61.35 (2024-03-12)

* The
  `@wsgi_app()`
  decorator now uses a different backend based on
  `a2wsgi`
  that streams requests in chunks, rather than buffering the entire request body.

### 0.61.32 (2024-03-11)

* Stubs/apps can now be “composed” from several smaller stubs using
  `stub.include(...)`
  . This allows more ergonomic setup of multi-file Modal apps.

### 0.61.31 (2024-03-08)

* The
  `Image.extend`
  method has been deprecated. This is a low-level interface and can be replaced by other
  `Image`
  methods that offer more flexibility, such as
  `Image.from_dockerfile`
  ,
  `Image.dockerfile_commands`
  , or
  `Image.run_commands`
  .

### 0.61.24 (2024-03-06)

* Fixes
  `modal volume put`
  to support uploading larger files, beyond 40 GiB.

### 0.61.22 (2024-03-05)

* Modal containers now display a warning message if lingering threads are present at container exit, which prevents runner shutdown.

### 0.61.17 (2024-03-05)

* Bug fix: Stopping an app while a container’s
  `@exit()`
  lifecycle methods are being run no longer interrupts the lifecycle methods.
* Bug fix: Worker preemptions no longer interrupt a container’s
  `@exit()`
  lifecycle method (until 30 seconds later).
* Bug fix: Async
  `@exit()`
  lifecycle methods are no longer skipped for sync functions.
* Bug fix: Stopping a sync function with
  `allow_concurrent_inputs>1`
  now actually stops the container. Previously, it would not propagate the signal to worker threads, so they would continue running.
* Bug fix: Input-level cancellation no longer skips the
  `@exit()`
  lifecycle method.
* Improve stability of container entrypoint against race conditions in task cancellation.

### 0.61.9 (2024-03-05)

* Fix issue with pdm where all installed packages would be automounted when using package cache (MOD-2485)

### 0.61.6 (2024-03-04)

* For modal functions/classes with
  `concurrency_limit < keep_warm`
  , we’ll raise an exception now. Previously we (silently) respected the
  `concurrency_limit`
  parameter.

### 0.61.1 (2024-03-03)

`modal run --interactive`
or
`modal run -i`
run the app in “interactive mode”. This allows any remote code to connect to the user’s local terminal by calling
`modal.interact()`
.

```
@stub.function()
def my_fn(x):
    modal.interact()

    x = input()
    print(f"Your number is {x}")
```

This means that you can dynamically start an IPython shell if desired for debugging:

```
@stub.function()
def my_fn(x):
    modal.interact()

    from IPython import embed
    embed()
```

For convenience, breakpoints automatically call
`interact()`
:

```
@stub.function()
def my_fn(x):
    breakpoint()
```

0.60
----

### 0.60.0 (2024-02-29)

* `Image.run_function`
  now allows you to pass args and kwargs to the function. Usage:

```
def my_build_function(name, size, *, variant=None):
    print(f"Building {name} {size} {variant}")

image = modal.Image.debian_slim().run_function(
    my_build_function, args=("foo", 10), kwargs={"variant": "bar"}
)
```

0.59
----

### 0.59.0 (2024-02-28)

* Mounted packages are now deduplicated across functions in the same stub
* Mounting of local Python packages are now marked as such in the mount creation output, e.g.
  `PythonPackage:my_package`
* Automatic mounting now includes packages outside of the function file’s own directory. Mounted packages are mounted in /root/

0.58
----

### 0.58.92 (2024-02-27)

* Most errors raised through usage of the CLI will now print a simple error message rather than showing a traceback from inside the
  `modal`
  library.
* Tracebacks originating from user code will include fewer frames from within
  `modal`
  itself.
* The new
  `MODAL_TRACEBACK`
  environment variable (and
  `traceback`
  field in the Modal config file) can override these behaviors so that full tracebacks are always shown.

### 0.58.90 (2024-02-27)

* Fixed a bug that could cause
  `cls`
  -based functions to to ignore timeout signals.

### 0.58.88 (2024-02-26)

* `volume get`
  performance is improved for large (> 100MB) files

### 0.58.79 (2024-02-23)

* Support for function parameters in methods decorated with
  `@exit`
  has been deprecated. Previously, exit methods were required to accept three arguments containing exception information (akin to
  `__exit__`
  in the context manager protocol). However, due to a bug, these arguments were always null. Going forward,
  `@exit`
  methods are expected to have no parameters.

### 0.58.75 (2024-02-23)

* Function calls can now be cancelled without killing the container running the inputs. This allows new inputs by different function calls to the same function to be picked up immediately without having to cold-start new containers after cancelling calls.

0.57
----

### 0.57.62 (2024-02-21)

* An
  `InvalidError`
  is now raised when a lifecycle decorator (
  `@build`
  ,
  `@enter`
  , or
  `@exit`
  ) is used in conjunction with
  `@method`
  . Previously, this was undefined and could produce confusing failures.

### 0.57.61 (2024-02-21)

* Reduced the amount of context for frames in modal’s CLI framework when showing a traceback.

### 0.57.60 (2024-02-21)

* The “dunder method” approach for class lifecycle management (
  `__build__`
  ,
  `__enter__`
  ,
  `__exit__`
  , etc.) is now deprecated in favor of the modal
  `@build`
  ,
  `@enter`
  , and
  `@exit`
  decorators.

### 0.57.52 (2024-02-17)

* In
  `modal token new`
  and
  `modal token set`
  , the
  `--no-no-verify`
  flag has been removed in favor of a
  `--verify`
  flag. This remains the default behavior.

### 0.57.51 (2024-02-17)

* Fixes a regression from 0.57.40 where
  `@enter`
  methods used a separate event loop.

### 0.57.42 (2024-02-14)

* Adds a new environment variable/config setting,
  `MODAL_FORCE_BUILD`
  /
  `force_build`
  , that coerces all images to be built from scratch, rather than loaded from cache.

### 0.57.40 (2024-02-13)

* The
  `@enter()`
  lifecycle method can now be used to run additional setup code prior to function checkpointing (when the class is decorated with
  `stub.cls(enable_checkpointing=True)`
  . Note that there are currently some limitations on function checkpointing:
  + Checkpointing only works for CPU memory; any GPUs attached to the function will not available
  + Networking is disabled while the checkpoint is being created
* Please note that function checkpointing is still a beta feature.

### 0.57.31 (2024-02-12)

* Fixed an issue with displaying deprecation warnings on Windows systems.

### 0.57.22 (2024-02-09)

* Modal client deprecation warnings are now highlighted in the CLI

### 0.57.16 (2024-02-07)

* Fixes a regression in container scheduling. Users on affected versions (
  **0.57.5**
  —
  **0.57.15**
  ) are encouraged to upgrade immediately.

### 0.57.15 (2024-02-07)

* The legacy
  `image_python_version`
  config option has been removed. Use the
  `python_version=`
  parameter on your image definition instead.

### 0.57.13 (2024-02-07)

* Adds support for mounting an S3 bucket as a volume.

### 0.57.9 (2024-02-07)

* Support for an implicit ‘default’ profile is now deprecated. If you have more than one profile in your Modal config file, one must be explicitly set to
  `active`
  (use
  `modal profile activate`
  or edit your
  `.modal.toml`
  file to resolve).
* An error is now raised when more than one profile is set to
  `active`
  .

### 0.57.2 (2024-02-06)

* Improve error message when generator functions are called with
  `.map(...)`
  .

### 0.57.0 (2024-02-06)

* Greatly improved streaming performance of generators and WebSocket web endpoints.
* **Breaking change:**
  You cannot use
  `.map()`
  to call a generator function. (In previous versions, this merged the results onto a single stream, but the behavior was undocumented and not widely used.)
* **Incompatibility:**
  Generator outputs are now on a different internal system. Modal code on client versions before 0.57 cannot trigger
  [deployed functions](https://modal.com/docs/guide/trigger-deployed-functions)

  with
  `.remote_gen()`
  that are on client version 0.57, and vice versa.

0.56
----

Note that in version 0.56 and prior, Modal used a different numbering system for patch releases.

### 0.56.4964 (2024-02-05)

* When using
  `modal token new`
  or
  `model token set`
  , the profile containing the new token will now be activated by default. Use the
  `--no-activate`
  switch to update the
  `modal.toml`
  file without activating the corresponding profile.

### 0.56.4953 (2024-02-05)

* The
  `modal profile list`
  output now indicates when the workspace is determined by a token stored in environment variables.

### 0.56.4952 (2024-02-05)

* Variadic parameters (e.g. \*args and \*\*kwargs) can now be used in scheduled functions as long as the function doesn’t have any other parameters without a default value

### 0.56.4903 (2024-02-01)

* `modal container exec`
  ’s
  `--no-tty`
  flag has been renamed to
  `--no-pty`
  .

### 0.56.4902 (2024-02-01)

* The singular form of the
  `secret`
  parameter in
  `Stub.function`
  ,
  `Stub.cls`
  , and
  `Image.run_function`
  has been deprecated. Please update your code to use the plural form instead:
  `secrets=[Secret(...)]`
  .

### 0.56.4885 (2024-02-01)

* In
  `modal profile list`
  , the user’s GitHub username is now shown as the name for the “Personal” workspace.

### 0.56.4874 (2024-01-31)

* The
  `modal token new`
  and
  `modal token set`
  commands now create profiles that are more closely associated with workspaces, and they have more explicit profile activation behavior:
  + By default, these commands will create/update a profile named after the workspace that the token points to, rather than a profile named “default”
  + Both commands now have an
    `--activate`
    flag that will activate the profile associated with the new token
  + If no other profiles exist at the time of creation, the new profile will have its
    `active`
    metadata set to True
* With these changes, we are moving away from the concept of a “default” profile. Implicit usage of the “default” profile will be deprecated in a future update.

### 0.56.4849 (2024-01-29)

* Adds tty support to
  `modal container exec`
  for fully-interactive commands. Example:
  `modal container exec [container-id] /bin/bash`

### 0.56.4792 (2024-01-26)

* The
  `modal profile list`
  command now shows the workspace associated with each profile.

### 0.56.4715 (2024-01-24)

* `Mount.from_local_python_packages`
  now places mounted packages at
  `/root`
  in the Modal runtime by default (used to be
  `/pkg`
  ). To override this behavior, the function now takes a
  `remote_dir: Union[str, PurePosixPath]`
  argument.

### 0.56.4707 (2024-01-23)

* The Modal client library is now compatible with Python 3.12, although there are a few limitations:

  + Images that use Python 3.12 without explicitly specifing it through
    `python_version`
    or
    `add_python`
    will not build
    properly unless the modal client is also running on Python 3.12.
  + The
    `conda`
    and
    `microconda`
    base images currently do not support Python 3.12 because an upstream dependency is not yet compatible.

### 0.56.4700 (2024-01-22)

* `gpu.A100`
  class now supports specifying GiB memory configuration using a
  `size: str`
  parameter. The
  `memory: int`
  parameter is deprecated.

### 0.56.4693 (2024-01-22)

* You can now execute commands in running containers with
  `modal container exec [container-id] [command]`
  .

### 0.56.4691 (2024-01-22)

* The
  `modal`
  cli now works more like the
  `python`
  cli in regard to script/module loading:
  + Running
    `modal my_dir/my_script.py`
    now puts
    `my_dir`
    on the PYTHONPATH.
  + `modal my_package.my_module`
    will now mount to /root/my\_package/my\_module.py in your Modal container, regardless if using automounting or not (and any intermediary
    `__init__.py`
    files will also be mounted)

### 0.56.4687 (2024-01-20)

* Modal now uses the current profile if
  `MODAL_PROFILE`
  is set to the empty string.

### 0.56.4649 (2024-01-17)

* Dropped support for building Python 3.7 based
  `modal.Image`
  s. Python 3.7 is end-of-life since late June 2023.

### 0.56.4620 (2024-01-16)

* modal.Stub.function now takes a
  `block_network`
  argument.

### 0.56.4616 (2024-01-16)

* modal.Stub now takes a
  `volumes`
  argument for setting the default volumes of all the stub’s functions, similarly to the
  `mounts`
  and
  `secrets`
  argument.

### 0.56.4590 (2024-01-13)

* `modal serve`
  : Setting MODAL\_LOGLEVEL=DEBUG now displays which files cause an app reload during serve

### 0.56.4570 (2024-01-12)

* `modal run`
  cli command now properly propagates
  `--env`
  values to object lookups in global scope of user code

[Changelog](#changelog)

[Latest](#latest)

[1.1.0 (2025-07-17)](#110-2025-07-17)

[1.0.5 (2025-06-27)](#105-2025-06-27)

[1.0.4 (2025-06-13)](#104-2025-06-13)

[1.0.3 (2025-06-05)](#103-2025-06-05)

[1.0.2 (2025-05-26)](#102-2025-05-26)

[1.0.1 (2025-05-19)](#101-2025-05-19)

[1.0.0 (2025-05-16)](#100-2025-05-16)

[0.77](#077)

[0.77.0 (2025-05-13)](#0770-2025-05-13)

[0.76](#076)

[0.76.3 (2025-05-12)](#0763-2025-05-12)

[0.76.0 (2025-05-12)](#0760-2025-05-12)

[0.75](#075)

[0.75.8 (2025-05-12)](#0758-2025-05-12)

[0.75.4 (2025-05-09)](#0754-2025-05-09)

[0.75.3 (2025-05-08)](#0753-2025-05-08)

[0.75.2 (2025-05-08)](#0752-2025-05-08)

[0.75.0 (2025-05-08)](#0750-2025-05-08)

[0.74](#074)

[0.74.63 (2025-05-08)](#07463-2025-05-08)

[0.74.61 (2025-05-07)](#07461-2025-05-07)

[0.74.60 (2025-05-07)](#07460-2025-05-07)

[0.74.56 (2025-05-06)](#07456-2025-05-06)

[0.74.53 (2025-05-06)](#07453-2025-05-06)

[0.74.51 (2025-05-06)](#07451-2025-05-06)

[0.74.49 (2025-05-06)](#07449-2025-05-06)

[0.74.48 (2025-05-05)](#07448-2025-05-05)

[0.74.46 (2025-05-05)](#07446-2025-05-05)

[0.74.39 (2025-04-30)](#07439-2025-04-30)

[0.74.36 (2025-04-29)](#07436-2025-04-29)

[0.74.35 (2025-04-29)](#07435-2025-04-29)

[0.74.32 (2025-04-29)](#07432-2025-04-29)

[0.74.25 (2025-04-25)](#07425-2025-04-25)

[0.74.23 (2025-04-25)](#07423-2025-04-25)

[0.74.22 (2025-04-24)](#07422-2025-04-24)

[0.74.21 (2025-04-24)](#07421-2025-04-24)

[0.74.18 (2025-04-23)](#07418-2025-04-23)

[0.74.12 (2025-04-18)](#07412-2025-04-18)

[0.74.7 (2025-04-17)](#0747-2025-04-17)

[0.74.6 (2025-04-17)](#0746-2025-04-17)

[0.74.0 (2025-04-15)](#0740-2025-04-15)

[0.73](#073)

[0.73.173 (2025-04-15)](#073173-2025-04-15)

[0.73.170 (2025-04-14)](#073170-2025-04-14)

[0.73.165 (2025-04-11)](#073165-2025-04-11)

[0.73.160 (2025-04-10)](#073160-2025-04-10)

[0.73.159 (2025-04-10)](#073159-2025-04-10)

[0.73.158 (2025-04-10)](#073158-2025-04-10)

[0.73.148 (2025-04-07)](#073148-2025-04-07)

[0.73.147 (2025-04-07)](#073147-2025-04-07)

[0.73.144 (2025-04-04)](#073144-2025-04-04)

[0.73.139 (2025-04-02)](#073139-2025-04-02)

[0.73.135 (2025-03-29)](#073135-2025-03-29)

[0.73.132 (2025-03-28)](#073132-2025-03-28)

[0.73.131 (2025-03-28)](#073131-2025-03-28)

[0.73.121 (2025-03-24)](#073121-2025-03-24)

[0.73.119 (2025-03-21)](#073119-2025-03-21)

[0.73.115 (2025-03-19)](#073115-2025-03-19)

[0.73.107 (2025-03-14)](#073107-2025-03-14)

[0.73.105 (2025-03-14)](#073105-2025-03-14)

[0.73.95 (2025-03-12)](#07395-2025-03-12)

[0.73.89 (2025-03-05)](#07389-2025-03-05)

[0.73.84 (2025-03-04)](#07384-2025-03-04)

[0.73.82 (2025-03-04)](#07382-2025-03-04)

[0.73.81 (2025-03-03)](#07381-2025-03-03)

[0.73.78 (2025-03-01)](#07378-2025-03-01)

[0.73.77 (2025-03-01)](#07377-2025-03-01)

[0.73.76 (2025-02-28)](#07376-2025-02-28)

[0.73.75 (2025-02-28)](#07375-2025-02-28)

[0.73.73 (2025-02-28)](#07373-2025-02-28)

[0.73.69 (2025-02-25)](#07369-2025-02-25)

[0.73.68 (2025-02-25)](#07368-2025-02-25)

[0.73.60 (2025-02-20)](#07360-2025-02-20)

[0.73.58 (2025-02-20)](#07358-2025-02-20)

[0.73.54 (2025-02-18)](#07354-2025-02-18)

[0.73.51 (2025-02-14)](#07351-2025-02-14)

[0.73.44 (2025-02-13)](#07344-2025-02-13)

[0.73.40 (2025-02-12)](#07340-2025-02-12)

[0.73.31 (2025-02-10)](#07331-2025-02-10)

[0.73.26 (2025-02-10)](#07326-2025-02-10)

[0.73.25 (2025-02-09)](#07325-2025-02-09)

[0.73.18 (2025-02-06)](#07318-2025-02-06)

[0.73.14 (2025-02-04)](#07314-2025-02-04)

[0.73.11 (2025-02-04)](#07311-2025-02-04)

[0.73.10 (2025-02-04)](#07310-2025-02-04)

[0.73.1 (2025-01-30)](#0731-2025-01-30)

[0.73.0 (2025-01-30)](#0730-2025-01-30)

[0.72](#072)

[0.72.56 (2025-01-28)](#07256-2025-01-28)

[0.72.54 (2025-01-28)](#07254-2025-01-28)

[0.72.48 (2025-01-24)](#07248-2025-01-24)

[0.72.39 (2025-01-22)](#07239-2025-01-22)

[0.72.33 (2025-01-20)](#07233-2025-01-20)

[0.72.30 (2025-01-18)](#07230-2025-01-18)

[0.72.24 (2025-01-17)](#07224-2025-01-17)

[0.72.22 (2025-01-17)](#07222-2025-01-17)

[0.72.17 (2025-01-16)](#07217-2025-01-16)

[0.72.16 (2025-01-16)](#07216-2025-01-16)

[0.72.15 (2025-01-15)](#07215-2025-01-15)

[0.72.8 (2025-01-10)](#0728-2025-01-10)

[0.72.0 (2025-01-09)](#0720-2025-01-09)

[0.71](#071)

[0.71.13 (2025-01-09)](#07113-2025-01-09)

[0.71.11 (2025-01-08)](#07111-2025-01-08)

[0.71.7 (2025-01-08)](#0717-2025-01-08)

[0.71.1 (2025-01-06)](#0711-2025-01-06)

[0.70](#070)

[0.70.1 (2024-12-27)](#0701-2024-12-27)

[0.69](#069)

[0.69.0 (2024-12-21)](#0690-2024-12-21)

[0.68](#068)

[0.68.53 (2024-12-20)](#06853-2024-12-20)

[0.68.44 (2024-12-19)](#06844-2024-12-19)

[0.68.43 (2024-12-19)](#06843-2024-12-19)

[0.68.39 (2024-12-18)](#06839-2024-12-18)

[0.68.29 (2024-12-17)](#06829-2024-12-17)

[0.68.28 (2024-12-17)](#06828-2024-12-17)

[0.68.27 (2024-12-17)](#06827-2024-12-17)

[0.68.24 (2024-12-16)](#06824-2024-12-16)

[0.68.21 (2024-12-13)](#06821-2024-12-13)

[0.68.15 (2024-12-13)](#06815-2024-12-13)

[0.68.11 (2024-12-13)](#06811-2024-12-13)

[0.68.6 (2024-12-12)](#0686-2024-12-12)

[0.68.5 (2024-12-12)](#0685-2024-12-12)

[0.68.2 (2024-12-11)](#0682-2024-12-11)

[0.67](#067)

[0.67.43 (2024-12-11)](#06743-2024-12-11)

[0.67.39 (2024-12-09)](#06739-2024-12-09)

[0.67.38 (2024-12-09)](#06738-2024-12-09)

[0.67.28 (2024-12-05)](#06728-2024-12-05)

[0.67.23 (2024-12-04)](#06723-2024-12-04)

[0.67.22 (2024-12-03)](#06722-2024-12-03)

[0.67.12 (2024-12-02)](#06712-2024-12-02)

[0.67.7 (2024-11-29)](#0677-2024-11-29)

[0.67.0 (2024-11-27)](#0670-2024-11-27)

[0.66](#066)

[0.66.49 (2024-11-26)](#06649-2024-11-26)

[0.66.45 (2024-11-26)](#06645-2024-11-26)

[0.66.40 (2024-11-23)](#06640-2024-11-23)

[0.66.30 (2024-11-21)](#06630-2024-11-21)

[0.66.12 (2024-11-19)](#06612-2024-11-19)

[0.66.0 (2024-11-15)](#0660-2024-11-15)

[0.65](#065)

[0.65.55 (2024-11-13)](#06555-2024-11-13)

[0.65.49 (2024-11-12)](#06549-2024-11-12)

[0.65.42 (2024-11-07)](#06542-2024-11-07)

[0.65.33 (2024-11-06)](#06533-2024-11-06)

[0.65.30 (2024-11-05)](#06530-2024-11-05)

[0.65.9 (2024-10-31)](#0659-2024-10-31)

[0.65.8 (2024-10-31)](#0658-2024-10-31)

[0.65.2 (2024-10-30)](#0652-2024-10-30)

[0.64](#064)

[0.64.227 (2024-10-25)](#064227-2024-10-25)

[0.64.223 (2024-10-24)](#064223-2024-10-24)

[0.64.218 (2024-10-23)](#064218-2024-10-23)

[0.64.198 (2024-10-18)](#064198-2024-10-18)

[0.64.194 (2024-10-18)](#064194-2024-10-18)

[0.64.193 (2024-10-18)](#064193-2024-10-18)

[0.64.187 (2024-10-16)](#064187-2024-10-16)

[0.64.185 (2024-10-15)](#064185-2024-10-15)

[0.64.184 (2024-10-15)](#064184-2024-10-15)

[0.64.182 (2024-10-15)](#064182-2024-10-15)

[0.64.181 (2024-10-14)](#064181-2024-10-14)

[0.64.180 (2024-10-14)](#064180-2024-10-14)

[0.64.168 (2024-10-03)](#064168-2024-10-03)

[0.64.153 (2024-09-30)](#064153-2024-09-30)

[0.64.142 (2024-09-25)](#064142-2024-09-25)

[0.64.139 (2024-09-25)](#064139-2024-09-25)

[0.64.123 (2024-09-18)](#064123-2024-09-18)

[0.64.119 (2024-09-17)](#064119-2024-09-17)

[0.64.118 (2024-09-17)](#064118-2024-09-17)

[0.64.112 (2024-09-15)](#064112-2024-09-15)

[0.64.109 (2024-09-13)](#064109-2024-09-13)

[0.64.100 (2024-09-11)](#064100-2024-09-11)

[0.64.99 (2024-09-11)](#06499-2024-09-11)

[0.64.97 (2024-09-11)](#06497-2024-09-11)

[0.64.87 (2024-09-05)](#06487-2024-09-05)

[0.64.67 (2024-08-30)](#06467-2024-08-30)

[0.64.48 (2024-08-21)](#06448-2024-08-21)

[0.64.38 (2024-08-16)](#06438-2024-08-16)

[0.64.33 (2024-08-16)](#06433-2024-08-16)

[0.64.32 (2024-08-16)](#06432-2024-08-16)

[0.64.26 (2024-08-15)](#06426-2024-08-15)

[0.64.24 (2024-08-14)](#06424-2024-08-14)

[0.64.18 (2024-08-12)](#06418-2024-08-12)

[0.64.8 (2024-08-06)](#0648-2024-08-06)

[0.64.7 (2024-08-05)](#0647-2024-08-05)

[0.64.2 (2024-08-02)](#0642-2024-08-02)

[0.64.0 (2024-07-29)](#0640-2024-07-29)

[0.63](#063)

[0.63.87 (2024-07-24)](#06387-2024-07-24)

[0.63.77 (2024-07-18)](#06377-2024-07-18)

[0.63.36 (2024-07-05)](#06336-2024-07-05)

[0.63.22 (2024-07-01)](#06322-2024-07-01)

[0.63.20 (2024-07-01)](#06320-2024-07-01)

[0.63.14 (2024-06-28)](#06314-2024-06-28)

[0.63.12 (2024-06-27)](#06312-2024-06-27)

[0.63.10 (2024-06-26)](#06310-2024-06-26)

[0.63.9 (2024-06-26)](#0639-2024-06-26)

[0.63.2 (2024-06-25)](#0632-2024-06-25)

[0.63.0 (2024-06-24)](#0630-2024-06-24)

[0.62](#062)

[0.62.236 (2024-06-21)](#062236-2024-06-21)

[0.62.230 (2024-06-18)](#062230-2024-06-18)

[0.62.224 (2024-06-17)](#062224-2024-06-17)

[0.62.223 (2024-06-14)](#062223-2024-06-14)

[0.62.220 (2024-06-12)](#062220-2024-06-12)

[0.62.219 (2024-06-12)](#062219-2024-06-12)

[0.62.208 (2024-06-08)](#062208-2024-06-08)

[0.62.201 (2024-06-04)](#062201-2024-06-04)

[0.62.199 (2024-06-04)](#062199-2024-06-04)

[0.62.190 (2024-05-29)](#062190-2024-05-29)

[0.62.186 (2024-05-29)](#062186-2024-05-29)

[0.62.185 (2024-05-28)](#062185-2024-05-28)

[0.62.181 (2024-05-24)](#062181-2024-05-24)

[0.62.178 (2024-05-21)](#062178-2024-05-21)

[0.62.175 (2024-05-17)](#062175-2024-05-17)

[0.62.174 (2024-05-17)](#062174-2024-05-17)

[0.62.173 (2024-05-17)](#062173-2024-05-17)

[0.62.172 (2024-05-17)](#062172-2024-05-17)

[0.62.166 (2024-05-14)](#062166-2024-05-14)

[0.62.165 (2024-05-13)](#062165-2024-05-13)

[0.62.162 (2024-05-13)](#062162-2024-05-13)

[0.62.159 (2024-05-10)](#062159-2024-05-10)

[0.62.156 (2024-05-09)](#062156-2024-05-09)

[0.62.150 (2024-05-08)](#062150-2024-05-08)

[0.62.149 (2024-05-08)](#062149-2024-05-08)

[0.62.148 (2024-05-08)](#062148-2024-05-08)

[0.62.146 (2024-05-07)](#062146-2024-05-07)

[0.62.144 (2024-05-06)](#062144-2024-05-06)

[0.62.141 (2024-05-03)](#062141-2024-05-03)

[0.62.131 (2024-05-01)](#062131-2024-05-01)

[0.62.130 (2024-05-01)](#062130-2024-05-01)

[0.62.116 (2024-04-26)](#062116-2024-04-26)

[0.62.114 (2024-04-25)](#062114-2024-04-25)

[0.62.110 (2024-04-25)](#062110-2024-04-25)

[0.62.109 (2024-04-24)](#062109-2024-04-24)

[0.62.108 (2024-04-24)](#062108-2024-04-24)

[0.62.98 (2024-04-21)](#06298-2024-04-21)

[0.62.81 (2024-04-18)](#06281-2024-04-18)

[0.62.72 (2024-04-16)](#06272-2024-04-16)

[0.62.70 (2024-04-16)](#06270-2024-04-16)

[0.62.69 (2024-04-16)](#06269-2024-04-16)

[0.62.67 (2024-04-15)](#06267-2024-04-15)

[0.62.65 (2024-04-15)](#06265-2024-04-15)

[0.62.55 (2024-04-11)](#06255-2024-04-11)

[0.62.53 (2024-04-10)](#06253-2024-04-10)

[0.62.25 (2024-04-01)](#06225-2024-04-01)

[0.62.15 (2024-03-29)](#06215-2024-03-29)

[0.62.3 (2024-03-27)](#0623-2024-03-27)

[0.62.1 (2024-03-27)](#0621-2024-03-27)

[0.62.0 (2024-03-26)](#0620-2024-03-26)

[0.61](#061)

[0.61.104 (2024-03-25)](#061104-2024-03-25)

[0.61.76 (2024-03-19)](#06176-2024-03-19)

[0.61.57 (2024-03-15)](#06157-2024-03-15)

[0.61.56 (2024-03-15)](#06156-2024-03-15)

[0.61.53 (2024-03-15)](#06153-2024-03-15)

[0.61.45 (2024-03-13)](#06145-2024-03-13)

[0.61.35 (2024-03-12)](#06135-2024-03-12)

[0.61.32 (2024-03-11)](#06132-2024-03-11)

[0.61.31 (2024-03-08)](#06131-2024-03-08)

[0.61.24 (2024-03-06)](#06124-2024-03-06)

[0.61.22 (2024-03-05)](#06122-2024-03-05)

[0.61.17 (2024-03-05)](#06117-2024-03-05)

[0.61.9 (2024-03-05)](#0619-2024-03-05)

[0.61.6 (2024-03-04)](#0616-2024-03-04)

[0.61.1 (2024-03-03)](#0611-2024-03-03)

[0.60](#060)

[0.60.0 (2024-02-29)](#0600-2024-02-29)

[0.59](#059)

[0.59.0 (2024-02-28)](#0590-2024-02-28)

[0.58](#058)

[0.58.92 (2024-02-27)](#05892-2024-02-27)

[0.58.90 (2024-02-27)](#05890-2024-02-27)

[0.58.88 (2024-02-26)](#05888-2024-02-26)

[0.58.79 (2024-02-23)](#05879-2024-02-23)

[0.58.75 (2024-02-23)](#05875-2024-02-23)

[0.57](#057)

[0.57.62 (2024-02-21)](#05762-2024-02-21)

[0.57.61 (2024-02-21)](#05761-2024-02-21)

[0.57.60 (2024-02-21)](#05760-2024-02-21)

[0.57.52 (2024-02-17)](#05752-2024-02-17)

[0.57.51 (2024-02-17)](#05751-2024-02-17)

[0.57.42 (2024-02-14)](#05742-2024-02-14)

[0.57.40 (2024-02-13)](#05740-2024-02-13)

[0.57.31 (2024-02-12)](#05731-2024-02-12)

[0.57.22 (2024-02-09)](#05722-2024-02-09)

[0.57.16 (2024-02-07)](#05716-2024-02-07)

[0.57.15 (2024-02-07)](#05715-2024-02-07)

[0.57.13 (2024-02-07)](#05713-2024-02-07)

[0.57.9 (2024-02-07)](#0579-2024-02-07)

[0.57.2 (2024-02-06)](#0572-2024-02-06)

[0.57.0 (2024-02-06)](#0570-2024-02-06)

[0.56](#056)

[0.56.4964 (2024-02-05)](#0564964-2024-02-05)

[0.56.4953 (2024-02-05)](#0564953-2024-02-05)

[0.56.4952 (2024-02-05)](#0564952-2024-02-05)

[0.56.4903 (2024-02-01)](#0564903-2024-02-01)

[0.56.4902 (2024-02-01)](#0564902-2024-02-01)

[0.56.4885 (2024-02-01)](#0564885-2024-02-01)

[0.56.4874 (2024-01-31)](#0564874-2024-01-31)

[0.56.4849 (2024-01-29)](#0564849-2024-01-29)

[0.56.4792 (2024-01-26)](#0564792-2024-01-26)

[0.56.4715 (2024-01-24)](#0564715-2024-01-24)

[0.56.4707 (2024-01-23)](#0564707-2024-01-23)

[0.56.4700 (2024-01-22)](#0564700-2024-01-22)

[0.56.4693 (2024-01-22)](#0564693-2024-01-22)

[0.56.4691 (2024-01-22)](#0564691-2024-01-22)

[0.56.4687 (2024-01-20)](#0564687-2024-01-20)

[0.56.4649 (2024-01-17)](#0564649-2024-01-17)

[0.56.4620 (2024-01-16)](#0564620-2024-01-16)

[0.56.4616 (2024-01-16)](#0564616-2024-01-16)

[0.56.4590 (2024-01-13)](#0564590-2024-01-13)

[0.56.4570 (2024-01-12)](#0564570-2024-01-12)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/app
================================================================================

`modal app`
===========

Manage deployed and running apps.

**Usage**
:

```
modal app [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `list`
  : List Modal apps that are currently deployed/running or recently stopped.
* `logs`
  : Show App logs, streaming while active.
* `rollback`
  : Redeploy a previous version of an App.
* `stop`
  : Stop an app.
* `history`
  : Show App deployment history, for a currently deployed app

`modal app list`
----------------

List Modal apps that are currently deployed/running or recently stopped.

**Usage**
:

```
modal app list [OPTIONS]
```

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

`modal app logs`
----------------

Show App logs, streaming while active.

**Examples:**

Get the logs based on an app ID:

```
modal app logs ap-123456
```

Get the logs for a currently deployed App based on its name:

```
modal app logs my-app
```

**Usage**
:

```
modal app logs [OPTIONS] [APP_IDENTIFIER]
```

**Arguments**
:

* `[APP_IDENTIFIER]`
  : App name or ID

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--timestamps`
  : Show timestamps for each log line
* `--help`
  : Show this message and exit.

`modal app rollback`
--------------------

Redeploy a previous version of an App.

Note that the App must currently be in a “deployed” state.
Rollbacks will appear as a new deployment in the App history, although
the App state will be reset to the state at the time of the previous deployment.

**Examples:**

Rollback an App to its previous version:

```
modal app rollback my-app
```

Rollback an App to a specific version:

```
modal app rollback my-app v3
```

Rollback an App using its App ID instead of its name:

```
modal app rollback ap-abcdefghABCDEFGH123456
```

**Usage**
:

```
modal app rollback [OPTIONS] [APP_IDENTIFIER] [VERSION]
```

**Arguments**
:

* `[APP_IDENTIFIER]`
  : App name or ID
* `[VERSION]`
  : Target version for rollback.

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal app stop`
----------------

Stop an app.

**Usage**
:

```
modal app stop [OPTIONS] [APP_IDENTIFIER]
```

**Arguments**
:

* `[APP_IDENTIFIER]`
  : App name or ID

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal app history`
-------------------

Show App deployment history, for a currently deployed app

**Examples:**

Get the history based on an app ID:

```
modal app history ap-123456
```

Get the history for a currently deployed App based on its name:

```
modal app history my-app
```

**Usage**
:

```
modal app history [OPTIONS] [APP_IDENTIFIER]
```

**Arguments**
:

* `[APP_IDENTIFIER]`
  : App name or ID

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

[modal app](#modal-app)

[modal app list](#modal-app-list)

[modal app logs](#modal-app-logs)

[modal app rollback](#modal-app-rollback)

[modal app stop](#modal-app-stop)

[modal app history](#modal-app-history)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/config
================================================================================

`modal config`
==============

Manage client configuration for the current profile.

Refer to
<https://modal.com/docs/reference/modal.config>

for a full explanation
of what these options mean, and how to set them.

**Usage**
:

```
modal config [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `show`
  : Show current configuration values (debugging command).
* `set-environment`
  : Set the default Modal environment for the active profile

`modal config show`
-------------------

Show current configuration values (debugging command).

**Usage**
:

```
modal config show [OPTIONS]
```

**Options**
:

* `--redact / --no-redact`
  : Redact the
  `token_secret`
  value. [default: redact]
* `--help`
  : Show this message and exit.

`modal config set-environment`
------------------------------

Set the default Modal environment for the active profile

The default environment of a profile is used when no —env flag is passed to
`modal run`
,
`modal deploy`
etc.

If no default environment is set, and there exists multiple environments in a workspace, an error will be raised
when running a command that requires an environment.

**Usage**
:

```
modal config set-environment [OPTIONS] ENVIRONMENT_NAME
```

**Arguments**
:

* `ENVIRONMENT_NAME`
  : [required]

**Options**
:

* `--help`
  : Show this message and exit.

[modal config](#modal-config)

[modal config show](#modal-config-show)

[modal config set-environment](#modal-config-set-environment)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/container
================================================================================

`modal container`
=================

Manage and connect to running containers.

**Usage**
:

```
modal container [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `list`
  : List all containers that are currently running.
* `logs`
  : Show logs for a specific container, streaming while active.
* `exec`
  : Execute a command in a container.
* `stop`
  : Stop a currently-running container and reassign its in-progress inputs.

`modal container list`
----------------------

List all containers that are currently running.

**Usage**
:

```
modal container list [OPTIONS]
```

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

`modal container logs`
----------------------

Show logs for a specific container, streaming while active.

**Usage**
:

```
modal container logs [OPTIONS] CONTAINER_ID
```

**Arguments**
:

* `CONTAINER_ID`
  : Container ID [required]

**Options**
:

* `--help`
  : Show this message and exit.

`modal container exec`
----------------------

Execute a command in a container.

**Usage**
:

```
modal container exec [OPTIONS] CONTAINER_ID COMMAND...
```

**Arguments**
:

* `CONTAINER_ID`
  : Container ID [required]
* `COMMAND...`
  : A command to run inside the container.

To pass command-line flags or options, add
`--`
before the start of your commands. For example:
`modal container exec <id> -- /bin/bash -c 'echo hi'`
[required]

**Options**
:

* `--pty / --no-pty`
  : Run the command using a PTY.
* `--help`
  : Show this message and exit.

`modal container stop`
----------------------

Stop a currently-running container and reassign its in-progress inputs.

This will send the container a SIGINT signal that Modal will handle.

**Usage**
:

```
modal container stop [OPTIONS] CONTAINER_ID
```

**Arguments**
:

* `CONTAINER_ID`
  : Container ID [required]

**Options**
:

* `--help`
  : Show this message and exit.

[modal container](#modal-container)

[modal container list](#modal-container-list)

[modal container logs](#modal-container-logs)

[modal container exec](#modal-container-exec)

[modal container stop](#modal-container-stop)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/deploy
================================================================================

`modal deploy`
==============

Deploy a Modal application.

**Usage:**
modal deploy my\_script.py
modal deploy -m my\_package.my\_mod

**Usage**
:

```
modal deploy [OPTIONS] APP_REF
```

**Arguments**
:

* `APP_REF`
  : Path to a Python file with an app to deploy [required]

**Options**
:

* `--name TEXT`
  : Name of the deployment.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--stream-logs / --no-stream-logs`
  : Stream logs from the app upon deployment. [default: no-stream-logs]
* `--tag TEXT`
  : Tag the deployment with a version.
* `-m`
  : Interpret argument as a Python module path instead of a file/script path
* `--help`
  : Show this message and exit.

[modal deploy](#modal-deploy)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/dict
================================================================================

`modal dict`
============

Manage
`modal.Dict`
objects and inspect their contents.

**Usage**
:

```
modal dict [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `create`
  : Create a named Dict object.
* `list`
  : List all named Dicts.
* `clear`
  : Clear the contents of a named Dict by deleting all of its data.
* `delete`
  : Delete a named Dict and all of its data.
* `get`
  : Print the value for a specific key.
* `items`
  : Print the contents of a Dict.

`modal dict create`
-------------------

Create a named Dict object.

Note: This is a no-op when the Dict already exists.

**Usage**
:

```
modal dict create [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal dict list`
-----------------

List all named Dicts.

**Usage**
:

```
modal dict list [OPTIONS]
```

**Options**
:

* `--json / --no-json`
  : [default: no-json]
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal dict clear`
------------------

Clear the contents of a named Dict by deleting all of its data.

**Usage**
:

```
modal dict clear [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal dict delete`
-------------------

Delete a named Dict and all of its data.

**Usage**
:

```
modal dict delete [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal dict get`
----------------

Print the value for a specific key.

Note: When using the CLI, keys are always interpreted as having a string type.

**Usage**
:

```
modal dict get [OPTIONS] NAME KEY
```

**Arguments**
:

* `NAME`
  : [required]
* `KEY`
  : [required]

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal dict items`
------------------

Print the contents of a Dict.

Note: By default, this command truncates the contents. Use the
`N`
argument to control the
amount of data shown or the
`--all`
option to retrieve the entire Dict, which may be slow.

**Usage**
:

```
modal dict items [OPTIONS] NAME [N]
```

**Arguments**
:

* `NAME`
  : [required]
* `[N]`
  : Limit the number of entries shown [default: 20]

**Options**
:

* `-a, --all`
  : Ignore N and print all entries in the Dict (may be slow)
* `-r, --repr`
  : Display items using
  `repr()`
  to see more details
* `--json / --no-json`
  : [default: no-json]
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

[modal dict](#modal-dict)

[modal dict create](#modal-dict-create)

[modal dict list](#modal-dict-list)

[modal dict clear](#modal-dict-clear)

[modal dict delete](#modal-dict-delete)

[modal dict get](#modal-dict-get)

[modal dict items](#modal-dict-items)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/environment
================================================================================

`modal environment`
===================

Create and interact with Environments

Environments are sub-divisons of workspaces, allowing you to deploy the same app
in different namespaces. Each environment has their own set of Secrets and any
lookups performed from an app in an environment will by default look for entities
in the same environment.

Typical use cases for environments include having one for development and one for
production, to prevent overwriting production apps when developing new features
while still being able to deploy changes to a live environment.

**Usage**
:

```
modal environment [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `list`
  : List all environments in the current workspace
* `create`
  : Create a new environment in the current workspace
* `delete`
  : Delete an environment in the current workspace
* `update`
  : Update the name or web suffix of an environment

`modal environment list`
------------------------

List all environments in the current workspace

**Usage**
:

```
modal environment list [OPTIONS]
```

**Options**
:

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

`modal environment create`
--------------------------

Create a new environment in the current workspace

**Usage**
:

```
modal environment create [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : Name of the new environment. Must be unique. Case sensitive [required]

**Options**
:

* `--help`
  : Show this message and exit.

`modal environment delete`
--------------------------

Delete an environment in the current workspace

Deletes all apps in the selected environment and deletes the environment irrevocably.

**Usage**
:

```
modal environment delete [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : Name of the environment to be deleted. Case sensitive [required]

**Options**
:

* `--confirm / --no-confirm`
  : Set this flag to delete without prompting for confirmation [default: no-confirm]
* `--help`
  : Show this message and exit.

`modal environment update`
--------------------------

Update the name or web suffix of an environment

**Usage**
:

```
modal environment update [OPTIONS] CURRENT_NAME
```

**Arguments**
:

* `CURRENT_NAME`
  : [required]

**Options**
:

* `--set-name TEXT`
  : New name of the environment
* `--set-web-suffix TEXT`
  : New web suffix of environment (empty string is no suffix)
* `--help`
  : Show this message and exit.

[modal environment](#modal-environment)

[modal environment list](#modal-environment-list)

[modal environment create](#modal-environment-create)

[modal environment delete](#modal-environment-delete)

[modal environment update](#modal-environment-update)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/launch
================================================================================

`modal launch`
==============

Open a serverless app instance on Modal.

This command is in preview and may change in the future.

**Usage**
:

```
modal launch [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `jupyter`
  : Start Jupyter Lab on Modal.
* `vscode`
  : Start Visual Studio Code on Modal.

`modal launch jupyter`
----------------------

Start Jupyter Lab on Modal.

**Usage**
:

```
modal launch jupyter [OPTIONS]
```

**Options**
:

* `--cpu INTEGER`
  : [default: 8]
* `--memory INTEGER`
  : [default: 32768]
* `--gpu TEXT`
* `--timeout INTEGER`
  : [default: 3600]
* `--image TEXT`
  : [default: ubuntu:22.04]
* `--add-python TEXT`
  : [default: 3.11]
* `--mount TEXT`
* `--volume TEXT`
* `--detach / --no-detach`
  : [default: no-detach]
* `--help`
  : Show this message and exit.

`modal launch vscode`
---------------------

Start Visual Studio Code on Modal.

**Usage**
:

```
modal launch vscode [OPTIONS]
```

**Options**
:

* `--cpu INTEGER`
  : [default: 8]
* `--memory INTEGER`
  : [default: 32768]
* `--gpu TEXT`
* `--image TEXT`
  : [default: debian:12]
* `--timeout INTEGER`
  : [default: 3600]
* `--mount TEXT`
* `--volume TEXT`
* `--detach / --no-detach`
  : [default: no-detach]
* `--help`
  : Show this message and exit.

[modal launch](#modal-launch)

[modal launch jupyter](#modal-launch-jupyter)

[modal launch vscode](#modal-launch-vscode)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/nfs
================================================================================

`modal nfs`
===========

Read and edit
`modal.NetworkFileSystem`
file systems.

**Usage**
:

```
modal nfs [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `list`
  : List the names of all network file systems.
* `create`
  : Create a named network file system.
* `ls`
  : List files and directories in a network file system.
* `put`
  : Upload a file or directory to a network file system.
* `get`
  : Download a file from a network file system.
* `rm`
  : Delete a file or directory from a network file system.
* `delete`
  : Delete a named, persistent modal.NetworkFileSystem.

`modal nfs list`
----------------

List the names of all network file systems.

**Usage**
:

```
modal nfs list [OPTIONS]
```

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

`modal nfs create`
------------------

Create a named network file system.

**Usage**
:

```
modal nfs create [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal nfs ls`
--------------

List files and directories in a network file system.

**Usage**
:

```
modal nfs ls [OPTIONS] VOLUME_NAME [PATH]
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `[PATH]`
  : [default: /]

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal nfs put`
---------------

Upload a file or directory to a network file system.

Remote parent directories will be created as needed.

Ending the REMOTE\_PATH with a forward slash (/), it’s assumed to be a directory and the file
will be uploaded with its current name under that directory.

**Usage**
:

```
modal nfs put [OPTIONS] VOLUME_NAME LOCAL_PATH [REMOTE_PATH]
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `LOCAL_PATH`
  : [required]
* `[REMOTE_PATH]`
  : [default: /]

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal nfs get`
---------------

Download a file from a network file system.

Specifying a glob pattern (using any
`*`
or
`**`
patterns) as the
`remote_path`
will download
all matching files, preserving their directory structure.

For example, to download an entire network file system into
`dump_volume`
:

```
modal nfs get <volume-name> "**" dump_volume
```

Use ”-” as LOCAL\_DESTINATION to write file contents to standard output.

**Usage**
:

```
modal nfs get [OPTIONS] VOLUME_NAME REMOTE_PATH [LOCAL_DESTINATION]
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `REMOTE_PATH`
  : [required]
* `[LOCAL_DESTINATION]`
  : [default: .]

**Options**
:

* `--force / --no-force`
  : [default: no-force]
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal nfs rm`
--------------

Delete a file or directory from a network file system.

**Usage**
:

```
modal nfs rm [OPTIONS] VOLUME_NAME REMOTE_PATH
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `REMOTE_PATH`
  : [required]

**Options**
:

* `-r, --recursive`
  : Delete directory recursively
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal nfs delete`
------------------

Delete a named, persistent modal.NetworkFileSystem.

**Usage**
:

```
modal nfs delete [OPTIONS] NFS_NAME
```

**Arguments**
:

* `NFS_NAME`
  : Name of the modal.NetworkFileSystem to be deleted. Case sensitive [required]

**Options**
:

* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

[modal nfs](#modal-nfs)

[modal nfs list](#modal-nfs-list)

[modal nfs create](#modal-nfs-create)

[modal nfs ls](#modal-nfs-ls)

[modal nfs put](#modal-nfs-put)

[modal nfs get](#modal-nfs-get)

[modal nfs rm](#modal-nfs-rm)

[modal nfs delete](#modal-nfs-delete)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/profile
================================================================================

`modal profile`
===============

Switch between Modal profiles.

**Usage**
:

```
modal profile [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `activate`
  : Change the active Modal profile.
* `current`
  : Print the currently active Modal profile.
* `list`
  : Show all Modal profiles and highlight the active one.

`modal profile activate`
------------------------

Change the active Modal profile.

**Usage**
:

```
modal profile activate [OPTIONS] PROFILE
```

**Arguments**
:

* `PROFILE`
  : Modal profile to activate. [required]

**Options**
:

* `--help`
  : Show this message and exit.

`modal profile current`
-----------------------

Print the currently active Modal profile.

**Usage**
:

```
modal profile current [OPTIONS]
```

**Options**
:

* `--help`
  : Show this message and exit.

`modal profile list`
--------------------

Show all Modal profiles and highlight the active one.

**Usage**
:

```
modal profile list [OPTIONS]
```

**Options**
:

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

[modal profile](#modal-profile)

[modal profile activate](#modal-profile-activate)

[modal profile current](#modal-profile-current)

[modal profile list](#modal-profile-list)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/queue
================================================================================

`modal queue`
=============

Manage
`modal.Queue`
objects and inspect their contents.

**Usage**
:

```
modal queue [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `create`
  : Create a named Queue.
* `delete`
  : Delete a named Queue and all of its data.
* `list`
  : List all named Queues.
* `clear`
  : Clear the contents of a queue by removing all of its data.
* `peek`
  : Print the next N items in the queue or queue partition (without removal).
* `len`
  : Print the length of a queue partition or the total length of all partitions.

`modal queue create`
--------------------

Create a named Queue.

Note: This is a no-op when the Queue already exists.

**Usage**
:

```
modal queue create [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal queue delete`
--------------------

Delete a named Queue and all of its data.

**Usage**
:

```
modal queue delete [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal queue list`
------------------

List all named Queues.

**Usage**
:

```
modal queue list [OPTIONS]
```

**Options**
:

* `--json / --no-json`
  : [default: no-json]
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal queue clear`
-------------------

Clear the contents of a queue by removing all of its data.

**Usage**
:

```
modal queue clear [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-p, --partition TEXT`
  : Name of the partition to use, otherwise use the default (anonymous) partition.
* `-a, --all`
  : Clear the contents of all partitions.
* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal queue peek`
------------------

Print the next N items in the queue or queue partition (without removal).

**Usage**
:

```
modal queue peek [OPTIONS] NAME [N]
```

**Arguments**
:

* `NAME`
  : [required]
* `[N]`
  : [default: 1]

**Options**
:

* `-p, --partition TEXT`
  : Name of the partition to use, otherwise use the default (anonymous) partition.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal queue len`
-----------------

Print the length of a queue partition or the total length of all partitions.

**Usage**
:

```
modal queue len [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-p, --partition TEXT`
  : Name of the partition to use, otherwise use the default (anonymous) partition.
* `-t, --total`
  : Compute the sum of the queue lengths across all partitions
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

[modal queue](#modal-queue)

[modal queue create](#modal-queue-create)

[modal queue delete](#modal-queue-delete)

[modal queue list](#modal-queue-list)

[modal queue clear](#modal-queue-clear)

[modal queue peek](#modal-queue-peek)

[modal queue len](#modal-queue-len)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/run
================================================================================

`modal run`
===========

Run a Modal function or local entrypoint.

`FUNC_REF`
should be of the format
`{file or module}::{function name}`
.
Alternatively, you can refer to the function via the app:

`{file or module}::{app variable name}.{function name}`

**Examples:**

To run the hello\_world function (or local entrypoint) in my\_app.py:

```
modal run my_app.py::hello_world
```

If your module only has a single app and your app has a
single local entrypoint (or single function), you can omit the app and
function parts:

```
modal run my_app.py
```

Instead of pointing to a file, you can also use the Python module path, which
by default will ensure that your remote functions will use the same module
names as they do locally.

```
modal run -m my_project.my_app
```

**Usage**
:

```
modal run [OPTIONS] FUNC_REF
```

**Options**
:

* `-w, --write-result TEXT`
  : Write return value (which must be str or bytes) to this local path.
* `-q, --quiet`
  : Don’t show Modal progress indicators.
* `-d, --detach`
  : Don’t stop the app if the local process dies or disconnects.
* `-i, --interactive`
  : Run the app in interactive mode.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `-m`
  : Interpret argument as a Python module path instead of a file/script path
* `--help`
  : Show this message and exit.

[modal run](#modal-run)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/secret
================================================================================

`modal secret`
==============

Manage secrets.

**Usage**
:

```
modal secret [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `list`
  : List your published secrets.
* `create`
  : Create a new secret.
* `delete`
  : Delete a named secret.

`modal secret list`
-------------------

List your published secrets.

**Usage**
:

```
modal secret list [OPTIONS]
```

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

`modal secret create`
---------------------

Create a new secret.

**Usage**
:

```
modal secret create [OPTIONS] SECRET_NAME [KEYVALUES]...
```

**Arguments**
:

* `SECRET_NAME`
  : [required]
* `[KEYVALUES]...`
  : Space-separated KEY=VALUE items.

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--from-dotenv PATH`
  : Path to a .env file to load secrets from.
* `--from-json PATH`
  : Path to a JSON file to load secrets from.
* `--force`
  : Overwrite the secret if it already exists.
* `--help`
  : Show this message and exit.

`modal secret delete`
---------------------

Delete a named secret.

**Usage**
:

```
modal secret delete [OPTIONS] SECRET_NAME
```

**Arguments**
:

* `SECRET_NAME`
  : Name of the modal.Secret to be deleted. Case sensitive [required]

**Options**
:

* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

[modal secret](#modal-secret)

[modal secret list](#modal-secret-list)

[modal secret create](#modal-secret-create)

[modal secret delete](#modal-secret-delete)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/serve
================================================================================

`modal serve`
=============

Run a web endpoint(s) associated with a Modal app and hot-reload code.

**Examples:**

```
modal serve hello_world.py
```

**Usage**
:

```
modal serve [OPTIONS] APP_REF
```

**Arguments**
:

* `APP_REF`
  : Path to a Python file with an app. [required]

**Options**
:

* `--timeout FLOAT`
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `-m`
  : Interpret argument as a Python module path instead of a file/script path
* `--help`
  : Show this message and exit.

[modal serve](#modal-serve)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/setup
================================================================================

`modal setup`
=============

Bootstrap Modal’s configuration.

**Usage**
:

```
modal setup [OPTIONS]
```

**Options**
:

* `--profile TEXT`
* `--help`
  : Show this message and exit.

[modal setup](#modal-setup)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/shell
================================================================================

`modal shell`
=============

Run a command or interactive shell inside a Modal container.

**Examples:**

Start an interactive shell inside the default Debian-based image:

```
modal shell
```

Start an interactive shell with the spec for
`my_function`
in your App
(uses the same image, volumes, mounts, etc.):

```
modal shell hello_world.py::my_function
```

Or, if you’re using a
[modal.Cls](https://modal.com/docs/reference/modal.Cls)

you can refer to a
`@modal.method`
directly:

```
modal shell hello_world.py::MyClass.my_method
```

Start a
`python`
shell:

```
modal shell hello_world.py --cmd=python
```

Run a command with your function’s spec and pipe the output to a file:

```
modal shell hello_world.py -c 'uv pip list' > env.txt
```

**Usage**
:

```
modal shell [OPTIONS] REF
```

**Arguments**
:

* `REF`
  : ID of running container, or path to a Python file containing a Modal App. Can also include a function specifier, like
  `module.py::func`
  , if the file defines multiple functions.

**Options**
:

* `-c, --cmd TEXT`
  : Command to run inside the Modal image. [default: /bin/bash]
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--image TEXT`
  : Container image tag for inside the shell (if not using REF).
* `--add-python TEXT`
  : Add Python to the image (if not using REF).
* `--volume TEXT`
  : Name of a
  `modal.Volume`
  to mount inside the shell at
  `/mnt/{name}`
  (if not using REF). Can be used multiple times.
* `--secret TEXT`
  : Name of a
  `modal.Secret`
  to mount inside the shell (if not using REF). Can be used multiple times.
* `--cpu INTEGER`
  : Number of CPUs to allocate to the shell (if not using REF).
* `--memory INTEGER`
  : Memory to allocate for the shell, in MiB (if not using REF).
* `--gpu TEXT`
  : GPUs to request for the shell, if any. Examples are
  `any`
  ,
  `a10g`
  ,
  `a100:4`
  (if not using REF).
* `--cloud TEXT`
  : Cloud provider to run the shell on. Possible values are
  `aws`
  ,
  `gcp`
  ,
  `oci`
  ,
  `auto`
  (if not using REF).
* `--region TEXT`
  : Region(s) to run the container on. Can be a single region or a comma-separated list to choose from (if not using REF).
* `--pty / --no-pty`
  : Run the command using a PTY.
* `-m`
  : Interpret argument as a Python module path instead of a file/script path
* `--help`
  : Show this message and exit.

[modal shell](#modal-shell)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/token
================================================================================

`modal token`
=============

Manage tokens.

**Usage**
:

```
modal token [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `set`
  : Set account credentials for connecting to Modal.
* `new`
  : Create a new token by using an authenticated web session.

`modal token set`
-----------------

Set account credentials for connecting to Modal.

If the credentials are not provided on the command line, you will be prompted to enter them.

**Usage**
:

```
modal token set [OPTIONS]
```

**Options**
:

* `--token-id TEXT`
  : Account token ID.
* `--token-secret TEXT`
  : Account token secret.
* `--profile TEXT`
  : Modal profile to set credentials for. If unspecified (and MODAL\_PROFILE environment variable is not set), uses the workspace name associated with the credentials.
* `--activate / --no-activate`
  : Activate the profile containing this token after creation. [default: activate]
* `--verify / --no-verify`
  : Make a test request to verify the new credentials. [default: verify]
* `--help`
  : Show this message and exit.

`modal token new`
-----------------

Create a new token by using an authenticated web session.

**Usage**
:

```
modal token new [OPTIONS]
```

**Options**
:

* `--profile TEXT`
  : Modal profile to set credentials for. If unspecified (and MODAL\_PROFILE environment variable is not set), uses the workspace name associated with the credentials.
* `--activate / --no-activate`
  : Activate the profile containing this token after creation. [default: activate]
* `--verify / --no-verify`
  : Make a test request to verify the new credentials. [default: verify]
* `--source TEXT`
* `--help`
  : Show this message and exit.

[modal token](#modal-token)

[modal token set](#modal-token-set)

[modal token new](#modal-token-new)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/cli/volume
================================================================================

`modal volume`
==============

Read and edit
`modal.Volume`
volumes.

Note: users of
`modal.NetworkFileSystem`
should use the
`modal nfs`
command instead.

**Usage**
:

```
modal volume [OPTIONS] COMMAND [ARGS]...
```

**Options**
:

* `--help`
  : Show this message and exit.

**Commands**
:

* `create`
  : Create a named, persistent modal.Volume.
* `get`
  : Download files from a modal.Volume object.
* `list`
  : List the details of all modal.Volume volumes in an Environment.
* `ls`
  : List files and directories in a modal.Volume volume.
* `put`
  : Upload a file or directory to a modal.Volume.
* `rm`
  : Delete a file or directory from a modal.Volume.
* `cp`
  : Copy within a modal.Volume.
* `delete`
  : Delete a named, persistent modal.Volume.
* `rename`
  : Rename a modal.Volume.

`modal volume create`
---------------------

Create a named, persistent modal.Volume.

**Usage**
:

```
modal volume create [OPTIONS] NAME
```

**Arguments**
:

* `NAME`
  : [required]

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--version INTEGER`
  : VolumeFS version. (Experimental)
* `--help`
  : Show this message and exit.

`modal volume get`
------------------

Download files from a modal.Volume object.

If a folder is passed for REMOTE\_PATH, the contents of the folder will be downloaded
recursively, including all subdirectories.

**Example**

```
modal volume get <volume_name> logs/april-12-1.txt
modal volume get <volume_name> / volume_data_dump
```

Use ”-” as LOCAL\_DESTINATION to write file contents to standard output.

**Usage**
:

```
modal volume get [OPTIONS] VOLUME_NAME REMOTE_PATH [LOCAL_DESTINATION]
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `REMOTE_PATH`
  : [required]
* `[LOCAL_DESTINATION]`
  : [default: .]

**Options**
:

* `--force / --no-force`
  : [default: no-force]
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal volume list`
-------------------

List the details of all modal.Volume volumes in an Environment.

**Usage**
:

```
modal volume list [OPTIONS]
```

**Options**
:

* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--json / --no-json`
  : [default: no-json]
* `--help`
  : Show this message and exit.

`modal volume ls`
-----------------

List files and directories in a modal.Volume volume.

**Usage**
:

```
modal volume ls [OPTIONS] VOLUME_NAME [PATH]
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `[PATH]`
  : [default: /]

**Options**
:

* `--json / --no-json`
  : [default: no-json]
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal volume put`
------------------

Upload a file or directory to a modal.Volume.

Remote parent directories will be created as needed.

Ending the REMOTE\_PATH with a forward slash (/), it’s assumed to be a directory
and the file will be uploaded with its current name under that directory.

**Usage**
:

```
modal volume put [OPTIONS] VOLUME_NAME LOCAL_PATH [REMOTE_PATH]
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `LOCAL_PATH`
  : [required]
* `[REMOTE_PATH]`
  : [default: /]

**Options**
:

* `-f, --force`
  : Overwrite existing files.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal volume rm`
-----------------

Delete a file or directory from a modal.Volume.

**Usage**
:

```
modal volume rm [OPTIONS] VOLUME_NAME REMOTE_PATH
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `REMOTE_PATH`
  : [required]

**Options**
:

* `-r, --recursive`
  : Delete directory recursively
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal volume cp`
-----------------

Copy within a modal.Volume. Copy source file to destination file or multiple source files to destination directory.

**Usage**
:

```
modal volume cp [OPTIONS] VOLUME_NAME PATHS...
```

**Arguments**
:

* `VOLUME_NAME`
  : [required]
* `PATHS...`
  : [required]

**Options**
:

* `-r, --recursive`
  : Copy directories recursively
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal volume delete`
---------------------

Delete a named, persistent modal.Volume.

**Usage**
:

```
modal volume delete [OPTIONS] VOLUME_NAME
```

**Arguments**
:

* `VOLUME_NAME`
  : Name of the modal.Volume to be deleted. Case sensitive [required]

**Options**
:

* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

`modal volume rename`
---------------------

Rename a modal.Volume.

**Usage**
:

```
modal volume rename [OPTIONS] OLD_NAME NEW_NAME
```

**Arguments**
:

* `OLD_NAME`
  : [required]
* `NEW_NAME`
  : [required]

**Options**
:

* `-y, --yes`
  : Run without pausing for confirmation.
* `-e, --env TEXT`
  : Environment to interact with.

If not specified, Modal will use the default environment of your current profile, or the
`MODAL_ENVIRONMENT`
variable.
Otherwise, raises an error if the workspace has multiple environments.

* `--help`
  : Show this message and exit.

[modal volume](#modal-volume)

[modal volume create](#modal-volume-create)

[modal volume get](#modal-volume-get)

[modal volume list](#modal-volume-list)

[modal volume ls](#modal-volume-ls)

[modal volume put](#modal-volume-put)

[modal volume rm](#modal-volume-rm)

[modal volume cp](#modal-volume-cp)

[modal volume delete](#modal-volume-delete)

[modal volume rename](#modal-volume-rename)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.App
================================================================================

modal.App
=========

```
class App(object)
```

A Modal App is a group of functions and classes that are deployed together.

The app serves at least three purposes:

* A unit of deployment for functions and classes.
* Syncing of identities of (primarily) functions and classes across processes
  (your local Python interpreter and every Modal container active in your application).
* Manage log collection for everything that happens inside your code.

**Registering functions with an app**

The most common way to explicitly register an Object with an app is through the
`@app.function()`
decorator. It both registers the annotated function itself and
other passed objects, like schedules and secrets, with the app:

```
import modal

app = modal.App()

@app.function(
    secrets=[modal.Secret.from_name("some_secret")],
    schedule=modal.Period(days=1),
)
def foo():
    pass
```

In this example, the secret and schedule are registered with the app.

```
def __init__(
    self,
    name: Optional[str] = None,
    *,
    image: Optional[_Image] = None,  # Default Image for the App (otherwise default to `modal.Image.debian_slim()`)
    secrets: Sequence[_Secret] = [],  # Secrets to add for all Functions in the App
    volumes: dict[Union[str, PurePosixPath], _Volume] = {},  # Volume mounts to use for all Functions
    include_source: bool = True,  # Default configuration for adding Function source file(s) to the Modal container
) -> None:
```

Construct a new app, optionally with default image, mounts, secrets, or volumes.

```
image = modal.Image.debian_slim().pip_install(...)
secret = modal.Secret.from_name("my-secret")
volume = modal.Volume.from_name("my-data")
app = modal.App(image=image, secrets=[secret], volumes={"/mnt/data": volume})
```

name
----

```
@property
def name(self) -> Optional[str]:
```

The user-provided name of the App.

is\_interactive
---------------

```
@property
def is_interactive(self) -> bool:
```

Whether the current app for the app is running in interactive mode.

app\_id
-------

```
@property
def app_id(self) -> Optional[str]:
```

Return the app\_id of a running or stopped app.

description
-----------

```
@property
def description(self) -> Optional[str]:
```

The App’s
`name`
, if available, or a fallback descriptive identifier.

lookup
------

```
@staticmethod
def lookup(
    name: str,
    *,
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
    create_if_missing: bool = False,
) -> "_App":
```

Look up an App with a given name, creating a new App if necessary.

Note that Apps created through this method will be in a deployed state,
but they will not have any associated Functions or Classes. This method
is mainly useful for creating an App to associate with a Sandbox:

```
app = modal.App.lookup("my-app", create_if_missing=True)
modal.Sandbox.create("echo", "hi", app=app)
```

set\_description
----------------

```
def set_description(self, description: str):
```

image
-----

```
@property
def image(self) -> _Image:
```

run
---

```
@contextmanager
def run(
    self,
    *,
    client: Optional[_Client] = None,
    detach: bool = False,
    interactive: bool = False,
    environment_name: Optional[str] = None,
) -> AsyncGenerator["_App", None]:
```

Context manager that runs an ephemeral app on Modal.

Use this as the main entry point for your Modal application. All calls
to Modal Functions should be made within the scope of this context
manager, and they will correspond to the current App.

**Example**

```
with app.run():
    some_modal_function.remote()
```

To enable output printing (i.e., to see App logs), use
`modal.enable_output()`
:

```
with modal.enable_output():
    with app.run():
        some_modal_function.remote()
```

Note that you should not invoke this in global scope of a file where you have
Modal Functions or Classes defined, since that would run the block when the Function
or Cls is imported in your containers as well. If you want to run it as your entrypoint,
consider protecting it:

```
if __name__ == "__main__":
    with app.run():
        some_modal_function.remote()
```

You can then run your script with:

```
python app_module.py
```

deploy
------

```
def deploy(
    self,
    *,
    name: Optional[str] = None,  # Name for the deployment, overriding any set on the App
    environment_name: Optional[str] = None,  # Environment to deploy the App in
    tag: str = "",  # Optional metadata that will be visible in the deployment history
    client: Optional[_Client] = None,  # Alternate client to use for RPCs
) -> typing_extensions.Self:
```

Deploy the App so that it is available persistently.

Deployed Apps will be avaible for lookup or web-based invocations until they are stopped.
Unlike with
`App.run`
, this method will return as soon as the deployment completes.

This method is a programmatic alternative to the
`modal deploy`
CLI command.

Examples:

```
app = App("my-app")
app.deploy()
```

To enable output printing (i.e., to see build logs), use
`modal.enable_output()`
:

```
app = App("my-app")
with modal.enable_output():
    app.deploy()
```

Unlike with
`App.run`
, Function logs will not stream back to the local client after the
App is deployed.

Note that you should not invoke this method in global scope, as that would redeploy
the App every time the file is imported. If you want to write a programmatic deployment
script, protect this call so that it only runs when the file is executed directly:

```
if __name__ == "__main__":
    with modal.enable_output():
        app.deploy()
```

Then you can deploy your app with:

```
python app_module.py
```

registered\_functions
---------------------

```
@property
def registered_functions(self) -> dict[str, _Function]:
```

All modal.Function objects registered on the app.

Note: this property is populated only during the build phase, and it is not
expected to work when a deplyoed App has been retrieved via
`modal.App.lookup`
.

registered\_classes
-------------------

```
@property
def registered_classes(self) -> dict[str, _Cls]:
```

All modal.Cls objects registered on the app.

Note: this property is populated only during the build phase, and it is not
expected to work when a deplyoed App has been retrieved via
`modal.App.lookup`
.

registered\_entrypoints
-----------------------

```
@property
def registered_entrypoints(self) -> dict[str, _LocalEntrypoint]:
```

All local CLI entrypoints registered on the app.

Note: this property is populated only during the build phase, and it is not
expected to work when a deplyoed App has been retrieved via
`modal.App.lookup`
.

registered\_web\_endpoints
--------------------------

```
@property
def registered_web_endpoints(self) -> list[str]:
```

Names of web endpoint (ie. webhook) functions registered on the app.

Note: this property is populated only during the build phase, and it is not
expected to work when a deplyoed App has been retrieved via
`modal.App.lookup`
.

local\_entrypoint
-----------------

```
def local_entrypoint(
    self, _warn_parentheses_missing: Any = None, *, name: Optional[str] = None
) -> Callable[[Callable[..., Any]], _LocalEntrypoint]:
```

Decorate a function to be used as a CLI entrypoint for a Modal App.

These functions can be used to define code that runs locally to set up the app,
and act as an entrypoint to start Modal functions from. Note that regular
Modal functions can also be used as CLI entrypoints, but unlike
`local_entrypoint`
,
those functions are executed remotely directly.

**Example**

```
@app.local_entrypoint()
def main():
    some_modal_function.remote()
```

You can call the function using
`modal run`
directly from the CLI:

```
modal run app_module.py
```

Note that an explicit
[`app.run()`](https://modal.com/docs/reference/modal.App#run)

is not needed, as an
[app](https://modal.com/docs/guide/apps)

is automatically created for you.

**Multiple Entrypoints**

If you have multiple
`local_entrypoint`
functions, you can qualify the name of your app and function:

```
modal run app_module.py::app.some_other_function
```

**Parsing Arguments**

If your entrypoint function take arguments with primitive types,
`modal run`
automatically parses them as
CLI options.
For example, the following function can be called with
`modal run app_module.py --foo 1 --bar "hello"`
:

```
@app.local_entrypoint()
def main(foo: int, bar: str):
    some_modal_function.call(foo, bar)
```

Currently,
`str`
,
`int`
,
`float`
,
`bool`
, and
`datetime.datetime`
are supported.
Use
`modal run app_module.py --help`
for more information on usage.

function
--------

```
@warn_on_renamed_autoscaler_settings
def function(
    self,
    _warn_parentheses_missing: Any = None,
    *,
    image: Optional[_Image] = None,  # The image to run as the container for the function
    schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function
    secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
    gpu: Union[
        GPU_T, list[GPU_T]
    ] = None,  # GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
    serialized: bool = False,  # Whether to send the function over using cloudpickle.
    network_file_systems: dict[
        Union[str, PurePosixPath], _NetworkFileSystem
    ] = {},  # Mountpoints for Modal NetworkFileSystems
    volumes: dict[
        Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
    ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
    # Specify, in fractional CPU cores, how many CPU cores to request.
    # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
    # CPU throttling will prevent a container from exceeding its specified limit.
    cpu: Optional[Union[float, tuple[float, float]]] = None,
    # Specify, in MiB, a memory request which is the minimum memory required.
    # Or, pass (request, limit) to additionally specify a hard limit in MiB.
    memory: Optional[Union[int, tuple[int, int]]] = None,
    ephemeral_disk: Optional[int] = None,  # Specify, in MiB, the ephemeral disk size for the Function.
    min_containers: Optional[int] = None,  # Minimum number of containers to keep warm, even when Function is idle.
    max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
    buffer_containers: Optional[int] = None,  # Number of additional idle containers to maintain under active load.
    scaledown_window: Optional[int] = None,  # Max time (in seconds) a container can remain idle while scaling down.
    proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
    retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
    timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
    name: Optional[str] = None,  # Sets the Modal name of the function within the app
    is_generator: Optional[
        bool
    ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object
    cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
    region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
    enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
    block_network: bool = False,  # Whether to block network access
    restrict_modal_access: bool = False,  # Whether to allow this function access to other Modal resources
    # Maximum number of inputs a container should handle before shutting down.
    # With `max_inputs = 1`, containers will be single-use.
    max_inputs: Optional[int] = None,
    i6pn: Optional[bool] = None,  # Whether to enable IPv6 container networking within the region.
    # Whether the file or directory containing the Function's source should automatically be included
    # in the container. When unset, falls back to the App-level configuration, or is otherwise True by default.
    include_source: Optional[bool] = None,
    experimental_options: Optional[dict[str, Any]] = None,
    # Parameters below here are experimental. Use with caution!
    _experimental_scheduler_placement: Optional[
        SchedulerPlacement
    ] = None,  # Experimental controls over fine-grained scheduling (alpha).
    _experimental_proxy_ip: Optional[str] = None,  # IP address of proxy
    _experimental_custom_scaling_factor: Optional[float] = None,  # Custom scaling factor
    _experimental_enable_gpu_snapshot: bool = False,  # Experimentally enable GPU memory snapshots.
    # Parameters below here are deprecated. Please update your code as suggested
    keep_warm: Optional[int] = None,  # Replaced with `min_containers`
    concurrency_limit: Optional[int] = None,  # Replaced with `max_containers`
    container_idle_timeout: Optional[int] = None,  # Replaced with `scaledown_window`
    allow_concurrent_inputs: Optional[int] = None,  # Replaced with the `@modal.concurrent` decorator
    allow_cross_region_volumes: Optional[bool] = None,  # Always True on the Modal backend now
    _experimental_buffer_containers: Optional[int] = None,  # Now stable API with `buffer_containers`
) -> _FunctionDecoratorType:
```

Decorator to register a new Modal Function with this App.

cls
---

```
@typing_extensions.dataclass_transform(field_specifiers=(parameter,), kw_only_default=True)
@warn_on_renamed_autoscaler_settings
def cls(
    self,
    _warn_parentheses_missing: Optional[bool] = None,
    *,
    image: Optional[_Image] = None,  # The image to run as the container for the function
    secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
    gpu: Union[
        GPU_T, list[GPU_T]
    ] = None,  # GPU request as string ("any", "T4", ...), object (`modal.GPU.A100()`, ...), or a list of either
    serialized: bool = False,  # Whether to send the function over using cloudpickle.
    network_file_systems: dict[
        Union[str, PurePosixPath], _NetworkFileSystem
    ] = {},  # Mountpoints for Modal NetworkFileSystems
    volumes: dict[
        Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]
    ] = {},  # Mount points for Modal Volumes & CloudBucketMounts
    # Specify, in fractional CPU cores, how many CPU cores to request.
    # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
    # CPU throttling will prevent a container from exceeding its specified limit.
    cpu: Optional[Union[float, tuple[float, float]]] = None,
    # Specify, in MiB, a memory request which is the minimum memory required.
    # Or, pass (request, limit) to additionally specify a hard limit in MiB.
    memory: Optional[Union[int, tuple[int, int]]] = None,
    ephemeral_disk: Optional[int] = None,  # Specify, in MiB, the ephemeral disk size for the Function.
    min_containers: Optional[int] = None,  # Minimum number of containers to keep warm, even when Function is idle.
    max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
    buffer_containers: Optional[int] = None,  # Number of additional idle containers to maintain under active load.
    scaledown_window: Optional[int] = None,  # Max time (in seconds) a container can remain idle while scaling down.
    proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.
    retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.
    timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.
    cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
    region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
    enable_memory_snapshot: bool = False,  # Enable memory checkpointing for faster cold starts.
    block_network: bool = False,  # Whether to block network access
    restrict_modal_access: bool = False,  # Whether to allow this class access to other Modal resources
    # Limits the number of inputs a container handles before shutting down.
    # Use `max_inputs = 1` for single-use containers.
    max_inputs: Optional[int] = None,
    i6pn: Optional[bool] = None,  # Whether to enable IPv6 container networking within the region.
    include_source: Optional[bool] = None,  # When `False`, don't automatically add the App source to the container.
    experimental_options: Optional[dict[str, Any]] = None,
    # Parameters below here are experimental. Use with caution!
    _experimental_scheduler_placement: Optional[
        SchedulerPlacement
    ] = None,  # Experimental controls over fine-grained scheduling (alpha).
    _experimental_proxy_ip: Optional[str] = None,  # IP address of proxy
    _experimental_custom_scaling_factor: Optional[float] = None,  # Custom scaling factor
    _experimental_enable_gpu_snapshot: bool = False,  # Experimentally enable GPU memory snapshots.
    # Parameters below here are deprecated. Please update your code as suggested
    keep_warm: Optional[int] = None,  # Replaced with `min_containers`
    concurrency_limit: Optional[int] = None,  # Replaced with `max_containers`
    container_idle_timeout: Optional[int] = None,  # Replaced with `scaledown_window`
    allow_concurrent_inputs: Optional[int] = None,  # Replaced with the `@modal.concurrent` decorator
    _experimental_buffer_containers: Optional[int] = None,  # Now stable API with `buffer_containers`
    allow_cross_region_volumes: Optional[bool] = None,  # Always True on the Modal backend now
) -> Callable[[Union[CLS_T, _PartialFunction]], CLS_T]:
```

Decorator to register a new Modal
[Cls](https://modal.com/docs/reference/modal.Cls)

with this App.

include
-------

```
def include(self, /, other_app: "_App") -> typing_extensions.Self:
```

Include another App’s objects in this one.

Useful for splitting up Modal Apps across different self-contained files.

```
app_a = modal.App("a")
@app.function()
def foo():
    ...

app_b = modal.App("b")
@app.function()
def bar():
    ...

app_a.include(app_b)

@app_a.local_entrypoint()
def main():
    # use function declared on the included app
    bar.remote()
```

[modal.App](#modalapp)

[name](#name)

[is\_interactive](#is_interactive)

[app\_id](#app_id)

[description](#description)

[lookup](#lookup)

[set\_description](#set_description)

[image](#image)

[run](#run)

[deploy](#deploy)

[registered\_functions](#registered_functions)

[registered\_classes](#registered_classes)

[registered\_entrypoints](#registered_entrypoints)

[registered\_web\_endpoints](#registered_web_endpoints)

[local\_entrypoint](#local_entrypoint)

[function](#function)

[cls](#cls)

[include](#include)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Client
================================================================================

modal.Client
============

```
class Client(object)
```

is\_closed
----------

```
def is_closed(self) -> bool:
```

hello
-----

```
def hello(self):
```

Connect to server and retrieve version information; raise appropriate error for various failures.

from\_credentials
-----------------

```
@classmethod
def from_credentials(cls, token_id: str, token_secret: str) -> "_Client":
```

Constructor based on token credentials; useful for managing Modal on behalf of third-party users.

**Usage:**

```
client = modal.Client.from_credentials("my_token_id", "my_token_secret")

modal.Sandbox.create("echo", "hi", client=client, app=app)
```

[modal.Client](#modalclient)

[is\_closed](#is_closed)

[hello](#hello)

[from\_credentials](#from_credentials)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.CloudBucketMount
================================================================================

modal.CloudBucketMount
======================

```
class CloudBucketMount(object)
```

Mounts a cloud bucket to your container. Currently supports AWS S3 buckets.

S3 buckets are mounted using
[AWS S3 Mountpoint](https://github.com/awslabs/mountpoint-s3)

.
S3 mounts are optimized for reading large files sequentially. It does not support every file operation; consult
[the AWS S3 Mountpoint documentation](https://github.com/awslabs/mountpoint-s3/blob/main/doc/SEMANTICS.md)

for more information.

**AWS S3 Usage**

```
import subprocess

app = modal.App()
secret = modal.Secret.from_name(
    "aws-secret",
    required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
    # Note: providing AWS_REGION can help when automatic detection of the bucket region fails.
)

@app.function(
    volumes={
        "/my-mount": modal.CloudBucketMount(
            bucket_name="s3-bucket-name",
            secret=secret,
            read_only=True
        )
    }
)
def f():
    subprocess.run(["ls", "/my-mount"], check=True)
```

**Cloudflare R2 Usage**

Cloudflare R2 is
[S3-compatible](https://developers.cloudflare.com/r2/api/s3/api/)

so its setup looks
very similar to S3. But additionally the
`bucket_endpoint_url`
argument must be passed.

```
import subprocess

app = modal.App()
secret = modal.Secret.from_name(
    "r2-secret",
    required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY"]
)

@app.function(
    volumes={
        "/my-mount": modal.CloudBucketMount(
            bucket_name="my-r2-bucket",
            bucket_endpoint_url="https://<ACCOUNT ID>.r2.cloudflarestorage.com",
            secret=secret,
            read_only=True
        )
    }
)
def f():
    subprocess.run(["ls", "/my-mount"], check=True)
```

**Google GCS Usage**

Google Cloud Storage (GCS) is
[S3-compatible](https://cloud.google.com/storage/docs/interoperability)

.
GCS Buckets also require a secret with Google-specific key names (see below) populated with
a
[HMAC key](https://cloud.google.com/storage/docs/authentication/managing-hmackeys#create)

.

```
import subprocess

app = modal.App()
gcp_hmac_secret = modal.Secret.from_name(
    "gcp-secret",
    required_keys=["GOOGLE_ACCESS_KEY_ID", "GOOGLE_ACCESS_KEY_SECRET"]
)

@app.function(
    volumes={
        "/my-mount": modal.CloudBucketMount(
            bucket_name="my-gcs-bucket",
            bucket_endpoint_url="https://storage.googleapis.com",
            secret=gcp_hmac_secret,
        )
    }
)
def f():
    subprocess.run(["ls", "/my-mount"], check=True)
```

```
def __init__(self, bucket_name: str, bucket_endpoint_url: Optional[str] = None, key_prefix: Optional[str] = None, secret: Optional[modal.secret._Secret] = None, oidc_auth_role_arn: Optional[str] = None, read_only: bool = False, requester_pays: bool = False) -> None
```

[modal.CloudBucketMount](#modalcloudbucketmount)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Cls
================================================================================

modal.Cls
=========

```
class Cls(modal.object.Object)
```

Cls adds method pooling and
[lifecycle hook](https://modal.com/docs/guide/lifecycle-functions)

behavior
to
[modal.Function](https://modal.com/docs/reference/modal.Function)

.

Generally, you will not construct a Cls directly.
Instead, use the
[`@app.cls()`](https://modal.com/docs/reference/modal.App#cls)

decorator on the App object.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

from\_name
----------

```
@classmethod
def from_name(
    cls: type["_Cls"],
    app_name: str,
    name: str,
    *,
    environment_name: Optional[str] = None,
) -> "_Cls":
```

Reference a Cls from a deployed App by its name.

This is a lazy method that defers hydrating the local
object with metadata from Modal servers until the first
time it is actually used.

```
Model = modal.Cls.from_name("other-app", "Model")
```

with\_options
-------------

```
@warn_on_renamed_autoscaler_settings
def with_options(
    self: "_Cls",
    *,
    cpu: Optional[Union[float, tuple[float, float]]] = None,
    memory: Optional[Union[int, tuple[int, int]]] = None,
    gpu: GPU_T = None,
    secrets: Collection[_Secret] = (),
    volumes: dict[Union[str, os.PathLike], _Volume] = {},
    retries: Optional[Union[int, Retries]] = None,
    max_containers: Optional[int] = None,  # Limit on the number of containers that can be concurrently running.
    buffer_containers: Optional[int] = None,  # Additional containers to scale up while Function is active.
    scaledown_window: Optional[int] = None,  # Max amount of time a container can remain idle before scaling down.
    timeout: Optional[int] = None,
    # The following parameters are deprecated
    concurrency_limit: Optional[int] = None,  # Now called `max_containers`
    container_idle_timeout: Optional[int] = None,  # Now called `scaledown_window`
    allow_concurrent_inputs: Optional[int] = None,  # See `.with_concurrency`
) -> "_Cls":
```

Override the static Function configuration at runtime.

This method will return a new instance of the cls that will autoscale independently of the
original instance. Note that options cannot be “unset” with this method (i.e., if a GPU
is configured in the
`@app.cls()`
decorator, passing
`gpu=None`
here will not create a
CPU-only instance).

**Usage:**

You can use this method after looking up the Cls from a deployed App or if you have a
direct reference to a Cls from another Function or local entrypoint on its App:

```
Model = modal.Cls.from_name("my_app", "Model")
ModelUsingGPU = Model.with_options(gpu="A100")
ModelUsingGPU().generate.remote(input_prompt)  # Run with an A100 GPU
```

The method can be called multiple times to “stack” updates:

```
Model.with_options(gpu="A100").with_options(scaledown_window=300)  # Use an A100 with slow scaledown
```

Note that container arguments (i.e.
`volumes`
and
`secrets`
) passed in subsequent calls
will not be merged.

with\_concurrency
-----------------

```
def with_concurrency(self: "_Cls", *, max_inputs: int, target_inputs: Optional[int] = None) -> "_Cls":
```

Create an instance of the Cls with input concurrency enabled or overridden with new values.

**Usage:**

```
Model = modal.Cls.from_name("my_app", "Model")
ModelUsingGPU = Model.with_options(gpu="A100").with_concurrency(max_inputs=100)
ModelUsingGPU().generate.remote(42)  # will run on an A100 GPU with input concurrency enabled
```

with\_batching
--------------

```
def with_batching(self: "_Cls", *, max_batch_size: int, wait_ms: int) -> "_Cls":
```

Create an instance of the Cls with dynamic batching enabled or overridden with new values.

**Usage:**

```
Model = modal.Cls.from_name("my_app", "Model")
ModelUsingGPU = Model.with_options(gpu="A100").with_batching(max_batch_size=100, batch_wait_ms=1000)
ModelUsingGPU().generate.remote(42)  # will run on an A100 GPU with input concurrency enabled
```

[modal.Cls](#modalcls)

[hydrate](#hydrate)

[from\_name](#from_name)

[with\_options](#with_options)

[with\_concurrency](#with_concurrency)

[with\_batching](#with_batching)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Cron
================================================================================

modal.Cron
==========

```
class Cron(modal.schedule.Schedule)
```

Cron jobs are a type of schedule, specified using the
[Unix cron tab](https://crontab.guru/)

syntax.

The alternative schedule type is the
[`modal.Period`](https://modal.com/docs/reference/modal.Period)

.

**Usage**

```
import modal
app = modal.App()

@app.function(schedule=modal.Cron("* * * * *"))
def f():
    print("This function will run every minute")
```

We can specify different schedules with cron strings, for example:

```
modal.Cron("5 4 * * *")  # run at 4:05am UTC every night
modal.Cron("0 9 * * 4")  # runs every Thursday at 9am UTC
```

We can also optionally specify a timezone, for example:

```
# Run daily at 6am New York time, regardless of whether daylight saving
# is in effect (i.e. at 11am UTC in the winter, and 10am UTC in the summer):
modal.Cron("0 6 * * *", timezone="America/New_York")
```

If no timezone is specified, the default is UTC.

```
def __init__(
    self,
    cron_string: str,
    timezone: str = "UTC",
) -> None:
```

Construct a schedule that runs according to a cron expression string.

[modal.Cron](#modalcron)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Dict
================================================================================

modal.Dict
==========

```
class Dict(modal.object.Object)
```

Distributed dictionary for storage in Modal apps.

Dict contents can be essentially any object so long as they can be serialized by
`cloudpickle`
. This includes other Modal objects. If writing and reading in different
environments (eg., writing locally and reading remotely), it’s necessary to have the
library defining the data type installed, with compatible versions, on both sides.
Additionally, cloudpickle serialization is not guaranteed to be deterministic, so it is
generally recommended to use primitive types for keys.

**Lifetime of a Dict and its items**

An individual Dict entry will expire after 7 days of inactivity (no reads or writes). The
Dict entries are written to durable storage.

Legacy Dicts (created before 2025-05-20) will still have entries expire 30 days after being
last added. Additionally, contents are stored in memory on the Modal server and could be lost
due to unexpected server restarts. Eventually, these Dicts will be fully sunset.

**Usage**

```
from modal import Dict

my_dict = Dict.from_name("my-persisted_dict", create_if_missing=True)

my_dict["some key"] = "some value"
my_dict[123] = 456

assert my_dict["some key"] == "some value"
assert my_dict[123] == 456
```

The
`Dict`
class offers a few methods for operations that are usually accomplished
in Python with operators, such as
`Dict.put`
and
`Dict.contains`
. The advantage of
these methods is that they can be safely called in an asynchronous context by using
the
`.aio`
suffix on the method, whereas their operator-based analogues will always
run synchronously and block the event loop.

For more examples, see the
[guide](https://modal.com/docs/guide/dicts-and-queues#modal-dicts)

.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

ephemeral
---------

```
@classmethod
@contextmanager
def ephemeral(
    cls: type["_Dict"],
    data: Optional[dict] = None,  # DEPRECATED
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
    _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
) -> Iterator["_Dict"]:
```

Creates a new ephemeral Dict within a context manager:

Usage:

```
from modal import Dict

with Dict.ephemeral() as d:
    d["foo"] = "bar"
```

```
async with Dict.ephemeral() as d:
    await d.put.aio("foo", "bar")
```

from\_name
----------

```
@staticmethod
def from_name(
    name: str,
    *,
    environment_name: Optional[str] = None,
    create_if_missing: bool = False,
) -> "_Dict":
```

Reference a named Dict, creating if necessary.

This is a lazy method that defers hydrating the local
object with metadata from Modal servers until the first
time it is actually used.

```
d = modal.Dict.from_name("my-dict", create_if_missing=True)
d[123] = 456
```

delete
------

```
@staticmethod
def delete(
    name: str,
    *,
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
):
```

clear
-----

```
@live_method
def clear(self) -> None:
```

Remove all items from the Dict.

get
---

```
@live_method
def get(self, key: Any, default: Optional[Any] = None) -> Any:
```

Get the value associated with a key.

Returns
`default`
if key does not exist.

contains
--------

```
@live_method
def contains(self, key: Any) -> bool:
```

Return if a key is present.

len
---

```
@live_method
def len(self) -> int:
```

Return the length of the Dict.

Note: This is an expensive operation and will return at most 100,000.

update
------

```
@live_method
def update(self, other: Optional[Mapping] = None, /, **kwargs) -> None:
```

Update the Dict with additional items.

put
---

```
@live_method
def put(self, key: Any, value: Any, *, skip_if_exists: bool = False) -> bool:
```

Add a specific key-value pair to the Dict.

Returns True if the key-value pair was added and False if it wasn’t because the key already existed and
`skip_if_exists`
was set.

pop
---

```
@live_method
def pop(self, key: Any) -> Any:
```

Remove a key from the Dict, returning the value if it exists.

keys
----

```
@live_method_gen
def keys(self) -> Iterator[Any]:
```

Return an iterator over the keys in this Dict.

Note that (unlike with Python dicts) the return value is a simple iterator,
and results are unordered.

values
------

```
@live_method_gen
def values(self) -> Iterator[Any]:
```

Return an iterator over the values in this Dict.

Note that (unlike with Python dicts) the return value is a simple iterator,
and results are unordered.

items
-----

```
@live_method_gen
def items(self) -> Iterator[tuple[Any, Any]]:
```

Return an iterator over the (key, value) tuples in this Dict.

Note that (unlike with Python dicts) the return value is a simple iterator,
and results are unordered.

[modal.Dict](#modaldict)

[hydrate](#hydrate)

[ephemeral](#ephemeral)

[from\_name](#from_name)

[delete](#delete)

[clear](#clear)

[get](#get)

[contains](#contains)

[len](#len)

[update](#update)

[put](#put)

[pop](#pop)

[keys](#keys)

[values](#values)

[items](#items)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Error
================================================================================

modal.Error
===========

```
class Error(Exception)
```

Base class for all Modal errors. See
[`modal.exception`](https://modal.com/docs/reference/modal.exception)

for the specialized error classes.

**Usage**

```
import modal

try:
    ...
except modal.Error:
    # Catch any exception raised by Modal's systems.
    print("Responding to error...")
```

[modal.Error](#modalerror)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.FilePatternMatcher
================================================================================

modal.FilePatternMatcher
========================

```
class FilePatternMatcher(modal.file_pattern_matcher._AbstractPatternMatcher)
```

Allows matching file Path objects against a list of patterns.

**Usage:**

```
from pathlib import Path
from modal import FilePatternMatcher

matcher = FilePatternMatcher("*.py")

assert matcher(Path("foo.py"))

# You can also negate the matcher.
negated_matcher = ~matcher

assert not negated_matcher(Path("foo.py"))
```

```
def __init__(self, *pattern: str) -> None:
```

Initialize a new FilePatternMatcher instance.

Args:
pattern (str): One or more pattern strings.

Raises:
ValueError: If an illegal exclusion pattern is provided.

can\_prune\_directories
-----------------------

```
def can_prune_directories(self) -> bool:
```

Returns True if this pattern matcher allows safe early directory pruning.

Directory pruning is safe when matching directories can be skipped entirely
without missing any files that should be included. This is for example not
safe when we have inverted/negated ignore patterns (e.g. ”!\*
*/*
.py”).

from\_file
----------

```
@classmethod
def from_file(cls, file_path: Union[str, Path]) -> "FilePatternMatcher":
```

Initialize a new FilePatternMatcher instance from a file.

The patterns in the file will be read lazily when the matcher is first used.

Args:
file\_path (Path): The path to the file containing patterns.

**Usage:**

```
from modal import FilePatternMatcher

matcher = FilePatternMatcher.from_file("/path/to/ignorefile")
```

[modal.FilePatternMatcher](#modalfilepatternmatcher)

[can\_prune\_directories](#can_prune_directories)

[from\_file](#from_file)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Function
================================================================================

modal.Function
==============

```
class Function(typing.Generic, modal.object.Object)
```

Functions are the basic units of serverless execution on Modal.

Generally, you will not construct a
`Function`
directly. Instead, use the
`App.function()`
decorator to register your Python functions with your App.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

update\_autoscaler
------------------

```
@live_method
def update_autoscaler(
    self,
    *,
    min_containers: Optional[int] = None,
    max_containers: Optional[int] = None,
    buffer_containers: Optional[int] = None,
    scaledown_window: Optional[int] = None,
) -> None:
```

Override the current autoscaler behavior for this Function.

Unspecified parameters will retain their current value, i.e. either the static value
from the function decorator, or an override value from a previous call to this method.

Subsequent deployments of the App containing this Function will reset the autoscaler back to
its static configuration.

Examples:

```
f = modal.Function.from_name("my-app", "function")

# Always have at least 2 containers running, with an extra buffer when the Function is active
f.update_autoscaler(min_containers=2, buffer_containers=1)

# Limit this Function to avoid spinning up more than 5 containers
f.update_autoscaler(max_containers=5)

# Extend the scaledown window to increase the amount of time that idle containers stay alive
f.update_autoscaler(scaledown_window=300)
```

from\_name
----------

```
@classmethod
def from_name(
    cls: type["_Function"],
    app_name: str,
    name: str,
    *,
    environment_name: Optional[str] = None,
) -> "_Function":
```

Reference a Function from a deployed App by its name.

This is a lazy method that defers hydrating the local
object with metadata from Modal servers until the first
time it is actually used.

```
f = modal.Function.from_name("other-app", "function")
```

get\_web\_url
-------------

```
@live_method
def get_web_url(self) -> Optional[str]:
```

URL of a Function running as a web endpoint.

remote
------

```
@live_method
def remote(self, *args: P.args, **kwargs: P.kwargs) -> ReturnType:
```

Calls the function remotely, executing it with the given arguments and returning the execution’s result.

remote\_gen
-----------

```
@live_method_gen
def remote_gen(self, *args, **kwargs) -> AsyncGenerator[Any, None]:
```

Calls the generator remotely, executing it with the given arguments and returning the execution’s result.

local
-----

```
def local(self, *args: P.args, **kwargs: P.kwargs) -> OriginalReturnType:
```

Calls the function locally, executing it with the given arguments and returning the execution’s result.

The function will execute in the same environment as the caller, just like calling the underlying function
directly in Python. In particular, only secrets available in the caller environment will be available
through environment variables.

spawn
-----

```
@live_method
def spawn(self, *args: P.args, **kwargs: P.kwargs) -> "_FunctionCall[ReturnType]":
```

Calls the function with the given arguments, without waiting for the results.

Returns a
[`modal.FunctionCall`](https://modal.com/docs/reference/modal.FunctionCall)

object
that can later be polled or waited for using
[`.get(timeout=...)`](https://modal.com/docs/reference/modal.FunctionCall#get)

.
Conceptually similar to
`multiprocessing.pool.apply_async`
, or a Future/Promise in other contexts.

get\_raw\_f
-----------

```
def get_raw_f(self) -> Callable[..., Any]:
```

Return the inner Python object wrapped by this Modal Function.

get\_current\_stats
-------------------

```
@live_method
def get_current_stats(self) -> FunctionStats:
```

Return a
`FunctionStats`
object describing the current function’s queue and runner counts.

map
---

```
@warn_if_generator_is_not_consumed(function_name="Function.map")
def map(
    self,
    *input_iterators: typing.Iterable[Any],  # one input iterator per argument in the mapped-over function/generator
    kwargs={},  # any extra keyword arguments for the function
    order_outputs: bool = True,  # return outputs in order
    return_exceptions: bool = False,  # propagate exceptions (False) or aggregate them in the results list (True)
    wrap_returned_exceptions: bool = True,
) -> AsyncOrSyncIterable:
```

Parallel map over a set of inputs.

Takes one iterator argument per argument in the function being mapped over.

Example:

```
@app.function()
def my_func(a):
    return a ** 2

@app.local_entrypoint()
def main():
    assert list(my_func.map([1, 2, 3, 4])) == [1, 4, 9, 16]
```

If applied to a
`app.function`
,
`map()`
returns one result per input and the output order
is guaranteed to be the same as the input order. Set
`order_outputs=False`
to return results
in the order that they are completed instead.

`return_exceptions`
can be used to treat exceptions as successful results:

```
@app.function()
def my_func(a):
    if a == 2:
        raise Exception("ohno")
    return a ** 2

@app.local_entrypoint()
def main():
    # [0, 1, UserCodeException(Exception('ohno'))]
    print(list(my_func.map(range(3), return_exceptions=True)))
```

starmap
-------

```
@warn_if_generator_is_not_consumed(function_name="Function.starmap")
def starmap(
    self,
    input_iterator: typing.Iterable[typing.Sequence[Any]],
    *,
    kwargs={},
    order_outputs: bool = True,
    return_exceptions: bool = False,
    wrap_returned_exceptions: bool = True,
) -> AsyncOrSyncIterable:
```

Like
`map`
, but spreads arguments over multiple function arguments.

Assumes every input is a sequence (e.g. a tuple).

Example:

```
@app.function()
def my_func(a, b):
    return a + b

@app.local_entrypoint()
def main():
    assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7]
```

for\_each
---------

```
def for_each(self, *input_iterators, kwargs={}, ignore_exceptions: bool = False):
```

Execute function for all inputs, ignoring outputs. Waits for completion of the inputs.

Convenient alias for
`.map()`
in cases where the function just needs to be called.
as the caller doesn’t have to consume the generator to process the inputs.

spawn\_map
----------

```
def spawn_map(self, *input_iterators, kwargs={}) -> None:
```

Spawn parallel execution over a set of inputs, exiting as soon as the inputs are created (without waiting
for the map to complete).

Takes one iterator argument per argument in the function being mapped over.

Example:

```
@app.function()
def my_func(a):
    return a ** 2

@app.local_entrypoint()
def main():
    my_func.spawn_map([1, 2, 3, 4])
```

Programmatic retrieval of results will be supported in a future update.

[modal.Function](#modalfunction)

[hydrate](#hydrate)

[update\_autoscaler](#update_autoscaler)

[from\_name](#from_name)

[get\_web\_url](#get_web_url)

[remote](#remote)

[remote\_gen](#remote_gen)

[local](#local)

[spawn](#spawn)

[get\_raw\_f](#get_raw_f)

[get\_current\_stats](#get_current_stats)

[map](#map)

[starmap](#starmap)

[for\_each](#for_each)

[spawn\_map](#spawn_map)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.FunctionCall
================================================================================

modal.FunctionCall
==================

```
class FunctionCall(typing.Generic, modal.object.Object)
```

A reference to an executed function call.

Constructed using
`.spawn(...)`
on a Modal function with the same
arguments that a function normally takes. Acts as a reference to
an ongoing function call that can be passed around and used to
poll or fetch function results at some later time.

Conceptually similar to a Future/Promise/AsyncResult in other contexts and languages.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

get
---

```
def get(self, timeout: Optional[float] = None) -> ReturnType:
```

Get the result of the function call.

This function waits indefinitely by default. It takes an optional
`timeout`
argument that specifies the maximum number of seconds to wait,
which can be set to
`0`
to poll for an output immediately.

The returned coroutine is not cancellation-safe.

get\_call\_graph
----------------

```
def get_call_graph(self) -> list[InputInfo]:
```

Returns a structure representing the call graph from a given root
call ID, along with the status of execution for each node.

See
[`modal.call_graph`](https://modal.com/docs/reference/modal.call_graph)

reference page
for documentation on the structure of the returned
`InputInfo`
items.

cancel
------

```
def cancel(
    self,
    # if true, containers running the inputs are forcibly terminated
    terminate_containers: bool = False,
):
```

Cancels the function call, which will stop its execution and mark its inputs as
[`TERMINATED`](https://modal.com/docs/reference/modal.call_graph#modalcall_graphinputstatus)

.

If
`terminate_containers=True`
- the containers running the cancelled inputs are all terminated
causing any non-cancelled inputs on those containers to be rescheduled in new containers.

from\_id
--------

```
@staticmethod
def from_id(function_call_id: str, client: Optional[_Client] = None) -> "_FunctionCall[Any]":
```

Instantiate a FunctionCall object from an existing ID.

Examples:

```
# Spawn a FunctionCall and keep track of its object ID
fc = my_func.spawn()
fc_id = fc.object_id

# Later, use the ID to re-instantiate the FunctionCall object
fc = _FunctionCall.from_id(fc_id)
result = fc.get()
```

Note that it’s only necessary to re-instantiate the
`FunctionCall`
with this method
if you no longer have access to the original object returned from
`Function.spawn`
.

gather
------

```
@staticmethod
def gather(*function_calls: "_FunctionCall[T]") -> typing.Sequence[T]:
```

Wait until all Modal FunctionCall objects have results before returning.

Accepts a variable number of
`FunctionCall`
objects, as returned by
`Function.spawn()`
.

Returns a list of results from each FunctionCall, or raises an exception
from the first failing function call.

Examples:

```
fc1 = slow_func_1.spawn()
fc2 = slow_func_2.spawn()

result_1, result_2 = modal.FunctionCall.gather(fc1, fc2)
```

*Added in v0.73.69*
: This method replaces the deprecated
`modal.functions.gather`
function.

[modal.FunctionCall](#modalfunctioncall)

[hydrate](#hydrate)

[get](#get)

[get\_call\_graph](#get_call_graph)

[cancel](#cancel)

[from\_id](#from_id)

[gather](#gather)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Image
================================================================================

modal.Image
===========

```
class Image(modal.object.Object)
```

Base class for container images to run functions in.

Do not construct this class directly; instead use one of its static factory methods,
such as
`modal.Image.debian_slim`
,
`modal.Image.from_registry`
, or
`modal.Image.micromamba`
.

add\_local\_file
----------------

```
def add_local_file(self, local_path: Union[str, Path], remote_path: str, *, copy: bool = False) -> "_Image":
```

Adds a local file to the image at
`remote_path`
within the container

By default (
`copy=False`
), the files are added to containers on startup and are not built into the actual Image,
which speeds up deployment.

Set
`copy=True`
to copy the files into an Image layer at build time instead, similar to how
[`COPY`](https://docs.docker.com/engine/reference/builder/#copy)

works in a
`Dockerfile`
.

copy=True can slow down iteration since it requires a rebuild of the Image and any subsequent
build steps whenever the included files change, but it is required if you want to run additional
build steps after this one.

*Added in v0.66.40*
: This method replaces the deprecated
`modal.Image.copy_local_file`
method.

add\_local\_dir
---------------

```
def add_local_dir(
    self,
    local_path: Union[str, Path],
    remote_path: str,
    *,
    copy: bool = False,
    # Predicate filter function for file exclusion, which should accept a filepath and return `True` for exclusion.
    # Defaults to excluding no files. If a Sequence is provided, it will be converted to a FilePatternMatcher.
    # Which follows dockerignore syntax.
    ignore: Union[Sequence[str], Callable[[Path], bool]] = [],
) -> "_Image":
```

Adds a local directory’s content to the image at
`remote_path`
within the container

By default (
`copy=False`
), the files are added to containers on startup and are not built into the actual Image,
which speeds up deployment.

Set
`copy=True`
to copy the files into an Image layer at build time instead, similar to how
[`COPY`](https://docs.docker.com/engine/reference/builder/#copy)

works in a
`Dockerfile`
.

copy=True can slow down iteration since it requires a rebuild of the Image and any subsequent
build steps whenever the included files change, but it is required if you want to run additional
build steps after this one.

**Usage:**

```
from modal import FilePatternMatcher

image = modal.Image.debian_slim().add_local_dir(
    "~/assets",
    remote_path="/assets",
    ignore=["*.venv"],
)

image = modal.Image.debian_slim().add_local_dir(
    "~/assets",
    remote_path="/assets",
    ignore=lambda p: p.is_relative_to(".venv"),
)

image = modal.Image.debian_slim().add_local_dir(
    "~/assets",
    remote_path="/assets",
    ignore=FilePatternMatcher("**/*.txt"),
)

# When including files is simpler than excluding them, you can use the `~` operator to invert the matcher.
image = modal.Image.debian_slim().add_local_dir(
    "~/assets",
    remote_path="/assets",
    ignore=~FilePatternMatcher("**/*.py"),
)

# You can also read ignore patterns from a file.
image = modal.Image.debian_slim().add_local_dir(
    "~/assets",
    remote_path="/assets",
    ignore=FilePatternMatcher.from_file("/path/to/ignorefile"),
)
```

*Added in v0.66.40*
: This method replaces the deprecated
`modal.Image.copy_local_dir`
method.

add\_local\_python\_source
--------------------------

```
def add_local_python_source(
    self, *modules: str, copy: bool = False, ignore: Union[Sequence[str], Callable[[Path], bool]] = NON_PYTHON_FILES
) -> "_Image":
```

Adds locally available Python packages/modules to containers

Adds all files from the specified Python package or module to containers running the Image.

Packages are added to the
`/root`
directory of containers, which is on the
`PYTHONPATH`
of any executed Modal Functions, enabling import of the module by that name.

By default (
`copy=False`
), the files are added to containers on startup and are not built into the actual Image,
which speeds up deployment.

Set
`copy=True`
to copy the files into an Image layer at build time instead. This can slow down iteration since
it requires a rebuild of the Image and any subsequent build steps whenever the included files change, but it is
required if you want to run additional build steps after this one.

**Note:**
This excludes all dot-prefixed subdirectories or files and all
`.pyc`
/
`__pycache__`
files.
To add full directories with finer control, use
`.add_local_dir()`
instead and specify
`/root`
as
the destination directory.

By default only includes
`.py`
-files in the source modules. Set the
`ignore`
argument to a list of patterns
or a callable to override this behavior, e.g.:

```
# includes everything except data.json
modal.Image.debian_slim().add_local_python_source("mymodule", ignore=["data.json"])

# exclude large files
modal.Image.debian_slim().add_local_python_source(
    "mymodule",
    ignore=lambda p: p.stat().st_size > 1e9
)
```

*Added in v0.67.28*
: This method replaces the deprecated
`modal.Mount.from_local_python_packages`
pattern.

from\_id
--------

```
@staticmethod
def from_id(image_id: str, client: Optional[_Client] = None) -> "_Image":
```

Construct an Image from an id and look up the Image result.

The ID of an Image object can be accessed using
`.object_id`
.

pip\_install
------------

```
def pip_install(
    self,
    *packages: Union[str, list[str]],  # A list of Python packages, eg. ["numpy", "matplotlib>=3.5.0"]
    find_links: Optional[str] = None,  # Passes -f (--find-links) pip install
    index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install
    extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install
    pre: bool = False,  # Passes --pre (allow pre-releases) to pip install
    extra_options: str = "",  # Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Install a list of Python packages using pip.

**Examples**

Simple installation:

```
image = modal.Image.debian_slim().pip_install("click", "httpx~=0.23.3")
```

More complex installation:

```
image = (
    modal.Image.from_registry(
        "nvidia/cuda:12.2.0-devel-ubuntu22.04", add_python="3.11"
    )
    .pip_install(
        "ninja",
        "packaging",
        "wheel",
        "transformers==4.40.2",
    )
    .pip_install(
        "flash-attn==2.5.8", extra_options="--no-build-isolation"
    )
)
```

pip\_install\_private\_repos
----------------------------

```
def pip_install_private_repos(
    self,
    *repositories: str,
    git_user: str,
    find_links: Optional[str] = None,  # Passes -f (--find-links) pip install
    index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install
    extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install
    pre: bool = False,  # Passes --pre (allow pre-releases) to pip install
    extra_options: str = "",  # Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
    gpu: GPU_T = None,
    secrets: Sequence[_Secret] = [],
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
) -> "_Image":
```

Install a list of Python packages from private git repositories using pip.

This method currently supports Github and Gitlab only.

* **Github:**
  Provide a
  `modal.Secret`
  that contains a
  `GITHUB_TOKEN`
  key-value pair
* **Gitlab:**
  Provide a
  `modal.Secret`
  that contains a
  `GITLAB_TOKEN`
  key-value pair

These API tokens should have permissions to read the list of private repositories provided as arguments.

We recommend using Github’s
[‘fine-grained’ access tokens](https://github.blog/2022-10-18-introducing-fine-grained-personal-access-tokens-for-github/)

.
These tokens are repo-scoped, and avoid granting read permission across all of a user’s private repos.

**Example**

```
image = (
    modal.Image
    .debian_slim()
    .pip_install_private_repos(
        "github.com/ecorp/private-one@1.0.0",
        "github.com/ecorp/private-two@main"
        "github.com/ecorp/private-three@d4776502"
        # install from 'inner' directory on default branch.
        "github.com/ecorp/private-four#subdirectory=inner",
        git_user="erikbern",
        secrets=[modal.Secret.from_name("github-read-private")],
    )
)
```

pip\_install\_from\_requirements
--------------------------------

```
def pip_install_from_requirements(
    self,
    requirements_txt: str,  # Path to a requirements.txt file.
    find_links: Optional[str] = None,  # Passes -f (--find-links) pip install
    *,
    index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install
    extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install
    pre: bool = False,  # Passes --pre (allow pre-releases) to pip install
    extra_options: str = "",  # Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Install a list of Python packages from a local
`requirements.txt`
file.

pip\_install\_from\_pyproject
-----------------------------

```
def pip_install_from_pyproject(
    self,
    pyproject_toml: str,
    optional_dependencies: list[str] = [],
    *,
    find_links: Optional[str] = None,  # Passes -f (--find-links) pip install
    index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install
    extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install
    pre: bool = False,  # Passes --pre (allow pre-releases) to pip install
    extra_options: str = "",  # Additional options to pass to pip install, e.g. "--no-build-isolation --no-clean"
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Install dependencies specified by a local
`pyproject.toml`
file.

`optional_dependencies`
is a list of the keys of the
optional-dependencies section(s) of the
`pyproject.toml`
file
(e.g. test, doc, experiment, etc). When provided,
all of the packages in each listed section are installed as well.

uv\_pip\_install
----------------

```
def uv_pip_install(
    self,
    *packages: Union[str, list[str]],  # A list of Python packages, eg. ["numpy", "matplotlib>=3.5.0"]
    requirements: Optional[list[str]] = None,  # Passes -r (--requirements) to uv pip install
    find_links: Optional[str] = None,  # Passes -f (--find-links) to uv pip install
    index_url: Optional[str] = None,  # Passes -i (--index-url) to uv pip install
    extra_index_url: Optional[str] = None,  # Passes --extra-index-url to uv pip install
    pre: bool = False,  # Allow pre-releases using uv pip install --prerelease allow
    extra_options: str = "",  # Additional options to pass to pip install, e.g. "--no-build-isolation"
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    uv_version: Optional[str] = None,  # uv version to use
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Install a list of Python packages using uv pip install.

**Examples**

Simple installation:

```
image = modal.Image.debian_slim().uv_pip_install("torch==2.7.1", "numpy")
```

This method assumes that:

* Python is on the
  `$PATH`
  and dependencies are installed with the first Python on the
  `$PATH`
  .
* Shell supports backticks for substitution
* `which`
  command is on the
  `$PATH`

Added in v1.1.0.

poetry\_install\_from\_file
---------------------------

```
def poetry_install_from_file(
    self,
    poetry_pyproject_toml: str,
    poetry_lockfile: Optional[str] = None,  # Path to lockfile. If not provided, uses poetry.lock in same directory.
    *,
    ignore_lockfile: bool = False,  # If set to True, do not use poetry.lock, even when present
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    # Selected optional dependency groups to install (See https://python-poetry.org/docs/cli/#install)
    with_: list[str] = [],
    # Selected optional dependency groups to exclude (See https://python-poetry.org/docs/cli/#install)
    without: list[str] = [],
    only: list[str] = [],  # Only install dependency groups specifed in this list.
    poetry_version: Optional[str] = "latest",  # Version of poetry to install, or None to skip installation
    # If set to True, use old installer. See https://github.com/python-poetry/poetry/issues/3336
    old_installer: bool = False,
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Install poetry
*dependencies*
specified by a local
`pyproject.toml`
file.

If not provided as argument the path to the lockfile is inferred. However, the
file has to exist, unless
`ignore_lockfile`
is set to
`True`
.

Note that the root project of the poetry project is not installed, only the dependencies.
For including local python source files see
`add_local_python_source`

Poetry will be installed to the Image (using pip) unless
`poetry_version`
is set to None.
Note that the interpretation of
`poetry_version="latest"`
depends on the Modal Image Builder
version, with versions 2024.10 and earlier limiting poetry to 1.x.

uv\_sync
--------

```
def uv_sync(
    self,
    uv_project_dir: str = "./",  # Path to local uv managed project
    *,
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    groups: Optional[list[str]] = None,  # Dependency group to install using `uv sync --group`
    extras: Optional[list[str]] = None,  # Optional dependencies to install using `uv sync --extra`
    frozen: bool = True,  # If True, then we run `uv sync --frozen` when a uv.lock file is present
    extra_options: str = "",  # Extra options to pass to `uv sync`
    uv_version: Optional[str] = None,  # uv version to use
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Creates a virtual environment with the dependencies in a uv managed project with
`uv sync`
.

**Examples**

```
image = modal.Image.debian_slim().uv_sync()
```

Added in v1.1.0.

dockerfile\_commands
--------------------

```
def dockerfile_commands(
    self,
    *dockerfile_commands: Union[str, list[str]],
    context_files: dict[str, str] = {},
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
    context_mount: Optional[_Mount] = None,  # Deprecated: the context is now inferred
    context_dir: Optional[Union[Path, str]] = None,  # Context for relative COPY commands
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    ignore: Union[Sequence[str], Callable[[Path], bool]] = AUTO_DOCKERIGNORE,
) -> "_Image":
```

Extend an image with arbitrary Dockerfile-like commands.

**Usage:**

```
from modal import FilePatternMatcher

# By default a .dockerignore file is used if present in the current working directory
image = modal.Image.debian_slim().dockerfile_commands(
    ["COPY data /data"],
)

image = modal.Image.debian_slim().dockerfile_commands(
    ["COPY data /data"],
    ignore=["*.venv"],
)

image = modal.Image.debian_slim().dockerfile_commands(
    ["COPY data /data"],
    ignore=lambda p: p.is_relative_to(".venv"),
)

image = modal.Image.debian_slim().dockerfile_commands(
    ["COPY data /data"],
    ignore=FilePatternMatcher("**/*.txt"),
)

# When including files is simpler than excluding them, you can use the `~` operator to invert the matcher.
image = modal.Image.debian_slim().dockerfile_commands(
    ["COPY data /data"],
    ignore=~FilePatternMatcher("**/*.py"),
)

# You can also read ignore patterns from a file.
image = modal.Image.debian_slim().dockerfile_commands(
    ["COPY data /data"],
    ignore=FilePatternMatcher.from_file("/path/to/dockerignore"),
)
```

entrypoint
----------

```
def entrypoint(
    self,
    entrypoint_commands: list[str],
) -> "_Image":
```

Set the entrypoint for the image.

shell
-----

```
def shell(
    self,
    shell_commands: list[str],
) -> "_Image":
```

Overwrite default shell for the image.

run\_commands
-------------

```
def run_commands(
    self,
    *commands: Union[str, list[str]],
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
) -> "_Image":
```

Extend an image with a list of shell commands to run.

micromamba
----------

```
@staticmethod
def micromamba(
    python_version: Optional[str] = None,
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
) -> "_Image":
```

A Micromamba base image. Micromamba allows for fast building of small Conda-based containers.

micromamba\_install
-------------------

```
def micromamba_install(
    self,
    # A list of Python packages, eg. ["numpy", "matplotlib>=3.5.0"]
    *packages: Union[str, list[str]],
    # A local path to a file containing package specifications
    spec_file: Optional[str] = None,
    # A list of Conda channels, eg. ["conda-forge", "nvidia"].
    channels: list[str] = [],
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Install a list of additional packages using micromamba.

from\_registry
--------------

```
@staticmethod
def from_registry(
    tag: str,
    secret: Optional[_Secret] = None,
    *,
    setup_dockerfile_commands: list[str] = [],
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    add_python: Optional[str] = None,
    **kwargs,
) -> "_Image":
```

Build a Modal Image from a public or private image registry, such as Docker Hub.

The image must be built for the
`linux/amd64`
platform.

If your image does not come with Python installed, you can use the
`add_python`
parameter
to specify a version of Python to add to the image. Otherwise, the image is expected to
have Python on PATH as
`python`
, along with
`pip`
.

You may also use
`setup_dockerfile_commands`
to run Dockerfile commands before the
remaining commands run. This might be useful if you want a custom Python installation or to
set a
`SHELL`
. Prefer
`run_commands()`
when possible though.

To authenticate against a private registry with static credentials, you must set the
`secret`
parameter to
a
`modal.Secret`
containing a username (
`REGISTRY_USERNAME`
) and
an access token or password (
`REGISTRY_PASSWORD`
).

To authenticate against private registries with credentials from a cloud provider,
use
`Image.from_gcp_artifact_registry()`
or
`Image.from_aws_ecr()`
.

**Examples**

```
modal.Image.from_registry("python:3.11-slim-bookworm")
modal.Image.from_registry("ubuntu:22.04", add_python="3.11")
modal.Image.from_registry("nvcr.io/nvidia/pytorch:22.12-py3")
```

from\_gcp\_artifact\_registry
-----------------------------

```
@staticmethod
def from_gcp_artifact_registry(
    tag: str,
    secret: Optional[_Secret] = None,
    *,
    setup_dockerfile_commands: list[str] = [],
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    add_python: Optional[str] = None,
    **kwargs,
) -> "_Image":
```

Build a Modal image from a private image in Google Cloud Platform (GCP) Artifact Registry.

You will need to pass a
`modal.Secret`
containing
[your GCP service account key data](https://cloud.google.com/iam/docs/keys-create-delete#creating)

as
`SERVICE_ACCOUNT_JSON`
. This can be done from the
[Secrets](https://modal.com/secrets)

page.
Your service account should be granted a specific role depending on the GCP registry used:

* For Artifact Registry images (
  `pkg.dev`
  domains) use
  the
  [“Artifact Registry Reader”](https://cloud.google.com/artifact-registry/docs/access-control#roles)

  role
* For Container Registry images (
  `gcr.io`
  domains) use
  the
  [“Storage Object Viewer”](https://cloud.google.com/artifact-registry/docs/transition/setup-gcr-repo)

  role

**Note:**
This method does not use
`GOOGLE_APPLICATION_CREDENTIALS`
as that
variable accepts a path to a JSON file, not the actual JSON string.

See
`Image.from_registry()`
for information about the other parameters.

**Example**

```
modal.Image.from_gcp_artifact_registry(
    "us-east1-docker.pkg.dev/my-project-1234/my-repo/my-image:my-version",
    secret=modal.Secret.from_name(
        "my-gcp-secret",
        required_keys=["SERVICE_ACCOUNT_JSON"],
    ),
    add_python="3.11",
)
```

from\_aws\_ecr
--------------

```
@staticmethod
def from_aws_ecr(
    tag: str,
    secret: Optional[_Secret] = None,
    *,
    setup_dockerfile_commands: list[str] = [],
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    add_python: Optional[str] = None,
    **kwargs,
) -> "_Image":
```

Build a Modal image from a private image in AWS Elastic Container Registry (ECR).

You will need to pass a
`modal.Secret`
containing
`AWS_ACCESS_KEY_ID`
,
`AWS_SECRET_ACCESS_KEY`
, and
`AWS_REGION`
to access the target ECR registry.

IAM configuration details can be found in the AWS documentation for
[“Private repository policies”](https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-policies.html)

.

See
`Image.from_registry()`
for information about the other parameters.

**Example**

```
modal.Image.from_aws_ecr(
    "000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:my-version",
    secret=modal.Secret.from_name(
        "aws",
        required_keys=["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY", "AWS_REGION"],
    ),
    add_python="3.11",
)
```

from\_dockerfile
----------------

```
@staticmethod
def from_dockerfile(
    path: Union[str, Path],  # Filepath to Dockerfile.
    *,
    context_mount: Optional[_Mount] = None,  # Deprecated: the context is now inferred
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    context_dir: Optional[Union[Path, str]] = None,  # Context for relative COPY commands
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
    add_python: Optional[str] = None,
    build_args: dict[str, str] = {},
    ignore: Union[Sequence[str], Callable[[Path], bool]] = AUTO_DOCKERIGNORE,
) -> "_Image":
```

Build a Modal image from a local Dockerfile.

If your Dockerfile does not have Python installed, you can use the
`add_python`
parameter
to specify a version of Python to add to the image.

**Usage:**

```
from modal import FilePatternMatcher

# By default a .dockerignore file is used if present in the current working directory
image = modal.Image.from_dockerfile(
    "./Dockerfile",
    add_python="3.12",
)

image = modal.Image.from_dockerfile(
    "./Dockerfile",
    add_python="3.12",
    ignore=["*.venv"],
)

image = modal.Image.from_dockerfile(
    "./Dockerfile",
    add_python="3.12",
    ignore=lambda p: p.is_relative_to(".venv"),
)

image = modal.Image.from_dockerfile(
    "./Dockerfile",
    add_python="3.12",
    ignore=FilePatternMatcher("**/*.txt"),
)

# When including files is simpler than excluding them, you can use the `~` operator to invert the matcher.
image = modal.Image.from_dockerfile(
    "./Dockerfile",
    add_python="3.12",
    ignore=~FilePatternMatcher("**/*.py"),
)

# You can also read ignore patterns from a file.
image = modal.Image.from_dockerfile(
    "./Dockerfile",
    add_python="3.12",
    ignore=FilePatternMatcher.from_file("/path/to/dockerignore"),
)
```

debian\_slim
------------

```
@staticmethod
def debian_slim(python_version: Optional[str] = None, force_build: bool = False) -> "_Image":
```

Default image, based on the official
`python`
Docker images.

apt\_install
------------

```
def apt_install(
    self,
    *packages: Union[str, list[str]],  # A list of packages, e.g. ["ssh", "libpq-dev"]
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    secrets: Sequence[_Secret] = [],
    gpu: GPU_T = None,
) -> "_Image":
```

Install a list of Debian packages using
`apt`
.

**Example**

```
image = modal.Image.debian_slim().apt_install("git")
```

run\_function
-------------

```
def run_function(
    self,
    raw_f: Callable[..., Any],
    *,
    secrets: Sequence[_Secret] = (),  # Optional Modal Secret objects with environment variables for the container
    volumes: dict[Union[str, PurePosixPath], Union[_Volume, _CloudBucketMount]] = {},  # Volume mount paths
    network_file_systems: dict[Union[str, PurePosixPath], _NetworkFileSystem] = {},  # NFS mount paths
    gpu: Union[GPU_T, list[GPU_T]] = None,  # Requested GPU or or list of acceptable GPUs( e.g. ["A10", "A100"])
    cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.
    memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.
    timeout: Optional[int] = 60 * 60,  # Maximum execution time of the function in seconds.
    cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.
    region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the function on.
    force_build: bool = False,  # Ignore cached builds, similar to 'docker build --no-cache'
    args: Sequence[Any] = (),  # Positional arguments to the function.
    kwargs: dict[str, Any] = {},  # Keyword arguments to the function.
    include_source: bool = True,  # Whether the builder container should have the Function's source added
) -> "_Image":
```

Run user-defined function
`raw_f`
as an image build step.

The function runs like an ordinary Modal Function, accepting a resource configuration and integrating
with Modal features like Secrets and Volumes. Unlike ordinary Modal Functions, any changes to the
filesystem state will be captured on container exit and saved as a new Image.

**Note**

Only the source code of
`raw_f`
, the contents of
`**kwargs`
, and any referenced
*global*
variables
are used to determine whether the image has changed and needs to be rebuilt.
If this function references other functions or variables, the image will not be rebuilt if you
make changes to them. You can force a rebuild by changing the function’s source code itself.

**Example**

```

def my_build_function():
    open("model.pt", "w").write("parameters!")

image = (
    modal.Image
        .debian_slim()
        .pip_install("torch")
        .run_function(my_build_function, secrets=[...], mounts=[...])
)
```

env
---

```
def env(self, vars: dict[str, str]) -> "_Image":
```

Sets the environment variables in an Image.

**Example**

```
image = (
    modal.Image.debian_slim()
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)
```

workdir
-------

```
def workdir(self, path: Union[str, PurePosixPath]) -> "_Image":
```

Set the working directory for subsequent image build steps and function execution.

**Example**

```
image = (
    modal.Image.debian_slim()
    .run_commands("git clone https://xyz app")
    .workdir("/app")
    .run_commands("yarn install")
)
```

cmd
---

```
def cmd(self, cmd: list[str]) -> "_Image":
```

Set the default entrypoint argument (
`CMD`
) for the image.

**Example**

```
image = (
    modal.Image.debian_slim().cmd(["python", "app.py"])
)
```

imports
-------

```
@contextlib.contextmanager
def imports(self):
```

Used to import packages in global scope that are only available when running remotely.
By using this context manager you can avoid an
`ImportError`
due to not having certain
packages installed locally.

**Usage:**

```
with image.imports():
    import torch
```

[modal.Image](#modalimage)

[add\_local\_file](#add_local_file)

[add\_local\_dir](#add_local_dir)

[add\_local\_python\_source](#add_local_python_source)

[from\_id](#from_id)

[pip\_install](#pip_install)

[pip\_install\_private\_repos](#pip_install_private_repos)

[pip\_install\_from\_requirements](#pip_install_from_requirements)

[pip\_install\_from\_pyproject](#pip_install_from_pyproject)

[uv\_pip\_install](#uv_pip_install)

[poetry\_install\_from\_file](#poetry_install_from_file)

[uv\_sync](#uv_sync)

[dockerfile\_commands](#dockerfile_commands)

[entrypoint](#entrypoint)

[shell](#shell)

[run\_commands](#run_commands)

[micromamba](#micromamba)

[micromamba\_install](#micromamba_install)

[from\_registry](#from_registry)

[from\_gcp\_artifact\_registry](#from_gcp_artifact_registry)

[from\_aws\_ecr](#from_aws_ecr)

[from\_dockerfile](#from_dockerfile)

[debian\_slim](#debian_slim)

[apt\_install](#apt_install)

[run\_function](#run_function)

[env](#env)

[workdir](#workdir)

[cmd](#cmd)

[imports](#imports)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.NetworkFileSystem
================================================================================

modal.NetworkFileSystem
=======================

```
class NetworkFileSystem(modal.object.Object)
```

A shared, writable file system accessible by one or more Modal functions.

By attaching this file system as a mount to one or more functions, they can
share and persist data with each other.

**Usage**

```
import modal

nfs = modal.NetworkFileSystem.from_name("my-nfs", create_if_missing=True)
app = modal.App()

@app.function(network_file_systems={"/root/foo": nfs})
def f():
    pass

@app.function(network_file_systems={"/root/goo": nfs})
def g():
    pass
```

Also see the CLI methods for accessing network file systems:

```
modal nfs --help
```

A
`NetworkFileSystem`
can also be useful for some local scripting scenarios, e.g.:

```
nfs = modal.NetworkFileSystem.from_name("my-network-file-system")
for chunk in nfs.read_file("my_db_dump.csv"):
    ...
```

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

from\_name
----------

```
@staticmethod
def from_name(
    name: str,
    *,
    environment_name: Optional[str] = None,
    create_if_missing: bool = False,
) -> "_NetworkFileSystem":
```

Reference a NetworkFileSystem by its name, creating if necessary.

This is a lazy method that defers hydrating the local object with
metadata from Modal servers until the first time it is actually
used.

```
nfs = NetworkFileSystem.from_name("my-nfs", create_if_missing=True)

@app.function(network_file_systems={"/data": nfs})
def f():
    pass
```

ephemeral
---------

```
@classmethod
@contextmanager
def ephemeral(
    cls: type["_NetworkFileSystem"],
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
    _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
) -> Iterator["_NetworkFileSystem"]:
```

Creates a new ephemeral network filesystem within a context manager:

Usage:

```
with modal.NetworkFileSystem.ephemeral() as nfs:
    assert nfs.listdir("/") == []
```

```
async with modal.NetworkFileSystem.ephemeral() as nfs:
    assert await nfs.listdir("/") == []
```

delete
------

```
@staticmethod
def delete(name: str, client: Optional[_Client] = None, environment_name: Optional[str] = None):
```

write\_file
-----------

```
@live_method
def write_file(self, remote_path: str, fp: BinaryIO, progress_cb: Optional[Callable[..., Any]] = None) -> int:
```

Write from a file object to a path on the network file system, atomically.

Will create any needed parent directories automatically.

If remote\_path ends with
`/`
it’s assumed to be a directory and the
file will be uploaded with its current name to that directory.

read\_file
----------

```
@live_method_gen
def read_file(self, path: str) -> Iterator[bytes]:
```

Read a file from the network file system

iterdir
-------

```
@live_method_gen
def iterdir(self, path: str) -> Iterator[FileEntry]:
```

Iterate over all files in a directory in the network file system.

* Passing a directory path lists all files in the directory (names are relative to the directory)
* Passing a file path returns a list containing only that file’s listing description
* Passing a glob path (including at least one \* or \*\* sequence) returns all files matching
  that glob path (using absolute paths)

add\_local\_file
----------------

```
@live_method
def add_local_file(
    self,
    local_path: Union[Path, str],
    remote_path: Optional[Union[str, PurePosixPath, None]] = None,
    progress_cb: Optional[Callable[..., Any]] = None,
):
```

add\_local\_dir
---------------

```
@live_method
def add_local_dir(
    self,
    local_path: Union[Path, str],
    remote_path: Optional[Union[str, PurePosixPath, None]] = None,
    progress_cb: Optional[Callable[..., Any]] = None,
):
```

listdir
-------

```
@live_method
def listdir(self, path: str) -> list[FileEntry]:
```

List all files in a directory in the network file system.

* Passing a directory path lists all files in the directory (names are relative to the directory)
* Passing a file path returns a list containing only that file’s listing description
* Passing a glob path (including at least one \* or \*\* sequence) returns all files matching
  that glob path (using absolute paths)

remove\_file
------------

```
@live_method
def remove_file(self, path: str, recursive=False):
```

Remove a file in a network file system.

[modal.NetworkFileSystem](#modalnetworkfilesystem)

[hydrate](#hydrate)

[from\_name](#from_name)

[ephemeral](#ephemeral)

[delete](#delete)

[write\_file](#write_file)

[read\_file](#read_file)

[iterdir](#iterdir)

[add\_local\_file](#add_local_file)

[add\_local\_dir](#add_local_dir)

[listdir](#listdir)

[remove\_file](#remove_file)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Period
================================================================================

modal.Period
============

```
class Period(modal.schedule.Schedule)
```

Create a schedule that runs every given time interval.

**Usage**

```
import modal
app = modal.App()

@app.function(schedule=modal.Period(days=1))
def f():
    print("This function will run every day")

modal.Period(hours=4)          # runs every 4 hours
modal.Period(minutes=15)       # runs every 15 minutes
modal.Period(seconds=math.pi)  # runs every 3.141592653589793 seconds
```

Only
`seconds`
can be a float. All other arguments are integers.

Note that
`days=1`
will trigger the function the same time every day.
This does not have the same behavior as
`seconds=84000`
since days have
different lengths due to daylight savings and leap seconds. Similarly,
using
`months=1`
will trigger the function on the same day each month.

This behaves similar to the
[dateutil](https://dateutil.readthedocs.io/en/latest/relativedelta.html)

package.

```
def __init__(
    self,
    *,
    years: int = 0,
    months: int = 0,
    weeks: int = 0,
    days: int = 0,
    hours: int = 0,
    minutes: int = 0,
    seconds: float = 0,
) -> None:
```

[modal.Period](#modalperiod)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Proxy
================================================================================

modal.Proxy
===========

```
class Proxy(modal.object.Object)
```

Proxy objects give your Modal containers a static outbound IP address.

This can be used for connecting to a remote address with network whitelist, for example
a database. See
[the guide](https://modal.com/docs/guide/proxy-ips)

for more information.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

from\_name
----------

```
@staticmethod
def from_name(
    name: str,
    *,
    environment_name: Optional[str] = None,
) -> "_Proxy":
```

Reference a Proxy by its name.

In contrast to most other Modal objects, new Proxy objects must be
provisioned via the Dashboard and cannot be created on the fly from code.

[modal.Proxy](#modalproxy)

[hydrate](#hydrate)

[from\_name](#from_name)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Queue
================================================================================

modal.Queue
===========

```
class Queue(modal.object.Object)
```

Distributed, FIFO queue for data flow in Modal apps.

The queue can contain any object serializable by
`cloudpickle`
, including Modal objects.

By default, the
`Queue`
object acts as a single FIFO queue which supports puts and gets (blocking and non-blocking).

**Usage**

```
from modal import Queue

# Create an ephemeral queue which is anonymous and garbage collected
with Queue.ephemeral() as my_queue:
    # Putting values
    my_queue.put("some value")
    my_queue.put(123)

    # Getting values
    assert my_queue.get() == "some value"
    assert my_queue.get() == 123

    # Using partitions
    my_queue.put(0)
    my_queue.put(1, partition="foo")
    my_queue.put(2, partition="bar")

    # Default and "foo" partition are ignored by the get operation.
    assert my_queue.get(partition="bar") == 2

    # Set custom 10s expiration time on "foo" partition.
    my_queue.put(3, partition="foo", partition_ttl=10)

    # (beta feature) Iterate through items in place (read immutably)
    my_queue.put(1)
    assert [v for v in my_queue.iterate()] == [0, 1]

# You can also create persistent queues that can be used across apps
queue = Queue.from_name("my-persisted-queue", create_if_missing=True)
queue.put(42)
assert queue.get() == 42
```

For more examples, see the
[guide](https://modal.com/docs/guide/dicts-and-queues#modal-queues)

.

**Queue partitions (beta)**

Specifying partition keys gives access to other independent FIFO partitions within the same
`Queue`
object.
Across any two partitions, puts and gets are completely independent.
For example, a put in one partition does not affect a get in any other partition.

When no partition key is specified (by default), puts and gets will operate on a default partition.
This default partition is also isolated from all other partitions.
Please see the Usage section below for an example using partitions.

**Lifetime of a queue and its partitions**

By default, each partition is cleared 24 hours after the last
`put`
operation.
A lower TTL can be specified by the
`partition_ttl`
argument in the
`put`
or
`put_many`
methods.
Each partition’s expiry is handled independently.

As such,
`Queue`
s are best used for communication between active functions and not relied on for persistent storage.

On app completion or after stopping an app any associated
`Queue`
objects are cleaned up.
All its partitions will be cleared.

**Limits**

A single
`Queue`
can contain up to 100,000 partitions, each with up to 5,000 items. Each item can be up to 1 MiB.

Partition keys must be non-empty and must not exceed 64 bytes.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

validate\_partition\_key
------------------------

```
@staticmethod
def validate_partition_key(partition: Optional[str]) -> bytes:
```

ephemeral
---------

```
@classmethod
@contextmanager
def ephemeral(
    cls: type["_Queue"],
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
    _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
) -> Iterator["_Queue"]:
```

Creates a new ephemeral queue within a context manager:

Usage:

```
from modal import Queue

with Queue.ephemeral() as q:
    q.put(123)
```

```
async with Queue.ephemeral() as q:
    await q.put.aio(123)
```

from\_name
----------

```
@staticmethod
def from_name(
    name: str,
    *,
    environment_name: Optional[str] = None,
    create_if_missing: bool = False,
) -> "_Queue":
```

Reference a named Queue, creating if necessary.

This is a lazy method the defers hydrating the local
object with metadata from Modal servers until the first
time it is actually used.

```
q = modal.Queue.from_name("my-queue", create_if_missing=True)
q.put(123)
```

delete
------

```
@staticmethod
def delete(name: str, *, client: Optional[_Client] = None, environment_name: Optional[str] = None):
```

clear
-----

```
@live_method
def clear(self, *, partition: Optional[str] = None, all: bool = False) -> None:
```

Clear the contents of a single partition or all partitions.

get
---

```
@live_method
def get(
    self, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
) -> Optional[Any]:
```

Remove and return the next object in the queue.

If
`block`
is
`True`
(the default) and the queue is empty,
`get`
will wait indefinitely for
an object, or until
`timeout`
if specified. Raises a native
`queue.Empty`
exception
if the
`timeout`
is reached.

If
`block`
is
`False`
,
`get`
returns
`None`
immediately if the queue is empty. The
`timeout`
is
ignored in this case.

get\_many
---------

```
@live_method
def get_many(
    self, n_values: int, block: bool = True, timeout: Optional[float] = None, *, partition: Optional[str] = None
) -> list[Any]:
```

Remove and return up to
`n_values`
objects from the queue.

If there are fewer than
`n_values`
items in the queue, return all of them.

If
`block`
is
`True`
(the default) and the queue is empty,
`get`
will wait indefinitely for
at least 1 object to be present, or until
`timeout`
if specified. Raises the stdlib’s
`queue.Empty`
exception if the
`timeout`
is reached.

If
`block`
is
`False`
,
`get`
returns
`None`
immediately if the queue is empty. The
`timeout`
is
ignored in this case.

put
---

```
@live_method
def put(
    self,
    v: Any,
    block: bool = True,
    timeout: Optional[float] = None,
    *,
    partition: Optional[str] = None,
    partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
) -> None:
```

Add an object to the end of the queue.

If
`block`
is
`True`
and the queue is full, this method will retry indefinitely or
until
`timeout`
if specified. Raises the stdlib’s
`queue.Full`
exception if the
`timeout`
is reached.
If blocking it is not recommended to omit the
`timeout`
, as the operation could wait indefinitely.

If
`block`
is
`False`
, this method raises
`queue.Full`
immediately if the queue is full. The
`timeout`
is
ignored in this case.

put\_many
---------

```
@live_method
def put_many(
    self,
    vs: list[Any],
    block: bool = True,
    timeout: Optional[float] = None,
    *,
    partition: Optional[str] = None,
    partition_ttl: int = 24 * 3600,  # After 24 hours of no activity, this partition will be deletd.
) -> None:
```

Add several objects to the end of the queue.

If
`block`
is
`True`
and the queue is full, this method will retry indefinitely or
until
`timeout`
if specified. Raises the stdlib’s
`queue.Full`
exception if the
`timeout`
is reached.
If blocking it is not recommended to omit the
`timeout`
, as the operation could wait indefinitely.

If
`block`
is
`False`
, this method raises
`queue.Full`
immediately if the queue is full. The
`timeout`
is
ignored in this case.

len
---

```
@live_method
def len(self, *, partition: Optional[str] = None, total: bool = False) -> int:
```

Return the number of objects in the queue partition.

iterate
-------

```
@warn_if_generator_is_not_consumed()
@live_method_gen
def iterate(
    self, *, partition: Optional[str] = None, item_poll_timeout: float = 0.0
) -> AsyncGenerator[Any, None]:
```

(Beta feature) Iterate through items in the queue without mutation.

Specify
`item_poll_timeout`
to control how long the iterator should wait for the next time before giving up.

[modal.Queue](#modalqueue)

[hydrate](#hydrate)

[validate\_partition\_key](#validate_partition_key)

[ephemeral](#ephemeral)

[from\_name](#from_name)

[delete](#delete)

[clear](#clear)

[get](#get)

[get\_many](#get_many)

[put](#put)

[put\_many](#put_many)

[len](#len)

[iterate](#iterate)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Retries
================================================================================

modal.Retries
=============

```
class Retries(object)
```

Adds a retry policy to a Modal function.

**Usage**

```
import modal
app = modal.App()

# Basic configuration.
# This sets a policy of max 4 retries with 1-second delay between failures.
@app.function(retries=4)
def f():
    pass

# Fixed-interval retries with 3-second delay between failures.
@app.function(
    retries=modal.Retries(
        max_retries=2,
        backoff_coefficient=1.0,
        initial_delay=3.0,
    )
)
def g():
    pass

# Exponential backoff, with retry delay doubling after each failure.
@app.function(
    retries=modal.Retries(
        max_retries=4,
        backoff_coefficient=2.0,
        initial_delay=1.0,
    )
)
def h():
    pass
```

```
def __init__(
    self,
    *,
    # The maximum number of retries that can be made in the presence of failures.
    max_retries: int,
    # Coefficent controlling how much the retry delay increases each retry attempt.
    # A backoff coefficient of 1.0 creates fixed-delay where the delay period always equals the initial delay.
    backoff_coefficient: float = 2.0,
    # Number of seconds that must elapse before the first retry occurs.
    initial_delay: float = 1.0,
    # Maximum length of retry delay in seconds, preventing the delay from growing infinitely.
    max_delay: float = 60.0,
):
```

Construct a new retries policy, supporting exponential and fixed-interval delays via a backoff coefficient.

[modal.Retries](#modalretries)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Sandbox
================================================================================

modal.Sandbox
=============

```
class Sandbox(modal.object.Object)
```

A
`Sandbox`
object lets you interact with a running sandbox. This API is similar to Python’s
[asyncio.subprocess.Process](https://docs.python.org/3/library/asyncio-subprocess.html#asyncio.subprocess.Process)

.

Refer to the
[guide](https://modal.com/docs/guide/sandbox)

on how to spawn and use sandboxes.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

create
------

```
@staticmethod
def create(
    *entrypoint_args: str,
    app: Optional["modal.app._App"] = None,  # Optionally associate the sandbox with an app
    image: Optional[_Image] = None,  # The image to run as the container for the sandbox.
    secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.
    network_file_systems: dict[Union[str, os.PathLike], _NetworkFileSystem] = {},
    timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.
    workdir: Optional[str] = None,  # Working directory of the sandbox.
    gpu: GPU_T = None,
    cloud: Optional[str] = None,
    region: Optional[Union[str, Sequence[str]]] = None,  # Region or regions to run the sandbox on.
    # Specify, in fractional CPU cores, how many CPU cores to request.
    # Or, pass (request, limit) to additionally specify a hard limit in fractional CPU cores.
    # CPU throttling will prevent a container from exceeding its specified limit.
    cpu: Optional[Union[float, tuple[float, float]]] = None,
    # Specify, in MiB, a memory request which is the minimum memory required.
    # Or, pass (request, limit) to additionally specify a hard limit in MiB.
    memory: Optional[Union[int, tuple[int, int]]] = None,
    block_network: bool = False,  # Whether to block network access
    # List of CIDRs the sandbox is allowed to access. If None, all CIDRs are allowed.
    cidr_allowlist: Optional[Sequence[str]] = None,
    volumes: dict[
        Union[str, os.PathLike], Union[_Volume, _CloudBucketMount]
    ] = {},  # Mount points for Modal Volumes and CloudBucketMounts
    pty_info: Optional[api_pb2.PTYInfo] = None,
    # List of ports to tunnel into the sandbox. Encrypted ports are tunneled with TLS.
    encrypted_ports: Sequence[int] = [],
    # List of encrypted ports to tunnel into the sandbox, using HTTP/2.
    h2_ports: Sequence[int] = [],
    # List of ports to tunnel into the sandbox without encryption.
    unencrypted_ports: Sequence[int] = [],
    # Reference to a Modal Proxy to use in front of this Sandbox.
    proxy: Optional[_Proxy] = None,
    # Enable verbose logging for sandbox operations.
    verbose: bool = False,
    # Enable memory snapshots.
    _experimental_enable_snapshot: bool = False,
    _experimental_scheduler_placement: Optional[
        SchedulerPlacement
    ] = None,  # Experimental controls over fine-grained scheduling (alpha).
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,  # *DEPRECATED* Optionally override the default environment
) -> "_Sandbox":
```

Create a new Sandbox to run untrusted, arbitrary code. The Sandbox’s corresponding container
will be created asynchronously.

**Usage**

```
app = modal.App.lookup('sandbox-hello-world', create_if_missing=True)
sandbox = modal.Sandbox.create("echo", "hello world", app=app)
print(sandbox.stdout.read())
sandbox.wait()
```

from\_id
--------

```
@staticmethod
def from_id(sandbox_id: str, client: Optional[_Client] = None) -> "_Sandbox":
```

Construct a Sandbox from an id and look up the Sandbox result.

The ID of a Sandbox object can be accessed using
`.object_id`
.

set\_tags
---------

```
def set_tags(self, tags: dict[str, str], *, client: Optional[_Client] = None):
```

Set tags (key-value pairs) on the Sandbox. Tags can be used to filter results in
`Sandbox.list`
.

snapshot\_filesystem
--------------------

```
def snapshot_filesystem(self, timeout: int = 55) -> _Image:
```

Snapshot the filesystem of the Sandbox.

Returns an
[`Image`](https://modal.com/docs/reference/modal.Image)

object which
can be used to spawn a new Sandbox with the same filesystem.

wait
----

```
def wait(self, raise_on_termination: bool = True):
```

Wait for the Sandbox to finish running.

tunnels
-------

```
def tunnels(self, timeout: int = 50) -> dict[int, Tunnel]:
```

Get Tunnel metadata for the sandbox.

Raises
`SandboxTimeoutError`
if the tunnels are not available after the timeout.

Returns a dictionary of
`Tunnel`
objects which are keyed by the container port.

NOTE: Previous to client
[v0.64.153](https://modal.com/docs/reference/changelog#064153-2024-09-30)

, this
returned a list of
`TunnelData`
objects.

reload\_volumes
---------------

```
def reload_volumes(self) -> None:
```

Reload all Volumes mounted in the Sandbox.

Added in v1.1.0.

terminate
---------

```
def terminate(self) -> None:
```

Terminate Sandbox execution.

This is a no-op if the Sandbox has already finished running.

poll
----

```
def poll(self) -> Optional[int]:
```

Check if the Sandbox has finished running.

Returns
`None`
if the Sandbox is still running, else returns the exit code.

exec
----

```
def exec(
    self,
    *cmds: str,
    pty_info: Optional[api_pb2.PTYInfo] = None,  # Deprecated: internal use only
    stdout: StreamType = StreamType.PIPE,
    stderr: StreamType = StreamType.PIPE,
    timeout: Optional[int] = None,
    workdir: Optional[str] = None,
    secrets: Sequence[_Secret] = (),
    # Encode output as text.
    text: bool = True,
    # Control line-buffered output.
    # -1 means unbuffered, 1 means line-buffered (only available if `text=True`).
    bufsize: Literal[-1, 1] = -1,
    # Internal option to set terminal size and metadata
    _pty_info: Optional[api_pb2.PTYInfo] = None,
):
```

Execute a command in the Sandbox and return a ContainerProcess handle.

See the
[`ContainerProcess`](https://modal.com/docs/reference/modal.container_process#modalcontainer_processcontainerprocess)

docs for more information.

**Usage**

```
app = modal.App.lookup("my-app", create_if_missing=True)

sandbox = modal.Sandbox.create("sleep", "infinity", app=app)

process = sandbox.exec("bash", "-c", "for i in $(seq 1 10); do echo foo $i; sleep 0.5; done")

for line in process.stdout:
    print(line)
```

open
----

```
def open(
    self,
    path: str,
    mode: Union["_typeshed.OpenTextMode", "_typeshed.OpenBinaryMode"] = "r",
):
```

[Alpha] Open a file in the Sandbox and return a FileIO handle.

See the
[`FileIO`](https://modal.com/docs/reference/modal.file_io#modalfile_iofileio)

docs for more information.

**Usage**

```
sb = modal.Sandbox.create(app=sb_app)
f = sb.open("/test.txt", "w")
f.write("hello")
f.close()
```

ls
--

```
def ls(self, path: str) -> list[str]:
```

[Alpha] List the contents of a directory in the Sandbox.

mkdir
-----

```
def mkdir(self, path: str, parents: bool = False) -> None:
```

[Alpha] Create a new directory in the Sandbox.

rm
--

```
def rm(self, path: str, recursive: bool = False) -> None:
```

[Alpha] Remove a file or directory in the Sandbox.

watch
-----

```
def watch(
    self,
    path: str,
    filter: Optional[list[FileWatchEventType]] = None,
    recursive: Optional[bool] = None,
    timeout: Optional[int] = None,
) -> Iterator[FileWatchEvent]:
```

[Alpha] Watch a file or directory in the Sandbox for changes.

stdout
------

```
@property
def stdout(self) -> _StreamReader[str]:
```

[`StreamReader`](https://modal.com/docs/reference/modal.io_streams#modalio_streamsstreamreader)

for
the sandbox’s stdout stream.

stderr
------

```
@property
def stderr(self) -> _StreamReader[str]:
```

[`StreamReader`](https://modal.com/docs/reference/modal.io_streams#modalio_streamsstreamreader)

for
the Sandbox’s stderr stream.

stdin
-----

```
@property
def stdin(self) -> _StreamWriter:
```

[`StreamWriter`](https://modal.com/docs/reference/modal.io_streams#modalio_streamsstreamwriter)

for
the Sandbox’s stdin stream.

returncode
----------

```
@property
def returncode(self) -> Optional[int]:
```

Return code of the Sandbox process if it has finished running, else
`None`
.

list
----

```
@staticmethod
def list(
    *, app_id: Optional[str] = None, tags: Optional[dict[str, str]] = None, client: Optional[_Client] = None
) -> AsyncGenerator["_Sandbox", None]:
```

List all Sandboxes for the current Environment or App ID (if specified). If tags are specified, only
Sandboxes that have at least those tags are returned. Returns an iterator over
`Sandbox`
objects.

[modal.Sandbox](#modalsandbox)

[hydrate](#hydrate)

[create](#create)

[from\_id](#from_id)

[set\_tags](#set_tags)

[snapshot\_filesystem](#snapshot_filesystem)

[wait](#wait)

[tunnels](#tunnels)

[reload\_volumes](#reload_volumes)

[terminate](#terminate)

[poll](#poll)

[exec](#exec)

[open](#open)

[ls](#ls)

[mkdir](#mkdir)

[rm](#rm)

[watch](#watch)

[stdout](#stdout)

[stderr](#stderr)

[stdin](#stdin)

[returncode](#returncode)

[list](#list)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.SandboxSnapshot
================================================================================

modal.SandboxSnapshot
=====================

```
class SandboxSnapshot(modal.object.Object)
```

> Sandbox memory snapshots are in
> **early preview**
> .

A
`SandboxSnapshot`
object lets you interact with a stored Sandbox snapshot that was created by calling
`._experimental_snapshot()`
on a Sandbox instance. This includes both the filesystem and memory state of
the original Sandbox at the time the snapshot was taken.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

from\_id
--------

```
@staticmethod
def from_id(sandbox_snapshot_id: str, client: Optional[_Client] = None):
```

Construct a
`SandboxSnapshot`
object from a sandbox snapshot ID.

[modal.SandboxSnapshot](#modalsandboxsnapshot)

[hydrate](#hydrate)

[from\_id](#from_id)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Secret
================================================================================

modal.Secret
============

```
class Secret(modal.object.Object)
```

Secrets provide a dictionary of environment variables for images.

Secrets are a secure way to add credentials and other sensitive information
to the containers your functions run in. You can create and edit secrets on
[the dashboard](https://modal.com/secrets)

, or programmatically from Python code.

See
[the secrets guide page](https://modal.com/docs/guide/secrets)

for more information.

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

from\_dict
----------

```
@staticmethod
def from_dict(
    env_dict: dict[
        str, Union[str, None]
    ] = {},  # dict of entries to be inserted as environment variables in functions using the secret
) -> "_Secret":
```

Create a secret from a str-str dictionary. Values can also be
`None`
, which is ignored.

Usage:

```
@app.function(secrets=[modal.Secret.from_dict({"FOO": "bar"})])
def run():
    print(os.environ["FOO"])
```

from\_local\_environ
--------------------

```
@staticmethod
def from_local_environ(
    env_keys: list[str],  # list of local env vars to be included for remote execution
) -> "_Secret":
```

Create secrets from local environment variables automatically.

from\_dotenv
------------

```
@staticmethod
def from_dotenv(path=None, *, filename=".env") -> "_Secret":
```

Create secrets from a .env file automatically.

If no argument is provided, it will use the current working directory as the starting
point for finding a
`.env`
file. Note that it does not use the location of the module
calling
`Secret.from_dotenv`
.

If called with an argument, it will use that as a starting point for finding
`.env`
files.
In particular, you can call it like this:

```
@app.function(secrets=[modal.Secret.from_dotenv(__file__)])
def run():
    print(os.environ["USERNAME"])  # Assumes USERNAME is defined in your .env file
```

This will use the location of the script calling
`modal.Secret.from_dotenv`
as a
starting point for finding the
`.env`
file.

A file named
`.env`
is expected by default, but this can be overridden with the
`filename`
keyword argument:

```
@app.function(secrets=[modal.Secret.from_dotenv(filename=".env-dev")])
def run():
    ...
```

from\_name
----------

```
@staticmethod
def from_name(
    name: str,
    *,
    environment_name: Optional[str] = None,
    required_keys: list[
        str
    ] = [],  # Optionally, a list of required environment variables (will be asserted server-side)
) -> "_Secret":
```

Reference a Secret by its name.

In contrast to most other Modal objects, named Secrets must be provisioned
from the Dashboard. See other methods for alternate ways of creating a new
Secret from code.

```
secret = modal.Secret.from_name("my-secret")

@app.function(secrets=[secret])
def run():
   ...
```

[modal.Secret](#modalsecret)

[hydrate](#hydrate)

[from\_dict](#from_dict)

[from\_local\_environ](#from_local_environ)

[from\_dotenv](#from_dotenv)

[from\_name](#from_name)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Tunnel
================================================================================

modal.Tunnel
============

```
class Tunnel(object)
```

A port forwarded from within a running Modal container. Created by
`modal.forward()`
.

**Important:**
This is an experimental API which may change in the future.

```
def __init__(self, host: str, port: int, unencrypted_host: str, unencrypted_port: int) -> None
```

url
---

```
@property
def url(self) -> str:
```

Get the public HTTPS URL of the forwarded port.

tls\_socket
-----------

```
@property
def tls_socket(self) -> tuple[str, int]:
```

Get the public TLS socket as a (host, port) tuple.

tcp\_socket
-----------

```
@property
def tcp_socket(self) -> tuple[str, int]:
```

Get the public TCP socket as a (host, port) tuple.

[modal.Tunnel](#modaltunnel)

[url](#url)

[tls\_socket](#tls_socket)

[tcp\_socket](#tcp_socket)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.Volume
================================================================================

modal.Volume
============

```
class Volume(modal.object.Object)
```

A writeable volume that can be used to share files between one or more Modal functions.

The contents of a volume is exposed as a filesystem. You can use it to share data between different functions, or
to persist durable state across several instances of the same function.

Unlike a networked filesystem, you need to explicitly reload the volume to see changes made since it was mounted.
Similarly, you need to explicitly commit any changes you make to the volume for the changes to become visible
outside the current container.

Concurrent modification is supported, but concurrent modifications of the same files should be avoided! Last write
wins in case of concurrent modification of the same file - any data the last writer didn’t have when committing
changes will be lost!

As a result, volumes are typically not a good fit for use cases where you need to make concurrent modifications to
the same file (nor is distributed file locking supported).

Volumes can only be reloaded if there are no open files for the volume - attempting to reload with open files
will result in an error.

**Usage**

```
import modal

app = modal.App()
volume = modal.Volume.from_name("my-persisted-volume", create_if_missing=True)

@app.function(volumes={"/root/foo": volume})
def f():
    with open("/root/foo/bar.txt", "w") as f:
        f.write("hello")
    volume.commit()  # Persist changes

@app.function(volumes={"/root/foo": volume})
def g():
    volume.reload()  # Fetch latest changes
    with open("/root/foo/bar.txt", "r") as f:
        print(f.read())
```

hydrate
-------

```
def hydrate(self, client: Optional[_Client] = None) -> Self:
```

Synchronize the local object with its identity on the Modal server.

It is rarely necessary to call this method explicitly, as most operations
will lazily hydrate when needed. The main use case is when you need to
access object metadata, such as its ID.

*Added in v0.72.39*
: This method replaces the deprecated
`.resolve()`
method.

read\_only
----------

```
def read_only(self) -> "_Volume":
```

Configure Volume to mount as read-only.

**Example**

```
import modal

volume = modal.Volume.from_name("my-volume", create_if_missing=True)

@app.function(volumes={"/mnt/items": volume.read_only()})
def f():
    with open("/mnt/items/my-file.txt") as f:
        return f.read()
```

The Volume is mounted as a read-only volume in a function. Any file system write operation into the
mounted volume will result in an error.

Added in v1.0.5.

from\_name
----------

```
@staticmethod
def from_name(
    name: str,
    *,
    environment_name: Optional[str] = None,
    create_if_missing: bool = False,
    version: "typing.Optional[modal_proto.api_pb2.VolumeFsVersion.ValueType]" = None,
) -> "_Volume":
```

Reference a Volume by name, creating if necessary.

This is a lazy method that defers hydrating the local
object with metadata from Modal servers until the first
time is is actually used.

```
vol = modal.Volume.from_name("my-volume", create_if_missing=True)

app = modal.App()

# Volume refers to the same object, even across instances of `app`.
@app.function(volumes={"/data": vol})
def f():
    pass
```

ephemeral
---------

```
@classmethod
@contextmanager
def ephemeral(
    cls: type["_Volume"],
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
    version: "typing.Optional[modal_proto.api_pb2.VolumeFsVersion.ValueType]" = None,
    _heartbeat_sleep: float = EPHEMERAL_OBJECT_HEARTBEAT_SLEEP,
) -> AsyncGenerator["_Volume", None]:
```

Creates a new ephemeral volume within a context manager:

Usage:

```
import modal
with modal.Volume.ephemeral() as vol:
    assert vol.listdir("/") == []
```

```
async with modal.Volume.ephemeral() as vol:
    assert await vol.listdir("/") == []
```

commit
------

```
@live_method
def commit(self):
```

Commit changes to the volume.

If successful, the changes made are now persisted in durable storage and available to other containers accessing
the volume.

reload
------

```
@live_method
def reload(self):
```

Make latest committed state of volume available in the running container.

Any uncommitted changes to the volume, such as new or modified files, may implicitly be committed when
reloading.

Reloading will fail if there are open files for the volume.

iterdir
-------

```
@live_method_gen
def iterdir(self, path: str, *, recursive: bool = True) -> Iterator[FileEntry]:
```

Iterate over all files in a directory in the volume.

Passing a directory path lists all files in the directory. For a file path, return only that
file’s description. If
`recursive`
is set to True, list all files and folders under the path
recursively.

listdir
-------

```
@live_method
def listdir(self, path: str, *, recursive: bool = False) -> list[FileEntry]:
```

List all files under a path prefix in the modal.Volume.

Passing a directory path lists all files in the directory. For a file path, return only that
file’s description. If
`recursive`
is set to True, list all files and folders under the path
recursively.

read\_file
----------

```
@live_method_gen
def read_file(self, path: str) -> Iterator[bytes]:
```

Read a file from the modal.Volume.

Note - this function is primarily intended to be used outside of a Modal App.
For more information on downloading files from a Modal Volume, see
[the guide](https://modal.com/docs/guide/volumes)

.

**Example:**

```
vol = modal.Volume.from_name("my-modal-volume")
data = b""
for chunk in vol.read_file("1mb.csv"):
    data += chunk
print(len(data))  # == 1024 * 1024
```

remove\_file
------------

```
@live_method
def remove_file(self, path: str, recursive: bool = False) -> None:
```

Remove a file or directory from a volume.

copy\_files
-----------

```
@live_method
def copy_files(self, src_paths: Sequence[str], dst_path: str, recursive: bool = False) -> None:
```

Copy files within the volume from src\_paths to dst\_path.
The semantics of the copy operation follow those of the UNIX cp command.

The
`src_paths`
parameter is a list. If you want to copy a single file, you should pass a list with a
single element.

`src_paths`
and
`dst_path`
should refer to the desired location
*inside*
the volume. You do not need to prepend
the volume mount path.

**Usage**

```
vol = modal.Volume.from_name("my-modal-volume")

vol.copy_files(["bar/example.txt"], "bar2")  # Copy files to another directory
vol.copy_files(["bar/example.txt"], "bar/example2.txt")  # Rename a file by copying
```

Note that if the volume is already mounted on the Modal function, you should use normal filesystem operations
like
`os.rename()`
and then
`commit()`
the volume. The
`copy_files()`
method is useful when you don’t have
the volume mounted as a filesystem, e.g. when running a script on your local computer.

batch\_upload
-------------

```
@live_method
def batch_upload(self, force: bool = False) -> "_AbstractVolumeUploadContextManager":
```

Initiate a batched upload to a volume.

To allow overwriting existing files, set
`force`
to
`True`
(you cannot overwrite existing directories with
uploaded files regardless).

**Example:**

```
vol = modal.Volume.from_name("my-modal-volume")

with vol.batch_upload() as batch:
    batch.put_file("local-path.txt", "/remote-path.txt")
    batch.put_directory("/local/directory/", "/remote/directory")
    batch.put_file(io.BytesIO(b"some data"), "/foobar")
```

delete
------

```
@staticmethod
def delete(name: str, client: Optional[_Client] = None, environment_name: Optional[str] = None):
```

rename
------

```
@staticmethod
def rename(
    old_name: str,
    new_name: str,
    *,
    client: Optional[_Client] = None,
    environment_name: Optional[str] = None,
):
```

[modal.Volume](#modalvolume)

[hydrate](#hydrate)

[read\_only](#read_only)

[from\_name](#from_name)

[ephemeral](#ephemeral)

[commit](#commit)

[reload](#reload)

[iterdir](#iterdir)

[listdir](#listdir)

[read\_file](#read_file)

[remove\_file](#remove_file)

[copy\_files](#copy_files)

[batch\_upload](#batch_upload)

[delete](#delete)

[rename](#rename)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.asgi_app
================================================================================

modal.asgi\_app
===============

```
def asgi_app(
    _warn_parentheses_missing=None,
    *,
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[[Union[_PartialFunction, NullaryFuncOrMethod]], _PartialFunction]:
```

Decorator for registering an ASGI app with a Modal function.

Asynchronous Server Gateway Interface (ASGI) is a standard for Python
synchronous and asynchronous apps, supported by all popular Python web
libraries. This is an advanced decorator that gives full flexibility in
defining one or more web endpoints on Modal.

**Usage:**

```
from typing import Callable

@app.function()
@modal.asgi_app()
def create_asgi() -> Callable:
    ...
```

To learn how to use Modal with popular web frameworks, see the
[guide on web endpoints](https://modal.com/docs/guide/webhooks)

.

[modal.asgi\_app](#modalasgi_app)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.batched
================================================================================

modal.batched
=============

```
def batched(
    _warn_parentheses_missing=None,
    *,
    max_batch_size: int,
    wait_ms: int,
) -> Callable[
    [Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
```

Decorator for functions or class methods that should be batched.

**Usage**

```
# Stack the decorator under `@app.function()` to enable dynamic batching
@app.function()
@modal.batched(max_batch_size=4, wait_ms=1000)
async def batched_multiply(xs: list[int], ys: list[int]) -> list[int]:
    return [x * y for x, y in zip(xs, ys)]

# call batched_multiply with individual inputs
# batched_multiply.remote.aio(2, 100)

# With `@app.cls()`, apply the decorator to a method (this may change in the future)
@app.cls()
class BatchedClass:
    @modal.batched(max_batch_size=4, wait_ms=1000)
    def batched_multiply(self, xs: list[int], ys: list[int]) -> list[int]:
        return [x * y for x, y in zip(xs, ys)]
```

See the
[dynamic batching guide](https://modal.com/docs/guide/dynamic-batching)

for more information.

[modal.batched](#modalbatched)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.call_graph
================================================================================

modal.call\_graph
=================

modal.call\_graph.InputInfo
---------------------------

```
class InputInfo(object)
```

Simple data structure storing information about a function input.

```
def __init__(self, input_id: str, function_call_id: str, task_id: str, status: modal.call_graph.InputStatus, function_name: str, module_name: str, children: list['InputInfo']) -> None
```

modal.call\_graph.InputStatus
-----------------------------

```
class InputStatus(enum.IntEnum)
```

Enum representing status of a function input.

The possible values are:

* `PENDING`
* `SUCCESS`
* `FAILURE`
* `INIT_FAILURE`
* `TERMINATED`
* `TIMEOUT`

[modal.call\_graph](#modalcall_graph)

[modal.call\_graph.InputInfo](#modalcall_graphinputinfo)

[modal.call\_graph.InputStatus](#modalcall_graphinputstatus)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.concurrent
================================================================================

modal.concurrent
================

```
def concurrent(
    _warn_parentheses_missing=None,
    *,
    max_inputs: int,  # Hard limit on each container's input concurrency
    target_inputs: Optional[int] = None,  # Input concurrency that Modal's autoscaler should target
) -> Callable[
    [Union[Callable[P, ReturnType], _PartialFunction[P, ReturnType, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
```

Decorator that allows individual containers to handle multiple inputs concurrently.

The concurrency mechanism depends on whether the function is async or not:

* Async functions will run inputs on a single thread as asyncio tasks.
* Synchronous functions will use multi-threading. The code must be thread-safe.

Input concurrency will be most useful for workflows that are IO-bound
(e.g., making network requests) or when running an inference server that supports
dynamic batching.

When
`target_inputs`
is set, Modal’s autoscaler will try to provision resources
such that each container is running that many inputs concurrently, rather than
autoscaling based on
`max_inputs`
. Containers may burst up to up to
`max_inputs`
if resources are insufficient to remain at the target concurrency, e.g. when the
arrival rate of inputs increases. This can trade-off a small increase in average
latency to avoid larger tail latencies from input queuing.

**Examples:**

```
# Stack the decorator under `@app.function()` to enable input concurrency
@app.function()
@modal.concurrent(max_inputs=100)
async def f(data):
    # Async function; will be scheduled as asyncio task
    ...

# With `@app.cls()`, apply the decorator at the class level, not on individual methods
@app.cls()
@modal.concurrent(max_inputs=100, target_inputs=80)
class C:
    @modal.method()
    def f(self, data):
        # Sync function; must be thread-safe
        ...
```

*Added in v0.73.148:*
This decorator replaces the
`allow_concurrent_inputs`
parameter
in
`@app.function()`
and
`@app.cls()`
.

[modal.concurrent](#modalconcurrent)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.config
================================================================================

modal.config
============

Modal intentionally keeps configurability to a minimum.

The main configuration options are the API tokens: the token id and the token secret.
These can be configured in two ways:

1. By running the
   `modal token set`
   command.
   This writes the tokens to
   `.modal.toml`
   file in your home directory.
2. By setting the environment variables
   `MODAL_TOKEN_ID`
   and
   `MODAL_TOKEN_SECRET`
   .
   This takes precedence over the previous method.

.modal.toml
-----------

The
`.modal.toml`
file is generally stored in your home directory.
It should look like this::

```
[default]
token_id = "ak-12345..."
token_secret = "as-12345..."
```

You can create this file manually, or you can run the
`modal token set ...`
command (see below).

Setting tokens using the CLI
----------------------------

You can set a token by running the command::

```
modal token set \
  --token-id <token id> \
  --token-secret <token secret>
```

This will write the token id and secret to
`.modal.toml`
.

If the token id or secret is provided as the string
`-`
(a single dash),
then it will be read in a secret way from stdin instead.

Other configuration options
---------------------------

Other possible configuration options are:

* `loglevel`
  (in the .toml file) /
  `MODAL_LOGLEVEL`
  (as an env var).
  Defaults to
  `WARNING`
  . Set this to
  `DEBUG`
  to see internal messages.
* `logs_timeout`
  (in the .toml file) /
  `MODAL_LOGS_TIMEOUT`
  (as an env var).
  Defaults to 10.
  Number of seconds to wait for logs to drain when closing the session,
  before giving up.
* `force_build`
  (in the .toml file) /
  `MODAL_FORCE_BUILD`
  (as an env var).
  Defaults to False.
  When set, ignores the Image cache and builds all Image layers. Note that this
  will break the cache for all images based on the rebuilt layers, so other images
  may rebuild on subsequent runs / deploys even if the config is reverted.
* `ignore_cache`
  (in the .toml file) /
  `MODAL_IGNORE_CACHE`
  (as an env var).
  Defaults to False.
  When set, ignores the Image cache and builds all Image layers. Unlike
  `force_build`
  ,
  this will not overwrite the cache for other images that have the same recipe.
  Subsequent runs that do not use this option will pull the
  *previous*
  Image from
  the cache, if one exists. It can be useful for testing an App’s robustness to
  Image rebuilds without clobbering Images used by other Apps.
* `traceback`
  (in the .toml file) /
  `MODAL_TRACEBACK`
  (as an env var).
  Defaults to False. Enables printing full tracebacks on unexpected CLI
  errors, which can be useful for debugging client issues.
* `log_pattern`
  (in the .toml file) / MODAL\_LOG\_PATTERN` (as an env var).
  Defaults to ”[modal-client] %(asctime)s %(message)s”
  The log formatting pattern that will be used by the modal client itself.
  See
  <https://docs.python.org/3/library/logging.html#logrecord-attributes>

  for available
  log attributes.

Meta-configuration
------------------

Some “meta-options” are set using environment variables only:

* `MODAL_CONFIG_PATH`
  lets you override the location of the .toml file,
  by default
  `~/.modal.toml`
  .
* `MODAL_PROFILE`
  lets you use multiple sections in the .toml file
  and switch between them. It defaults to “default”.

modal.config.Config
-------------------

```
class Config(object)
```

Singleton that holds configuration used by Modal internally.

```
def __init__(self):
```

### get

```
def get(self, key, profile=None, use_env=True):
```

Looks up a configuration value.

Will check (in decreasing order of priority):

1. Any environment variable of the form MODAL\_FOO\_BAR (when use\_env is True)
2. Settings in the user’s .toml configuration file
3. The default value of the setting

### override\_locally

```
def override_locally(self, key: str, value: str):
    # Override setting in this process by overriding environment variable for the setting
    #
    # Does NOT write back to settings file etc.
```

### to\_dict

```
def to_dict(self):
```

modal.config.config\_profiles
-----------------------------

```
def config_profiles():
```

List the available modal profiles in the .modal.toml file.

modal.config.config\_set\_active\_profile
-----------------------------------------

```
def config_set_active_profile(env: str) -> None:
```

Set the user’s active modal profile by writing it to the
`.modal.toml`
file.

[modal.config](#modalconfig)

[.modal.toml](#modaltoml)

[Setting tokens using the CLI](#setting-tokens-using-the-cli)

[Other configuration options](#other-configuration-options)

[Meta-configuration](#meta-configuration)

[modal.config.Config](#modalconfigconfig)

[get](#get)

[override\_locally](#override_locally)

[to\_dict](#to_dict)

[modal.config.config\_profiles](#modalconfigconfig_profiles)

[modal.config.config\_set\_active\_profile](#modalconfigconfig_set_active_profile)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.container_process
================================================================================

modal.container\_process
========================

modal.container\_process.ContainerProcess
-----------------------------------------

```
class ContainerProcess(typing.Generic)
```

```
def __init__(
    self,
    process_id: str,
    client: _Client,
    stdout: StreamType = StreamType.PIPE,
    stderr: StreamType = StreamType.PIPE,
    exec_deadline: Optional[float] = None,
    text: bool = True,
    by_line: bool = False,
) -> None:
```

### stdout

```
@property
def stdout(self) -> _StreamReader[T]:
```

StreamReader for the container process’s stdout stream.

### stderr

```
@property
def stderr(self) -> _StreamReader[T]:
```

StreamReader for the container process’s stderr stream.

### stdin

```
@property
def stdin(self) -> _StreamWriter:
```

StreamWriter for the container process’s stdin stream.

### returncode

```
@property
def returncode(self) -> int:
```

### poll

```
def poll(self) -> Optional[int]:
```

Check if the container process has finished running.

Returns
`None`
if the process is still running, else returns the exit code.

### wait

```
def wait(self) -> int:
```

Wait for the container process to finish running. Returns the exit code.

[modal.container\_process](#modalcontainer_process)

[modal.container\_process.ContainerProcess](#modalcontainer_processcontainerprocess)

[stdout](#stdout)

[stderr](#stderr)

[stdin](#stdin)

[returncode](#returncode)

[poll](#poll)

[wait](#wait)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.current_function_call_id
================================================================================

modal.current\_function\_call\_id
=================================

```
def current_function_call_id() -> Optional[str]:
```

Returns the function call ID for the current input.

Can only be called from Modal function (i.e. in a container context).

```
from modal import current_function_call_id

@app.function()
def process_stuff():
    print(f"Starting to process input from {current_function_call_id()}")
```

[modal.current\_function\_call\_id](#modalcurrent_function_call_id)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.current_input_id
================================================================================

modal.current\_input\_id
========================

```
def current_input_id() -> Optional[str]:
```

Returns the input ID for the current input.

Can only be called from Modal function (i.e. in a container context).

```
from modal import current_input_id

@app.function()
def process_stuff():
    print(f"Starting to process {current_input_id()}")
```

[modal.current\_input\_id](#modalcurrent_input_id)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.enable_output
================================================================================

modal.enable\_output
====================

```
@contextlib.contextmanager
def enable_output(show_progress: bool = True) -> Generator[None, None, None]:
```

Context manager that enable output when using the Python SDK.

This will print to stdout and stderr things such as

1. Logs from running functions
2. Status of creating objects
3. Map progress

Example:

```
app = modal.App()
with modal.enable_output():
    with app.run():
        ...
```

[modal.enable\_output](#modalenable_output)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.enter
================================================================================

modal.enter
===========

```
def enter(
    _warn_parentheses_missing=None,
    *,
    snap: bool = False,
) -> Callable[[Union[_PartialFunction, NullaryMethod]], _PartialFunction]:
```

Decorator for methods which should be executed when a new container is started.

See the
[lifeycle function guide](https://modal.com/docs/guide/lifecycle-functions#enter)

for more information.

[modal.enter](#modalenter)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.exception
================================================================================

modal.exception
===============

modal.exception.AuthError
-------------------------

```
class AuthError(modal.exception.Error)
```

Raised when a client has missing or invalid authentication.

modal.exception.ClientClosed
----------------------------

```
class ClientClosed(modal.exception.Error)
```

modal.exception.ConnectionError
-------------------------------

```
class ConnectionError(modal.exception.Error)
```

Raised when an issue occurs while connecting to the Modal servers.

modal.exception.DeprecationError
--------------------------------

```
class DeprecationError(UserWarning)
```

UserWarning category emitted when a deprecated Modal feature or API is used.

modal.exception.DeserializationError
------------------------------------

```
class DeserializationError(modal.exception.Error)
```

Raised to provide more context when an error is encountered during deserialization.

modal.exception.ExecutionError
------------------------------

```
class ExecutionError(modal.exception.Error)
```

Raised when something unexpected happened during runtime.

modal.exception.FilesystemExecutionError
----------------------------------------

```
class FilesystemExecutionError(modal.exception.Error)
```

Raised when an unknown error is thrown during a container filesystem operation.

modal.exception.FunctionTimeoutError
------------------------------------

```
class FunctionTimeoutError(modal.exception.TimeoutError)
```

Raised when a Function exceeds its execution duration limit and times out.

modal.exception.InputCancellation
---------------------------------

```
class InputCancellation(BaseException)
```

Raised when the current input is cancelled by the task

Intentionally a BaseException instead of an Exception, so it won’t get
caught by unspecified user exception clauses that might be used for retries and
other control flow.

modal.exception.InteractiveTimeoutError
---------------------------------------

```
class InteractiveTimeoutError(modal.exception.TimeoutError)
```

Raised when interactive frontends time out while trying to connect to a container.

modal.exception.InternalFailure
-------------------------------

```
class InternalFailure(modal.exception.Error)
```

Retriable internal error.

modal.exception.InvalidError
----------------------------

```
class InvalidError(modal.exception.Error)
```

Raised when user does something invalid.

modal.exception.ModuleNotMountable
----------------------------------

```
class ModuleNotMountable(Exception)
```

modal.exception.MountUploadTimeoutError
---------------------------------------

```
class MountUploadTimeoutError(modal.exception.TimeoutError)
```

Raised when a Mount upload times out.

modal.exception.NotFoundError
-----------------------------

```
class NotFoundError(modal.exception.Error)
```

Raised when a requested resource was not found.

modal.exception.OutputExpiredError
----------------------------------

```
class OutputExpiredError(modal.exception.TimeoutError)
```

Raised when the Output exceeds expiration and times out.

modal.exception.PendingDeprecationError
---------------------------------------

```
class PendingDeprecationError(UserWarning)
```

Soon to be deprecated feature. Only used intermittently because of multi-repo concerns.

modal.exception.RemoteError
---------------------------

```
class RemoteError(modal.exception.Error)
```

Raised when an error occurs on the Modal server.

modal.exception.RequestSizeError
--------------------------------

```
class RequestSizeError(modal.exception.Error)
```

Raised when an operation produces a gRPC request that is rejected by the server for being too large.

modal.exception.SandboxTerminatedError
--------------------------------------

```
class SandboxTerminatedError(modal.exception.Error)
```

Raised when a Sandbox is terminated for an internal reason.

modal.exception.SandboxTimeoutError
-----------------------------------

```
class SandboxTimeoutError(modal.exception.TimeoutError)
```

Raised when a Sandbox exceeds its execution duration limit and times out.

modal.exception.SerializationError
----------------------------------

```
class SerializationError(modal.exception.Error)
```

Raised to provide more context when an error is encountered during serialization.

modal.exception.ServerWarning
-----------------------------

```
class ServerWarning(UserWarning)
```

Warning originating from the Modal server and re-issued in client code.

modal.exception.TimeoutError
----------------------------

```
class TimeoutError(modal.exception.Error)
```

Base class for Modal timeouts.

modal.exception.VersionError
----------------------------

```
class VersionError(modal.exception.Error)
```

Raised when the current client version of Modal is unsupported.

modal.exception.VolumeUploadTimeoutError
----------------------------------------

```
class VolumeUploadTimeoutError(modal.exception.TimeoutError)
```

Raised when a Volume upload times out.

modal.exception.simulate\_preemption
------------------------------------

```
def simulate_preemption(wait_seconds: int, jitter_seconds: int = 0):
```

Utility for simulating a preemption interrupt after
`wait_seconds`
seconds.
The first interrupt is the SIGINT signal. After 30 seconds, a second
interrupt will trigger.

This second interrupt simulates SIGKILL, and should not be caught.
Optionally add between zero and
`jitter_seconds`
seconds of additional waiting before first interrupt.

**Usage:**

```
import time
from modal.exception import simulate_preemption

simulate_preemption(3)

try:
    time.sleep(4)
except KeyboardInterrupt:
    print("got preempted") # Handle interrupt
    raise
```

See
<https://modal.com/docs/guide/preemption>

for more details on preemption
handling.

[modal.exception](#modalexception)

[modal.exception.AuthError](#modalexceptionautherror)

[modal.exception.ClientClosed](#modalexceptionclientclosed)

[modal.exception.ConnectionError](#modalexceptionconnectionerror)

[modal.exception.DeprecationError](#modalexceptiondeprecationerror)

[modal.exception.DeserializationError](#modalexceptiondeserializationerror)

[modal.exception.ExecutionError](#modalexceptionexecutionerror)

[modal.exception.FilesystemExecutionError](#modalexceptionfilesystemexecutionerror)

[modal.exception.FunctionTimeoutError](#modalexceptionfunctiontimeouterror)

[modal.exception.InputCancellation](#modalexceptioninputcancellation)

[modal.exception.InteractiveTimeoutError](#modalexceptioninteractivetimeouterror)

[modal.exception.InternalFailure](#modalexceptioninternalfailure)

[modal.exception.InvalidError](#modalexceptioninvaliderror)

[modal.exception.ModuleNotMountable](#modalexceptionmodulenotmountable)

[modal.exception.MountUploadTimeoutError](#modalexceptionmountuploadtimeouterror)

[modal.exception.NotFoundError](#modalexceptionnotfounderror)

[modal.exception.OutputExpiredError](#modalexceptionoutputexpirederror)

[modal.exception.PendingDeprecationError](#modalexceptionpendingdeprecationerror)

[modal.exception.RemoteError](#modalexceptionremoteerror)

[modal.exception.RequestSizeError](#modalexceptionrequestsizeerror)

[modal.exception.SandboxTerminatedError](#modalexceptionsandboxterminatederror)

[modal.exception.SandboxTimeoutError](#modalexceptionsandboxtimeouterror)

[modal.exception.SerializationError](#modalexceptionserializationerror)

[modal.exception.ServerWarning](#modalexceptionserverwarning)

[modal.exception.TimeoutError](#modalexceptiontimeouterror)

[modal.exception.VersionError](#modalexceptionversionerror)

[modal.exception.VolumeUploadTimeoutError](#modalexceptionvolumeuploadtimeouterror)

[modal.exception.simulate\_preemption](#modalexceptionsimulate_preemption)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.exit
================================================================================

modal.exit
==========

```
def exit(_warn_parentheses_missing=None) -> Callable[[NullaryMethod], _PartialFunction]:
```

Decorator for methods which should be executed when a container is about to exit.

See the
[lifeycle function guide](https://modal.com/docs/guide/lifecycle-functions#exit)

for more information.

[modal.exit](#modalexit)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.fastapi_endpoint
================================================================================

modal.fastapi\_endpoint
=======================

```
def fastapi_endpoint(
    _warn_parentheses_missing=None,
    *,
    method: str = "GET",  # REST method for the created endpoint.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Custom fully-qualified domain name (FQDN) for the endpoint.
    docs: bool = False,  # Whether to enable interactive documentation for this endpoint at /docs.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[
    [Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
```

Convert a function into a basic web endpoint by wrapping it with a FastAPI App.

Modal will internally use
[FastAPI](https://fastapi.tiangolo.com/)

to expose a
simple, single request handler. If you are defining your own
`FastAPI`
application
(e.g. if you want to define multiple routes), use
`@modal.asgi_app`
instead.

The endpoint created with this decorator will automatically have
[CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)

enabled
and can leverage many of FastAPI’s features.

For more information on using Modal with popular web frameworks, see our
[guide on web endpoints](https://modal.com/docs/guide/webhooks)

.

*Added in v0.73.82*
: This function replaces the deprecated
`@web_endpoint`
decorator.

[modal.fastapi\_endpoint](#modalfastapi_endpoint)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.file_io
================================================================================

modal.file\_io
==============

modal.file\_io.FileIO
---------------------

```
class FileIO(typing.Generic)
```

[Alpha] FileIO handle, used in the Sandbox filesystem API.

The API is designed to mimic Python’s io.FileIO.

Currently this API is in Alpha and is subject to change. File I/O operations
may be limited in size to 100 MiB, and the throughput of requests is
restricted in the current implementation. For our recommendations on large file transfers
see the Sandbox
[filesystem access guide](https://modal.com/docs/guide/sandbox-files)

.

**Usage**

```
import modal

app = modal.App.lookup("my-app", create_if_missing=True)

sb = modal.Sandbox.create(app=app)
f = sb.open("/tmp/foo.txt", "w")
f.write("hello")
f.close()
```

```
def __init__(self, client: _Client, task_id: str) -> None:
```

### create

```
@classmethod
def create(
    cls, path: str, mode: Union["_typeshed.OpenTextMode", "_typeshed.OpenBinaryMode"], client: _Client, task_id: str
) -> "_FileIO":
```

Create a new FileIO handle.

### read

```
def read(self, n: Optional[int] = None) -> T:
```

Read n bytes from the current position, or the entire remaining file if n is None.

### readline

```
def readline(self) -> T:
```

Read a single line from the current position.

### readlines

```
def readlines(self) -> Sequence[T]:
```

Read all lines from the current position.

### write

```
def write(self, data: Union[bytes, str]) -> None:
```

Write data to the current position.

Writes may not appear until the entire buffer is flushed, which
can be done manually with
`flush()`
or automatically when the file is
closed.

### flush

```
def flush(self) -> None:
```

Flush the buffer to disk.

### seek

```
def seek(self, offset: int, whence: int = 0) -> None:
```

Move to a new position in the file.

`whence`
defaults to 0 (absolute file positioning); other values are 1
(relative to the current position) and 2 (relative to the file’s end).

### ls

```
@classmethod
def ls(cls, path: str, client: _Client, task_id: str) -> list[str]:
```

List the contents of the provided directory.

### mkdir

```
@classmethod
def mkdir(cls, path: str, client: _Client, task_id: str, parents: bool = False) -> None:
```

Create a new directory.

### rm

```
@classmethod
def rm(cls, path: str, client: _Client, task_id: str, recursive: bool = False) -> None:
```

Remove a file or directory in the Sandbox.

### watch

```
@classmethod
def watch(
    cls,
    path: str,
    client: _Client,
    task_id: str,
    filter: Optional[list[FileWatchEventType]] = None,
    recursive: bool = False,
    timeout: Optional[int] = None,
) -> Iterator[FileWatchEvent]:
```

### close

```
def close(self) -> None:
```

Flush the buffer and close the file.

modal.file\_io.FileWatchEvent
-----------------------------

```
class FileWatchEvent(object)
```

FileWatchEvent(paths: list[str], type: modal.file\_io.FileWatchEventType)

```
def __init__(self, paths: list[str], type: modal.file_io.FileWatchEventType) -> None
```

modal.file\_io.FileWatchEventType
---------------------------------

```
class FileWatchEventType(enum.Enum)
```

An enumeration.

The possible values are:

* `Unknown`
* `Access`
* `Create`
* `Modify`
* `Remove`

modal.file\_io.delete\_bytes
----------------------------

```
async def delete_bytes(file: "_FileIO", start: Optional[int] = None, end: Optional[int] = None) -> None:
```

Delete a range of bytes from the file.

`start`
and
`end`
are byte offsets.
`start`
is inclusive,
`end`
is exclusive.
If either is None, the start or end of the file is used, respectively.

modal.file\_io.replace\_bytes
-----------------------------

```
async def replace_bytes(file: "_FileIO", data: bytes, start: Optional[int] = None, end: Optional[int] = None) -> None:
```

Replace a range of bytes in the file with new data. The length of the data does not
have to be the same as the length of the range being replaced.

`start`
and
`end`
are byte offsets.
`start`
is inclusive,
`end`
is exclusive.
If either is None, the start or end of the file is used, respectively.

[modal.file\_io](#modalfile_io)

[modal.file\_io.FileIO](#modalfile_iofileio)

[create](#create)

[read](#read)

[readline](#readline)

[readlines](#readlines)

[write](#write)

[flush](#flush)

[seek](#seek)

[ls](#ls)

[mkdir](#mkdir)

[rm](#rm)

[watch](#watch)

[close](#close)

[modal.file\_io.FileWatchEvent](#modalfile_iofilewatchevent)

[modal.file\_io.FileWatchEventType](#modalfile_iofilewatcheventtype)

[modal.file\_io.delete\_bytes](#modalfile_iodelete_bytes)

[modal.file\_io.replace\_bytes](#modalfile_ioreplace_bytes)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.forward
================================================================================

modal.forward
=============

```
@contextmanager
def forward(port: int, *, unencrypted: bool = False, client: Optional[_Client] = None) -> Iterator[Tunnel]:
```

Expose a port publicly from inside a running Modal container, with TLS.

If
`unencrypted`
is set, this also exposes the TCP socket without encryption on a random port
number. This can be used to SSH into a container (see example below). Note that it is on the public Internet, so
make sure you are using a secure protocol over TCP.

**Important:**
This is an experimental API which may change in the future.

**Usage:**

```
import modal
from flask import Flask

app = modal.App(image=modal.Image.debian_slim().pip_install("Flask"))
flask_app = Flask(__name__)

@flask_app.route("/")
def hello_world():
    return "Hello, World!"

@app.function()
def run_app():
    # Start a web server inside the container at port 8000. `modal.forward(8000)` lets us
    # expose that port to the world at a random HTTPS URL.
    with modal.forward(8000) as tunnel:
        print("Server listening at", tunnel.url)
        flask_app.run("0.0.0.0", 8000)

    # When the context manager exits, the port is no longer exposed.
```

**Raw TCP usage:**

```
import socket
import threading

import modal

def run_echo_server(port: int):
    """Run a TCP echo server listening on the given port."""
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.bind(("0.0.0.0", port))
    sock.listen(1)

    while True:
        conn, addr = sock.accept()
        print("Connection from:", addr)

        # Start a new thread to handle the connection
        def handle(conn):
            with conn:
                while True:
                    data = conn.recv(1024)
                    if not data:
                        break
                    conn.sendall(data)

        threading.Thread(target=handle, args=(conn,)).start()

app = modal.App()

@app.function()
def tcp_tunnel():
    # This exposes port 8000 to public Internet traffic over TCP.
    with modal.forward(8000, unencrypted=True) as tunnel:
        # You can connect to this TCP socket from outside the container, for example, using `nc`:
        #  nc <HOST> <PORT>
        print("TCP tunnel listening at:", tunnel.tcp_socket)
        run_echo_server(8000)
```

**SSH example:**
This assumes you have a rsa keypair in
`~/.ssh/id_rsa{.pub}`
, this is a bare-bones example
letting you SSH into a Modal container.

```
import subprocess
import time

import modal

app = modal.App()
image = (
    modal.Image.debian_slim()
    .apt_install("openssh-server")
    .run_commands("mkdir /run/sshd")
    .add_local_file("~/.ssh/id_rsa.pub", "/root/.ssh/authorized_keys", copy=True)
)

@app.function(image=image, timeout=3600)
def some_function():
    subprocess.Popen(["/usr/sbin/sshd", "-D", "-e"])
    with modal.forward(port=22, unencrypted=True) as tunnel:
        hostname, port = tunnel.tcp_socket
        connection_cmd = f'ssh -p {port} root@{hostname}'
        print(f"ssh into container using: {connection_cmd}")
        time.sleep(3600)  # keep alive for 1 hour or until killed
```

If you intend to use this more generally, a suggestion is to put the subprocess and port
forwarding code in an
`@enter`
lifecycle method of an @app.cls, to only make a single
ssh server and port for each container (and not one for each input to the function).

[modal.forward](#modalforward)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.gpu
================================================================================

modal.gpu
=========

**GPU configuration shortcodes**

You can pass a wide range of
`str`
values for the
`gpu`
parameter of
[`@app.function`](https://modal.com/docs/reference/modal.App#function)

.

For instance:

* `gpu="H100"`
  will attach 1 H100 GPU to each container
* `gpu="L40S"`
  will attach 1 L40S GPU to each container
* `gpu="T4:4"`
  will attach 4 T4 GPUs to each container

You can see a list of Modal GPU options in the
[GPU docs](https://modal.com/docs/guide/gpu)

.

**Example**

```
@app.function(gpu="A100-80GB:4")
def my_gpu_function():
    ... # This will have 4 A100-80GB with each container
```

**Deprecation notes**

An older deprecated way to configure GPU is also still supported,
but will be removed in future versions of Modal. Examples:

* `gpu=modal.gpu.H100()`
  will attach 1 H100 GPU to each container
* `gpu=modal.gpu.T4(count=4)`
  will attach 4 T4 GPUs to each container
* `gpu=modal.gpu.A100()`
  will attach 1 A100-40GB GPUs to each container
* `gpu=modal.gpu.A100(size="80GB")`
  will attach 1 A100-80GB GPUs to each container

modal.gpu.A100
--------------

```
class A100(modal.gpu._GPUConfig)
```

[NVIDIA A100 Tensor Core](https://www.nvidia.com/en-us/data-center/a100/)

GPU class.

The flagship data center GPU of the Ampere architecture. Available in 40GB and 80GB GPU memory configurations.

```
def __init__(
    self,
    *,
    count: int = 1,  # Number of GPUs per container. Defaults to 1.
    size: Union[str, None] = None,  # Select GB configuration of GPU device: "40GB" or "80GB". Defaults to "40GB".
):
```

modal.gpu.A10G
--------------

```
class A10G(modal.gpu._GPUConfig)
```

[NVIDIA A10G Tensor Core](https://www.nvidia.com/en-us/data-center/products/a10-gpu/)

GPU class.

A mid-tier data center GPU based on the Ampere architecture, providing 24 GB of memory.
A10G GPUs deliver up to 3.3x better ML training performance, 3x better ML inference performance,
and 3x better graphics performance, in comparison to NVIDIA T4 GPUs.

```
def __init__(
    self,
    *,
    # Number of GPUs per container. Defaults to 1.
    # Useful if you have very large models that don't fit on a single GPU.
    count: int = 1,
):
```

modal.gpu.Any
-------------

```
class Any(modal.gpu._GPUConfig)
```

Selects any one of the GPU classes available within Modal, according to availability.

```
def __init__(self, *, count: int = 1):
```

modal.gpu.H100
--------------

```
class H100(modal.gpu._GPUConfig)
```

[NVIDIA H100 Tensor Core](https://www.nvidia.com/en-us/data-center/h100/)

GPU class.

The flagship data center GPU of the Hopper architecture.
Enhanced support for FP8 precision and a Transformer Engine that provides up to 4X faster training
over the prior generation for GPT-3 (175B) models.

```
def __init__(
    self,
    *,
    # Number of GPUs per container. Defaults to 1.
    # Useful if you have very large models that don't fit on a single GPU.
    count: int = 1,
):
```

modal.gpu.L4
------------

```
class L4(modal.gpu._GPUConfig)
```

[NVIDIA L4 Tensor Core](https://www.nvidia.com/en-us/data-center/l4/)

GPU class.

A mid-tier data center GPU based on the Ada Lovelace architecture, providing 24GB of GPU memory.
Includes RTX (ray tracing) support.

```
def __init__(
    self,
    count: int = 1,  # Number of GPUs per container. Defaults to 1.
):
```

modal.gpu.L40S
--------------

```
class L40S(modal.gpu._GPUConfig)
```

[NVIDIA L40S](https://www.nvidia.com/en-us/data-center/l40s/)

GPU class.

The L40S is a data center GPU for the Ada Lovelace architecture. It has 48 GB of on-chip
GDDR6 RAM and enhanced support for FP8 precision.

```
def __init__(
    self,
    *,
    # Number of GPUs per container. Defaults to 1.
    # Useful if you have very large models that don't fit on a single GPU.
    count: int = 1,
):
```

modal.gpu.T4
------------

```
class T4(modal.gpu._GPUConfig)
```

[NVIDIA T4 Tensor Core](https://www.nvidia.com/en-us/data-center/tesla-t4/)

GPU class.

A low-cost data center GPU based on the Turing architecture, providing 16GB of GPU memory.

```
def __init__(
    self,
    count: int = 1,  # Number of GPUs per container. Defaults to 1.
):
```

modal.gpu.parse\_gpu\_config
----------------------------

```
def parse_gpu_config(value: GPU_T) -> api_pb2.GPUConfig:
```

[modal.gpu](#modalgpu)

[modal.gpu.A100](#modalgpua100)

[modal.gpu.A10G](#modalgpua10g)

[modal.gpu.Any](#modalgpuany)

[modal.gpu.H100](#modalgpuh100)

[modal.gpu.L4](#modalgpul4)

[modal.gpu.L40S](#modalgpul40s)

[modal.gpu.T4](#modalgput4)

[modal.gpu.parse\_gpu\_config](#modalgpuparse_gpu_config)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.interact
================================================================================

modal.interact
==============

```
def interact() -> None:
```

Enable interactivity with user input inside a Modal container.

See the
[interactivity guide](https://modal.com/docs/guide/developing-debugging#interactivity)

for more information on how to use this function.

[modal.interact](#modalinteract)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.io_streams
================================================================================

modal.io\_streams
=================

modal.io\_streams.StreamReader
------------------------------

```
class StreamReader(typing.Generic)
```

Retrieve logs from a stream (
`stdout`
or
`stderr`
).

As an asynchronous iterable, the object supports the
`for`
and
`async for`
statements. Just loop over the object to read in chunks.

**Usage**

```
from modal import Sandbox

sandbox = Sandbox.create(
    "bash",
    "-c",
    "for i in $(seq 1 10); do echo foo; sleep 0.1; done",
    app=running_app,
)
for message in sandbox.stdout:
    print(f"Message: {message}")
```

### file\_descriptor

```
@property
def file_descriptor(self) -> int:
```

Possible values are
`1`
for stdout and
`2`
for stderr.

### read

```
def read(self) -> T:
```

Fetch the entire contents of the stream until EOF.

**Usage**

```
from modal import Sandbox

sandbox = Sandbox.create("echo", "hello", app=running_app)
sandbox.wait()

print(sandbox.stdout.read())
```

modal.io\_streams.StreamWriter
------------------------------

```
class StreamWriter(object)
```

Provides an interface to buffer and write logs to a sandbox or container process stream (
`stdin`
).

### write

```
def write(self, data: Union[bytes, bytearray, memoryview, str]) -> None:
```

Write data to the stream but does not send it immediately.

This is non-blocking and queues the data to an internal buffer. Must be
used along with the
`drain()`
method, which flushes the buffer.

**Usage**

```
from modal import Sandbox

sandbox = Sandbox.create(
    "bash",
    "-c",
    "while read line; do echo $line; done",
    app=running_app,
)
sandbox.stdin.write(b"foo\n")
sandbox.stdin.write(b"bar\n")
sandbox.stdin.write_eof()

sandbox.stdin.drain()
sandbox.wait()
```

### write\_eof

```
def write_eof(self) -> None:
```

Close the write end of the stream after the buffered data is drained.

If the process was blocked on input, it will become unblocked after
`write_eof()`
. This method needs to be used along with the
`drain()`
method, which flushes the EOF to the process.

### drain

```
def drain(self) -> None:
```

Flush the write buffer and send data to the running process.

This is a flow control method that blocks until data is sent. It returns
when it is appropriate to continue writing data to the stream.

**Usage**

```
writer.write(data)
writer.drain()
```

Async usage:

```
writer.write(data)  # not a blocking operation
await writer.drain.aio()
```

[modal.io\_streams](#modalio_streams)

[modal.io\_streams.StreamReader](#modalio_streamsstreamreader)

[file\_descriptor](#file_descriptor)

[read](#read)

[modal.io\_streams.StreamWriter](#modalio_streamsstreamwriter)

[write](#write)

[write\_eof](#write_eof)

[drain](#drain)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.is_local
================================================================================

modal.is\_local
===============

```
def is_local() -> bool:
```

Returns if we are currently on the machine launching/deploying a Modal app

Returns
`True`
when executed locally on the user’s machine.
Returns
`False`
when executed from a Modal container in the cloud.

[modal.is\_local](#modalis_local)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.method
================================================================================

modal.method
============

```
def method(
    _warn_parentheses_missing=None,
    *,
    # Set this to True if it's a non-generator function returning
    # a [sync/async] generator object
    is_generator: Optional[bool] = None,
) -> _MethodDecoratorType:
```

Decorator for methods that should be transformed into a Modal Function registered against this class’s App.

**Usage:**

```
@app.cls(cpu=8)
class MyCls:

    @modal.method()
    def f(self):
        ...
```

[modal.method](#modalmethod)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.parameter
================================================================================

modal.parameter
===============

```
def parameter(*, default: Any = _no_default, init: bool = True) -> Any:
```

Used to specify options for modal.cls parameters, similar to dataclass.field for dataclasses

```
class A:
    a: str = modal.parameter()
```

If
`init=False`
is specified, the field is not considered a parameter for the
Modal class and not used in the synthesized constructor. This can be used to
optionally annotate the type of a field that’s used internally, for example values
being set by @enter lifecycle methods, without breaking type checkers, but it has
no runtime effect on the class.

[modal.parameter](#modalparameter)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.web_endpoint
================================================================================

modal.web\_endpoint
===================

```
def web_endpoint(
    _warn_parentheses_missing=None,
    *,
    method: str = "GET",  # REST method for the created endpoint.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    docs: bool = False,  # Whether to enable interactive documentation for this endpoint at /docs.
    custom_domains: Optional[
        Iterable[str]
    ] = None,  # Create an endpoint using a custom domain fully-qualified domain name (FQDN).
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[
    [Union[_PartialFunction[P, ReturnType, ReturnType], Callable[P, ReturnType]]],
    _PartialFunction[P, ReturnType, ReturnType],
]:
```

Register a basic web endpoint with this application.

DEPRECATED: This decorator has been renamed to
`@modal.fastapi_endpoint`
.

This is the simple way to create a web endpoint on Modal. The function
behaves as a
[FastAPI](https://fastapi.tiangolo.com/)

handler and should
return a response object to the caller.

Endpoints created with
`@modal.web_endpoint`
are meant to be simple, single
request handlers and automatically have
[CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS)

enabled.
For more flexibility, use
`@modal.asgi_app`
.

To learn how to use Modal with popular web frameworks, see the
[guide on web endpoints](https://modal.com/docs/guide/webhooks)

.

[modal.web\_endpoint](#modalweb_endpoint)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.web_server
================================================================================

modal.web\_server
=================

```
def web_server(
    port: int,
    *,
    startup_timeout: float = 5.0,  # Maximum number of seconds to wait for the web server to start.
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[[Union[_PartialFunction, NullaryFuncOrMethod]], _PartialFunction]:
```

Decorator that registers an HTTP web server inside the container.

This is similar to
`@asgi_app`
and
`@wsgi_app`
, but it allows you to expose a full HTTP server
listening on a container port. This is useful for servers written in other languages like Rust,
as well as integrating with non-ASGI frameworks like aiohttp and Tornado.

**Usage:**

```
import subprocess

@app.function()
@modal.web_server(8000)
def my_file_server():
    subprocess.Popen("python -m http.server -d / 8000", shell=True)
```

The above example starts a simple file server, displaying the contents of the root directory.
Here, requests to the web endpoint will go to external port 8000 on the container. The
`http.server`
module is included with Python, but you could run anything here.

Internally, the web server is transparently converted into a web endpoint by Modal, so it has
the same serverless autoscaling behavior as other web endpoints.

For more info, see the
[guide on web endpoints](https://modal.com/docs/guide/webhooks)

.

[modal.web\_server](#modalweb_server)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs/reference/modal.wsgi_app
================================================================================

modal.wsgi\_app
===============

```
def wsgi_app(
    _warn_parentheses_missing=None,
    *,
    label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.
    custom_domains: Optional[Iterable[str]] = None,  # Deploy this endpoint on a custom domain.
    requires_proxy_auth: bool = False,  # Require Modal-Key and Modal-Secret HTTP Headers on requests.
) -> Callable[[Union[_PartialFunction, NullaryFuncOrMethod]], _PartialFunction]:
```

Decorator for registering a WSGI app with a Modal function.

Web Server Gateway Interface (WSGI) is a standard for synchronous Python web apps.
It has been
[succeeded by the ASGI interface](https://asgi.readthedocs.io/en/latest/introduction.html#wsgi-compatibility)

which is compatible with ASGI and supports additional functionality such as web sockets.
Modal supports ASGI via
[`asgi_app`](https://modal.com/docs/reference/modal.asgi_app)

.

**Usage:**

```
from typing import Callable

@app.function()
@modal.wsgi_app()
def create_wsgi() -> Callable:
    ...
```

To learn how to use this decorator with popular web frameworks, see the
[guide on web endpoints](https://modal.com/docs/guide/webhooks)

.

[modal.wsgi\_app](#modalwsgi_app)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/docs
================================================================================

Modal Documentation
===================

Modal provides a serverless cloud for engineers and researchers who want
to build compute-intensive applications without thinking about
infrastructure.

Run generative AI models, large-scale batch workflows, job queues, and more,
all faster than ever before.

[Try the playground](/playground)

![Modal container](https://modal-cdn.com/tmpyuueeabu_646b2904.webp)

[Guide

Everything you need to know to run code on Modal. Dive deep into all of our features and best practices.](/docs/guide)
[Examples

Powerful applications built with Modal. Explore guided starting points for your use case.](/docs/examples)
[Reference

Technical information about the Modal API. Quickly refer to basic descriptions of various programming functionalities.](/docs/reference)
[Playground

Interactive tutorials to learn how to start using Modal. Run serverless cloud functions from your browser.](/playground/get_started)

---

[Guide

Everything you need to know to run code on Modal. Dive deep into all of our features and best practices.](/docs/guide)

---

[Examples

Powerful applications built with Modal. Explore guided starting points for your use case.](/docs/examples)

---

[Reference

Technical information about the Modal API. Quickly refer to basic descriptions of various programming functionalities.](/docs/reference)

---

[Playground

Interactive tutorials to learn how to start using Modal. Run serverless cloud functions from your browser.](/playground/get_started)

Featured Examples
-----------------

[View all](/docs/examples)

[### Deploy an OpenAI-compatible LLM service

Run large language models with a drop-in replacement for the OpenAI API](/docs/examples/vllm_inference)

[### Custom pet art from Flux with Hugging Face and Gradio

Fine-tune an image generation model on pictures of your pet](/docs/examples/dreambooth_app)

[### Run llama.cpp

Run DeepSeek-R1 and Phi-4 on llama.cpp](/docs/examples/llama_cpp)

[### Sandbox a LangGraph agent's code

Run an LLM coding agent that runs its own language models](/docs/examples/agent)

[### Transcribe speech in batches with Whisper

Turn audio bytes into text at scale](/docs/examples/batched_whisper)

[### Voice chat with LLMs

Build an interactive voice chat app](/docs/examples/llm-voice-chat)

[### Edit images with Flux Kontext

Transform images with SotA diffusion models](/docs/examples/image_to_image)

[### Fold proteins with Boltz-2

Predict molecular structures and binding affinities from sequences with SotA open source models](/docs/examples/boltz_predict)

[### Serverless WebRTC

Stream YOLO detections on webcam footage in real time](/docs/examples/webrtc_yolo)

[### Serve diffusion models

Serve Flux on Modal with optimizations for blazingly fast inference](/docs/examples/flux)

[### Serverless TensorRT-LLM (LLaMA 3 8B)

Run interactive language model applications](/docs/examples/trtllm_latency)

[### Transcribe speech with Kyutai STT

Stream transcripts at the speed of speech](/docs/examples/streaming_kyutai_stt)

[### Star in custom music videos

Fine-tune a Wan2.1 video model on your face and run it in parallel](/docs/examples/music-video-gen)

[### Create music

Turn prompts into music with MusicGen](/docs/examples/musicgen)

[### RAG Chat with PDFs

Use ColBERT-style, multimodal embeddings with a Vision-Language Model to answer questions about documents](/docs/examples/chat_with_pdf_vision)

[### Bring images to life

Prompt a generative video model to animate an image](/docs/examples/image_to_video)

[### Fast podcast transcriptions

Build an end-to-end podcast transcription app that leverages dozens of containers for super-fast processing](/docs/examples/whisper-transcriber)

[### Build a protein folding dashboard

Serve a web UI for a protein model with ESM3, Molstar, and Gradio](/docs/examples/esm3)

[### Deploy a Hacker News Slackbot

Periodically post new Hacker News posts to Slack](/docs/examples/hackernews_alerts)

[### Fold proteins with Chai-1

Predict molecular structures from sequences with SotA open source models](/docs/examples/chai1)

[### Retrieval-Augmented Generation (RAG) for Q&A

Build a question-answering web endpoint that can cite its sources](/docs/examples/potus_speech_qanda)

[### Document OCR job queue

Use Modal as an infinitely scalable job queue that can service async tasks from a web app](/docs/examples/doc_ocr_jobs)

[### Parallel processing of Parquet files on S3

Analyze data from the Taxi and Limousine Commission of NYC in parallel](/docs/examples/s3_bucket_mount)

![Modal logo](/_app/immutable/assets/logotype.CAx-nu9G.svg)

© 2025

[About](/company)
[Status](https://status.modal.com/)
[Changelog](/docs/reference/changelog)
[Documentation](/docs/guide)
[Slack Community](/slack)
[Pricing](/pricing)
[Examples](/docs/examples)


================================================================================
SOURCE URL: https://modal.com/blog/5-best-gpus-for-machine-learning-compared
================================================================================

5 Best GPUs for Machine Learning in 2025: A Complete Guide
==========================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Graphics Processing Units (GPUs) have become essential for modern machine
learning workloads. Their parallel processing capabilities make them ideal for
heavy numerical calculations common in both traditional ML and large language
models (LLMs). For a deeper understanding of GPU architecture, terminology, and
utilization,
check out our comprehensive
[GPU Glossary](/gpu-glossary)

and
[GPU utilization guide](/blog/gpu-utilization-guide)

.

Understanding GPU Requirements for ML
-------------------------------------

### Memory (VRAM) Requirements

* **Large Language Models**
  : 40GB+ for models like Llama 70B
* **Image Generation**
  : 16GB+ for models like SDXL
* **Traditional ML**
  : Often 8-16GB is sufficient
* **Data Processing**
  : Sometimes GPU memory isn’t the bottleneck

Beyond VRAM: Other Critical GPU Characteristics
-----------------------------------------------

While VRAM capacity often gets the most attention, several other GPU characteristics can significantly impact machine learning performance:

### Memory Bandwidth

Memory bandwidth determines how quickly data can move between VRAM and the GPU’s compute units. For example:

* [H200’s HBM3e](https://www.nvidia.com/en-us/data-center/h200/)

  memory provides 4.8 TB/s bandwidth
* [A100’s HBM2e](https://www.nvidia.com/en-us/data-center/a100/)

  offers 2 TB/s
* [L40S’s GDDR6](https://www.nvidia.com/en-us/data-center/l40s/)

  delivers 864 GB/s

Memory bandwidth becomes a bottleneck in several scenarios:

1. **Large model inference**
   : When running models that barely fit in VRAM:

   * Weight loading becomes more frequent
   * Memory swapping may occur
   * Cache misses increase
2. **Multi-GPU training**
   : When scaling across multiple GPUs:

   * Weight updates require frequent memory access
   * Gradient communication needs high bandwidth
   * Data loading can become memory-bound

This can be a bottleneck for multi-GPU workloads (e.g. training large models).

### GPU Interconnect

Multi-GPU workloads depend heavily on inter-GPU communication:

* [NVLink](https://www.nvidia.com/en-us/data-center/nvlink/)

  provides high-bandwidth GPU-to-GPU connections
* [NVSwitch](https://www.nvidia.com/en-us/data-center/nvlink/)

  enables all-to-all GPU communication
* [PCIe](https://pcisig.com/specifications)

  connections offer lower bandwidth but more flexibility

For instance, when running large models like
[Llama 3 405B](/blog/how_to_run_llama_405b_article)

across multiple GPUs, the interconnect speed can become the primary bottleneck.

### Compute Architecture

Different GPU generations offer varying features:

* [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/)

  : Specialized for matrix multiplication
* [Ray Tracing (RT) Cores](https://developer.nvidia.com/rtx/ray-tracing?sortBy=developer_learning_library%2Fsort%2Ftitle%3Aasc)

  : Accelerate ray tracing operations
* [Clock speeds](https://www.nvidia.com/en-us/data-center/technologies/)

  : Affect raw computing power
* [Cache hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy)

  : Impact data access speeds

As discussed in our
[GPU utilization guide](/blog/gpu-utilization-guide)

, maximizing GPU performance requires understanding and optimizing for all these characteristics, not just VRAM capacity.

Matching GPUs to ML Tasks
-------------------------

### Image and Video Processing

* **Image Generation**
  :
  [H100](/blog/nvidia-h100-price-article)

  or
  [A100](/blog/nvidia-a100-price-article)

  recommended
* **Video Processing**
  : L40S or H100
* **OCR/Computer Vision**
  : L40S sufficient

### Traditional ML

* **Training**
  : A100 or L40S
* **Inference**
  : L40S often sufficient
* **Data Processing**
  : Consider CPU for some tasks

### Language Models

* **Large Models (70B+)**
  : H100 or H200
* **Medium Models (7-70B)**
  : A100 80GB
* **Small Models (less than 7B)**
  : A100 40GB or L40S

Top 5 GPUs for Machine Learning
-------------------------------

### 1. NVIDIA L40S

Best value for many ML tasks, with excellent availability.

* **VRAM**
  : 48GB GDDR6
* **Performance**
  : Strong for traditional ML
* **Best For**
  : Computer vision, smaller LLMs
* **Availability**
  : Excellent
* **Cost**
  : Lower than A100
* **ROI**
  : Best for most common ML tasks

### 2. NVIDIA A100 40GB

Balanced option for medium-scale workloads.

* **VRAM**
  : 40GB HBM2e
* **Performance**
  : Similar to L40S
* **Best For**
  : Medium-sized models
* **Availability**
  : Very good
* **Cost**
  : Similar to L40S
* **ROI**
  : Good for specific workloads

### 3. NVIDIA H100

The current industry standard for high-end ML.

* **VRAM**
  : 80GB HBM3
* **Performance**
  : Excellent for all ML tasks
* **Best For**
  : Model training, inference for larger models
* **Availability**
  : Generally available
* **Cost**
  : High but justified for heavy usage
* **ROI**
  : Good for large-scale training, production workloads

### 4. NVIDIA A100 80GB

Proven workhorse for ML workloads.

* **VRAM**
  : 80GB HBM2e
* **Performance**
  : Strong for most tasks
* **Best For**
  : Large model inference
* **Availability**
  : Widely available
* **Cost**
  : Lower than H100
* **ROI**
  : Excellent for most use cases

### 5. NVIDIA H200

The newest and most powerful GPU, but limited availability.

* **VRAM**
  : 141GB HBM3e
* **Performance**
  : 1.9x faster than H100
* **Best For**
  : Largest models, cutting-edge research
* **Availability**
  : Limited, not widely accessible
* **Cost**
  : Premium pricing
* **ROI**
  : Best for specific high-end needs

Performance Comparison
----------------------

| GPU | VRAM (GB) | Relative Performance | Cost Efficiency | Availability |
| --- | --- | --- | --- | --- |
| L40S | 48 | ⭐⭐⭐ | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |
| A100 40GB | 40 | ⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐⭐ |
| H100 | 80 | ⭐⭐⭐⭐ | ⭐⭐⭐ | ⭐⭐⭐ |
| A100 80GB | 80 | ⭐⭐⭐ | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |
| H200 | 141 | ⭐⭐⭐⭐⭐ | ⭐⭐ | ⭐ |

Accessing GPU Computing
-----------------------

While you can purchase these GPUs directly, most organizations are better served by cloud GPU providers. Options include:

1. **Major Cloud Providers**
   :

   * [AWS](https://aws.amazon.com/ec2/instance-types/p4/)
   * [Google Cloud](https://cloud.google.com/compute/gpus-pricing)
2. **Specialized GPU Providers**
   :

   * [Modal](https://modal.com)
   * [RunPod](https://runpod.io)
   * [Lambda Labs](https://lambdalabs.com)

### Why Choose Modal for GPU Computing?

[Modal](https://modal.com)

offers several advantages for ML workloads:

1. **Instant Access**
   : No waiting for GPU availability
2. **Automatic Scaling**
   : Pay only for what you use
3. **Simple Deployment**
   : Python-native interface
4. **Cost Effective**
   : No long-term commitments

### Example: Running ML on Modal

```
import modal

app = modal.App("ml-workload")

@app.function(gpu="A100")
def train_model():
    # Your ML code here
    pass
```

Ready to start running ML workloads on powerful GPUs?
[Try Modal free](https://modal.com/signup)

or check out our
[documentation](/docs/guide/gpu)

for more examples.

Additional Resources
--------------------

* [Modal’s GPU Glossary](/blog/open-source-gpu-glossary)

  - Comprehensive guide to GPU terminology
* [GPU Utilization Guide](/blog/gpu-utilization-guide)

  - Deep dive into maximizing GPU performance
* [Cold Start Guide](/docs/guide/cold-start)

  - Tips for reducing model startup times
* [H100 vs. A100](/blog/gpu-types)

  - Comparing H100 and A100 GPUs
* [Future of AI Infrastructure](/blog/the-future-of-ai-needs-more-flexible-gpu-capacity)

  - Trends in GPU computing

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/6-best-code-embedding-models-compared
================================================================================

6 Best Code Embedding Models Compared: A Complete Guide
=======================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Modern AI-powered code editors like
[Cursor](https://cursor.sh)

and
[Windsurf](https://windsurf.ai)

have transformed how developers interact with their codebases. Their ability to understand context, suggest relevant code snippets, and navigate large repositories feels almost magical. Behind this magic lies embedding models that have been optimized for understanding code.

Embedding models convert text (or code) into dense vector representations, but their effectiveness depends heavily on what they were trained on. For example, in a general-purpose embedding model, the word “snowflake” might be closest to words like “rain” or “winter”. But in a model trained on technical documentation, the same word “snowflake” would be closer to “databricks” or “redshift” because they’re all data warehousing platforms.

Why Use Code-Optimized Embedding Models?
----------------------------------------

Understanding code involves distinct challenges that differ from those of general text comprehension. It necessitates algorithmic thinking and must accommodate intricate syntax rules, including keywords, control structures, nesting, and formatting.

Common Use Cases for Code Embeddings
------------------------------------

1. **Semantic Code Search**
   : Find similar code snippets across large codebases
2. **Code Completion**
   : Enhance IDE suggestions with semantic understanding
3. **Repository Analysis**
   : Identify duplicate code and analyze dependencies
4. **Docstring-to-Code**
   : Retrieving code snippets using function docstring queries
5. **Text-to-Code**
   : Retrieving code snippets using natural language queries

Top Code Embedding Models Compared
----------------------------------

### 1. VoyageCode3 (Latest Release)

[VoyageCode3](https://huggingface.co/voyageai/voyage-code-3)

is specifically designed for code understanding tasks.

* **Context Length**
  : 32K tokens
* **Key Features**
  :
  + Supports embeddings of 2048, 1024, 512, and 256 dimensions
  + Multiple embedding quantization options (float, int8, uint8, binary, ubinary)
  + Trained on trillions of tokens with carefully tuned code-to-text ratio
  + Comprehensive dataset with docstring-code and code-code pairs across 300+ programming languages
* **How to access**
  :
  [Voyage API](https://docs.voyageai.com/docs/embeddings)

  or
  [SageMaker](https://aws.amazon.com/marketplace/pp/prodview-d5nri3kbddsrw?sr=0-2&ref_=beagle&applicationId=AWSMPContessa)

### 2. OpenAI Text Embedding 3 Large

[text-embedding-3-large](https://platform.openai.com/docs/guides/embeddings)

is OpenAI’s latest embedding model, showing strong performance across both text and code tasks.

* **Model Size**
  : Not disclosed
* **Context Length**
  : 8191 tokens
* **Output Dimensions**
  : 3072
* **Key Features**
  :
  + Superior cross-domain performance
  + High-dimensional embeddings for better separation
  + Excellent code understanding despite being a general model
* **How to access**
  :
  [OpenAI API](https://platform.openai.com/docs/guides/embeddings)

### 3. Jina Code Embeddings V2

[Jina Code V2](https://huggingface.co/jinaai/jina-embeddings-v2-base-code)

excels at code similarity tasks.

* **Model Size**
  : 137M parameters
* **Context Length**
  : 8192 tokens
* **License**
  : Apache 2.0
* **Key Features**
  :
  + Fast inference times
  + Optimized for code search
  + Extensive language support
* **How to access**
  :
  [Jina API](https://jina.ai/api-dashboard/embedding)

  ,
  [SageMaker](https://aws.amazon.com/marketplace/seller-profile?id=seller-stch2ludm6vgy)

  ,
  [HuggingFace](https://huggingface.co/jinaai/jina-embeddings-v2)

  (open weights, run on your own infra)

### 4. Nomic Embed Code

[Nomic Embed Code](https://huggingface.co/nomic-ai/nomic-embed-code)

is a state-of-the-art code embedding model that excels at code retrieval tasks.

* **Model Size**
  : 7B parameters
* **Context Length**
  : 2048 tokens
* **License**
  : Apache 2.0
* **Key Features**
  :
  + Supports multiple programming languages (Python, Java, Ruby, PHP, JavaScript, Go)
  + Trained on CoRNStack dataset with dual-consistency filtering
  + Fully open-source with model weights, training data, and evaluation code
  + Strong performance across all supported languages (81.7% on Python, 80.5% on Java, etc.)
* **How to access**
  : Open weights, run on your own infra

### 5. CodeSage Large V2

[CodeSage Large V2](https://huggingface.co/codesage/codesage-large-v2)

is a
powerful code embedding model with a Transformer encoder architecture that
supports a wide range of source code understanding tasks.

* **Model Size**
  : 1.3B parameters
* **Context Length**
  : 2048 tokens
* **License**
  : Apache 2.0
* **Key Features**
  :
  + Flexible embedding dimensions through Matryoshka Representation Learning
  + Two-stage training: masked language modeling with identifier deobfuscation, followed by contrastive learning
  + Enhanced semantic search performance through consistency filtering
  + Trained on The Stack V2 dataset with improved data quality
  + Available in three sizes: 130M (Small), 356M (Base), and 1.3B (Large)
* **How to access**
  : Open weights, run on your own infra

### 6. CodeRankEmbed

[CodeRankEmbed](https://huggingface.co/nomic-ai/CodeRankEmbed)

is a specialized bi-encoder for code retrieval.

* **Model Size**
  : 137M parameters
* **Context Length**
  : 8192 tokens
* **License**
  : MIT
* **Key Features**
  :
  + State-of-the-art code retrieval performance
  + High-quality contrastive learning
  + Optimized for code search tasks
* **How to access**
  : Open weights, run on your own infra

Performance Benchmarks
----------------------

[CodeSearchNet](https://github.com/github/CodeSearchNet)

and
[MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

provide standardized comparisons for code embedding models. Key metrics include:

* Code search performance
* Cross-language understanding
* Semantic similarity accuracy
* Resource efficiency

Hosting and Serving Embedding Models
------------------------------------

While some of these embedding models are available exclusively through
hosted APIs, others offer the option to be hosted on your own
infrastructure. For production use cases, you’ll want to:

1. Host the model on GPU-enabled infrastructure for optimal performance
2. Use an inference server to handle requests efficiently
3. Implement proper batching and caching

The most popular inference server options are:

* **[Sentence Transformers](https://www.sbert.net/)**
  : The go-to Python library for embedding models, offering:

  + Simple API for batched inference
  + Automatic GPU acceleration
  + Built-in caching
  + Wide model compatibility
* **[Text Embeddings Inference](https://github.com/huggingface/text-embeddings-inference)**
  : Hugging Face’s Rust-based server that provides:

  + Higher throughput
  + Lower latency
  + Better memory efficiency
  + Native quantization support

For most teams, starting with Sentence Transformers is the right choice due to its ease of use and Python-native implementation. As your needs grow, you can explore more optimized solutions like Text Embeddings Inference.

Running Code Embeddings at Scale
--------------------------------

[Modal](https://modal.com)

provides serverless GPU infrastructure ideal for running code embedding models at scale. With Modal, you can:

1. Deploy models with automatic scaling
2. Process millions of code snippets efficiently
3. Pay only for actual compute time
4. Access the latest GPU hardware

Ready to start embedding code at scale?
[Try Modal
free](https://modal.com/signup)

or check out an
[embedding model inference](/docs/examples/text_embeddings_inference#run-textembeddingsinference-tei-on-modal)

example.

Additional Resources
--------------------

* [Embedding Model Fine-tuning](/blog/fine-tuning-embeddings)
* [Run an Embedding Model with Text Embeddings Inference on Modal](/docs/examples/text_embeddings_inference#run-textembeddingsinference-tei-on-modal)
* [Embed a Large Dataset with Modal](/blog/embedding-wikipedia)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/8-top-open-source-ocr-models-compared
================================================================================

8 Top Open-Source OCR Models Compared: A Complete Guide
=======================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Optical Character Recognition (OCR) technology has seen remarkable advancement
in recent years. While hosted solutions like
[Azure Computer
Vision](https://azure.microsoft.com/products/ai-services/ai-vision)

and
[Mistral
OCR](https://mistral.ai/news/mistral-large/)

offer convenient APIs, many
organizations need open-source alternatives. Whether for compliance with data
privacy regulations or cost optimization at scale, you still need self-hosted
OCR models for many use cases.

Takeaways
---------

* Extracting from scanned documents:
  [Tesseract](https://github.com/tesseract-ocr/tesseract)
* Extracting from handwritten documents:
  [TrOCR](https://huggingface.co/microsoft/trocr-base-handwritten)
* Extracting from structured documents:
  [PaddleOCR](https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/install/pip/macos-pip_en.html)
* Extracting from complex documents:
  [Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL)
* Extracting from visual documents:
  [Llama 3.2 Vision](https://ai.meta.com/llama/)

Traditional ML vs. LLM-Based OCR
--------------------------------

OCR models broadly fall into two categories:

1. **Traditional ML Models**
   : Purpose-built for text extraction, these models often use specialized computer vision architectures and post-processing pipelines.
2. **LLM-Based Models**
   : Newer multimodal large language models that can perform OCR as part of their general visual understanding capabilities.

You should generally start with more traditional OCR models, which are fast,
cheap, and often very accurate, even for structured data like tables (you may
need to fiddle around with some configuration options). For complex diagrams or
other tricky cases, you may need to use an LLM-based OCR model, which will incur
higher latency and cost.

Traditional ML-Based OCR Models
-------------------------------

### 1. Tesseract OCR

[Tesseract](https://github.com/tesseract-ocr/tesseract)

is the most widely-used open-source OCR engine.

* **Key Features**
  :
  + Supports 100+ languages
  + LSTM-based neural network architecture
  + Extensive documentation and community
  + Apache 2.0 license
* **Best For**
  : General document processing, especially printed text
* **GPU Support**
  : Limited, primarily CPU-based

### 2. EasyOCR

[EasyOCR](https://github.com/JaidedAI/EasyOCR)

provides a Python-first approach to OCR.

* **Key Features**
  :
  + Simple Python API
  + 80+ supported languages
  + Built on PyTorch
  + Apache 2.0 license
  + Currently no support for handwritten text, but coming soon
* **Best For**
  : Quick integration in Python projects
* **GPU Support**
  : Native
  [GPU acceleration](https://github.com/JaidedAI/EasyOCR?tab=readme-ov-file#usage)

### 3. PaddleOCR

[PaddleOCR](https://www.paddlepaddle.org.cn/en/install/quick?docurl=/documentation/docs/en/install/pip/macos-pip_en.html)

is a lightweight OCR toolkit developed by PaddlePaddle.

* **Key Features**
  :
  + PP-OCRv4 with high accuracy for Chinese and English
  + Support for 80+ languages
  + Layout analysis and table recognition
  + Formula recognition capabilities
  + Apache 2.0 license
* **Best For**
  : Complex document processing, especially for Chinese text and
  structured documents
* **GPU Support**
  : Easiest with
  [Docker image](https://hub.docker.com/layers/paddlepaddle/paddle/latest-dev-cuda12.3-cudnn9.0-trt8.6-gcc12.2/images/sha256-afc69baadc57cc151953aa31426028f1f60a1371629056a3457d080316be6cbb?context=explore)

### 4. docTR

[docTR](https://github.com/mindee/doctr)

is a comprehensive document text recognition library developed by Mindee.

* **Key Features**
  :
  + Multiple text detection architectures (DBNet, LinkNet, FAST)
  + Various text recognition models (CRNN, SAR, MASTER, ViTSTR)
  + Support for both PyTorch and TensorFlow
  + Apache 2.0 license
* **Best For**
  : Flexible OCR pipeline with choice of architectures
* **GPU Support**
  :
  [Docker image for GPU support](https://github.com/mindee/doctr?tab=readme-ov-file#using-gpu-with-doctr-docker-images)

LLM-Based OCR Models
--------------------

### 5. Microsoft TrOCR

[TrOCR](https://huggingface.co/microsoft/trocr-base-handwritten)

uses transformer architecture for OCR tasks.

* **Key Features**
  :
  + Transformer-based architecture
  + Strong handwriting recognition
  + Multiple language models available
  + MIT license
* **Best For**
  : Handwritten text recognition
* **GPU Support**
  : Full GPU acceleration

### 6. Donut

[Donut](https://github.com/clovaai/donut)

is an OCR-free document understanding transformer developed by Clova AI.

* **Key Features**
  :
  + End-to-end document understanding without OCR
  + Strong performance on structured documents
  + Support for document classification and information extraction
  + MIT license
* **Best For**
  : Document understanding without traditional OCR pipeline
* **GPU Support**
  : Full GPU acceleration

### 7. Qwen2.5-VL

[Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL)

is a powerful multimodal model that excels at visual language tasks.

* **Key Features**
  :
  + Advanced visual language understanding
  + High accuracy on complex document layouts
  + Support for multiple languages
  + Apache 2.0 license
* **Best For**
  : Complex visual language understanding tasks
* **GPU Support**
  : Full GPU acceleration

### 8. Llama 3.2 Vision

[Llama 3.2 Vision](https://ai.meta.com/llama/)

offers OCR as part of its multimodal capabilities.

* **Key Features**
  :
  + General visual understanding
  + Contextual text extraction
  + Multiple languages
  + Llama 2 Community License
* **Best For**
  : Combined visual-language tasks
* **GPU Support**
  : Requires GPU for inference

Running OCR Models at Scale
---------------------------

Most modern OCR models benefit significantly from GPU acceleration. While traditional models like Tesseract can run on CPU, newer transformer-based and LLM models require GPUs for practical inference speeds.

[Modal](https://modal.com)

provides serverless GPU infrastructure ideal for running OCR workloads at scale. With Modal, you can:

1. Deploy any open-source OCR model
2. Automatically scale based on demand
3. Pay only for actual processing time
4. Access the latest GPU hardware

Ready to start processing documents at scale?
[Try Modal](https://modal.com/signup)

or check out our
[documentation](/docs/guide/gpu)

for more examples.

Additional Resources
--------------------

* [Modal’s Guide to GPU Computing](/docs/guide/gpu)
* [Document OCR on Modal Example](/docs/examples/doc_ocr_jobs)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/a1111-vs-comfyui
================================================================================

A1111 vs ComfyUI
================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

![a1111_vs_comfyui](https://modal-cdn.com/a1111_vs_comfyui.png)

Source: https://www.reddit.com/r/StableDiffusion/comments/155uo4w/how\_it\_feels\_to\_switch\_from\_automatic1111\_to/

[AUTOMATIC1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

(often shortened to A1111) and
[ComfyUI](https://github.com/comfyanonymous/ComfyUI)

are two popular open source web UIs for Stable Diffusion. Both allow you to interactively develop image generation pipelines, so which one should you use?

| A1111 | ComfyUI |
| --- | --- |
| Beginner-friendly | Steeper learning curve |
| Built-in common workflows (e.g. text-to-image, image-to-image) | More flexibility in customizing workflows |
| Fewer cutting-edge features | Strong momentum and pace of development |

Takeaways
---------

* If you are a beginner and just want to
  **quickly generate AI art**
  : Use A1111
* If you are somewhat familiar with Stable Diffusion and want to
  **run more complex image generation pipelines**
  : Use ComfyUI

What is A1111?
--------------

[A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui)

is one of the first open-source web UIs created for Stable Diffusion. The project is technically called “Stable Diffusion web UI” and the author’s handle is AUTOMATIC1111. However, over time people started calling the project by the author’s handle or just A1111 for short.

A1111 was created in August 2022, coinciding with the first public release of Stable Diffusion. It quickly became one of the most popular user interfaces for running Stable Diffusion locally.

Built using
[Gradio](https://github.com/gradio-app/gradio)

, the UI feels immediately familiar, and this is its greatest strength; it’s very easy to jump in and quickly generate AI art.

Inpainting (i.e. editing specific areas of an image) is a good example of a workflow that A1111 makes easy:

1. Generate the image in the
   `txt2img`
   tab
   ![a1111_txt2img](https://modal-cdn.com/a1111_txt2img.png)
2. Click a button to send the generated image to the
   `img2img`
   inpaint tab
   ![a1111_img2img](https://modal-cdn.com/a1111_img2img.png)
3. Brush over the area you’d like to draw over (a.k.a. the mask)
4. Update your prompt
   ![a1111_inpainting](https://modal-cdn.com/a1111_inpainting.png)

Though A1111 is very user-friendly, many complain about its
[reliability](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/9157)

. It’s also not exactly on the cutting edge; as of this writing it still
[does not support Flux](https://github.com/AUTOMATIC1111/stable-diffusion-webui/discussions/16314)

. Many users have started switching over to newer frameworks such as
[Forge](https://github.com/lllyasviel/stable-diffusion-webui-forge)

,
[Fooocus](https://github.com/lllyasviel/Fooocus)

, or
[ComfyUI](https://github.com/comfyanonymous/ComfyUI)

.

What is ComfyUI?
----------------

ComfyUI has become one of the fastest growing open-source web UIs for Stable Diffusion. ComfyUI was created in Jan 2023 and has positioned itself as a more powerful and flexible version of A1111.

In ComfyUI, you define workflows down to the node level in a flowchart-like UI. The ability to tweak node-level parameters and also write custom nodes gives you more flexibility compared to A1111.

Here are some particularly impressive ComfyUI workflow examples:

* [Image to sketch](https://openart.ai/workflows/terrier_outgoing_67/turn-your-photo-to-sketch-style/IsCl46lqFJH5lH0CMWAa)
* [Image to manga](https://openart.ai/workflows/monkey_well-worn_31/realistic-photo-to-manga-style/Cl3xyeUX0EVQXY4bVnPN)
* [Animating images](https://openart.ai/workflows/xiongmu/multi-image-deformation-animate/xXGtkyDZKq6vFv3EhZuY)

ComfyUI’s community and pace of development is also growing faster. Most recently, the author of ComfyUI,
[comfyanonymous](https://github.com/comfyanonymous)

, founded
[Comfy Org](https://www.comfy.org/)

, a team dedicated to improving the reliability of core ComfyUI. They’ve already released:

* [comfy-cli](https://github.com/Comfy-Org/comfy-cli)

  , a handy tool for easily installing ComfyUI, managing custom nodes, and running workflows programmatically
* [Comfy Registry](https://registry.comfy.org/)

  , a custom node “app store” to standardize and improve the reliability of custom node development

Below is a
[simple workflow](https://openart.ai/workflows/reverentelusarca/flux-simple-workflow-schnell/40OkdaB23J2TMTXHmxxu)

for text to image using Flux on ComfyUI:
![comfyui_flux](https://modal-cdn.com/comfyui_flux.png)

You can run this workflow on Modal with this
[gist](https://gist.github.com/kning/97c3ef37cdfea87489768872dbe2d25d)

.

Conclusion
----------

A1111 is great for beginners who want to quickly generate AI art using fairly straightforward workflows like text-to-image or image-to-image.

While ComfyUI has a steeper learning curve, more advanced users will be able to design powerful workflows, especially when leveraging the rich custom node ecosystem. Because of its strong momentum and pace of development, it’s likely also more future-proof.

A1111 vs ComfyUI doesn’t necessarily have to mean one vs. the other. Many users use A1111 for rapid prototyping and also ComfyUI for serving complex workflows in production.

Running ComfyUI on Modal
------------------------

Check out our
[ComfyUI example](/docs/examples/comfyapp)

to get started with ComfyUI.

By running image generation on Modal, you can:

* Avoid the headache of locally installing these pieces of software
* Only pay for the GPU time you use
* Host your workflow behind a production-ready API endpoint that
  [autoscales](https://modal.com/blog/scaling-comfyui)

  to handle any traffic load

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/aws-lambda-price-article
================================================================================

How much is AWS Lambda?
=======================

![author](https://modal-cdn.com/kenny-ning.jpg)

Kenny Ning

Growth Engineer

AWS Lambda is the most well-known serverless product on the market. The way it works is it lets you publish code as a Lambda function that gets triggered by some event (e.g. an object appears in an S3 bucket).

AWS Lambda charges you based on what you use. This can be nice for running bursty, unpredictable workloads especially compared to using EC2 where you may be over / under-provisioned at any given moment.

How AWS Lambda pricing works
----------------------------

AWS Lambda charges
[per GB per second](https://aws.amazon.com/lambda/pricing/)

of compute time plus number of requests.

|  | Cost | Free per month |
| --- | --- | --- |
| Compute time | $0.0000166667 / GB-s | 400,000 GB-s |
| Requests | $0.20 per 1M requests | 1 million requests |

### Compute Time

AWS Lambda pricing starts at $0.0000166667 per GB per second of runtime (GB-s). This assumes an x86 instance running in us-east-1.

This base rate card decreases as compute goes up. For example, if you exceed 6 billion GB-seconds per month, successive compute gets billed at $0.000015 per GB-second.

Your first 400,000 GB-s per month are free.

### Requests

On top of compute time, AWS Lambda also charges $0.20 per 1 million requests. Your first 1 million requests are free.

### Additional charges

There are other charges you may incur, including ephemeral storage, provisioned concurrency, and data transfer across regions / clouds.

Example
-------

Let’s say you have a Lambda function that process JSON data in a file and gets triggered every time a file appears in an S3 bucket:

* Number of requests per month: 2M
* Average compute time per job: 1 second
* 10GB memory required

Compute seconds = 2M \* 1 second = 2M

GB-seconds = 2M compute seconds \* 10 GB = 20M

Compute cost = (20M - 400,000) \* $0.0000166667 = $326.67

Request cost = (2M - 1M) \* $0.20 = $0.20

Total cost =
**$326.87 per month**

Comparison with Modal
---------------------

Modal charges for CPU cores and memory separately as opposed to a single GB-second $ rate. If we know that a 1.8 GB Lambda function is
[equal to one vCPU](https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html)

, then we can infer the above example to use 5.6 vCPUs = 2.8 physical CPU cores on Modal.

So the cost of the above example on Modal would look like:

* CPU cost: 2.8 \* $0.0000131 \* 2M compute seconds = $73.36
* Memory cost: 10GB \* $0.00000222 \* 2M = $44.4
* Total cost = $117.76 - $30 free credits =
  **$87.76 per month**

Moving the workload above to Modal from Lambda would reduce your cost by 73%. On top of that, Modal does not charge for number of requests or storage.

Conclusion
----------

AWS Lambda charges per GB per second of compute time plus number of requests.

While AWS Lambda is the de-facto incumbent in the serverless space, its pricing model is unnecessarily complicated and you can probably find better rates on pure serverless providers like Modal.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/aws-lambda-s3-file
================================================================================

Upload files to S3 with AWS Lambda and AWS API Gateway in TypeScript: A Step-by-Step Guide
==========================================================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Are you looking to create a serverless solution for uploading JPEG images to Amazon S3 using AWS API Gateway and Lambda? This guide will walk you through creating an API Gateway endpoint that uploads JPEG images to an S3 bucket using AWS Lambda, with the added feature of using a “filename” parameter to name the S3 object. We’ll be using TypeScript to write our Lambda function, ensuring type safety and improved developer experience.

Prerequisites
-------------

Before we begin, make sure you have:

1. An AWS account
2. AWS CLI installed and configured
3. Node.js and npm installed
4. TypeScript installed globally (
   `npm install -g typescript`
   )
5. AWS SAM CLI installed

Step 1: Set Up the Project
--------------------------

First, let’s create a new directory for our project and initialize it:

```
mkdir aws-image-upload
cd aws-image-upload
npm init -y
npm install aws-sdk @types/aws-lambda @types/node typescript
npm install --save-dev @types/aws-sdk
```

Create a
`tsconfig.json`
file in the root directory:

```
{
  "compilerOptions": {
    "target": "es2018",
    "module": "commonjs",
    "strict": true,
    "esModuleInterop": true,
    "outDir": "./dist"
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules"]
}
```

Step 2: Create the Lambda Function
----------------------------------

Create a new file
`src/index.ts`
and add the following code:

```
import { APIGatewayProxyHandler } from "aws-lambda";
import { S3 } from "aws-sdk";

const s3 = new S3();

export const handler: APIGatewayProxyHandler = async (event) => {
  if (!event.body || !event.queryStringParameters?.filename) {
    return {
      statusCode: 400,
      body: JSON.stringify({ message: "Missing file or filename parameter" }),
    };
  }

  const filename = event.queryStringParameters.filename;
  const fileContent = Buffer.from(event.body, "base64");

  if (
    !filename.toLowerCase().endsWith(".jpg") &&
    !filename.toLowerCase().endsWith(".jpeg")
  ) {
    return {
      statusCode: 400,
      body: JSON.stringify({ message: "File must be a JPEG image" }),
    };
  }

  const params = {
    Bucket: process.env.S3_BUCKET_NAME!,
    Key: filename,
    Body: fileContent,
    ContentType: "image/jpeg",
  };

  try {
    const result = await s3.upload(params).promise();
    return {
      statusCode: 200,
      body: JSON.stringify({
        message: "File uploaded successfully",
        fileUrl: result.Location,
      }),
    };
  } catch (error) {
    console.error("Error uploading file:", error);
    return {
      statusCode: 500,
      body: JSON.stringify({ message: "Error uploading file" }),
    };
  }
};
```

This Lambda function does the following:

1. Checks for the presence of the file content and filename parameter
2. Verifies that the file is a JPEG image
3. Uploads the file to S3 using the provided filename
4. Returns a success message with the file URL or an error message

Step 3: Create the SAM Template
-------------------------------

Create a
`template.yaml`
file in the root directory:

```
AWSTemplateFormatVersion: "2010-09-09"
Transform: AWS::Serverless-2016-10-31
Description: API Gateway for uploading JPEG images to S3

Resources:
  ImageUploadFunction:
    Type: AWS::Serverless::Function
    Properties:
      Handler: dist/index.handler
      Runtime: nodejs20.x
      CodeUri: ./
      Environment:
        Variables:
          S3_BUCKET_NAME: !Ref UploadBucket
      Policies:
        - S3CrudPolicy:
            BucketName: !Ref UploadBucket
      Events:
        UploadAPI:
          Type: Api
          Properties:
            Path: /upload
            Method: post

  UploadBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub ${AWS::StackName}-uploads
      AccessControl: Private

Outputs:
  ApiUrl:
    Description: URL of the API endpoint
    Value: !Sub "https://${ServerlessRestApi}.execute-api.${AWS::Region}.amazonaws.com/Prod/upload"
```

This SAM template sets up:

1. A Lambda function with the necessary permissions
2. An S3 bucket for storing the uploaded images
3. An API Gateway endpoint that triggers the Lambda function

Step 4: Build and Deploy
------------------------

First, compile your TypeScript code:

```
npx tsc
```

Then, use SAM CLI to build and deploy your application:

```
sam build
sam deploy --guided
```

Follow the prompts to complete the deployment. Once finished, you’ll receive an API endpoint URL.

Step 5: Test Your API
---------------------

You can test your new API using cURL. Replace
`YOUR_API_ENDPOINT`
with the URL provided after deployment:

```
curl -X POST \
  'YOUR_API_ENDPOINT?filename=test-image.jpg' \
  --data-binary '@/path/to/your/image.jpg' \
  -H 'Content-Type: image/jpeg'
```

This command sends a POST request to your API with the image file and the filename parameter.

Conclusion
----------

Congratulations! You’ve successfully created an AWS API Gateway endpoint that uploads JPEG images to an S3 bucket using AWS Lambda and TypeScript. This serverless solution provides a scalable and efficient way to handle image uploads in your applications.

Remember to implement additional security measures such as authentication and input validation based on your specific use case.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/aws-lambda-vs-google-cloud-functions-article
================================================================================

AWS Lambda vs. Google Cloud functions: a comprehensive comparison
=================================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Both
[AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/welcome.html)

and
[Google Cloud Functions](https://cloud.google.com/functions/docs)

offer serverless execution environments for building and connecting cloud services. They allow developers to write single-purpose functions that are triggered by events or HTTP requests, enabling rapid development and scalable applications.

This article provides an in-depth comparison of these two services, examining key factors such as features, performance, pricing, and integration capabilities.

Key features comparison
-----------------------

| Feature | AWS Lambda | Google Cloud Functions |
| --- | --- | --- |
| Supported Languages | Node.js, Python, Java, Ruby, C#, Go, PowerShell | Node.js, Python, Go, Java, .NET, Ruby, PHP |
| Maximum Execution Time | 900 seconds (15 minutes) | 3,600 seconds (60 minutes) for 2nd gen |
| Pricing Model | Per-request and per 1ms of execution time | Per-request and per 100ms of execution time |
| Free Tier (monthly) | 1 million requests, 400,000 GB-seconds (memory) | 2 million requests, 360,000 GB-seconds (memory), 180,000 GHz-seconds (CPU) |
| Cold Start Mitigation | Provisioned Concurrency | Minimum Instances |
| VPC Support | Yes | Yes |
| Ecosystem Integration | Deep integration with AWS services | Deep integration with Google Cloud services |
| Event Triggers | Wide range of AWS service triggers | HTTP, Cloud Storage, Pub/Sub, Firestore, and 90+ via Eventarc |
| Monitoring | CloudWatch | Cloud Monitoring |
| Deployment Tools | AWS CLI, AWS SAM, CloudFormation | gcloud CLI, Cloud Console, Terraform |
| Maximum Memory | 10 GB | 32 GB |
| Maximum Concurrent Executions | 1,000 per region | 1,000 per function |
| GPU Support | No | Yes (in preview) |

Event-driven capabilities
-------------------------

Both platforms offer robust event-driven architectures:

### AWS Lambda

Supports a wide range of
[trigger types](https://docs.aws.amazon.com/lambda/latest/dg/lambda-services.html)

, including:

* HTTP/HTTPS (via API Gateway)
* Amazon S3, DynamoDB, SQS, SNS
* AWS CloudWatch Events/EventBridge
* AWS CloudWatch Logs, CodeCommit, Kinesis
* AWS IoT, Alexa Skills Kit

### Google Cloud Functions

Supports various
[trigger types](https://cloud.google.com/functions/docs/calling)

:

* HTTP triggers
* Event triggers:
  + Pub/Sub, Cloud Storage, Firestore
  + Generalized Eventarc triggers (90+ event sources via Cloud Audit Logs)

Resource configuration and scaling
----------------------------------

### AWS Lambda

* [Automatically allocates CPU power](https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html)

  proportional to configured memory

### Google Cloud Functions

* Allows
  [separate CPU and memory configuration](https://cloud.google.com/functions/docs/configuring/memory)

Cold start mitigation
---------------------

Both AWS Lambda and Google Cloud Functions offer strategies to mitigate cold starts:

### AWS Lambda

* [Provisioned Concurrency](https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html)

  : Keeps a specified number of initialized instances ready to respond to invocations.

### Google Cloud Functions

* [Minimum Instances](https://cloud.google.com/functions/docs/configuring/min-instances)

  : Keeps a specified number of instances warm and ready to serve requests.
* CPU Allocation: Allocating more CPU can help reduce cold start times for compute-intensive functions.

Pricing structure
-----------------

Both services use a pay-per-use model with some differences:

### AWS Lambda

* [Charges in 1ms increments](https://aws.amazon.com/lambda/pricing/)

  after the first 100ms
* Pricing based on number of requests, memory allocated to the function, and execution duration

### Google Cloud Functions

* [Charges in 100ms increments](https://cloud.google.com/functions/pricing)
* Pricing based on invocations, CPU and memory allocation, and execution time

### Pricing comparison

| Service | Free tier | Compute pricing | Request pricing | Additional charges |
| --- | --- | --- | --- | --- |
| AWS Lambda | 1M free requests per month and 400,000 GB-seconds of compute time | $0.0000166667 per GB-second (includes CPU and memory) | $0.20 per 1M requests | $0.09 per GB outbound data transfer |
| Google Cloud Functions | 2M free requests, 180,000 vCPU-seconds, and 360,000 GB-seconds per month | $0.00002400 per vCPU-second (CPU) and $0.00000250 per GB-second (memory) | $0.40 per 1M requests | $0.12 per GB outbound data transfer |

Note:

1. AWS Lambda pricing is based on the amount of memory you allocate to your function and the time it runs.
2. Google Cloud Functions prices CPU and memory separately, allowing for more granular resource allocation.
3. Prices shown are for Tier 1 regions in Google Cloud.
4. Google Cloud Functions offers committed use discounts (CUD) for longer-term commitments, which can reduce costs further.
5. For both services, actual costs may vary based on specific configuration and usage patterns.

GPU support
-----------

### AWS Lambda

AWS Lambda does not offer native GPU support, but provides alternatives:

* Can trigger
  [GPU-enabled EC2 instances](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/accelerated-computing-instances.html)

  or ECS tasks
* Integrates with
  [AWS Batch](https://docs.aws.amazon.com/batch/latest/userguide/gpu-jobs.html)

  for GPU-accelerated batch processing
* Works with
  [Amazon SageMaker](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-gpu-instances.html)

  for ML inference on GPUs

### Google Cloud Functions

* Offers
  [GPU support](https://cloud.google.com/blog/products/serverless/google-cloud-functions-is-now-cloud-run-functions)

  in preview for Cloud Run functions (2nd gen). You can currently access only
  [1 Nvidia L4 GPU](https://cloud.google.com/run/docs/configuring/services/gpu)

  per Cloud Run instance.
* Can trigger
  [Compute Engine instances with GPUs](https://cloud.google.com/compute/docs/gpus)
* Works with
  [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus)

  for containerized GPU tasks

Please check the
[official pricing page](https://cloud.google.com/run/pricing#gpu-pricing)

for the most up-to-date and detailed information.

VPC support
-----------

VPC support is crucial for security and access to private resources:

### AWS Lambda

* [Native VPC integration](https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html)
* Functions can run in private subnets
* Direct access to VPC resources without public exposure
* Security group association for traffic control
* Automatic ENI management
* Potential longer cold starts for VPC-connected functions

### Google Cloud Functions

* [Native VPC support](https://cloud.google.com/functions/docs/networking/connecting-vpc)

  (2nd gen only)
* Serverless VPC Access Connector feature for accessing VPC resources
* Primarily facilitates outbound connections to VPC
* Shared VPC support across projects

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/azure-function-pricing-guide
================================================================================

Azure Functions pricing: Consumption vs. Flex Consumption
=========================================================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

[Azure Functions](https://azure.microsoft.com/en-us/products/functions)

is a flexible and cost-effective serverless solution for running your code in the cloud. This guide breaks down the differences between the two main plans: Consumption and Flex Consumption.

Consumption vs. Flex Consumption
--------------------------------

Azure Functions comes in two pay as you go plans:

1. **Consumption plan**
   : The original offering, priced based on function execution time and total number of executions.
2. **Flex Consumption plan**
   :
   [Azure’s newer offering](https://learn.microsoft.com/en-us/azure/azure-functions/flex-consumption-plan)

   with more flexibility and features, including “Always Ready” instances, memory configuration, and virtual network integration.

Overall, the Flex Consumption plan is slightly more expensive than the base Consumption Plan but offers more features and better functionality. Azure officially recommends this plan over the original Consumption plan.

Consumption plan pricing
------------------------

Consumption plan Azure Functions are
[priced](https://azure.microsoft.com/en-us/pricing/details/functions/)

based on the following components (East US region):

1. **Execution Time**
   : $0.000016 per GB-second (first 400,000 GB-s per month free)
2. **Total executions**
   : $0.20 per million executions (first 1 million executions per month free)

The cost of the individual components are added together to get the total cost.

Instance sizes are non-configurable and limited to 1.5 GB.

Interestingly, this pricing model and rates are identical to
[AWS Lambda pricing](/blog/aws-lambda-price-article)

.

### Pricing example

Let’s say your function runs 3 million times per month for 1 second each time:

|  | Calculation | Total |
| --- | --- | --- |
| Execution Time | 3 million executions x 1 second x 1.5 GB | 4,500,000 GB-s |
| Billable Execution Time | 4,500,000 GB-s - 400,000 GB-s | 4,100,000 GB-s |
| Execution Time Cost | 4,100,000 GB-s x $0.000016 | **$65.60** |
| Billable Executions | 3 million - 1 million | 2 million |
| Executions Cost | 2 x $0.20 | **$0.40** |
| Total Cost | $65.50 + $0.40 | **$66.00** |

Flex Consumption plan pricing
-----------------------------

Like the base Consumption plan, Flex Consumption charges by Execution Time and Total Executions, but at higher base rates.

1. **Execution Time**
   : $0.000026 per GB-second (first 100,000 GB-s per month free)
2. **Total Executions**
   : $0.40 per million executions (first 1 million executions per month free)

If you use the Always Ready feature (pre-provisioned instances), the pricing is slightly different:

1. **Baseline**
   : $0.000004 per GB-second (first 100,000 GB-s per month free)
2. **Execution Time**
   : $0.000016 per GB-second
3. **Total Executions**
   : $0.40 per million executions

You will be billed at the “Baseline” rate based on the memory you select, even for idle time.

### Resource allocation

The
[default instance size](https://learn.microsoft.com/en-us/azure/azure-functions/flex-consumption-plan#instance-memory)

is 2,048 MB (2 GiB). The other options are 512 MB and 4,096 MB.

### Flex Consumption pricing example

Let’s say your function runs for 1 second with 2,048 MB of allocated memory 3 million times per month, but with Always Ready enabled. Note that with Always Ready enabled, there is no free grant.

|  | Calculation | Total |
| --- | --- | --- |
| Baseline Time | 2,592,000 seconds (30 day month) x 2 GB | 5,184,000 GB-s |
| Baseline Cost | 5,184,000 GB-s x $0.000004 | **$20.74** |
| Execution Time | 3 million executions x 1 second x 2 GB | 6,000,000 GB-s |
| Execution Time Cost | 6,000,000 GB-s x $0.000016 | **$96.00** |
| Executions Cost | 3 x $0.40 | **$1.20** |
| Total Cost | $20.74 + $96.00 + $1.20 | **$117.94** |

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/batch_vs_stream
================================================================================

Batch processing vs. stream processing by example
=================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Batch processing and stream processing are two different approaches to data processing. Batch processing involves collecting data over time and processing it in large chunks at scheduled intevals. Stream processing, on the other hand, processes data in real time as it arrives. In this blog post, we’ll illustrate them both with examples.

|  | Batch Processing | Stream Processing |
| --- | --- | --- |
| Implementation difficulty | Traditionally easier to implement and to handle failures, since there is built-in slack time | More complex, often requires specialized infrastructure, difficult to handle failures and recovery |
| Latency | Higher latency (minutes to hours) | Low latency (seconds or less) |
| Cost | Generally more cost effective, especially because you can run batch jobs in off-peak hours | Can be more expensive due to constant processing |

What is batch processing?
-------------------------

Batch processing involves collecting data over time and processing it in large chunks at scheduled intervals. This method is ideal for handling large volumes of data where immediate results are not critical.

### Examples

* Web scraping scripts that collect data from PDF files on a fixed hourly schedule
* Processing new files from a file store every 10 minutes or hour
* Scheduled reading of messages from a queue (e.g., every 10 minutes)
* Manually initiated processes for handling accumulated data
* Hourly data transfers from OLTP storage or NoSQL databases to data lakes/warehouses

### Technologies

Batch processing is traditionally how data warehouses and ETL pipelines operate, going back to the days of “big data” and Hadoop. Many of the orchestrators and frameworks below, then, were built in the batch processing paradigm.

* [Hadoop](https://hadoop.apache.org/)
* [Spark](https://spark.apache.org/)
* [Airflow](https://airflow.apache.org/)
* [Luigi](https://github.com/spotify/luigi)

What is stream processing?
--------------------------

Stream processing, in contrast, handles each data item in real-time as it arrives. This method is perfect for scenarios requiring immediate data processing and analysis. Anything labeled “real-time” or “live” likely involves stream processing.

It’s important to note that stream processing isn’t just short-interval batch processing! Batch processing processes data in discrete chunks even if those chunks are small!

### Examples

* Real-time event processing through APIs and message queues
* Continuous data processing from IoT devices like cars or weather stations
* Instant processing of social media posts for immediate timeline updates
* Trigger-based processing when new data appears (e.g., file uploads)

### Technologies

Traditionally, stream processing has been more complex to implement than batch processing. In stream processing, one has to deal with things like state management, consistency, and ensuring that each event is processed exactly once. There are a number of frameworks that have been developed to enable this and make it easier.

#### Message brokers

* [Kafka](https://kafka.apache.org/)
* [AWS SQS](https://aws.amazon.com/sqs/)
* [RabbitMQ](https://www.rabbitmq.com/)

Message brokers like Apache Kafka and Amazon SQS (Simple Queue Service) play a crucial role in enabling stream processing. They ingest and buffer streaming data, ensure reliable delivery of messages/events, and allow multiple consumers to read from the same stream of data.

#### Streaming frameworks

* [Flink](https://flink.apache.org/)
* [Kafka Streams](https://kafka.apache.org/documentation/streams/)

Streaming frameworks like Apache Flink and Kafka Streams provide the tools to process and analyze streaming data in real-time. They offer features like windowing, state management, and fault tolerance to handle the complexities of stream processing.

#### Streaming databases

* [Materialize](https://materialize.com/)
* [Apache Pinot](https://pinot.apache.org/)
* [Apache Kinesis](https://aws.amazon.com/kinesis/)

Streaming databases are a new category of databases designed to handle real-time data processing and analytics. They provide low-latency access to streaming data, enabling real-time dashboards, analytics, and decision-making.

Use cases
---------

**Feature engineering for rec sys:**
use batch processing (okay for models to train on data lag)

**Fraud detection:**
use stream processing (can’t wait a day for fraud)

**BI / Analytics:**
use batch processing (easier to manage, generally okay with 1 day of data lag)

**Trading desk:**
use stream processing (need to be able to react to real time news / data)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/best-open-source-llms
================================================================================

Best open-source LLMs in 2025
=============================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

These days, it seems like hardly a week goes by without a
tech giant or AI startup announcing a new open-source large language model and claiming that it’s best in class.

With so many options available, it can be overwhelming to navigate the landscape and find the best open-source LLM for your specific use case.

In this blog post, we’ll explore some of the best open-source LLMs, evaluating them based on factors such as performance, size, ease of use,
and suitability for various tasks like writing, coding, and fine-tuning. We’ll
also discuss where to find and run these models, including how to serve them
efficiently using Modal.

Table of contents
-----------------

* [What is an open-source LLM and why would you use one?](#what-is-an-open-source-llm-and-why-would-you-use-one)
* [How to evaluate open-source LLMs](#how-to-evaluate-open-source-llms)
* [Best overall open-source LLM: DeepSeek-V3](#best-overall-open-source-llm)
* [Best open-source LLM for chat: Meta-LLama-3.1-8B-Instruct](#best-open-source-llm-for-chat)
* [Best open-source LLM for coding: Qwen2.5-Coder-32B-Instruct](#best-open-source-llm-for-coding)
* [Best open-source LLM for fine-tuning: Mixtral-8x7B-Instruct](#best-open-source-llm-for-fine-tuning)
* [Where to find open-source LLMs](#where-to-find-open-source-llms)
* [Where to run open-source LLMs](#where-to-run-open-source-llms)
* [How to serve open-source LLMs blazingly fast](#how-to-serve-open-source-llms-blazingly-fast)
* [Run open-source LLMs on Modal](#run-open-source-llms-on-modal)

What is an open-source LLM and why would you use one?
-----------------------------------------------------

An open-source language model (LLM) is a model that is made
available to the public for free, with its weights and architecture openly
accessible.

Versus closed-source and proprietary LLMs like OpenAI’s GPT and
Anthropic’s Claude, open-source LLMs offer several advantages:

* **Open licenses**
  :

  Open-source LLMs are released under permissive licenses like Apache 2.0 and MIT, allowing developers to
  use, modify, and distribute the models freely.
* **Publicly available weights**
  :

  The pre-trained weights of open-source LLMs are readily available, enabling
  developers to fine-tune the models for specific tasks without starting from
  scratch.
* **Cheaper**
  :

  There are a lot of caveats here (OpenAI GPT-3.5 is the most cost-effective option for many, many use cases), but if you have high-volume use cases with consistent load, you can save money by running open-source LLMs on your own infrastructure or through cost-effective cloud providers.

  For example, Ramp recently
  [cut their infrastructure costs for automated receipt processing](/blog/ramp-case-study)

  by 79% by switching from OpenAI to running an open-source LLM on Modal.
* **Fast inference speeds and no rate-limiting**
  :

  Open-source LLMs also offer the potential for faster inference speeds, as you
  have control over the hardware and optimization techniques employed.
  Additionally, you are not subject to the rate limits and usage restrictions
  often imposed by commercial LLM providers.

With the plethora of open-source LLMs
now coming out, there’s a lot of noise, and not every new model warrants your
attention. You should consider a new open-source LLM when:

1. It achieves state-of-the-art performance in specific dimensions, such as accuracy,
   efficiency, or model size, compared to existing open-source models.
2. It surpasses the performance of cutting-edge proprietary models, such as GPT-4/5.
3. It offers performance comparable to cutting-edge proprietary models while being
   significantly more cost-effective (e.g., 10x cheaper).

How to evaluate open-source LLMs
--------------------------------

When evaluating open-source LLMs, consider the following
factors:

* **Quality**
  : How accurate and coherent are the model’s outputs?

While a lot of open-source LLMs claim that they are the leader on various benchmarks, we recommend just trying out the models on prompts that resemble your specific use case.

* **Speed**
  : How quickly can the model generate outputs?

Models of comparable size should have similar inference speeds, but this can still vary based on optimization techniques and inference providers.

In general, you should be looking at something like 30-50 tokens/second for a 70B model, and 100-200 tokens/second for a 7B model.

* **Cost**
  : What are the costs associated with running the model?

While open-source LLMs are ostensibly “free” to use, you still need to consider the cost of running the model on your own infrastructure or through a cloud provider.

Here, you should consider not only the server time - whether that’s spinning up your own servers or using one of the newer serverless providers like
[Modal](https://modal.com/)

- but also the cost of maintenance, migration, and developer training.

There are a
[number of sites](https://artificialanalysis.ai/)

that aggregate the metrics mentioned above for the various open-source LLMs and cloud providers.

![artificialanalysis.ai](/_app/immutable/assets/artificial-analysis.d4HmTyGz.png)

### What’s the difference between the 7B and the 70B versions?

When considering open-source LLMs, you’ll often come across models with the same name but different parameter counts, such as Llama3-8B and Llama3-70B. These numbers refer to the number of relationships the AI can
build internally with the training data.

A 70B model has 10 times the number of internal inter-token relationships compared to a 7B model, allowing it to
capture more subtle patterns and nuances in the data.
While larger models often exhibit better performance, they also come with higher computational costs and
longer inference times.

We recommend that you start out by trying the 7B models,
which can generally run on consumer hardware, even mobile phones, and if the
models prove insufficient for your use case, then move on to the larger models.

### What’s the difference between Instruct vs. non-Instruct versions?

Open-source LLMs can come in both instruct and non-instruct versions. Instruct
models are fine-tuned to follow instructions and are generally more suitable for
task-oriented applications, while non-instruct models are more open-ended and
can be used for creative generation tasks.

Best overall open-source LLM
----------------------------

[DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3)

is roughly comparable to GPT-4o in terms of quality, and excels at
general writing, coding, and reasoning tasks.

Users report that it is not as good for coding as Claude-3.5-Sonnet, but it is
significantly cheaper to run.

You can try a hosted version of DeepSeek-V3 on
[DeepSeek](https://www.deepseek.com/)

’s website.

Best open-source LLM for chat
-----------------------------

[Meta-LLama-3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)

is designed for conversational applications, making it a strong choice for chatbots and other AI assistants.

It excels at general task following and content generation.

Best open-source LLM for math
-----------------------------

[DeepSeek-R1-Distill-Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)

is a 32B parameter distilled version of the DeepSeek-R1 model, with very good
performance on general math.

DeepSeek-R1-Distill-Qwen-32B is a smaller version of the full DeepSeek-R1 model that is more
efficient and cheaper to run (Qwen-32B fine-tuned on
hundreds of thousands of samples of DeepSeek-R1 outputs).

Best open-source LLM for coding
-------------------------------

[Qwen2.5-Coder-32B-Instruct](https://qwenlm.github.io/blog/qwen2.5-coder-family/)

is a 32B parameter Qwen
model that is fine-tuned for coding tasks.

It has competitive performance with GPT-4o on code generation and code repair,
and is familiar with 40+ programming languages.

Best open-source LLM for fine-tuning
------------------------------------

If you plan to fine-tune your LLM, we recommend:

* [Mixtral-8x7B-Instruct](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)

A larger model with a mixture-of-experts architecture that allows
for efficient fine-tuning.

You can fine-tune Mixtral on Modal Labs using our
[fine-tuning template](https://github.com/modal-labs/llm-finetuning)

.

Where to find open-source LLMs
------------------------------

One of the best places to find open-source LLMs is
[Hugging Face](https://huggingface.co/models)

. Hugging Face is a
community-driven platform that hosts a wide variety of open-source models,
including LLMs, and provides tools for easy integration and fine-tuning.

Where to run open-source LLMs
-----------------------------

There are several options for running open-source
LLMs, including:

* Cloud platforms like
  [AWS](https://aws.amazon.com/)

  ,
  [Google
  Cloud](https://cloud.google.com/)

  , and
  [Azure](https://azure.microsoft.com/)
* On-premise hardware
* Specialized AI platforms like
  [Modal](https://modal.com/)

  ,
  [Together.ai](https://together.ai/)

  , and
  [Fireworks.ai](https://fireworks.ai/)

How to serve open-source LLMs blazingly fast
--------------------------------------------

To serve your open-source LLM fast, you will likely need to use one of the below
LLM inference engines. The two best engines in 2025 are:

* [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)

  : A platform for optimizing and
  accelerating AI models on NVIDIA GPUs. Best performance but annoying to set up. To run an open-source LLM with TensorRT-LLM on Modal, check out our
  [TensorRT-LLM example](/docs/examples/trtllm_llama)

  .
* [vLLM](https://github.com/vllm-project/vllm)

  : UC Berkeley’s open-source option, relatively easy to use. To run an open-source LLM with vLLM on Modal, check out our
  [example](/docs/examples/vllm_inference)

  .

Run open-source LLMs on Modal
-----------------------------

[Modal](https://modal.com/)

is a serverless cloud computing platform that makes it easy to run open-source
LLMs in the cloud. With Modal, you can:

* Easily
  [deploy and scale open-source LLMs](/use-cases/language-models)

  , with less waste. Modal automatically spins up new containers when you have more demand and scales down when you have less.
* Access powerful
  [GPU resources](/docs/guide/gpu)

  on-demand.

To get started with running open-source LLMs on Modal, check out our
[documentation](/docs/guide)

and
[examples](/docs/examples)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/best-text-to-image-model-article
================================================================================

Stable Diffusion 3.5 vs. Flux
=============================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

[Stable Diffusion 3.5](https://stability.ai/news/introducing-stable-diffusion-3-5)

and
[Flux](https://github.com/black-forest-labs/flux)

are two of the top text-to-image models currently. This post compares and contrasts their outputs on a variety of prompts and shares some basic statistics about running the model.

### Image Comparison

#### Stable Diffusion 3.5 (left) vs. Flux1.dev (right)

![sd-v-flux-1](/_app/immutable/assets/sd-v-flux-1.osKdmOe1.jpg)

From top:

* `A cyberpunk character standing in a rain-soaked alley, with neon signs and holograms reflecting off their metallic outfit, holding a high-tech weapon, in an edgy, modern style.`
* `A dynamic scene of a superhero flying above a bustling city, with their cape flowing and dramatic lighting highlighting their powerful pose, in a comic book-inspired style.`
* `An intense battle scene between a knight and a monstrous creature in a stormy, dramatic environment, with glowing weapons and a sense of movement, in a cinematic style.`
* `An abstract, futuristic cityscape composed of geometric shapes and vibrant colors, with no clear sense of gravity or orientation, in a surreal and modern style.`

![sd-v-flux-2](/_app/immutable/assets/sd-v-flux-2.DZ69sklA.jpg)

From top:

* `A serene underwater scene featuring a coral reef teeming with vibrant marine life, and a mermaid gracefully swimming amidst the fish, in a soft and enchanting style.`
* `A serene Japanese garden in spring, with cherry blossoms in full bloom, a small pond with koi fish, and a traditional wooden bridge, in a realistic and tranquil style.`
* `A quaint, colorful village in a lush valley, with cobblestone streets, flower-filled gardens, and a crystal-clear river running through it, in a cheerful and idyllic style.`
* `A mysterious forest filled with oversized mushrooms that glow softly in the dark, with a fairy-like creature perched on one, in a mystical and enchanting style.`

![sd-v-flux-3](/_app/immutable/assets/sd-v-flux-3.DK_oDfC2.jpg)

From top:

* `A giant, ancient tree with a glowing, hollow trunk, surrounded by tiny, glowing creatures that look like fireflies, in a peaceful and magical atmosphere.`
* `A group of astronauts exploring a distant alien planet, with strange, glowing plants and a vibrant, otherworldly atmosphere, in a cinematic style.`
* `A majestic dragon soaring over snow-capped mountains, with intricate scales that shimmer in the sunlight, rendered in a highly detailed, photorealistic style.`
* `A futuristic laboratory filled with holographic displays, advanced machinery, and a scientist analyzing a glowing, otherworldly artifact, in a sleek, sci-fi style.`

### [Flux](https://github.com/black-forest-labs/flux) by [Black Forest Labs](https://blackforestlabs.ai/)

* **Key Features**
  :

  + Offers a versatile lineup with four main model variants tailored for different users:
    - **FLUX1.1 [pro]**
      and
      **FLUX.1 [pro]**
      are their managed product, available only through their API and through partners like Replicate.
    - **FLUX.1 [dev]**
      is an open-weight, guidance-distilled model intended for non-commercial use; it balances quality and efficiency. It offers similar quality to FLUX.1 [pro] but is more efficient.
    - **FLUX.1 [schnell]**
      is the fastest model optimized for local and personal use. It is openly licensed under Apache 2.0.
* **Open Source**
  : Yes for
  **FLUX.1 [dev]**
  and
  **FLUX.1 [schnell]**
* **Size**
  : 12B parameters for
  **FLUX.1 [dev]**
  and
  **FLUX.1 [schnell]**
* **GPU Needed**
  : A100 or H100
* **How to run**
  :

  + [Flux tutorial on Modal (optimized)](/docs/examples/flux)
  + [How to run Flux on Modal (simple)](/blog/how-to-run-flux1-dev-on-modal)

### [Stable Diffusion 3.5](https://stability.ai/news/introducing-stable-diffusion-3-5) by Stability

* **Key Features**
  :

  + Supports a wide range of output styles, including photorealism and stylized art.
  + Fast inference with the Large Turbo variant (~2 seconds on an A100).
  + Versatile for both commercial and personal projects due to its community license.
* **Open Source**
  : Yes
* **Size**
  : Multiple options, including 8.1B parameters for
  [Large](https://huggingface.co/stabilityai/stable-diffusion-3.5-large)

  and 2.5B for
  [Medium](https://huggingface.co/stabilityai/stable-diffusion-3.5-medium)
* **GPU Needed**
  : A100 for Large, A10 for Medium
* **How to run**
  :

  + [Stable Diffusion CLI on Modal (optimized)](/docs/examples/stable_diffusion_cli)
  + [How to run Stable Diffusion 3.5 Large on Modal (simple)](/blog/how-to-run-stable-diffusion-3-5-large-on-modal)
  + [How to run Stable Diffusion 3.5 Medium on Modal (simple)](/blog/how-to-run-stable-diffusion-3-5-medium-on-modal)

### Conclusion

You can run any open-source text-to-image on Modal’s
[serverless GPUs](https://modal.com/docs/guide/gpu#gpu-acceleration)

.
[Here](https://modal.com/docs/examples)

are a few guides to get you started.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/cache-dict-launch
================================================================================

Modal's Serverless KV Store Gets Its Limit Raised to Infinity
=============================================================

![author](https://modal-cdn.com/blog/images/dshaar-modal.webp)

[Daniel Shaar

@dshaar\_](https://twitter.com/dshaar_)

Member of Technical Staff

Modal’s
[Dict](/docs/guide/dicts)

primitive provides users with a simple TTL’ed (time-to-live) key-value store that can be accessed from any container within the same workspace environment. Dicts are well-suited for things like caching the results of function calls and communicating state changes among a fleet of containers.

Today, we’re excited to announce some major improvements to Dicts, including smarter caching, a new locking feature, and data durability!

🧑‍🚀 A few small changes to Dict, a giant leap for Dict use
----------------------------------------------------------

Here’s what we’ve changed:

|  | Legacy Dicts | New Dicts |
| --- | --- | --- |
| Storage limit | 10GiB | Unlimited |
| Item expiry policy | 30 days since last write | 7 days since last write **OR read** |
| Locking primitive | N/A | `.put()` now supports a `skip_if_exists` flag |
| Durability | ❌ | ✅ |

These changes will apply to all
**newly**
created Dicts. Some cool things we think these features enable:

* LRU-like caching: now that reading extends an item’s TTL, hot cache entries will stick around for as long as they’re needed. And with unlimited items, there’s no need to worry about evicting useful data.
* Distributed locking: in the event that many containers try to perform a redundant operation or state change, you can guarantee “exactly once” semantics using
  `skip_if_exists`
  .

With these new properties, let’s see how we can better tackle a common use case for Dicts: reducing backend load by caching function call results.

🧱 Building a request cache, Dict by Dict
----------------------------------------

Let’s look at a common app structure
**without Dicts**
that we may want to build and optimize on Modal. In this example, we have an “expensive” function that takes a while to run, along with a high concurrency web endpoint that simply calls out to the function.

```
@app.function()
def expensive_function(x: int) -> int:
    time.sleep(30)
    return x ** 2

@app.function(image=modal.Image.debian_slim().pip_install("fastapi[standard]"))
@modal.concurrent(max_inputs=100)
@modal.fastapi_endpoint()
def expensive_function_endpoint(x: int) -> int:
    expensive_function_modal = modal.Function.from_name(APP_NAME, "expensive_function")
    return expensive_function_modal.remote(x)
```

After running this app in production for a while, we discover that users are issuing the same few requests to the web endpoint—who knew that figuring out 13 squared is 169 is all the rage. Not only that, but our app typically sees bursts of traffic for these hot requests.

As was commonly done by Modal users with the previous version of Dicts, we can define some sort of request caching class to wrap our function calls. A sample interface could look like:

```
class RequestCacher():
    """Utility class using `modal.Dict` to issue deduped requests and cache the results."""

    def __init__(self, function: modal.Function):
        self.function = function.hydrate()
        self.cache = modal.Dict.from_name(f"{function.object_id}-cache", create_if_missing=True)

    def _fetch_cached_result(self, request_id: bytes) -> Any:
        pass  # Used by `.call()`.

    def call(self, request_id: bytes, *args, **kwargs) -> Any:
        pass
```

We go ahead and implement some straightforward caching logic—check the cache, if the entry isn’t there, then make the call ourselves and add it to the cache when we’re done. Problem solved!

Or so we thought… turns out those bursts of traffic contain many requests that come in all at once, so we still end up making a bunch of expensive requests to our backend code before the cache is populated. We could work hard to narrow down that race condition window and handle the edge cases. But really, wouldn’t it be great if we could guarantee that we only make the one call to our expensive function?

With Dicts, we can make something quite snazzy to do just this!

🔒 Deduping requests—pop it, lock it
-----------------------------------

![Diagram of request cacher](https://modal-cdn.com/blog/images/request-cacher-flow-2.webp)

Glossing over how a production version of this (that we hope to release in our client 👀) would handle various failure modes, the request handling logic now looks like:

* Try to “acquire a lock” by putting a
  `pending`
  entry in the Dict if it doesn’t already exist.
* If someone else is / was working on the request, we read the Dict entry and poll for / fetch the result.
* Otherwise, assuming we successfully wrote the
  `pending`
  entry:
  + We
    [`.spawn()`](https://modal.com/docs/guide/job-queue#creating-jobs-with-spawn)

    a function call and insert its handle as an
    `in_progress`
    Dict entry.
  + Once the function call is complete, we insert the result as a
    `completed`
    Dict entry.

Here’s a sample implementation:

```
    def call(self, request_id: bytes, *args, **kwargs) -> Any:
        pending = _CacheEntry(_RequestState.PENDING, time.time())
        if not self.cache.put(request_id, pending, skip_if_exists=True):
            # This request is already being worked on or done.
            return self._fetch_cached_result(request_id)

        # Issue the request and populate the cache with the function call handle.
        function_call = self.function.spawn(*args, **kwargs)
        in_progress = _CacheEntry(_RequestState.IN_PROGRESS, function_call)
        self.cache.put(request_id, in_progress)

        # Once the function call completes, populate the cache with the result.
        result = function_call.get()
        completed = _CacheEntry(_RequestState.COMPLETED, result)
        self.cache.put(request_id, completed)
        return result
```

The proof is in the dashboard, so here I’ve issued 3 identical requests to our web endpoint. The second request was deduped against the first, and the third request just got the result from cache:

![Screenshot of not-so-expensive-function-endpoint dashboard](https://modal-cdn.com/blog/images/not-so-expensive-function-endpoint-dash.webp)

This not only speeds up our customer experience significantly, but we also ended up calling the expensive function only once—success!

![Screenshot of expensive-function dashboard](https://modal-cdn.com/blog/images/expensive-function-dash.webp)

💸 Shut up and give me my Dicts
------------------------------

Whether it’s caching, locking, or some other state management, just create a new Dict to get started! For more details, check out our
[docs](/docs/guide/dicts)

. Caveat: to use the
`skip_if_exists`
flag, you may need to upgrade your client version.

Got questions? Come hang out in our
[community Slack](/slack)

—we’d love to hear what you’re building.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/comfyui-custom-nodes
================================================================================

Top ComfyUI custom node packs
=============================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

One of ComfyUI’s greatest strengths as a diffusion model platform is its rich custom node ecosystem. Many talented developers have written their own custom nodes that greatly expand the capabilities of ComfyUI, from image processing and animation to general usability and ergonomics.

Because there are so many custom nodes of varying quality and support it can feel intimidating to get started working with them. To understand this ecosystem better, we asked our good friends at
[Comfy Deploy](https://www.comfydeploy.com/)

to pull the
**top 5 most used custom node packs**
across the thousands of deployed ComfyUI workflows on their platform.

If you’re someone who’s always wanted to try out ComfyUI custom nodes but don’t know where to start, this list is for you.

|  | Pack | Description |
| --- | --- | --- |
| 1 | [WAS Node Suite](#1-was-node-suite) | Hundreds of nodes across image processing and more |
| 2 | [ComfyUI Impact Pack](#2-comfyui-impact-pack) | Image enhancement nodes especially for face detailing |
| 3 | [ComfyUI IPAdapter Plus](#3-comfyui-ipadapater-plus) | Style transfer of reference images |
| 4 | [ComfyUI Essentials](#4-comfyui-essentials) | Quality-of-life improvement nodes |
| 5 | [KJNodes for ComfyUI](#5-kjnodes-for-comfyui) | Quality-of-life and simple image transformation nodes |

In this post, we’ll go through each of these packs and provide a simple:

* Description and use case
* [Modal App](https://modal.com/docs/reference/modal.App)

  that builds the image with the custom node pack’s dependencies and serves an interactive ComfyUI session
* JSON workflow you can drag and drop into the session

To run each example:

* Run
  `modal serve <custom_node>_example.py`
* Click the UI link after the image builds to open an interactive ComfyUI web session
* Drag and drop the JSON workflow into the UI

#0. ComfyUI Manager
-------------------

It goes without saying you need
[ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager)

if you want to run custom nodes. ComfyUI Manager is the primary entrypoint for most ComfyUI users for discovering and installing custom nodes, so it deserves the first spot on any list.

![ComfyUI-Manager](https://modal-cdn.com/cdnbot/tmpaf2lnq56_8db49573.webp)

Keep in mind that if you’re using ComfyUI with Modal, the ComfyUI Manager Menu does not work for downloading custom nodes because your filesystem is not persisted across sessions. That being said, our
[ComfyUI example](/docs/examples/comfyapp)

uses
[comfy-cli](https://github.com/Comfy-Org/comfy-cli)

, which uses ComfyUI Manager CLI in the backend to manage the custom node installation.

#1. WAS Node Suite
------------------

[WAS Node Suite](https://github.com/WASasquatch/was-node-suite-comfyui)

is the most popular ComfyUI custom node pack and contains hundreds of nodes across image processing, prompt processing, and general workflow improvements.

In the example below, we use WAS Node Suite’s Image Generate Gradient, Image Gradient Map, and Image Blending Mode nodes to apply a warm, autumn-y filter (adapted from
[this example](https://github.com/WASasquatch/was-node-suite-comfyui/wiki/Workflow-Examples#autumn-color-tone-with-gradient-mapping-download)

).

![WAS Node Suite](https://modal-cdn.com/cdnbot/tmprandcvj5_9551a1fa.webp)

Sample code
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/comfyui/was_node_suite)

.

#2. ComfyUI Impact Pack
-----------------------

[ComfyUI Impact Pack](https://github.com/ltdrdata/ComfyUI-Impact-Pack)

from ltdrdata (also the creator of ComfyUI Manager) is another popular extension focused on image enhancement, especially around face detailing and image segmentation.

In the example below (adapted from
[this workflow](https://github.com/ltdrdata/ComfyUI-extension-tutorials/blob/Main/ComfyUI-Impact-Pack/workflow/simple.png)

), we first generate an AI image with the prompt “A smiling woman with short hair”.

![Impact_1](https://modal-cdn.com/cdnbot/comfyui-impact-workflow9vdxj9fa_074474d4.webp)

In classic AI art fashion, we get a (slightly horrifying) distorted face. Now let’s pass the image through the FaceDetailer node to make it look more realistic.

![Impact_2](https://modal-cdn.com/cdnbot/comfyui-impact-workflowbfobyley_bb367f51.webp)

Much better. This example also shows how FaceDetailer can crop the headshot and segment out the face when the UltralyticsDetectionProvider and SAMLoader nodes are passed in.

Sample code
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/comfyui/impact)

.

#3. ComfyUI IPAdapater Plus
---------------------------

[ComfyUI IPAdapter Plus](https://github.com/cubiq/ComfyUI_IPAdapter_plus)

is the ComfyUI implementation of
[IP-Adapter](https://github.com/tencent-ailab/IP-Adapter/)

(Image Prompt adapter) which allows you to pass reference images to the image generation process to achieve style transfer e.g. “x in the art style of y”.

In the example below (adapted from
[this video](https://www.youtube.com/watch?v=_JzDcgKgghY)

), we use the IPAdapter Unified Loader and IPAdapter nodes with the weight\_type set to “style transfer” to generate a painting of a golden retriever in the style of “Starry Night”:

![ip-adapter](https://modal-cdn.com/cdnbot/ip-adapterfsr4d0su_4ab4c249.webp)

Sample code
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/comfyui/ip_adapter)

.

#4. ComfyUI Essentials
----------------------

[ComfyUI Essentials](https://github.com/cubiq/ComfyUI_essentials)

(developed by the same author as ComfyUI IPAdapter Plus) contains mostly quality-of-life workflow improvement nodes:

* Printing image dimensions to the console (Get Image Size, Console Debug)
* Simple image transformations (Image Resize, Image Crop, Image Flip)
* Simple mask transformations (Mask Blur, Mask Flip)

The workflow below (adapted from
[this workflow](https://github.com/cubiq/ComfyUI_essentials/blob/main/workflow_all_nodes.json)

) gives a quick glance into all of the Essentials nodes:

![essentials](https://modal-cdn.com/cdnbot/essentialsnla_ahpo_c7484206.webp)

Sample code
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/comfyui/essentials)

.

#5. KJNodes for ComfyUI
-----------------------

[KJNodes for ComfyUI](https://github.com/kijai/ComfyUI-KJNodes)

contains quality-of-life and simple image transformation nodes. For instance, the Color Match node allows you to apply a reference image’s color palette to a target image.

In the example below, I applied the iconic red hallway palette from “In the Mood for Love” to a picture I took of a Portuguese tram.

![kjnodes](https://modal-cdn.com/cdnbot/kjnodes8bw9dqhp_210601df.webp)

Sample code
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/comfyui/kjnodes)

.

Conclusion
----------

The rapid development of the ComfyUI custom node ecosystem historically has made the developer experience painful, but thankfully there have been a lot of recent improvements:

* [Comfy Org](https://www.comfy.org/)

  ’s work establishing the
  [Comfy Registry](https://docs.comfy.org/registry/overview)

  to standardize custom node development
* Tools like
  [Comfy Deploy](https://www.comfydeploy.com/)

  that make it easy to productionize a ComfyUI workflow
* Serverless compute platforms like
  [Modal](https://modal.com/)

  that allow you to programmatically define your ComfyUI environment (models, custom nodes) and iterate quickly while only paying for the compute you use

Make you sure to check out our official
[ComfyUI example](https://modal.com/docs/examples/comfyapp)

which you can combine with these simple code snippets to serve your workflow via API.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/comfyui-prototype-to-production
================================================================================

How to convert a ComfyUI workflow to Python code
================================================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Data Engineer

*Edit 2024-08-26: Our latest recommended solution for productionizing a ComfyUI workflow is detailed in
[this example](/docs/examples/comfyapp)

. As a result, this post has been largely re-written to focus on the specific use case of converting a ComfyUI JSON workflow to Python*
.

![ComfyUI workflow diagram](https://modal-cdn.com/cdnbot/comfy-ui-diagram.jpg)

[ComfyUI](https://github.com/comfyanonymous/ComfyUI)

is a popular no-code,
visual editor for building complex image generation workflows. While ComfyUI started as a prototyping / experimental playground for Stable Diffusion, increasingly more users are using it to deploy image generation pipelines in production.

Modal is a great solution for this and our
[ComfyUI example](/docs/examples/comfyapp)

walks you through the step-by-step process of serving your ComfyUI workflow behind an API endpoint.
**This is our recommended solution**
for productionizing ComfyUI in most cases.

However, some users prefer defining and iterating on their
**ComfyUI workflows in Python**
. For example, if you’re doing some complex user prompt handling in your workflow, Python is arguably easier to work with than handling the raw workflow JSON object. In this blog post, we’ll show you how to convert your ComfyUI workflow to executable Python code as an alternative design to serving a workflow in production.

Export your workflow as JSON
----------------------------

The native representation of a ComfyUI workflow is in JSON. First, we need to extract that representation from the UI.

1. Click the gear icon in the top right of the menu box:

![ComfyUI menu box with gear circled](https://modal-cdn.com/comfy_menu.png)

2. Check Enable Dev mode Options:

![ComfyUI enable dev options selection](https://modal-cdn.com/comfy_dev_mode.png)

3. Click Save (API Format) option in your menu:

![ComfyUI menu with api json cirlced](https://modal-cdn.com/save_api_format.png)

Save the file as
`workflow_api.json`
.

Convert JSON to Python
----------------------

We’ll use the
[ComfyUI to Python Extension](https://github.com/pydn/ComfyUI-to-Python-Extension)

to convert the JSON from the previous step to Python code. This tool requires ComfyUI to be installed on the host machine, so we’ll use Modal to install ComfyUI in a container and then run the tool in that container.

Save the following script to a file called
`comfypython.py`
in the same directory as the
`workflow_api.json`
you created in the previous step.

```
import pathlib

import modal

comfyui_python_remote_path = pathlib.Path("/root/comfy/ComfyUI/ComfyUI-to-Python-Extension")
image = (  # build up a Modal Image to run ComfyUI, step by step
    modal.Image.debian_slim(  # start from basic Linux with Python
        python_version="3.11"
    )
    .apt_install("git")  # install git to clone ComfyUI
    .pip_install("comfy-cli==1.0.33")  # install comfy-cli
    .run_commands(  # use comfy-cli to install the ComfyUI repo and its dependencies
        "comfy --skip-prompt install --nvidia"
    )
    .run_commands(  # download all models and custom nodes required in your workflow
        "comfy --skip-prompt model download --url https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/resolve/main/512-inpainting-ema.safetensors --relative-path models/checkpoints"
    )
    .run_commands(
        # As of this writing, main is broken, so using this fork's fix
        "git clone -b fix/custom_module_init https://github.com/albertpurnama/ComfyUI-to-Python-Extension/ /root/comfy/ComfyUI/ComfyUI-to-Python-Extension",
        f"cd {comfyui_python_remote_path} && pip install -r requirements.txt",
    )
)
app = modal.App(name="comfy-python", image=image)

@app.function(
    gpu="A10G",
    mounts=[
        modal.Mount.from_local_file(
            pathlib.Path(__file__).parent / "workflow_api.json",
            comfyui_python_remote_path / "workflow_api.json",
        )
    ],
)
def comfyui_to_python():
    """
    Put the workflow json you want to convert into the same directory as this script
    """
    import subprocess

    result = subprocess.run(["python", "comfyui_to_python.py"], cwd=comfyui_python_remote_path)
    if result.returncode != 0:
        raise RuntimeError(f"Exited unexpectedly with code {result.returncode}")
    else:
        try:
            return (comfyui_python_remote_path / "workflow_api.py").read_text()
        except FileNotFoundError:
            print("Error: File workflow_api.py not found.")

@app.local_entrypoint()
def fetch_comyfui_to_python():
    """
    Write the generated python to _generated_workflow_api.py in the same directory
    """
    (pathlib.Path(__file__).parent / "_generated_workflow_api.py").write_text(comfyui_to_python.remote())
```

At a high level, this script will convert a JSON node representation:

```
"2": {
  "inputs": {
    "ckpt_name": "512-inpainting-ema.ckpt"
  },
  "class_type": "CheckpointLoaderSimple",
  "_meta": {
    "title": "Load Checkpoint"
  }
}
```

Into a Python object:

```
from nodes import (
    ...
    CheckpointLoaderSimple,
    ...
)

checkpointloadersimple = CheckpointLoaderSimple()
checkpointloadersimple_2 = checkpointloadersimple.load_checkpoint(
    ckpt_name="512-inpainting-ema.ckpt"
)
```

Run
`modal run comfypython::fetch_comfyui_to_python`
to convert
`workflow_api.json`
into a Python file called
`_generated_workflow_api.py`
in your local directory.

Run the Python workflow
-----------------------

Now we can run this generated code and fetch the generated images. Add the following to the
`comfypython.py`
script we created in the previous step:

```
@app.function(
    gpu="A10G",
    # Mount the generated workflow Python code
    mounts=[
        modal.Mount.from_local_file(
            pathlib.Path(__file__).parent / "_generated_workflow_api.py",
            comfyui_python_remote_path / "_generated_workflow_api.py",
        )
    ],
)
def run_comfyui_python():
    import subprocess

    result = subprocess.run(["python", "_generated_workflow_api.py"], cwd=comfyui_python_remote_path)
    if result.returncode != 0:
        raise RuntimeError(f"Exited unexpectedly with code {result.returncode}")
    else:
        # Iterate through the output directory and return the generated images
        output_dir = comfyui_python_remote_path.parent / "output"
        image_bytes = []
        for file in output_dir.iterdir():
            if file.name.endswith(".png"):
                with open(file, "rb") as f:
                    image_bytes.append(f.read())
        return image_bytes

@app.local_entrypoint()
def fetch_images():
    image_bytes = run_comfyui_python.remote()
    # Write image bytes to local files
    for i, val in enumerate(image_bytes):
        pathlib.Path(f"image_{i}.png").write_bytes(val)
```

Run
`modal run comfypython.py::fetch_images`
to run the Python workflow and write the generated images to your local directory.

Conclusion
----------

Some of our users have had success using this approach to establish the foundation of a Python-based ComfyUI workflow, from which they can continue to iterate. For example:

* Add command-line arguments to
  `_generated_workflow_api.py`
  to handle prompts
* Turn
  `run_comfyui_python`
  into a
  [web endpoint](/docs/guide/webhooks)

  that can receive requests via API

Generally speaking though, we don’t recommend this approach anymore to productionize your ComfyUI pipeline because the extra step required to convert from JSON to Python isn’t worth the marginal ergonomic beneftis of coding in Python. This is especially true if your workflow contains a lot of models or custom nodes. Your best bet is to follow our
[ComfyUI example](/docs/examples/comfyapp)

which directly serves your ComfyUI workflow JSON.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/contextual-case-study
================================================================================

How Contextual AI automated CI with Modal GPUs
==============================================

Cutting edge platforms like Contextual AI often find that their software development practices require more flexible resources than legacy providers can offer. With Modal, Contextual AI was able to automate and parallelize their continuous integration (CI) on GPUs.

About Contextual AI
-------------------

[Contextual AI](https://contextual.ai/)

offers an end-to-end platform for building RAG 2.0 (retrieval-augmented generation) enterprise AI applications. The product integrates the entire RAG pipeline into a single optimized system which can be specialized for customer needs, delivering greater accuracy and transparency for knowledge-intensive tasks. The company is led by CEO Douwe Kiela, who pioneered the industry-standard
[RAG technique](https://arxiv.org/abs/2005.11401)

, and CTO Amanpreet Singh, who was a research engineer at Hugging Face and Meta’s Fundamental AI Research team.

A bottleneck on testing
-----------------------

CI is a practice where engineers integrate their code changes frequently, and each integration is verified by an automated build and automated tests. Because Contextual AI’s product uses LLMs, they needed a way to run CI using GPUs. There were two scenarios when they ran test suites:

1. Before a pull request (PR) was merged, they would run a large suite of small tests to ensure that the PR didn’t break any plumbing in the product. To optimize for efficiency, they used tiny, several-MB models as stand-ins.
2. Once a day, they would run more in-depth “quality” tests using larger models that customers would actually use, to ensure there were no regressions in model output.

Developers had to run these tests manually on in-house GPU nodes, which was inconvenient and time-consuming. It was easy to forget to run the tests before merging PRs, resulting in broken master code that would slow down the whole team.

“

Previously you were just trusting that people would trigger these GPU tests manually before they merged code. I would have to ask, “Well, did you run the tests?” before I approved the PRs. But now we’re 50 people and you can’t rely on that.

”

— Stas Bekman,

ML Engineer at Contextual AI

Another pain point was procuring GPUs on demand. While Contextual AI had a massive quantity of GPUs reserved with GCP, the research team’s training and prototyping needs took priority. It didn’t make sense for CI to divert resources away from them, which is why
[Stas Bekman](https://github.com/stas00/)

, an ML engineer at Contextual AI, wanted to find a reliable external provider.

Stas
[searched for CI-on-GPUs options](https://x.com/StasBekman/status/1724271487375569269)

, but didn’t find a good fit. Their CI required at least two GPUs but neither GitHub nor CircleCI provided more than one GPU per job. Furthermore, the GPUs they had available were old, slow, and expensive.

Back in his time at Hugging Face, Stas used an AWS on-demand GPU instance to solve this problem, but it wasn’t ideal. Updating the machine image was slow and cumbersome, and it could take 5+ minutes just to get an instance running. Often times CI would fail because no instance could be found, even when he tried searching across multiple availability zones. He wanted to avoid repeating the same mistake at Contextual AI.

Parallelizable CI on Modal GPUs
-------------------------------

After making a request on Twitter for suggestions, Stas decided to try Modal because he could access flexible configurations of GPUs on-demand. This is what the CI workflow looked like:

1. PR is submitted on GitHub.
2. A GitHub Action is triggered which calls a Modal Function. The Function has multiple GPUs attached and uses an image with custom requirements and
   `pytest`
   installed.
3. The Modal Function invokes
   `pytest`
   as a subprocess to run a suite of tests.
4. The first time the Function runs, Modal builds and caches the custom image. On subsequent runs, no image rebuild is needed, allowing the tests to start running within 30 seconds of job submission.

Simplified pattern of CI using Modal:

```
import modal

image = (
    modal.Image.debian_slim()
    .pip_install("pytest")
    .pip_install_from_requirements("requirements.txt")
)

app = modal.App("ci-testing", image=image)

@app.function(gpu="any", mounts=[tests])
def pytest():
    import subprocess

    subprocess.run(["pytest", "-vs"], check=True, cwd="/root")
```

This workflow allowed Contextual AI to fully automate their test suite. As a result, they can maximize their developer iteration speed while maintaining a high quality bar. Other key benefits:

* GitHub Actions can directly trigger Modal, so there’s no need to manage self-hosted runners.
* Modal spins up GPUs for each job submission, allowing CI for multiple PRs to run in parallel.
* Modal bills by usage, which keeps costs low. Because image builds are cached, 99% of what’s billed is actual test run-time.

“

I was shocked at the amazing support I received from Modal's team. They quickly created a sample repo that catered exactly to our needs and within a few hours we had our CI running. In this day and age it's very difficult to find excellent technical support within seconds of posting a request. It has been an amazing experience for our team collaborating with Modal.

”

— Stas Bekman,

ML Engineer at Contextual AI

All of this has been enabled by Modal’s custom infrastructure—including our own file system and scheduler—for running containers in the cloud. Modal can spin up GPU-enabled containers in as little as one second, which helps companies iterate fast and scale up to large production workloads.

Interested in CI on Modal? Check out our
[sample repo](https://github.com/modal-labs/ci-on-modal/tree/main)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/cron-jobs
================================================================================

How to run cron jobs
====================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Data Engineer

What is a cron job?
-------------------

A cron job is a scheduled task. You can use cron jobs to automate work by running a script at a regular interval (e.g. every hour, every day).

There are many use cases for cron jobs:

* Running data jobs
* Sending reports or alerts
* System maintenance e.g. removing old Docker images

Cron syntax
-----------

A cron schedule is specified with 5 numbers that specify (in order):

* Minute (0-59)
* Hour (0-23)
* Day of month (1-31)
* Month (1-12)
* Day of week (0-6)

The most common configuration you’ll see is
`* * * * *`
, which means “run every minute”. The second most common configuration is something like
`0 * * * *`
, which means “run hourly” or more specifically “run at minute zero of every hour”.

Cron uses the time zone of the host machine, which is likely UTC.

Here’s some other examples:

| Cron Example | Explanation | Syntax Note |
| --- | --- | --- |
| 0,30 \* \* \* \* | Twice an hour, once at the 0 minute mark and again at the 30 minute mark | `,` means “and” |
| 0 6 \* \* 0 | Every Sunday at 6am UTC | Last value is day of week (0=Sunday) |
| \* \* 1 3,9 \* | Every minute on the 1st of March and September | 3rd and 4th values represent day of month and the month |
| \* \* \* \* 1-5 | Every minute, Mon-Fri | `-` defines a range of values |
| \*/5 \* \* \* \* | Every 5 minutes | `/` defines step values or intervals |

[crontab.guru](https://crontab.guru/)

is a great tool to double check your cron expressions.

Crontab
-------

The crontab is the file that specifies all your cron jobs using the cron syntax described above. On computers with a Unix-like operating system (e.g. MacOS, most cloud servers), you can add a cron job to the crontab by typing
`crontab -e`
from your terminal and adding your job in the format
`<cron schedule syntax> <path to executable script>`
. For example,

`*/5 * * * * /Users/modal/run_job.sh`

Once this is added to the crontab, your computer will execute the
`/Users/modal/run_job.sh`
script every 5 minutes.

Limitations of cron
-------------------

### Primarily made for shell scripts

This makes running data jobs annoying since they are most often written in Python or SQL and require additional configuration to get working.

### No monitoring

Cron jobs do not provide notifications or output when a job fails.

### Requires cloud provisioning

If you want to run cron jobs in the cloud, you need to set up an EC2 and go through the manual process of uploading your script, editing the crontab remotely, making sure the script stays updated, etc.

An alternative to cron jobs are orchestrator tools like Airflow, but this is often overkill for simple scheduled tasks.

How to run cron jobs on Modal
-----------------------------

We believe Modal is the best way to
[run cron jobs in the cloud](https://modal.com/docs/guide/cron)

. Write your Python code locally, and decorate it with a
[modal.Period](/docs/reference/modal.Period#modalperiod)

:

```
@app.function(schedule=modal.Period(days=1))
def f():
    print("This function will run every day")
```

Or if you prefer using cron syntax, you can attach that as well:

```
@app.function(schedule=modal.Cron("0 0 * * *"))
def f():
    print("This function will run every day at midnight UTC")
```

You can now schedule your Python script directly and get basic monitoring, all without having to touch any cloud providers or edit any crontab files.

Examples (built on Modal cron jobs)
-----------------------------------

* [Scrape a comedy club’s website every 10 minutes](https://twitter.com/_nateraw/status/1779588270424277483)

  and text yourself when new shows are added
* [Schedule ETL jobs](https://modal.com/blog/etl)
* Build a
  [Hacker News Slackbot](https://modal.com/docs/examples/hackernews_alerts#defining-the-schedule-and-deploying)

  that queries the Hacker News API every day and posts the results to a Slack channel

Conclusion
----------

Cron has been around for decades and is one of the most battle-tested pieces of software out there. You can’t go wrong using cron jobs for simple scheduled tasks, and Modal has made it even easier to manage cron jobs for your data and automation needs.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/dagster-vs-airflow-article
================================================================================

Dagster vs. Airflow: a comprehensive comparison
===============================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Data orchestration tools play a crucial role in modern data engineering workflows. Two popular options in this space are
[Apache Airflow](https://airflow.apache.org/)

and
[Dagster](https://dagster.io/)

. While both aim to help data teams build and manage complex data pipelines, they take different approaches and are better suited for different use cases. This article will compare Dagster and Airflow to help you understand their key differences and choose the right tool for your needs.

Overview
--------

### Airflow

Airflow is a highly popular, open-source workflow management system known for its flexibility, ease of use, and strong community support. It uses Python to define workflows as Directed Acyclic Graphs (DAGs), allowing users to easily schedule, monitor, and manage complex data pipelines. Airflow’s key strengths include:

* Massive ecosystem of plugins and integrations
* Cloud-native design for easy connection to various services
* Web-based UI for workflow visualization and management
* Highly flexible and customizable

### Dagster

Dagster, a newer entrant in the workflow management space, was developed to address some of the limitations of early Airflow versions. It focuses heavily on data quality, testing, and analytics. While also using DAGs and a Python-based API, Dagster’s approach differs in several key areas:

* Built-in data quality checks at each pipeline step
* Strong focus on testing and debugging workflows
* Asset-centric approach to data pipelines

Key Differences
---------------

### 1. Workflow Focus

#### Airflow

Airflow excels at managing complex, branching workflows with conditional logic. It’s designed to handle intricate business logic and automate tasks that data engineers might otherwise do manually across multiple systems. These can be more generic, non data-related tasks. Airflow allows you to:

* Create advanced DAGs with numerous branching possibilities
* Implement conditional workflows based on various factors (e.g., day of the week, data conditions)
* Automate and orchestrate tasks across your entire data stack

#### Dagster

Dagster’s workflow focus is more centered around data collection, processing, and visualization. It’s particularly well-suited for analytics-focused tasks. Dagster workflows typically involve:

* Collecting data from APIs
* Processing and transforming data
* Visualizing results
* Emphasizing metadata and data source information

### 2. Data Quality and Testing

#### Airflow

While Airflow doesn’t have built-in data quality checks, its modular nature allows integration with external tools:

* Can leverage tools like
  [Great Expectations](https://greatexpectations.io/)

  for data quality checks
* Requires manual implementation of quality checks within DAGs
* Offers flexibility to choose and implement preferred testing frameworks

#### Dagster

Dagster places a strong emphasis on data quality and testing:

* Built-in capability to include data quality checks within DAGs
* Automated testing framework for debugging workflows
* Provides detailed information on step success/failure and causes of errors

### 3. Community Support and Ecosystem

#### Airflow

Airflow boasts a massive and growing community:

* Over 10 million downloads per month
* 30 million package downloads by provider monthly
* Thousands of providers and integrations available
* Large, active community (30,000+ members in
  [Slack](https://apache-airflow-slack.herokuapp.com/)

  )

#### Dagster

As a newer, proprietary solution, Dagster has a smaller but growing community:

* Exact download numbers not published
* Approximately 250,000 monthly website visits
* Over 3,000 community members across various organizations

### 4. Language and Coding Approach

#### Airflow

Airflow is purely Python-based:

* DAGs and workflows defined in Python
* Can incorporate SQL, Bash commands, etc., through operators
* Offers granular control over task logic and data passing between tasks

#### Dagster

Dagster uses a Python-based API:

* Workflows built around data assets
* Heavy use of decorators and API calls
* Focus on orchestrating Python functions for data processing

When to Choose Airflow or Dagster
---------------------------------

Consider Airflow if:

* You need a highly flexible and customizable workflow management system
* Your use cases involve complex, branching workflows with conditional logic
* You want to leverage a vast ecosystem of plugins and integrations
* You need to orchestrate tasks across multiple systems in your data stack

Consider Dagster if:

* Your primary focus is on data quality and testing throughout the pipeline
* You’re mainly working with data collection, processing, and visualization tasks
* You prefer a more structured approach to defining data assets and their relationships
* You want built-in testing and debugging capabilities

Conclusion
----------

Both Airflow and Dagster are powerful tools for data orchestration, but they cater to different needs and preferences. Airflow’s flexibility, extensive ecosystem, and ability to handle complex workflows make it a solid choice for many teams, especially those dealing with intricate data pipelines across multiple systems. Dagster’s focus on data quality, built-in testing, and analytics-centric approach make it appealing for teams prioritizing these aspects in their data workflows.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/dollars-per-token-considered-harmful
================================================================================

Dollars per token considered harmful
====================================

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

Token Producer & Dollar Consumer, Modal

It is no secret that open source, self-hosted large language model inference has grown up in the shadow of proprietary services, like the APIs provided by OpenAI, Anthropic, and Alphabet.

What is less obvious is that the choices and priorities of those providers have shaped the expectations and discourse of the field in ways that are actively harmful to the inevitable move away from them.

One of the most pernicious of these subtle influences is in the pricing model: “dollars per token”.

### When you serve your own language model inference, you must think in terms of dollars per request, not dollars per token.

Here’s why:

* you are running language model inference in service of a language model application, and
* the users of your application don’t care about tokens, they care about requests, and therefore
* so should you.

You are running language model inference in service of a language model application.
------------------------------------------------------------------------------------

The API providers are running language model inference
*as a service*
. Running that service incurs costs which they recoup,
ideally but optionally plus a profit margin, by charging their users.

These costs roughly scale with the sizes of requests (both inputs and outputs).
These sizes are measured in complex non-linear transformations of Unicode bytes called
*tokens*
, rather than something sensible like bytes or characters,
due to contemporary model architecture skill issues.

Because token counts and costs scale together and sit at the boundary between the API and the people who build on it, they make for a good pricing mechanism: count tokens, charge dollars.

But when teams run their own language model inference, they are generally not running inference as a service.
Instead, they have built some application of language models, like a support chatbot or an AI boyfriend or a meme coin shill,
and they want to support that application at reduced cost, with more control, and/or with tighter data governance.

The users of your application don’t care about tokens, they care about requests.
--------------------------------------------------------------------------------

When the users of your B2B dog-sitting marketplace sit down to ask your chatbot whether pit bulls cost extra, they aren’t counting tokens.
Introducing usage-based per-token billing will confuse and anger them.
They aren’t thinking about language models at all! They are thinking about asking for help and getting it.

This is a
*request*
, part of a user workflow that they pay you to help complete.
As your application grows and more users hit your chatbot to inquire regarding per-breed pricing, the number of requests will scale, and so will your costs.

So should you.
--------------

These requests are a key part of the boundary between you and your users, just as tokens are for the boundary between developers and LLM API providers.
As the engineer of the LLM engine that supports the service for your users, tokens are part of your internal reasoning, but they are secondary.

Let’s consider a few questions that come up when evaluating LLM self-hosting and see why a per-request framing is so helpful.

*What latency is acceptable? Can I hit that latency?*

Well, how quickly do users need a response to a request? Once you have that, you can ask how many tokens are in a typical request and response.
Get those numbers, then compare them to published results for time-to-first-token and inter-token latency using a tool like our
[LLM Engine Advisor](/llm-almanac/advisor)

— or run the benchmarks yourself using our framework,
[`stopwatch`](https://github.com/modal-labs/stopwatch)

.

If you just think in terms of aggregate “tokens per second”, you can’t get meaningful numbers for latency estimation.

*How many replicas do we need to serve our traffic?*

Well, how many requests does a user make per second, and how many users are online at once?
(Note: that’s probably variable, so you’ll need to think about
[managing your GPU allocation](/blog/gpu-utilization-guide)

too!).
That will give you an estimate of the requests per second you need to serve.

For a given latency target on these queries, a single replica of your LLM engine will be able to serve a certain number of concurrent requests.
The aggregate load of requests is then split among your replicas.

Again, if you think of your workload in terms of aggregate “tokens per second”, without considering requests, you’ll be unable to properly understand the load a single replica can handle.

Put simply: tokens cannot be arbitrarily split among replicas. Requests can. At best, you can be token-count-aware when routing requests.

*How much will this cost me?*

When you’re hosting your own LLM inference, you are paying for compute.

Costs for compute are measured in dollars over time — even for on-premises deployments, where capital costs are amortized over useful lifespans, on top of the time-denominated operating expenses that fully define costs in cloud deployments.

So once you’ve done the work to determine the number of requests you can support per replica while hitting your latency requirements and the number of replicas you need, you’re ready to determine the cost!
You just take the dollars per second per replica offered by your compute provider times the number of replicas you need to get a total in terms of dollars per second to serve the workload.

### Dollars ÷ seconds = dollars ÷ seconds ÷ replicas × replicas

Tokens are, quite literally, no longer part of the equation.

*Is the cost worth it?*

This final question, typically the most important question teams face when considering whether to build their own LLM inference, is best considered with no regard at all to tokens.

When costs are framed in dollars per request, the end user perspective is brought back to the center, where it belongs, and conversations are elevated to the level where engineering can act in concert with product, design, and revenue.

Is $1 per request “worth it”? Yes, if satisfying those requests leads to a >1% increase in conversion rate for users with a life-time-value of $100!
Is 10¢ per request “worth it”? No, if your users make 1k requests a month but only pay you $20!

Introducing the sizes of requests (denominated in tokens) into this discussion adds an extra dimension of variation that’s pure nuisance.
It’s the concern of an organization selling language model inference
*per se*
, not one building an application of language models or a system that includes that application.
And the dominance of those organizations, especially the ones selling proprietary models, is how we’ve ended up with this confused approach.

If you’d like to keep your dollars per request down while taking language model inference into your own hands, try Modal.
-------------------------------------------------------------------------------------------------------------------------

We learned these lessons working with a variety of teams that are taking advantage of advances in open weights language models and open source language model inference engines
to build high-throughput, low-latency, low-cost, high-control LLM applications on our serverless infrastructure platform,
[Modal](/)

.

If you’d like to read more about running your own language model inference,
check out the
[executive summary of our LLM engine benchmarks](/llm-almanac/summary)

or
[dive into those benchmark results directly](/llm-almanac/advisor)

.
If you’re interested in more hard-won insights gained helping teams break free of “AI from an API”,
check out
[our guide to thinking about GPU costs and optimizations](/blog/gpu-utilization-guide)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/embedding-models-article
================================================================================

Top embedding models for RAG
============================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

If you’re building a Retrieval-Augmented Generation (RAG) system, choosing the right embedding model is crucial for good performance.

This is because the embedding model directly affects the quality and relevance of retrieved information. Different models excel at capturing semantic relationships and contextual nuances.

For example, an embedding model trained on medical data will interpret the phrase “she scrubbed” differently than a general-purpose model.

Top embedding models for RAG
----------------------------

Some top embedding models to consider when you are evaluating for RAG are:

1. **[intfloat/e5-large-v2](https://huggingface.co/intfloat/e5-large-v2)**
   : This model is designed for efficient embedding generation and is suitable for various NLP tasks.
2. **[Salesforce/SFR-Embedding-2\_R](https://huggingface.co/Salesforce/SFR-Embedding-2_R)**
   : Developed by Salesforce, this model enhances text retrieval and semantic search capabilities.
3. **[Alibaba-NLP/gte-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)**
   : A high-performance model with 7 billion parameters, ideal for complex embedding tasks.
4. **[Alibaba-NLP/gte-Qwen2-1.5B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct)**
   : This 1.5 billion parameter model offers a balance between performance and resource requirements.
5. **[intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)**
   : A 0.5 billion parameter multilingual model that supports various languages.
6. **[jinaai/jina-embeddings-v2-base-en](https://huggingface.co/jinaai/jina-embeddings-v2-base-en)**
   : A 0.1 billion parameter model designed for English text embeddings.
7. **[jinaai/jina-embeddings-v2-base-code](https://huggingface.co/jinaai/jina-embeddings-v2-base-code)**
   : This model, also with 0.1 billion parameters, is optimized for code embeddings.
8. **[BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)**
   : This model is specifically designed for English embedding tasks and is part of the BGE family.

The MTEB leaderboard: A benchmark for embedding models
------------------------------------------------------

The
[Massive Text Embedding Benchmark (MTEB) leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

on Hugging Face evaluates embedding models across various tasks, providing a standardized comparison of performance in classification, clustering, retrieval, and semantic textual similarity.

While the MTEB leaderboard is a valuable resource, it’s important to remember that a high ranking does not necessarily mean a model is the best fit for every specific use case.

The only definitive way to determine whether an embedding model will perform well for your needs is to experiment with it and optimize it alongside various other parameters in a RAG system. Here are some general tips to using the MTEB leaderboard:

1. **Retrieval score**
   : Make sure to click into the “retrieval” task tab, and select the language that your use case is in.

![mteb-leaderboard](/_app/immutable/assets/mteb-leaderboard.EOwx2w0H.jpg)

2. **Sequence Length**
   : This indicates how many tokens a model can process into a single embedding. It’s best to limit input to a paragraph of text, with models supporting up to 512 tokens being sufficient for most cases.
3. **Model Size**
   : The model’s size impacts usability. The larger the model, the more expensive it is to run and the higher the latency. As a rule of thumb, start with a small model (~500 million parameters) and work up from there.

How to serve embedding models fast
----------------------------------

To use embedding models effectively in production environments, it’s essential to employ efficient serving frameworks. The
[text-embeddings-inference](https://github.com/huggingface/text-embeddings-inference)

library from Hugging Face is an excellent choice for fast and scalable embedding model deployment.

To run a text embedding model with
`text-embeddings-inference`
on Modal’s serverless computing platform, follow
[this example](/docs/examples/text_embeddings_inference)

.

Fine-tuning embedding models
----------------------------

Fine-tuning embedding models can significantly enhance their performance for specific use cases. By taking a pre-trained model and training it further on your own dataset, you can tailor the embeddings to better capture the nuances and characteristics relevant to your application. For a writeup for how you can fine-tune an embedding model on Modal, and get better performance than proprietary models, read our blog post
[here](/blog/fine-tuning-embeddings)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/embedding-wikipedia
================================================================================

Embedding English Wikipedia in under 15 minutes
===============================================

![author](https://modal-cdn.com/jason-liu.jpg)

[Jason Liu

@jxnlco](https://twitter.com/jxnlco)

AI Engineer

[View on GitHub](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/embeddings/wikipedia)

Text embeddings are a key component of production-ready applications using large
language models (LLMs). A text embedding model transforms chunks of text into
vectors of floating-point numbers that represent their semantic meaning,
allowing us to quantitatively compare strings for similarity. Creating
embeddings on a large corpus of text enables us to build applications like
search and recommendation engines, as well as give additional context to LLMs
for Retrieval-Augmented Generation (RAG) on custom documents.

Embedding models behind APIs like OpenAI’s
[`text-embedding-ada-002`](https://platform.openai.com/docs/api-reference/embeddings)

are a great way to get started with building for these use cases. However, as
you gather user data and tailor your applications using that data, you will
likely get
**higher-quality results at lower cost**
if you used this data to
[fine-tune](/docs/examples/llm-finetuning)

an open-source embedding model. This
requires setting up large-scale embedding jobs, which can be a challenge due to
rate limits, infrastructure complexity, and the infeasibility of getting a large
number of GPUs for short bursts of time. So what can we do? Enter Modal.

Modal provides a serverless solution for organizations grappling with scaling
workloads. Modal’s technology enables rapid scaling across many GPUs, which we
can use to run large-scale workloads, such as generating embeddings for a
massive text dataset, at lightning speed. In this post, we’ll go over everything
you need to embed the entire English Wikipedia in just 15 minutes using Hugging
Face’s
[Text Embedding Inference](https://huggingface.co/docs/text-embeddings-inference/index)

service on Modal. Using Modal’s serverless solution, this job comes out to just
over $15.

More specifically, we will:

1. Discuss the advantages of using open source models.
2. Explain the fundamentals of using Modal.
3. Guide you through the necessary code to implement our embedding client on the
   Wikipedia dataset.

Shortening the embedding generation time from multiple hours to just a few
minutes enables more frequent experimentation, which is crucial for continuous
model fine-tuning in production use cases (as you have to regenerate embeddings
for your entire corpus of data every time). In future posts, we’ll delve into
using Modal for other aspects of this workflow (running grid search on and
fine-tuning your own embedding models) to create more tailored user experiences.

Why open-source models?
-----------------------

Closed-source models are a great way to get started with creating and using
embeddings, but they run into two critical limitations in production:

1. As you run your model in production, you gather a corpus of rich preference
   data that can be used to improve the performance of your model. However,
   fine-tuning proprietary models with this custom data you’ve gathered is
   either impossible or highly cost-prohibitive.
2. Remote APIs have a number of drawbacks, such as rate limits, unreliable tail
   latencies, and high costs associated with tokens rather than compute time.

For these reasons, we believe that open-source embedding models that
progressively get better with fine-tuning are best suited for embedding use
cases like RAG workflows in production. Thousands of
[open-source models](https://huggingface.co/models?pipeline_tag=sentence-similarity&sort=trending)

are available on Hugging Face.

Why Modal?
----------

Model makes it easy to run your code in the cloud and push to production. By
only paying for what you use, and abstracting away all the complexity of
deploying and serving, Modal provides a simplified process to help you focus on
what’s important—your product.

> To follow along with some of these examples, you’ll need to
> [create a Modal account](/signup)
>
> . You’ll get $30 out of the box and all of
> the features to try out immediately. Once you’ve done so, make sure to
> [install the Modal Python package](https://pypi.org/project/modal/)
>
> using a
> virtual environment of your choice, and you can run all of the code we provide
> below.

Modal Concepts
--------------

Before we dive into the code, let’s take a look at some of the key concepts that
Modal provides that will allow us to run our embedding job quickly and
efficiently. In order to understand that, we’ll need to look at two concepts - a
`Function`
and a
[`Volume`](/docs/guide/volumes)

.

### Functions

Modal functions package the code you want to run, along with their environment.
They describe the image, the requirements, and the storage we want to attach in
order to get the job done.

```
import modal

app = modal.App()

pandas_image = modal.Image.debian_slim().pip_install("pandas")
volume = modal.Volume.from_name("embedding-wikipedia", create_if_missing=True)

@app.function(image=pandas_image, gpu="A100", volumes={"/root/foo": volume})
def my_fn():
    # perform tasks here
```

Using Modal functions, you could for example, provision on-demand GPUs for
fine-tuning workloads, define endpoints to serve large language models at scale,
and even spin up hundreds of containers to process large datasets in parallel.

### Volumes

In order to load large datasets and models efficiently, we can use Modal’s
[Volumes](/docs/guide/volumes)

feature. Volumes are a way to mount data into
your containers and allow you to read and write to them as if they were a local
file system. You can create a new volume using the
`modal volume create`
command.

Embedding Wikipedia
-------------------

Now that we’ve got a good understanding of some key concepts that Modal
provides, let’s load the
`wikipedia`
dataset in a persistent volume we’ve
created called
`embedding-wikipedia`
, set up the Hugging Face inference server,
and run our distributed batch GPU job to embed the entire dataset.

> The Hugging Face inference server is a fast way to get started to test
> different models from Hugging Face. They offer an easy-to-use client and a
> wide range of configurations to make the most out of your infrastructure.

### Loading the Dataset

We’ll be using the Hugging Face
`datasets`
library to download the dataset
before saving it explicitly into a directory of our choice for future use. In
order to do so, we’ll create a file called
[`download.py`](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/embeddings/wikipedia/download.py)

,
where we’ll create our first
[Modal image](/docs/guide/images)

with
the
`datasets`
package installed.

> Note here that we explicitly need to commit and save new changes to our
> volume. If not, these changes will be discarded once the container is shut
> down. See more information in our docs
> [here](/docs/guide/volumes#volume-commits-and-reloads)
>
> .

```
import modal

volume = modal.Volume.from_name("embedding-wikipedia")
image = modal.Image.debian_slim().pip_install("datasets")

app = modal.App(image=image)
cache_dir = "/data"

@app.function(volumes={cache_dir: volume}, timeout=3000)
def download_dataset(cache=False) -> None:
    from datasets import load_dataset

    # Download and save the dataset locally
    dataset = load_dataset("wikipedia", "20220301.en", num_proc=10)
    dataset.save_to_disk(f"{cache_dir}/wikipedia")

    # Commit and save to the volume
    volume.commit()
```

You can then run this file by using the command

```
modal run download.py::download_dataset
```

### Hugging Face Embedding Inference Server

For our embedding function, we’ll be using the Hugging Face
[Text Embedding Inference](https://github.com/huggingface/text-embeddings-inference)

server. We’ll walk through how to leverage caching of model weights by defining
another custom Modal image, managing container state through a Modal
`cls`
, and
lastly, leveraging this new container in our other functions.

### Parameters

Let’s start by defining some parameters for the
`Text Embedding Inference`
program. In our case, we’re specifying the specific embedding model we’re using
and increasing the maximum batch size so that we can speed up our embedding job.

```
MODEL_ID = "BAAI/bge-small-en-v1.5"
BATCH_SIZE = 768

LAUNCH_FLAGS = [
    "--model-id",
    MODEL_ID,
    "--port",
    "8000",
    "--max-client-batch-size",
    str(BATCH_SIZE),
    "--max-batch-tokens",
    str(BATCH_SIZE * 512),
]
```

### Defining Our Image

We’ll be using the recommended image for A10G GPUs for this example. If you’d
like to explore other GPU models, you should make sure to download the correct
model listed
[here](https://huggingface.co/docs/text-embeddings-inference/supported_models)

.
Note that we also override the default entrypoint so that it is compatible with
Modal.

```
tei_image = (
    Image.from_registry(
        "ghcr.io/huggingface/text-embeddings-inference:86-0.4.0",
        add_python="3.10",
    )
    .entrypoint([])
    .pip_install("httpx", "numpy")
)
```

### Creating our Modal Class

Using a Modal class enhances control over a container’s lifecycle (see more
[here](/docs/guide/lifecycle-functions)

):

1. Initialize once at boot with
   **@enter**
   .
2. Handle calls from other functions using
   **@method**
   decorators.
3. Clean up at shutdown with
   **@exit**
   .

We initialize a server at boot, spinning out an inference server that maintains
its state for subsequent requests and optimizes initialization costs. Modal
simplifies lifecycle management by requiring only a couple function definitions
and a decorator. Additionally, we configure the app class for specific images
and GPUs through
[`app.cls`](https://modal.com/docs/reference/modal.App#cls)

parameters. Once we’ve set this up, most of our code will focus on preparing our
data and efficiently sending it to the
`TextEmbeddingsInference`
servers.

```
import modal

GPU_CONFIG = "A10G"

def spawn_server() -> subprocess.Popen:
    import socket

    process = subprocess.Popen(["text-embeddings-router"] + LAUNCH_FLAGS)

    # Poll until webserver at 127.0.0.1:8000 accepts connections before running inputs.
    while True:
        try:
            socket.create_connection(("127.0.0.1", 8000), timeout=1).close()
            print("Webserver ready!")
            return process
        except (socket.timeout, ConnectionRefusedError):
            # Check if launcher webserving process has exited.
            # If so, a connection can never be made.
            retcode = process.poll()
            if retcode is not None:
                raise RuntimeError(f"launcher exited unexpectedly with code {retcode}")

@app.cls(
    gpu=GPU_CONFIG,
    image=tei_image, # This is defined above
)
class TextEmbeddingsInference:

    @modal.enter()
    def open_connection(self):
        # If the process is running for a long time, the client does not seem to close the connections, results in a pool timeout
        from httpx import AsyncClient

        self.process = spawn_server()
        self.client = AsyncClient(base_url="http://127.0.0.1:8000", timeout=30)

    @modal.exit()
    def terminate_connection(self, exc_type, exc_value, traceback):
        self.process.terminate()

    async def _embed(self, chunk_batch):
        texts = [chunk[3] for chunk in chunk_batch]
        res = await self.client.post("/embed", json={"inputs": texts})
        return np.array(res.json())

    @modal.method()
    async def embed(self, chunks):
        """Embeds a list of texts.  id, url, title, text = chunks[0]"""

        # in order to send more data per request, we batch requests to
        # `TextEmbeddingsInference` and make concurrent requests to the endpoint
        coros = [
            self._embed(chunk_batch)
            for chunk_batch in generate_batches(chunks, batch_size=BATCH_SIZE)
        ]

        embeddings = np.concatenate(await asyncio.gather(*coros))
        return chunks, embeddings
```

### Generating Embeddings

Let’s take stock of what we’ve achieved so far:

* We first created a Modal
  `App`
  .
* Then, we created a persistent
  `Volume`
  that could store data in between our
  script runs and downloaded the entirety of English Wikipedia into it.
* Next, we put together our first Modal
  `cls`
  object using the Text Embedding
  Inference image from Docker and attached an
  `A10G`
  GPU to the class.
* Lastly, we defined a method we could call from other app functions using the
  `@method`
  decorator.

Now, let’s see how to use the dataset that we downloaded with our container to
embed all of Wikipedia. We’ll first write a small function to split our dataset
into batches before seeing how we can get our custom Modal
`cls`
object to embed
all of the chunks.

### Chunking Text

We’ll be using the
[BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)

model in
order to embed all of our content. This model has state-of-the-art benchmark
results at great peformance. It has a maximum sequence length of 512 tokens so
we can’t pass in an entire chunk of text at once. Instead, we’ll split it into
chunks of 400 characters for simplicity using the function below, but in
practice you’ll want to split it more intelligently and include overlap between
chunks to avoid losing information.

```
def generate_chunks_from_dataset(xs, chunk_size: int = 400):
    for data in xs:
        id_ = data["id"]
        url = data["url"]
        title = data["title"]
        text = data["text"]
        for chunk_start in range(0, len(text), chunk_size):
            yield (
                id_,
                url,
                title,
                text[chunk_start : chunk_start + chunk_size],
            )
```

To amortize the overhead of data transfer, we batch our
`generate_chunks_from_dataset`
chunks into batches of 512 chunks each. This
allows us to pass in a batch of 512 chunks to our Modal
`cls`
object to embed at
once.

```
def generate_batches(xs, batch_size=512):
    batch = []
    for x in xs:
        batch.append(x)
        if len(batch) == batch_size:
            yield batch
            batch = []
    if batch:
        yield batch
```

### Mapping the embedding function

After creating a function to batch our dataset, we can now pass these chunks to
our Modal
`cls`
object for embedding. We use a custom image with the
`datasets`
library installed to easily load our dataset from disk. Additionally, we have
logic to extract a subset of the dataset.

To call our custom Modal
`cls`
object and use the
`.embed`
function with our
data batches, we simply use the
`.map`
function. Modal takes care of managing
the containers, serializing and deserializing inputs, and handling the lifecycle
of each container.

```
@app.function(
    image=Image.debian_slim().pip_install("datasets"),
    volumes={cache_dir: volume},
    timeout=5000,
)
def embed_dataset():
    dataset = load_from_disk(f"{cache_dir}/wikipedia")
    model = TextEmbeddingsInference()

    text_chunks = generate_chunks_from_dataset(dataset["train"], chunk_size=512)
    batches = generate_batches(text_chunks, batch_size=batch_size)

    # Collect the chunks and embeddings
    for batch_chunks, batch_embeddings in model.embed.map(batches, order_outputs=False):
        ...

    return
```

Once we have this function we can use
`modal run`
on this
[`main.py`](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/embeddings/wikipedia/main.py)

file to execute the specific function:

```
modal run main.py::embed_dataset
```

[

](https://modal-cdn.com/embedding-wikipedia-demo.mp4)

Further customization
---------------------

### Deploying on a schedule

In a production setting, you might want to run this on a schedule as new data
comes in or as you get more user data. This allows you update data and models in
production without having to worry about the underlying infrastructure. You just
need to modify the
`@app.function`
decorator to add in a
`schedule`
parameter.
This can be modified to any arbitrary period that you’d like to use depending on
your use case.

```
@app.function(..., schedule=modal.Period(days=1))
def my_function():
    pass
```

We can then deploy this function using the command

```
modal deploy --name wikipedia-embedding main.py
```

If you’d like to change the frequency, just change the schedule parameter and
re-deploy, and you’re good to go!

### Uploading your dataset

If you check out our example code, you’ll notice that we’ve uploaded the
embedded dataset to a
[public Hugging Face dataset](https://huggingface.co/datasets/567-labs/wikipedia-bge-small-en-v1.5-full)

.
We provide some details in the README on how to do this. In practice, how you
handle this data will depend on your use case. You can also can follow similar
steps to upload it to a private dataset or insert it into your favorite vector
database.

### GPUs go brr

For free accounts, Modal caps the concurrent number of GPUs that can be used
to 10. Using 10 GPUs in parallel still greatly speeds up the embedding job, but
if you are on a
[paid plan](https://modal.com/pricing)

, the GPU limit can be
raised.

All we really need to do then is crank up the value of
`max_containers`
to a
number like 50, and we’ll end up with 50 separate containers (each with their
own
`A10G`
GPU) processing batches of text to be embedded.

```
@app.cls(
    gpu=GPU_CONFIG,
    image=tei_image,
    max_containers=50,  # Number of concurrent containers that can be spawned to handle the task
)
class TextEmbeddingsInference:
    # Rest of code below
```

Conclusion
----------

In this post, we show how to use some of Modal’s abstractions to run massive
parallelizable jobs at scale. Having the ability to scale unlocks new business
use cases for companies that can now iterate on production models more quickly
and efficiently. By shortening the feedback loop with Modal’s serverless GPUs,
teams are free to focus on experimentation and deployment.

We’ve uploaded our full code
[here](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/embeddings/wikipedia)

,
which helps you quickly get started and also showcases how to upload your own
generated embeddings to Hugging Face. You can also check out some
[example datasets](https://huggingface.co/567-labs)

that contain embeddings we
computed using some popular open source embedding models.

Try running your own large-scale batch jobs by
[creating your free Modal account](https://modal.com/signup)

, and follow
[@modal\_labs](https://twitter.com/modal_labs)

on X/Twitter to stay posted on
upcoming posts on further customizing your embeddings workflow.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/etl
================================================================================

Why you should move your ETL stack to Modal
===========================================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Data Engineer

![ETL diagram](https://modal-cdn.com/cdnbot/etl-diagram.jpg)

ETL (Extract, Transform, Load) is the process of moving data from point A to
point B.

Most commonly, ETL means moving data from some
**source system**
(e.g. a
production database, Slack API) into an analytical
**data warehouse**
(e.g.
Snowflake) where the data is easier to combine and analyze. Most data teams use
a vendor like Fivetran or an orchestration platform like Airflow to do this.

Modal is a great solution for ETL if you are primarily looking for:

* **Cost savings on large-scale data transfers.**
  Modal’s usage-based pricing
  means you pay for how much compute you use and not how many rows you sync,
  making it a far more cost-effective option for moving large amounts of data
* **An**
  **easy and flexible platform for custom code.**
  Orchestration platforms
  like Airflow are notoriously difficult to set up and are often overkill for
  95% of data jobs that call for a simple cron-like scheduling pattern. Modal is
  the easiest way to get those kinds of custom ETL jobs running.

In this post, I’ll walk through two examples inspired by how we do our internal
analytics that clearly show the cost and flexibility advantages of using Modal
for ETL.

Example 1: Copy 12m ClickHouse rows to Snowflake at .01% of the cost of Fivetran
--------------------------------------------------------------------------------

We use ClickHouse to serve metrics on resource usage and run time for our
customers’ jobs. We’d like to move this data into Snowflake so that we can
combine this data with other information we have on our customers and answer
questions like “what is the conversion from a Modal workspace creation to using
1 hour of compute?”.

First, we extract from ClickHouse using their native
[Python connector](https://clickhouse.com/docs/en/integrations/python#querying-data-with-clickhouse-connect--advanced-usage)

:

```
def extract_from_clickhouse(date):
    import clickhouse_connect

    query = f"""
        select
            timestamp_minute,
            workspace_id,
            billing_type,
            cpu_ns / 3600e9 as cpu_hr,
            mem_ns / 3600e9 as mem_hr,
            gpu_ns / 3600e9 as gpu_hr
        from metrics
        prewhere toDate(timestamp_minute) == '{date}'
    """

    client = clickhouse_connect.get_client(
        host=os.environ["CLICKHOUSE_HOST"]
        port=os.environ["CLICHOUSE_PORT"]
        username="default",
        password=os.environ["CLICKHOUSE_PASSWORD"],
        secure=True,
    )
    result = client.query(query)
    print(f"Fetched clickhouse data for {date}")
    return result.result_rows
```

This returns the query results as a list of tuples, where each tuple is a row.
Then we batch load the results into Snowflake:

```
def load_to_snowflake(data: list[tuple], date):
    target_table = 'USAGE_BY_MINUTE'
    batch_size = 10000
    insert_sql = f"""
    insert into CLICKHOUSE.{target_table} (timestamp_minute, workspace_id, billing_type, cpu_hr, mem_hr, gpu_hr, inserted_at)
    values (%s, %s, %s, %s, %s, %s, current_timestamp())
    """
    for i in range(0, len(data), batch_size):
        batch = data[i : i + batch_size]
        print(f"Loading batch {date}:{i}-{i+batch_size}")
        cursor.executemany(insert_sql, batch)
        conn.commit()

    # Close the cursor and connection
    cursor.close()
    conn.close()

    print(f"Data inserted successfully.")
```

Here we are using Snowflake’s
[executemany](https://community.snowflake.com/s/article/How-To-Insert-JSON-data-using-Executemany-in-Python)

function, which batch inserts 10,000 rows at a time. We set the batch size to
10,000 because Snowflake’s insert statement has a limit of
[16,384 rows](https://docs.snowflake.com/en/sql-reference/sql/insert#usage-notes)

in a single call.

Now, we add some Modal 🪄magic🪄:

```
@app.function(
    secrets=[
        modal.Secret.from_name("snowflake-secret"),
        modal.Secret.from_name("clickhouse-prod")
    ],
    timeout=3000
)
def run_etl(date):
    results = extract_from_clickhouse(date)
    load_to_snowflake(results, date)

@app.local_entrypoint()
def main():
    dates = [
        '2024-04-07',
        '2024-04-08',
        '2024-04-09',
        '2024-04-10',
        '2024-04-11'
    ]
    run_etl.for_each(dates)
```

We use
`@app.function`
to execute
`run_etl`
in the cloud with the following
parameters:

* Database credentials as environment variables via
  [Secrets](https://modal.com/docs/guide/secrets)
* A
  [timeout](https://modal.com/docs/guide/timeouts#timeouts)

  of 50 minutes
  (default is 5 minutes)

In
`main()`
, we kick off 5
`run_etl`
jobs in parallel by date using
[for\_each](https://modal.com/docs/reference/modal.Function#for_each)

to greatly
speed up processing time.

Here are the statistics of an example run:

![ETL example metrics](https://modal-cdn.com/cdnbot/etl-example-metrics.png)

This job copied 12m rows from Clickhouse to Snowflake in 16 minutes using:

* **5 CPUs**
  : at $0.192 / CPU hour that comes out to $0.26
* **4.4 GiB**
  of memory: at $0.024 / GiB per hour that comes out to $0.03

Even if Fivetran had a ClickHouse connector
(
[it doesn’t](https://www.fivetran.com/connectors?q=clickhouse&noresults=true)

at the time of this writing), syncing 12m rows would
[cost ~$3300](https://www.fivetran.com/pricing)

. The total cost of this Modal
job is
**$0.29**
(0.01% of Fivetran).

You could argue that the Modal job costs more in developer time compared to an
ETL vendor. In my opinion (and hopefully yours too after reading the code
snippets!), this example was quite simple; I’d estimate an analytics engineer
could write this in less than a day and spend at most a few hours a month
maintaining it.

And this is where the real cost savings come in: by making your engineers more
productive. In this next example, we’ll show how easy it is to write your own
custom data jobs on Modal.

Example 2: Enrich user data with the Github API
-----------------------------------------------

Most of our customers first sign up using their username. However, we also want
to know what company they work for so we can see if they would be interested in
our
[Team or Enterprise tier](https://modal.com/pricing)

. One way to get that
information is from a user’s Github profile:

![ETL Github screenshot](https://modal-cdn.com/cdnbot/etl-github-screenshot.png)

The only thing we know about the ComfyUI creator...

First, we extract some user ids and associated Github usernames from our data
warehouse:

```
def get_usernames():
    import snowflake.connector
    conn = snowflake.connector.connect(
        user="snowflake_user",
        password=os.environ["SNOWFLAKE_PASSWORD"],
        account=os.environ["SNOWFLAKE_ACCOUNT"],
    )
    cursor = conn.cursor()
    q = """
    select
        id,
        github_username

    from user

    where github_username is not null
    """
    cursor.execute(q)
    df = cursor.fetch_pandas_all()
    print(f"Got {df.shape[0]} rows.")
    cursor.close()
    conn.close()
    return df
```

Then, we write a function to query the
[Github API](https://github.com/PyGithub/PyGithub)

for a user’s company:

```
def get_company(user):
    from github import Auth, Github, GithubException

    auth = Auth.Token(os.environ['PAT'])

    g = Github(auth=auth)

    try:
        user = g.get_user(user)
    except GithubException:
        print(f"Request for {user} failed, skipping.")
        return None

    return user.company
```

Finally, we apply that function on our user data to get an enriched dataset with
a user’s Github-listed company. To query the Github API, first create a
[personal access token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens)

and add it to Modal as a Secret:

```
def get_user_companies(df):
    print("Querying Github API...")
    df['company'] = df['GITHUB_USERNAME'].apply(get_company)
    return df

@app.function(
    secrets=[
        modal.Secret.from_name("kenny-github-secret"),
        modal.Secret.from_name("snowflake-secret")
    ],
)
def main():
    users_df = get_usernames()
    enriched_df = get_user_companies(users_df)
    print(enriched_df.head())
```

Running this script gives us:

```
Got 100 rows.
Querying Github API...
Request for xxxx failed, skipping.
       ID GITHUB_USERNAME          company
0  us-abc        xxxxxxxx  Duke University
1  us-def      xxxxxxxxxx             None
2  us-ghi         xxxxxxx             None
3  us-jkl    xxxxxxxxxxxx             None
4  us-mno       xxxxxxxxx             None
```

Looks like we need to schedule some college tours, starting with Duke 🔵😈

Let’s say you want to schedule this to run every day. This is as simple as
attaching a
[Period](https://modal.com/docs/reference/modal.Period)

or
[Cron](https://modal.com/docs/reference/modal.Cron)

argument into
`@app.function`
:

```
@app.function(
    secrets=[
        modal.Secret.from_name("kenny-github-secret"),
        modal.Secret.from_name("snowflake-secret")
    ],
    # run this cloud function every day at 6am UTC
    schedule=modal.Cron("0 6 * * *")
)
```

The ETL vendors want you to be afraid of writing custom code, but hopefully this
example shows you how easy it is to add your own custom logic to make simple,
yet powerful data enrichments.

Conclusion (when to not use Modal for ETL)
------------------------------------------

Traditional ETL solutions are still quite powerful when it comes to:

* **Common connectors with small-medium data volumes**
  : we still have a lot of
  respect for companies like Fivetran, who have really nailed the user
  experience for the most common ETL use cases, like syncing Zendesk tickets or
  a production Postgres read replica into Snowflake. The only criticism we have
  is the pricing model, especially for larger data volumes.
* **Long-running, business-critical, multi-stage pipelines**
  : this is where you
  will get the value from an orchestration platform like Airflow e.g. function
  caching, partial retries, granular observability metrics. For what it’s worth,
  Modal is also actively thinking about how to address some of these use cases
  better.

The data community is going through a sea change, where people are realizing
that writing custom code is actually an
**asset**
and not a cost. It reduces
your risk of vendor lock-in, expands your universe of data solutions, and is
orders of magnitude cheaper. Powered by Modal, your ETL process can finally
unlock the flexibility, speed, and cost savings necessary in the new modern data
era.

More examples
-------------

Check out these other Modal examples for common data and analytics use cases:

* [Deploying a dbt project](https://github.com/modal-labs/modal-examples/blob/main/10_integrations/dbt/dbt_duckdb.py)

  that transforms S3 data using the duckdb adapter
* [Hosting Streamlit apps](https://modal.com/docs/examples/serve_streamlit)
* [Sending daily reports to Google Sheets](https://modal.com/docs/examples/db_to_sheet)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fast-cheap-batch-transcription
================================================================================

Transcribe speech 100x faster and 100x cheaper with open models
===============================================================

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

Developer Advocate

![author](https://modal-cdn.com/blog/images/shababo.webp)

Ben Shababo

ML Engineer

![author](https://modal-cdn.com/lucy-zhang.png)

[Lucy Zhang

@lu\_cyzhang](https://twitter.com/lu_cyzhang)

Forward Deployed Engineer

![](https://modal-cdn.com/blog/images/top-pareto.webp)

Open ASR models have arrived.
-----------------------------

Since ChatGPT showed the world that generative modeling and artificial intelligence are ready for industrial-scale commercial applications, the most performant modeling and intelligence services have largely been provided by proprietary APIs.

But over the past year, open weights models have caught up across a range of domains from
[language](/examples/llama_cpp)

to
[images](/blog/flux-3x-faster)

to
[video](/docs/examples/ltx)

. Open source frameworks to run those models, like
[PyTorch](/docs/examples/torch_profiling)

and
[vLLM](/docs/examples/vllm_inference)

, have kept pace.

And you might have missed it, but in just the past few months, there’s been a wave of highly performant open weights models focused on automated speech recognition (ASR), aka speech-to-text (STT), aka transcription.

These models - which include NVIDIA’s
[Parakeet](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2)

&
[Canary](https://huggingface.co/nvidia/canary-1b-flash)

and Kyutai’s
[STT](https://kyutai.org/next/stt)

- report incredible accuracy (word-error rate, WER), speed (real-time factor, RTFx), and include additional features like multiple languages, word-level timestamps, or vocal activity detection (VAD).

| **Model** | **[ESB](https://arxiv.org/abs/2210.13352)  WER, English (%)** | **RTFx** | **Languages** | **Timestamps** | **VAD** |
| --- | --- | --- | --- | --- | --- |
| nvidia/parakeet-tdt-0.6b-v2 | 6.05 | 3386.02 | ✅ | ✅ | ❌ |
| nvidia/canary-1b-flash | 6.35 | 1045.75 | ✅✅✅✅ | ✅ | ❌ |
| kyutai/stt-2.6b-en | 6.4 | 88.37 | ✅ | ✅ | ✅ |

### Some CEO math: Modal + Open ASR > 100x

A quick back-of-the-envelope calculation was enough to get us pretty excited about these new models.

* Proprietary APIs charge about 0.4¢ per minute of audio
* The Open ASR leaderboard indicates an RTFx, aka audio minutes per wall clock minute, of several thousand on A100s
* An A100 or L40S GPU on Modal is currently about 4¢ per minute

Even making pessimistic assumptions about overhead, the napkin math indicates it should be possible to hit 100x cheaper per minute of audio when running transcription.

We heard some rumors this was true (from people churning off proprietary APIs and onto our platform), so we decided to see for ourselves.

We

* implemented a batch transcription service on Modal
* using NVIDIA’s
  [parakeet-tdt-0.6b-v2](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2)

  (English) and
  [canary-1b-flash](https://huggingface.co/nvidia/canary-1b-flash)

  (multilingual)
* and compared it to a popular proprietary API
* on roughly a week (7 \* 24 hours) of speech data from the
  [ESB](https://arxiv.org/abs/2210.13352)

  benchmarking
  [datasets](https://huggingface.co/datasets/hf-audio/esb-datasets-test-only-sorted)

  used in the
  [HuggingFace ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard)

  .

A few optimizations and experiments later and we were able to deploy a transcription service that
**is over 100x faster or 100x cheaper than the proprietary API we tested while matching its error rate.**
(Technically, the open models had a slightly lower error-rate on this dataset…)

That’s

* one week of audio
* transcribed in just one minute
* for just $1.

![](https://modal-cdn.com/blog/images/main-pareto.webp)

|  | Relative ETE Throughput | Cost Savings |
| --- | --- | --- |
| **English only** |  |  |
| Parakeet, Fastest | **112x** | 60x |
| Parakeet, Cheapest | 25x | **200x** |
| Proprietary API | 1x | 1x |
| **Multilingual** |  |  |
| Canary, Fastest | **80x** | 55x |
| Canary, Cheapest | 12x | **152x** |
| Proprietary API | 1x | 1x |

How we transcribed a week of audio in a minute for a dollar.
------------------------------------------------------------

We want to share some of the details of our implementation and optimization process related to batching and distributing transcription requests across workers.

Hopefully these can help you optimize your own transcription service and build more performant Modal apps.

### The real world use case: transcribing many hours of audio per hour.

Before we jump into the engineering, let’s make sure we’re clear on the use cases we’re trying to optimize the system for. A good idea for any performance engineering task!

The ideal use case for large-scale batch transcription is a company that’s collecting many, many seconds of audio per second. Say a call center where
[your call may be recorded for quality assurance](https://www.scorebuddyqa.com/blog/this-call-is-being-recorded-for-quality-assurance)

, generating thousands of hours of new recordings every hour. The data is
~~dumped on S3~~
loaded into a data lake and every hour we need to load up the new files and transcribe them.

Or you might be a foundation model company that’s looking for language tokens wherever you can find them. You’ve scraped audio files from all over the Internet and want to transcribe them so that they can be used for next-token-prediction training of a text-based language model.

In both cases, we can assume that at rest, the data lives in some object/file-like cloud storage, and the figures of merit are

* how quickly we can finish processing all of the data
* how much the processing costs

Batch transcription is qualitatively different from a streaming scenario where we have many users who each require a low-latency transcription service for real-time applications. The two are evaluated with different metrics and require different implementation choices.

If you’re interested in streaming transcription, check out our
[Kyutai STT example](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/speech-to-text/streaming_kyutai_stt.py)

. For use cases somewhere between pure batch jobs and streaming, you might consider dynamically batched transcription (code sample
[here](/docs/examples/batched_whisper)

).

### Designing a benchmark for batch audio transcription

**We sanity-checked quality via WER and the ESB/HuggingFace Open ASR Leaderboard benchmark.**

One advantage of choosing models included in the HuggingFace ASR Leaderboard is that we can use the same datasets and WER scoring code. This allows us to verify our deployments haven’t introduced any bugs or somehow reduced accuracy.

It also helps ground our accuracy comparison to the proprietary API in the full leaderboard results.

**But we measured end-to-end performance, not just model execution time.**

ASR models report speed as RTFx which only considers transcription time. This makes sense if you’re running inference locally (relative to the audio byte producer and the text consumer). But for distributed cloud services, you need to consider data transfer times. For a cloud service with horizontal scaling, you also need to consider cold start times.

![](https://modal-cdn.com/blog/images/local-remote-distributed.webp)

We get why model providers want to focus on the time the model takes. But to us, it seems unfair to ignore these parts of the process. Most of us ultimately care about the total time our jobs take, not just the fancy “AI” part. To account for this, we report end-to-end throughput which measures the duration at the client and includes cold-starts, network latencies, and other forms of data movement.

### Architecting batch transcription with Modal

We used Modal to set up
[batch processing](/docs/guide/batch-processing)

of transcription using both open weights models and a proprietary API. And in both cases we spent time optimizing throughput.

The main difference between the two architectures is that we provision GPUs and perform transcription on the Modal container itself when using open models whereas we provision CPU containers which make parallel HTTP requests when using the proprietary API. We don’t include the costs of CPUs in any of our numbers. Compared to GPUs and API requests, they might as well be free!

### Deploying NVIDIA’s ASR models on Modal GPUs

NVIDIA lays claim to many of the most accurate and fastest open ASR models. We opted to use their
**[parakeet-tdt-0.6b-v2](https://huggingface.co/nvidia/parakeet-tdt-0.6b-v2)**
model, since it reports 3x faster inference than any model with a comparable error rate, as well as its sister model,
**[canary-1b-flash](https://huggingface.co/nvidia/canary-1b-flash)

,**
which has a lower RTFx but handles multiple languages (English, Français, Español, und Deutsch).

We built our service on NVIDIA’s
[NeMo framework](https://docs.nvidia.com/nemo-framework/index.html)

, which makes it relatively simple to switch between any of NVIDIA’s ASR models.

It was a pretty small lift to take the NeMo code from the
[HuggingFace Open ASR GitHub](https://github.com/huggingface/open_asr_leaderboard/tree/main/nemo_asr)

repo and turn it into a distributed batch service that will dynamically provision GPUs when we spin up a job. A basic demo fits in
[a single Python file](/docs/examples/batched_whisper)

— no YAML, no problems.

![](https://modal-cdn.com/blog/images/app-diagram-gpu.webp)

**Saturating a proprietary API from Modal CPUs**

We selected a popular and competitive proprietary ASR API and tested the base level of service. To provide a fair comparison on throughput, we tested multiple approaches to saturate request rates using a distributed Modal deployment. The core strategy to maximize throughput was to distribute the transcription requests over a number of workers equal to maximum concurrency limit of the API.

![](https://modal-cdn.com/blog/images/app-diagram-api.webp)

**We observed throttling but we ignored it to be more fair.**

Our request throughput was potentially throttled during runs over full the dataset. To be as fair as possible, we report the maximum throughput we observed. While we can’t be sure we are measuring the same thing, our estimated maximum throughput matched the rate reported on the API’s documentation and marketing site.

Doing some 100x engineering
---------------------------

Distributing a computation - whether it’s across the
[streaming multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)

in a single GPU or across a global fleet of cloud instances - can result in impressive throughput gains. But it can alternatively result in a reduction in throughput depending on how effectively your implementation moves data around and
[makes use of the available hardware](https://www.youtube.com/watch?v=y-UGrYbJsJk)

.

Determining the best implementation is a combination of thoughtful design when architecting the system and experimentation with the systems knobs — you know, “engineering”.

### Distributing data into batches

We can think about some of our optimizations as packing problems - both in terms of memory and time. When we don’t pack memory efficiently we end up performing more iterations or requests than is necessary. When we don’t pack time efficiently, we end up waiting on work while also having empty compute lanes. This is the difference between a balanced and unbalanced load distribution.

In other words, we want to fill all of the available lanes all of the time.

![](https://modal-cdn.com/blog/images/balance-unbalanced-load.webp)

In our system, we need to distribute data into batches over requests to our Modal Function as well as within each GPU.

**Batching audio files into requests**

To balance the workload across our Modal autoscaling pool, we want to match the distribution of data sent in each request in the following ways

* the number of bytes
* the number of audio files
* total duration of audio files to ensure that each worker spends about the same amount of time downloading data as well as performing transcription.

Fortunately, for large batch jobs we can simply shuffle the data before dealing it into request batches to ensure the distributions are matched. Be sure not to skip this step. There can be correlations between sample ordering and duration baked into your tables; or the data may come pre-sorted as is the case with the ESB/HF datasets.

**Batching bytes to make GPUs go brrrt**

Most, if not all, of the entrants on the ASR Leaderboard perform GPU inference in batches. For Parakeet this is as simple as adding the
`batch_size`
keyword argument to the
`transcribe`
call.

Batch inference makes better use of the GPU,
[a massively parallel, throughput-oriented computing device](https://www.youtube.com/watch?v=y-UGrYbJsJk&t=1s&pp=ygUkd2hhdCBldmVyeSBhaSBlbmdpbmVlciBuZWVkcyB0byBrbm93)

. It exposes more parallelism to the underlying
[program runtime](/gpu-glossary/device-software/cuda-programming-model)

and
[hardware](/gpu-glossary/device-hardware/tensor-core)

. Our batch size is limited by the
[GPU’s memory](/gpu-glossary/device-hardware/gpu-ram)

.

Luckily, the NVIDIA submissions to the HuggingFace leaderboard give us some idea of what GPU batch size to use. And during experiments we found that a range of values performed more or less equally well.

But what about packing in time?

Longer recordings take longer to process. This has important consequences for how we distribute audio into batches because a batch is only complete when all of its elements have been processed. This leads to a strategy of matching audio duration within GPU batches to maximize execution time within each lane of the GPU.

![](https://modal-cdn.com/blog/images/gpu-batching.webp)

To implement this strategy, sort your audio segments before mapping your inference calls. Remember, we’re talking about sorting the audio for a single GPU worker/request. We shuffled before batching into requests to balance the workload, but now we want to sort each of those requests to maximize the efficiency of GPU batching.

If you are getting PTSD flashbacks to sort phases in Hadoop Map-Reduce, 1) we feel you and 2)
[we’re hiring](/careers)

and we promise you won’t ever see HDFS in prod. While the fix itself is relatively simple, the effect can be quite large, as recording lengths are often exponentially distributed (presumably reflecting underlying
[memoryless dynamics](https://www.probabilitycourse.com/chapter4/4_2_2_exponential.php)

).

**Downloading data to workers**

We also need to move the data to each worker. And the best solution is highly dependent on the location and state of your data at rest.

In our setup, we start with audio segments saved as WAV files on a
[Modal Volume](/docs/guide/volumes)

. Before transcription begins, we download the files to local disk. To minimize transfer time, we use Python’s
`multiprocessing.ThreadPool`
to saturate the network bandwidth with parallel requests. Avoiding disk writes by keeping files in memory might speed things up a bit, but SSD throughput is so much higher than network throughput that we didn’t think it worth trying.

### Selecting a GPU type and number of requests

There are two more design decisions that will have a large impact on our cost and throughput: GPU model and number of requests.

These choices interact with each other in somewhat complex ways: for instance, big GPUs are faster but more expensive. Instead of trying to reason through which combination will lead to the best results, we chose to empirically determine the optimal configurations - MLEs in the audience, think hyperparameter fine-tuning.

Because we have two figures of merit, cost and throughput, there is not necessarily a single “optimal configuration”. There is instead a Pareto frontier of configurations (per model) that dominate over all others but represent distinct trade-offs of cost and speed.

Note that the proprietary inference service is not shown on this chart — if we were to draw it, it’d be somewhere near the word “proprietary” at the beginning of this sentence.

![](https://modal-cdn.com/blog/images/configs-pareto.webp)

For our batch transcription service, we can choose different combinations of GPU model and number of workers depending on whether we want to save money or save time or maybe split the difference. But regardless of exactly which configuration we choose, deploying the top open ASR models on Modal is competitive alternative to proprietary services.

Run your own large-scale, high-performance audio transcription on Modal in minutes!
-----------------------------------------------------------------------------------

Modal users like
[Substack](/blog/substack-case-study)

and Zencastr run their transcription workloads at scale on Modal — no proprietary APIs or AWS YAML Engineer certifications needed, just open source models, a Python file or two, and a Modal API key to provision the resources on our platform.

Learn more about boosting your
[audio inference](/use-cases/audio)

services with Modal, quickly deploy your own batch transcription service using
[the code from this post](https://github.com/modal-labs/open-batch-transcription)

, or check out our other
[speech-to-text examples](https://github.com/modal-labs/modal-examples/tree/main/06_gpu_and_ml/speech-to-text)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/faster-transcription
================================================================================

5 Ways to Speed Up Whisper Transcription
========================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

How to use Whisper
------------------

[Whisper](https://openai.com/index/whisper/)

is a state-of-the-art open-source speech-to-text model developed by OpenAI, designed to convert audio into accurate text.

To get started with Whisper, you have two primary options:

* **OpenAI API**
  : Access Whisper’s capabilities through the
  [OpenAI API](https://platform.openai.com/docs/guides/speech-to-text)

  .
* **Self-hosted deployment**
  : Deploy the
  [open-source Whisper library](https://github.com/openai/whisper)

  on your own hardware, such as
  [Modal](/docs/examples/whisper-transcriber)

  , to maintain control over your transcription processes. This option allows you to utilize Whisper as:
  + A command-line tool for quick and straightforward transcription tasks.
  + A
    [Python library](https://pypi.org/project/whisper/)

    for more complex integrations and custom applications.

Your Whisper implementation is too slow. Now what?
--------------------------------------------------

Let’s say that you choose to self-host Whisper. If, for whatever reason, you
find that your Whisper implementation is too slow for your needs, here are some
strategies to speed it up:

### 1. Leverage GPU Acceleration 🚀

The single most effective way to speed up Whisper is to run it on a GPU. By
offloading computations from CPU to GPU, you can achieve dramatically faster
inference times, especially for larger versions of Whisper.

To run Whisper on a GPU, first make sure that you have the
[CUDA drivers](/docs/guide/cuda)

installed.

You can usually do this by installing
`torch`
with CUDA support. Then ensure that your GPU is being used by Whisper by setting the
`device`
argument to
`cuda`
.

```
import whisper

model = whisper.load_model(model_size, device="cuda")
```

2. Choose Smaller Models 🎯
--------------------------

Whisper offers multiple model sizes, each with different speed-accuracy tradeoffs:

* **tiny**
  : Fastest but least accurate
* **base**
  : Good balance for many use cases
* **small**
  : More accurate than base, still reasonably fast
* **medium**
  : Better accuracy, slower processing
* **large**
  : Most accurate, but slowest

If speed is crucial, consider using
`base`
or
`small`
models. They often provide sufficient accuracy while processing audio significantly faster than larger models.

3. Process Audio Chunks in Parallel ⚡
-------------------------------------

For long audio files like podcasts or meeting recordings, parallel processing can dramatically reduce total transcription time. Here’s how:

1. Split your audio into smaller chunks (e.g., 30-second segments)
2. Process multiple chunks simultaneously
3. Combine the results

If you are self-hosting Whisper on a platform like Modal, you can use Modal’s
[`.map`](/docs/guide/scale)

feature to process audio chunks in parallel.

Here is
[an example](/docs/examples/whisper-transcriber)

of how you can use Modal to transcribe hour-long podcasts in
under a minute.

4. Implement Real-Time Streaming 🔄
----------------------------------

If you need real-time transcription, the standard open-source Whisper library (which processes 30-second chunks) won’t cut it. Instead, use
[Whisper Streaming](https://github.com/ufal/whisper_streaming)

, which enables:

* Live audio processing
* Immediate transcription output
* Lower latency for interactive applications

For optimal streaming performance, pair it with
[Faster-Whisper](https://github.com/guillaumekln/faster-whisper)

as the backend.

5. Try Optimized Whisper Variants 🔧
-----------------------------------

Several optimized versions of Whisper offer significant speed improvements:

* [WhisperX](https://github.com/m-bain/whisperX)

  : Enhanced speed and word-level timestamps
* [Faster-Whisper](https://github.com/guillaumekln/faster-whisper)

  : Optimized for GPU performance
* [Whisper.cpp](https://github.com/ggerganov/whisper.cpp)

  : Efficient C++ implementation

These variants can provide substantial performance gains while maintaining accuracy.

Deploy Fast Whisper on Modal
----------------------------

Want to implement these optimizations without managing infrastructure?
[Modal](https://modal.com)

offers serverless GPU-powered compute that makes it easy to:

* ✅ Run Whisper on powerful GPUs
* ✅ Scale automatically with demand
* ✅ Pay only for actual usage
* ✅ Focus on building, not infrastructure

Start deploying high-performance Whisper on
[Modal](https://modal.com)

today!

// … previous content remains the same …

Additional Resources
--------------------

### Official Whisper Documentation

* [Whisper Official Repository](https://github.com/openai/whisper)
* [OpenAI Whisper Homepage](https://openai.com/index/whisper/)
* [OpenAI API Documentation](https://platform.openai.com/docs/guides/speech-to-text)

### Optimized Implementations

* [WhisperX GitHub Repository](https://github.com/m-bain/whisperX)
* [Faster-Whisper GitHub Repository](https://github.com/guillaumekln/faster-whisper)
* [Whisper.cpp GitHub Repository](https://github.com/ggerganov/whisper.cpp)
* [Whisper Streaming GitHub Repository](https://github.com/ufal/whisper_streaming)

### Deployment & Infrastructure

* [Modal GPU Guide](/docs/guide/gpu)
* [Modal CUDA Setup Guide](/docs/guide/cuda)
* [Modal Scaling Guide](/docs/guide/scale)
* [Example of how to transcribe podcasts in parallel on Modal](/docs/examples/whisper-transcriber)

### Python Packages

* [Whisper PyPI Package](https://pypi.org/project/whisper/)
* [PyTorch with CUDA](https://pytorch.org/get-started/locally/)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fine-tuning-embeddings
================================================================================

Beating Proprietary Models with a Quick Fine-Tune
=================================================

![author](https://modal-cdn.com/jason-liu.jpg)

[Jason Liu

@jxnlco](https://twitter.com/jxnlco)

AI Engineer

![author](https://modal-cdn.com/ivan-leo.jpg)

[Ivan Leo

@ivanleomk](https://twitter.com/ivanleomk)

Software Engineer

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

AI Engineer

[View on GitHub](https://github.com/567-labs/fastllm/blob/main/applications/finetune-quora-embeddings/Readme.md)

With just a handful of examples, a fine-tuned open source embedding model can
provide greater accuracy at a lower price than proprietary models like OpenAI’s
`text-embedding-3-small`
. In this article, we’ll explain how to create one with Modal.
First, we’ll cover the basics of fine-tuning. Then, we’ll talk about an
experiment we ran to determine how much fine-tuning data we needed for a simple
question-answering use case.

The why of fine-tuning
----------------------

### Open models get you started

Custom models matter. That’s how Netflix keeps suggesting better movies
and how Spotify manages to find a new anthem for your daylist. By tracking when you finish the
movie you chose or whether you skipped a song, these companies accumulate data.
They use that data to improve their internal embedding models and recommendation
systems, which lead to better suggestions and a better experience for
you. That can even lead to more engagement from more users, leading to more
data, leading to better models, in a virtuous cycle known as the
[*data flywheel*](https://eugeneyan.com/writing/more-patterns/#data-flywheel-to-continuously-improve--build-a-moat)

.

![The Data Flywheel: more users means more data means better models means more users](https://modal-cdn.com/cdnbot/fine-tuning-embeddings-data-flywheel.png)

The data flywheel: more users means more data means better models means more users.

Large, ML-forward organizations like Netflix and Spotify have taken advantage of
data flywheels to create their own models from scratch, and now they have lots of data.
But when you’re just starting out on a new company or project, you don’t always have the
data you need. Bootstrapping a data flywheel in the 2010s required substantial
creativity or resource investment.

But the advent in the 2020s of highly capable, generic pre-trained models with
permissive licenses has dramatically simplified that bootstrap step. You can
start with one of these models, trained to recognize patterns in a large,
diverse dataset, and expect them to perform reasonably well on your task.

In a
[previous blog post](/blog/embedding-wikipedia)

, we demonstrated this by deploying
[an off-the-shelf model](https://huggingface.co/BAAI/bge-small-en-v1.5)

on hundreds of GPUs using Modal’s autoscaling infrastructure,
embedding the entirety of English Wikipedia in under 15 minutes.

### Fine-tuning kicks off the data flywheel

The availability of both these models and the infrastructure to run them easily
is great news for organizations which are just starting out and don’t have any
user data yet. But it’s critical to move as fast as possible to a custom model
that delivers better performance than the off-the-shelf model.
Luckily, data accumulates quickly: just a few dozen users
interacting with a service only 3-4 times per day can create hundreds
of datapoints in a matter of days.

And that’s all the data we needed to train a model that could
beat OpenAI’s
`text-embedding-3-small`
at identifying textual similarity on a
sample dataset.

The same scalable, serverless infrastructure on Modal that we used to create embeddings
with the off-the-shelf model can also be used to customize it, a process called
*fine-tuning*
.
The end result is an ML application with superior performance at a significantly
reduced operational expense: the first step to starting your own data flywheel.

The how of fine-tuning: datasets, models, and infrastructure
------------------------------------------------------------

When fine-tuning a model, there are a number of design decisions that must be
made. We review a few of them here.

### Finding or creating a dataset

Though much of the discussion and research in machine learning is around models, any
ML engineer worth their salt will tell you that the dataset is the most critical
component.

Embedding models are generally trained on datasets made out of pairs of items,
where some pairs are marked as “similar” (like sentences from the same
paragraph) and some are marked as “different” (like two sentences chosen at
random). The same principle could be applied to longer texts than sentences —
paragraphs, pages, documents — or it could be applied to things other than text
— images, songs, user clickstreams — or it could be applied to multiple
modalities at once — images and their captions, songs and their lyrics, user
clickstreams and purchased products.

We’ll be using the
[Quora dataset](https://huggingface.co/datasets/quora)

, which
contains pairs of questions from posts on Quora, where some pairs are questions
that have been marked as duplicates.

![Quora dataset in the Hugging Face dataset viewer](https://modal-cdn.com/cdnbot/fine-tuning-embeddings-quora-dataset-hf-viewer.png)

Sample rows from the Quora duplicates dataset in the
[Hugging Face dataset viewer](https://huggingface.co/datasets/quora/viewer/default/train)
.

You can review the dataset in an interactive viewer
[here](https://huggingface.co/datasets/quora/viewer/default/train)

. Some of the
question pairs, like “Can I hack my Charter Motorolla DCX3400?” and “How do I
hack Motorola DCX3400 for free internet?”, are quite similar, but are not
duplicates, aka they are “hard negatives”.

Together, that makes the model we’re training here potentially useful for
retrieval-augmented generation (RAG) chatbots. In embedding-based RAG for
chatbots, a large corpus of text must be searched for a small number of passages
that “match” a query from a user, aka likely contain the answer. This dataset
will train the model to be sensitive to very small differences in the topic of a
question. Near duplicates can also be removed before retrieval or before
training other models, a technique known as “semantic deduplication”.

### Choosing a base model

We’ll primarily be focusing here on permissively licensed, weights-available
models. These models have weights that you can download and modify the same way
you download and modify open source code. For that reason, we refer to them here
as “open source” models, even though there is no
[Open Source Initiative-sanctioned](https://opensource.org/)

definition of “open source” that applies to models.
Models are commonly released via Hugging Face’s git LFS-based model repository hub,
and that’s where we’ll get our models.

Alternatively, we could have used an API to fine-tune a proprietary model, as is
offered by some embedding API services. In addition to concerns about cost, we
find that fine-tuning a model is sufficiently complex and use-case-specific that
controlling the training process is necessary.

How do you choose between available models? Each model is trained differently
and with a specific use-case in mind. Most critically, a model is trained on a
specific modality or modalities (text, images, audio, video, et cetera) and a
specific set of data. Once you have narrowed down to models that handle the
modalities in your use case, compare their performance on public benchmarks,
like
[MTEB](https://huggingface.co/spaces/mteb/leaderboard)

. In addition to task
performance, review the model’s performance in terms of resource requirements
and throughput/latency, again via public benchmarking data (hardware providers
like
[Lambda Labs](https://lambdalabs.com/blog)

are a good resource here).

For example, the embedding dimension, or the number of entries in the output
embeddings of the model, is an important consideration. Larger vectors can store
more information, leading to better task performance, but can result in
significantly higher costs (costs scaling more like RAM than like disk) as we embed
more data over time. When fine-tuning, we can adjust this dimension.

### Acquiring training infrastructure

Fine-tuning a model requires significant compute resources. Even models that can
later be run satisfactorily on CPUs, even client or edge CPUs, are frequently
trained on GPUs, which can achieve high throughput on easily parallelizable workloads
like training.

For a typical fine-tuning job, we need one to eight server-grade GPUs. More than
eight GPUs generally requires distributing training over multiple nodes, due to
connectivity constraints, which significantly increases both hardware cost and
engineering complexity.

But server-grade GPUs are scarce these days, meaning they are expensive to
purchase or rent and cloud providers frequently require reservations of a
minimum size and duration. But fine-tuning jobs are less like production
workflows (always on, reasonably predictable traffic) and more like development
workflows (intermittent, unpredictable). Combined, these phenomena have lead to
massive over-allocation and over-spending, with organizations reporting
*peak*
utilizations of about 60% on average, according to
[this survey from ClearML and the AI Infrastructure Alliance](https://go.clear.ml/the-state-of-ai-infrastructure-at-scale-2024)

— and even less off-peak.

![Survey results on GPU allocation from the AI Infrastructure Alliance](https://modal-cdn.com/cdnbot/fine-tuning-embeddings-ai-infra-survey-gpu-allocation.png)

Source:
[The State of AI Infrastructure at Scale 2024](https://go.clear.ml/the-state-of-ai-infrastructure-at-scale-2024)

Modal solves this problem: it provides autoscaling infrastructure, including
GPUs, so you only pay for what you use (aka it is “serverless”). Modal also
offers a Pythonic, infrastructure-from-code interface, empowering data
scientists and ML researchers to own and control their infrastructure.

With these resources in hand, we need to determine how to scope our model
training process. The more time and money we spend on training, iterating on
hyperparameters and data tweaks, the better our task performance will become,
but with diminishing returns. In general, we recommend either training to
satisfaction on some metric (e.g. at least 90% accuracy) or selecting a number
of metrics to satisfy and one metric to maximize (e.g. highest accuracy we can
get with recall ≥ 50%), then setting a hard limit on resources and time.

Running a grid search over fine-tuning hyperparameters
------------------------------------------------------

Determining how to train an entirely new model architecture or on an entirely
new task is a research project, and should be scoped accordingly. But
fine-tuning is simpler — we can use pre-existing training recipes, like the
ones used to train the model (if it has been open-sourced). But there is still
room for experimentation, including on many of the considerations listed above.

We selected as experimental parameters the three we considered most important:
which pre-trained model should we train, on how much data, and with how many
output dimensions? Because these experimental parameters determine the values of
the parameters (the weights and biases) in the model, they are known as
*hyperparameters*
.

The simplest way to explore hyperparameters is to define a set of possible
values for each hyperparameter and then check all combinations — a
*grid search*
.
This is a brute-force approach, but, as we’ll see below, it is effective and easy to parallelize.

We added two additional models to the original
`bge-base-en-v1.5`
model we used
in the Wikipedia embedding example and we tried two different embedding
dimensions. For each of these configurations, we tested a different dataset
size, ranging from one hundred to over one hundred thousand samples:

```
MODELS = [
    "BAAI/bge-base-en-v1.5",
    "sentence-transformers/all-mpnet-base-v2",
    "jinaai/jina-embeddings-v2-small-en",
]
DATASET_SIZE = [
    100, 200, 400, 800, 1600, 3200,
    6400, 12800, 25600, 51200, 102400,
]
DENSE_OUT_FEATURES = [256, 512]
```

All other hyperparameters were held fixed.

Next, we generated all possible combinations of
`model`
,
`dataset_size`
and
`dense_out_features`
using the
`product`
function provided by the standard
library module
`itertools`
, which creates an iterator that returns every
possible combination of elements from each of the input lists (aka the Cartesian
`product`
). We then use these combinations to generate configuration objects for
our model fine-tuning process:

```
def generate_configs():
    for model, sample_size, dense_out_features in product(
        MODELS, DATASET_SIZE, DENSE_OUT_FEATURES
    ):
        yield grid_search_config(model, sample_size, dense_out_features)
```

Whatever our fine-tuning process is, it takes
`config`
s and produces some
dictionary of
`results`
. We wrap it in a function that we decorate with
[`@app.function()`](/docs/reference/modal.App#function)

to make it runnable on
Modal’s autoscaling infrastructure, as in the pseudo-code below.

```
@app.function(gpu="A10G")  # configure autoscaling and other infra parameters here
def objective(config) -> dict:
    model = Model.from_config(config)
    model.setup()
    results = model.train()
    return results
```

From there, scaling is as simple as calling
`objective.map`
to
[run experiments in parallel](/docs/guide/scale)

. We wrap this in a function
that we decorate with
`@app.local_entrypoint()`
so that we can launch
experiments from the command line with
[`modal run`](/docs/reference/cli/run)

.

```
@app.local_entrypoint():
def run():
    results = []
    for experiment_result in objective.map(generate_configs()):
        results.append(experiment_result)

    df = pd.DataFrame(results).sort_values("metric_accuracy", ascending=False)
    df.to_csv("trial_results.csv", index=False)
```

Our training process fits on a single GPU, and each experiment runs in parallel,
so we can scale it all the way up to the maximum number of simultaneous GPU workers
allowed in Modal — in the thousands, at time of writing. For large training jobs,
this can mean the difference between getting results next week or after lunch.

### Beating proprietary models with a few hundred examples

The figure below summarizes the results of our experiment, showing the error
rate (fraction of predictions that are incorrect) of the models we trained on
the Quora dataset as a function of the number of dataset examples used during
fine-tuning, with one plot for each of the three models. The performance of the
OpenAI
`text-embedding-3-small`
model is shown for comparison. For completeness, we
show the two different embedding dimension sizes we tested, though we didn’t
observe a difference in performance between them for any setting of the other
hyperparameters.

![Error rate as a function of dataset size](https://modal-cdn.com/cdnbot/fine-tuning-embeddings-error-rate-vs-dataset-size.png)

Error rate as a function of dataset size for three models fine-tuned on the Quora dataset.

We see a few patterns commonly observed in fine-tuning in these three cases:

* For the
  `jina-embeddings-v2-small-en`
  model, the error rate is higher
  than the baseline and never goes down. As always, it’s plausible that
  different settings of the other hyperparameters we didn’t tune might lead to
  improved performance from this model. These are the kind of results you don’t want to
  get from a hyperparameter search, because it’s not clear what to do next.
* For the
  `all-mpnet-base-v2`
  model, the error rate is
  lower than the baseline after just 100 examples, but we don’t observe much
  improvement, even out to three orders of magnitude more examples.
* For the
  `bge-base-en-v1.5`
  model, the error rate starts out higher than
  the baseline model, but rapidly improves with more data, convincingly beating
  the baseline by 200 examples and still improving at 100,000 examples.

Reviewing these results, we’d move forward with the fine-tuned
`bge-base-en-v1.5`
model, especially if we expected to be able to collect
more data via a data flywheel in the future.
We’d most likely select the 256-dimensional embeddings, as they are cheaper to
produce and store than the 512-dimensional embeddings, and we didn’t observe
an accuracy benefit from using the larger embeddings.

You might object that the improvements over the baseline are small in
absolute terms — an error rate of 17% versus an error rate of 13%. But
relatively, that is a large difference: a full quarter of the mistakes that the
baseline model makes are avoided by the fine-tuned model. This phenomenon gets
stronger as the error rate decreases: a system with 99% reliability can be used
in situations where one with 95% reliability is inadmissible, even though the
magnitude of the difference seems small.

Next steps
----------

In this article, we’ve shown how to fine-tune an open source embedding model to
beat a proprietary model on a simple question-answering task. We’ve also
discussed the considerations that go into fine-tuning a model and how to run a
grid search over hyperparameters using Modal. We’ve shown that even with just a
few hundred examples, we can achieve better performance than proprietary models.

Moving forward, the next step in fine-tuning would be to operationalize this
process, so that we can collect more data and iterate on the model. With full
automation, we could even turn the model into a continuously-improving system,
powered by the additional data we collect.

Far more than models, these pipelines and processes for converting data into
useful features for users are the output of machine learning teams. With open
source models and serverless infrastructure, building them is easier than ever.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fine-tuning-flux-style-lora
================================================================================

Fine-tuning a FLUX.1-dev style LoRA
===================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

[FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)

, a 12B parameter model developed by
[Black Forest Labs](https://blackforestlabs.ai/)

, is one of the hottest open-source text-to-image AI models on the market today.

But what if you want to use Flux to generate images that adhere to a specific artistic style or theme? Can you fine-tune Flux?

The answer is yes!

In
[a previous blog post](/blog/fine-tuning-stable-diffusion)

, we fine-tuned Stable Diffusion 1.5 on
[Heroicons](https://heroicons.com/)

, a set of freely-available icons from the makers of
[Tailwind CSS](https://tailwindcss.com)

. Our results were decent, but a bit noisy and more detailed and complex than the very clean, abstract Heroicon style. The training also took on the order of hours, because we did a full fine-tune.

In this follow-up blog post, we switched to FLUX.1-dev and got better results. We fine-tuned it on Heroicons using the Dreambooth with LoRA technique. The Flux models are well-known for their superior performance in generating clean lines and text, which aligns with our results. We end up with a fine-tuned Flux style LoRA that allows us to generate an infinite icon library.

Here are some example output images:

![heroicon-fine-tuned](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/heroicons-fine-tuned-grid.jpg)

From top left:
`Donald Trump`
,
`Beyonce`
,
`Barack Obama`
,
`Hillary Clinton`
,
`cocktail`
,
`castle`
,
`phone`
,
`tent`
,
`golden retriever`
,
`heart`
,
`Mt. Everest`
,
`dress`

You can play around with the LoRA yourself
[here](https://modal-labs-ren-dev--example-dreambooth-flux-fastapi-app.modal.run)

.

We’ll cover everything from curating the dataset to the GPU and fine-tuning technique used. Fine-tuning is run on
[Modal](https://modal.com/docs)

, a scalable, serverless cloud computing platform that abstracts away the complexities of infrastructure management.

Table of contents
-----------------

* [Experiment quick facts](#experiment-quick-facts)
* [Fine-tuning technique](#fine-tuning-technique)
* [Preparing the dataset](#preparing-the-dataset)
* [Training on Modal](#training-on-modal)
* [Hyperparameter optimization](#hyperparameter-optimization)
* [Results](#results)

Experiment quick facts:
-----------------------

* **Dataset:**
  [yirenlu/heroicons-subset-25-images](https://huggingface.co/datasets/yirenlu/heroicons-subset-25-images)
* **Number of training images:**
  25
* **Training images format:**
  PNG
* **Resolution of training images:**
  512x512
* **Fine-tuning technique:**
  Dreambooth LoRA
* **Fine-tuning script:**
  [Diffusers
  `train_dreambooth_lora_flux.py`](https://github.com/huggingface/diffusers/blob/abd922bd0c43a504e47eca2ed354c3634bd00834/examples/text_to_image/train_text_to_image.py)

  script
* **GPU:**
  1 A100
* **Cost:**
  ~$2 for rank 16 LoRA for 4000 training steps
* **Hyperparameters:**
  See section on
  [hyperparameter optimization](#hyperparameter-optimization)
* **Replicating this experiment on Modal:**
  :
  [Example](/docs/examples/dreambooth_app)

Fine-tuning technique
---------------------

For our experiment, we use a fine-tuning technique called
[Dreambooth](https://dreambooth.github.io/)

that teaches Flux a new concept (i.e. a style or a character) by associating a special word with the example images. In particular, we use a LoRA implementation of Dreambooth that allows you to achieve full fine-tuning-like performance but with much less memory.

In LoRA fine-tuning, instead of updating all the parameters of a model during training, you introduce low-rank matrices that capture the essential changes needed for adaptation. During the fine-tuning process, only these low-rank “adapters” are updated. Then on inference, you load the base model, which remains unchanged, followed by the LoRA adapters. Compared to full fine-tuning, this approach offers faster training times and lower memory usage.

At this point, you might be wondering, how do you choose between full fine-tuning and LoRA fine-tuning?

**Generally speaking, the best practice is to start with LoRA fine-tuning, and then, if the results are not adequate, move on to full fine-tuning.**

You might have also heard of other optimization techniques like
[qLoRA](/blog/lora-qlora)

, where the base model and the LoRA adapters are further quantized to cut down on memory usage.

Should you use qLoRA to fine-tune Flux?

The answer is that it’s probably not necessary. You can run a LoRA fine-tune on a single A100 40GB GPU without needing to quantize down to 8-bits.

Preparing the Dataset
---------------------

![heroicon-training-data](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/heroicons-training-data.jpg)

Some examples of our training data

In our previous attempt at fine-tuning Heroicons, we used the entire 300+ Heroicons icons that are publically available as our training set.

With LoRA, we can fine-tune with fewer images. Instead of the hundreds we would need for full fine-tuning, we only need around 20-25 images, so we curate 25 images. Some things to note about the training data:

* **Variety of images:**
  For LoRA, more important than the number of images is the variety and representativeness of the images. For example, we deliberately choose icons that represent both tangible objects and more abstract concepts, as well as icons that contain both primarily straight lines/angles as well as curves.
* **Captioning:**
  To train a style LoRA, it’s very helpful to provide individual captions for the images. The nice things about Heroicons is that each Heroicon image file is already named, so it’s easy to convert those names into appropriate captions. Our captions take the form
  `"an HCON, a black and white minimalist icon of a <object>."`
  Note that this means that at inference time, we will need to prompt with something similar to the caption, i.e.
  `"an HCON, a black and white minimalist icon of Barack Obama"`
  in order to trigger the style.

3. **Dreambooth keyword:**
   As previously mentioned, Dreambooth is a technique that teaches Flux a new concept (i.e. a style or a character) by associating a special word with the example images. In our case, that special word is
   `HCON`
   . At inference time,
   `HCON`
   must be present in the prompt.

The final dataset used for our LoRA fine-tuning is
[here](https://huggingface.co/datasets/yirenlu/heroicons-subset-25-images)

.

Training on Modal
-----------------

To fine-tune on Modal, we can adapt
[this Dreambooth example](/docs/examples/dreambooth_app)

, which shows you how to run a
[Diffusers](https://github.com/huggingface/diffusers)

fine-tuning script on Modal.

Diffusers is a HuggingFace-produced library that provides a set of easy-to-use scripts for fine-tuning Diffusion models on custom datasets. You can see an up-to-date list of all their scripts in their
[`examples`
subdirectory](https://github.com/huggingface/diffusers/tree/main/examples)

.

For this fine-tuning task, we will be using the
[`train_dreambooth_lora_flux.py`](https://github.com/huggingface/diffusers/blob/abd922bd0c43a504e47eca2ed354c3634bd00834/examples/text_to_image/train_text_to_image.py)

script. This script does a Dreambooth fine-tune with LoRA.

### Setting up accounts

If you’re following along or using this blog post as a template for your own fine-tuning experiments,
make sure you have the following set up before you use the scripts above:

* A HuggingFace account (sign up
  [here](https://huggingface.co/join)

  if you don’t have one).
* A Modal account (sign up
  [here](https://modal.com/signup)

  if you don’t have one).

Hyperparameter optimization
---------------------------

Fine-tuning a model is highly sensitive to the selection of hyperparameters. These parameters significantly influence the training process and the final performance of the model. Key hyperparameters to consider include the number of training steps, the learning rate, and, specifically for LoRA fine-tuning, the rank. In our experiments, we explored a range of values around commonly used configurations, varying the following parameters:

* **Rank**
  : The higher the rank chosen, the closer it approximates full fine-tuning. On the other hand, the higher the rank chosen, the more memory and time it takes to train. In general, the LoRA rank chosen should correspond to the complexity of the style. For simpler styles, you can probably get away with ranks like
  `4`
  ,
  `8,`
  or
  `16`
  . For more complex styles, you will probably need rank
  `32`
  or
  `64`
  . In general, training a style LoRA requires a higher rank than training a character LoRA. This makes intuitive sense - style LoRAs need to capture more nuanced details.
* **Learning Rate**
  : The standard learning rate for full fine-tuning of Diffusion models is typically set at
  `1e-6`
  . LoRA fine-tuning, however, generally allows for higher learning rates, because it only updates a small subset of parameters compared to full fine-tuning, making it less prone to overfitting.
* **Max training steps**
  : This parameter defines the total number of training iterations the model will undergo. A full fine-tune of a diffusion model will often require 10,000 steps, but you can generally get pretty good LoRA results in less than 5000 steps.

In addition to these primary hyperparameters, we also utilized the following hyperparameters:

```
resolution: int = 512
train_batch_size: int = 1
gradient_accumulation_steps: int = 1
lr_scheduler: str = "constant"
lr_warmup_steps: int = 0
seed: int = 0
```

### Performing hyperparameter search with Modal

Modal makes it easy to scale up our training
— running tens or hundreds, etc, of copies of the training script at once,
each with different hyperparameters.

To do this, we first set up a Python class with the different hyperparameter values we want to search through.

```
@dataclass
class SweepConfig(TrainConfig):
"""Configuration for hyperparameter sweep"""

# Sweep parameters
learning_rates = [8e-5, 2e-4]
train_steps = [1000, 1500, 3000, 4000]
ranks = [4, 8, 16]
```

Next, we write a function that generates all possible combinations of the hyperparameters:

```
def generate_sweep_configs(sweep_config: SweepConfig):
"""Generate all combinations of hyperparameters"""
param_combinations = itertools.product(
    sweep_config.learning_rates,
    sweep_config.train_steps,
    sweep_config.ranks,
)

return [
    {
        "learning_rate": lr,
        "max_train_steps": steps,
        "rank": rank,
        "output_dir": Path(MODEL_DIR)
        / f"lr_{lr}_steps_{steps}_rank_{rank}", # store the different LoRAs in different directories within the same volume
    }
    for lr, steps, rank in param_combinations
]
```

Finally, we use the local entrypoint to orchestrate the hyperparameter sweep using
[`.map()`](/docs/guide/scale#parallel-execution-of-inputs)

:

```
@app.local_entrypoint()
def run()
import wandb

sweep_config = SweepConfig()
app_config = AppConfig()
configs = generate_sweep_configs(sweep_config)

results_by_rank = {}  # Dictionary to store results for each rank

# Log results to wandb
with wandb.init(
    project="flux-lora-sweep-heroicons",
    name="hyperparameter_sweep",
) as run:
    for config in train.map(configs):

        learning_rate = config['learning_rate']
        rank = config['rank']
        max_train_steps = config['max_train_steps']

        for image, prompt in Model(
            learning_rate, rank, max_train_steps
        ).inference.starmap(
            [(x, app_config) for x in sweep_config.heroicon_test_prompts]
        ):
            results_by_rank[rank][prompt][steps] = wandb.Image(image)

    # log results to wandb
    run.log()
```

Results
-------

#### Learning rate `8e-5`

Below, we see the results across rank 4, 8, and 16 for a progressive number of training steps.

![lr_8e-5_rank_4](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/lr_8e-5_rank_4.jpg)

![lr_8e-5_rank_8](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/lr_8e-5_rank_8.jpg)

![lr_8e-5_rank_16](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/lr_8e-5_rank_16.jpg)

#### Learning rate `2e-4`

Below, we see the results across rank 4, 8, and 16 for a progressive number of training steps.

![lr_2e-4_rank_4](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/lr_2e-4_rank_4.jpg)

![lr_2e-4_rank_8](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/lr_2e-4_rank_8.jpg)

![lr_2e-4_rank_16](https://modal-public-assets.s3.us-east-1.amazonaws.com/article-assets/lr_2e-4_rank_16.jpg)

There’s obviously a lot of subjectivity when it comes to deciding which hyperparameter combination gives the best results, but to our eyes, at least, it appears that the LoRA with rank
`16`
, trained for
`4000`
steps at a learning rate of
`2e-4`
(the last image), gives the best results in terms of being clean, well-structured, and appropriately representing the prompt concept.

Some further observations:

* There’s some overfitting and graininess with the lower learning rates, particularly when trained for an extended number of steps.
* Although the general guideline suggests that simpler styles require lower ranks, our findings indicate that the lower ranks produced results that were unexpectedly more complex and noisy. It seems that a higher rank was necessary to effectively capture the true “style” of Heroicons, which are inherently abstract and conceptual icons.
* The base
  `FLUX.1-dev`
  model was initially trained on
  `1024x1024`
  images, while our dataset consisted of lower-resolution
  `512x512`
  images. As a potential improvement, we could consider resizing our fine-tuning dataset to
  `1024x1024`
  to evaluate whether the outputs improve.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fine-tuning-llms
================================================================================

Best frameworks for fine-tuning LLMs in 2025
============================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

In this article, we cover the state-of-the-art frameworks for fine-tuning large language models (LLMs) in 2025. These frameworks make it faster, cheaper, and simpler to fine-tune models like LLaMA 3/LLaMA3.1, Mistral, Mixtral, or Pythia on your own data.

Table of contents
-----------------

* [What are Axolotl, Unsloth, and Torchtune?](#what-are-axolotl-unsloth-and-torchtune)
* [Takeaways](#takeaways)
* [Axolotl](#axolotl)
* [Unsloth](#unsloth)
* [Torchtune](#torchtune)
* [Conclusion](#conclusion)

What are Axolotl, Unsloth, and Torchtune?
-----------------------------------------

Axolotl, Unsloth, and Torchtune are three of the most popular frameworks for fine-tuning large language models.

These frameworks simplify the fine-tuning process. Users can easily apply state-of-the-art optimization techniques to fine-tuning open weights models to their specific datasets without needing to implement training procedures from scratch.

They are also generally designed to optimize the fine-tuning process, potentially offering faster training speeds and reduced memory usage.

Takeaways
---------

* If you are a beginner: Use Axolotl.
* If you have limited GPU resources: Use Unsloth.
* If you prefer working directly with PyTorch: Use Torchtune.
* If you want to train on more than one GPU: Use Axolotl.

Axolotl
-------

![axolotl](/_app/immutable/assets/axolotl.CwAsO-9x.png)

[Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl)

is a wrapper for lower-level Hugging Face libraries like
[Transformers](https://huggingface.co/docs/transformers/en/training)

, retaining most of the granular control they offer while being much easier to use. It frees you up to focus more on your data rather than the technical details of the fine-tuning process.

Axolotl comes with lots of built-in default values and optimizations, reducing the need for manual configuration. The tool includes clever features like sample packing, which can improve training efficiency.

With Axolotl, you can train open weights models like LLaMA 3/LLaMA 3.1, Pythia, and Falcon, all available on Hugging Face.

You should consider using Axolotl if you don’t want to get too deep into the math of LLMs and just want to fine-tune a model.

Unsloth
-------

![unsloth](/_app/immutable/assets/unsloth.BmcAUDZL.png)

[Unsloth](https://github.com/unslothai/unsloth)

, built by
[Daniel Han Chen](https://x.com/danielhanchen)

, who was previously a Nvidia engineer, is designed to dramatically improve the speed and efficiency of LLM fine-tuning. It allows you to fine-tune Llama 3.1, Mistral, Phi & Gemma LLMs 2-5x faster with 80% less memory usage compared to
[FA2](https://github.com/Dao-AILab/flash-attention)

(Flash Attention 2).

Unsloth achieves these improvements without degradation of accuracy, since it doesn’t rely on approximation or quanization. Instead, the Unsloth team achieved the improvement in speed with a fast, custom attention implementation in
[Triton](https://openai.com/index/triton/)

, OpenAI’s high-level language for GPU kernels.

The main goal of Unsloth is to make it possible for everyone to fine-tune their language models, even with very limited GPU resources. Consider using Unsloth if you only have access to smaller or older GPUs.

So for example, if you are trying to fine-tune something on the free tier of Google Colab, which gives you a single Tesla T4 GPU, then Unsloth might be the the choice for you. (Note that Unsloth does not support multi-GPU training, so if you have a large GPU cluster, you should Axolotl instead.)

Torchtune
---------

[Torchtune](https://github.com/pytorch/torchtune)

is a PyTorch-native library for easily fine-tuning LLMs. It offers a lean, extensible, abstraction-free design that’s just pure PyTorch.

The library is designed with memory efficiency in mind, with recipes tested on consumer GPUs with 24GB VRAM. Torchtune provides excellent interoperability with popular libraries across the PyTorch ecosystem, as well as recipes for parameter-efficient techniques like qLoRA and LoRA, in addition to full fine-tuning.

Torchtune is an excellent choice if you prefer working directly with PyTorch without additional abstractions, need flexibility and extensibility in your fine-tuning pipeline, or want to leverage a wide range of integrations with popular AI tools and platforms.

Conclusion
----------

The choice between these tools ultimately depends on your specific requirements, hardware constraints, and level of expertise.

In the vast majority of cases, and especially if you are a beginner, we recommend using Axolotl.

To fine-tune with Axolotl using
[Modal](https://modal.com)

, check out our
[fine-tuning starter code](https://github.com/modal-labs/llm-finetuning)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fine-tuning-llms-hyperparameters-glossary-article
================================================================================

Glossary: LLM fine-tuning hyperparameters
=========================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

### Learning rate

The learning rate is a scalar that determines the step size at each iteration while moving toward a minimum of the loss function. The bigger the learning rate, the faster fine-tuning goes, but you have to balance that against the risk of overshooting the optimal solution or causing unstable training.

The specific value of the learning rate will be dependent on the optimizer you choose. The default for the most popular
[Adam optimizer](https://arxiv.org/abs/1412.6980)

is 0.001.

Learning rates are often expressed in exponential terms, such as 1e-5 or 5e-4.

### Learning rate (LR) scheduler

Instead of manually adjusting the learning rate, you can choose to use a learning rate (LR) scheduler that dynamically adjusts the learning rate during fine-tuning.

[Linear decay](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LinearLR.html)

is a common choice, where the learning rate is gradually reduced over time to facilitate smoother model convergence. In contrast,
[cosine annealing](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/schedules/CosineDecay)

follows a cosine pattern, starting with a high learning rate that rapidly decreases to a minimum before increasing again. This process simulates a restart of the learning process, reusing good weights as the starting point, known as a “warm restart.” This differs from a “cold restart,” where a new set of small random numbers is used as the starting point.

### Optimizer

The optimizer is the algorithm responsible for adjusting a model’s parameters to minimize the loss function.

[AdamW](https://openreview.net/forum?id=Bkg6RiCqY7)

is the most popular optimizer for deep learning training/fine-tuning use cases due to its simplicity, efficiency, and robustness. It’s a modification of the Adam optimizer that controls overfitting and improves the generalizations of the models.

AdamW has 32-bit, 8-bit, and paged versions. The 32-bit version is resource-intensive, requiring additional memory for optimizer states. In contrast,
[AdamW 8-bit](https://huggingface.co/docs/bitsandbytes/v0.44.1/optimizers)

performs similarly but with reduced GPU memory usage, making it a recommended choice. The paged version is useful in distributed settings.

### Batch size

The number of training examples used in one iteration. Larger batch sizes can lead to faster training but may require more memory. The default batch size is often set to 32.

### Number of epochs

An epoch is one complete pass through the entire training dataset. The number of epochs determines how many times the model will see the entire dataset during training. The default number of epochs is often set to 3. Increasing it can allow the model to see the data more times, which can improve performance. However, it can also increase the risk of overfitting.

### Warmup steps

The number of training steps for which the learning rate is gradually increased from a small value to the initial learning rate. This can help stabilize training in the early stages. The default number of warmup steps is often set to 0.1 x total training steps.

### Weight decay

A regularization technique that adds a penalty term to the loss function to prevent overfitting by keeping the weights small. The default weight decay is often set to 0.01. Increasing it can improve generalization, especially for large models or complex tasks. However, it can also slow down learning.

### Packing

Batches have a pre-defined sequence length. Instead of assigning one batch per sample, we can combine multiple small samples in one batch, increasing efficiency.

Tips for hyperparameter tuning
------------------------------

1. **Start with a small learning rate and gradually increase**
   : A smaller learning rate, like 0.0001, can help with convergence, especially at the beginning of training.
2. **Use hyperparameter search**
   : Employ techniques like grid search, random search, or Bayesian optimization to find optimal hyperparameter combinations.
   [Modal](/docs)

   makes it fast and easy to do massively parallel hyperparameter searches on thousands of containers.
3. **Monitor validation performance**
   : Keep an eye on the model’s performance on a held-out validation set to avoid overfitting. You can use tools like Weights and Biases to visualize the performance of different permutations of hyperparameters on the held-out set.
4. **Experiment with different techniques**
   : Try combining different fine-tuning methods, such as LoRA with QAT, to achieve better results.

For practical examples of LLM fine-tuning using Modal, check out our
[LLM fine-tuning guide](/docs/examples/llm-finetuning)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fine-tuning-mochi-video
================================================================================

Create a custom video generator by fine-tuning a Mochi LoRA on Modal
====================================================================

One month ago,
[Genmo released Mochi 1](https://www.genmo.ai/blog)

, an open state-of-the-art video generation model. Given a text prompt, Mochi 1 generates matching short video clips — like Stable Diffusion or Flux do for images and ChatGPT does for conversations. The results are stunning, especially for those of us who remember when GANs
[struggled](https://machinelearningmastery.com/how-to-develop-a-generative-adversarial-network-for-a-cifar-10-small-object-photographs-from-scratch/)

to make 32x32 pixel frog images from CIFAR-10.

[

](https://modal-cdn.com/mochi-sequence-demo.mp4)

Videos generated by Mochi 1 have realistic imagery and high-fidelity motion.

As an open (Apache 2.0-licensed!) foundation model, Mochi 1 is designed so that others can build on top of it. And today, Genmo just made that much easier by releasing a collection of scripts and sample code for fine-tuning Mochi 1 with LoRA.

Here are some of our results from fine-tuning Mochi on short clips of objects dissolving:

[

](https://modal-cdn.com/mochi-dissolve-finetune-fwd-bwd.mp4)

Generations from a Mochi model fine-tuned on videos of objects dissolving, played forwards and backwards in a loop.

[Check out our fine-tuning recipe](https://github.com/genmoai/mochi/tree/main/contrib/modal)

to create your own custom video generator on Modal.

What is fine-tuning? What is LoRA?
----------------------------------

Mochi 1 has been trained on data that is broadly representative of the kinds of videos people want to watch.
You can use it off the shelf via text prompting (see
[our docs](/docs/examples/mochi)

for how to run baseline Mochi inference on Modal).

But as with image generation models, fine-tuning the model by continuing its training process on new data allows for superior control of style and increased character consistency across generations.

* **Foundation models aren’t always masters of your task**
  : While Mochi excels at video generation in general, your specific use case might require specialization in particular styles, subjects, or motions that aren’t well-represented in the training data.
* **Proprietary data needs proprietary weights**
  : If you have unique video content that defines your brand or style, fine-tuning helps Mochi learn these distinctive characteristics.
* **Improve consistency**
  : Fine-tuned models typically require less prompt engineering to produce consistent results for your specific use case.

Training neural networks like Mochi is more resource-intensive than running their inference, since training involves running inference and using the results to update the model.
In particular, vanilla training techniques require much more memory, since they maintain model parameters and parameter updates.

[LoRA (Low-Rank Adaptation)](https://modal.com/blog/lora-qlora)

dramatically reduces the memory required for fine-tuning by training a separate set of parameters,
a model
*adapter*
, that are run in parallel with the base foundation model’s parameters, but are much smaller.
With LoRA, the Mochi 1 model can be
[fine-tuned on a single H100 GPU](https://github.com/genmoai/mochi/tree/main/contrib/modal)

.

These smaller parameter sets are also easier to train and require less data — as few as fifteen video clips of only a few seconds each.

What is Modal?
--------------

Modal is a high-performance AI infrastructure platform perfect for demanding generative modeling tasks like Mochi fine-tuning and inference.

Our platform offers:

* Access to powerful H100 and A100 GPUs without server management or up-front reservations
* Resources that automatically scale up and down based on your needs
* Usage-based pricing
* Enterprise-grade security and reliability

Tips for fine-tuning Mochi
--------------------------

* If the videos you want to train on don’t come with captions, try
  [ChatGPT](https://chat.com)

  or Gemini’s
  [AIStudio](https://aistudio.google.com/prompts/new_chat)

  .
* Captions should be fairly detailed (>50 words). Here’s an example:

  ```
  High-resolution footage depicting the controlled dissolution of a metallic robot.
  The robot, characterized by a polished chrome finish and retro design,
  holds a bouquet of various colored daisies (yellow, orange, pink, white, blue).
  The dissolution process initiates subtly, progressing from localized melting at the joints and edges.
  The metal transitions from a solid state to a liquid state exhibiting properties consistent with a high-viscosity,
  reflective substance.  The liquid metal flows downward under the influence of gravity, creating visible pooling and surface tension effects.
  Observe the displacement of the flowers as they are immersed in the molten metal.
  The liquid metal maintains a high degree of reflectivity, showcasing the distorted image of the background environment.
  ```

* You can get good results with LoRA fine-tuning with around 15 training clips.

[Try it on Modal](https://github.com/genmoai/mochi/tree/main/contrib/modal)

!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fine-tuning-stable-diffusion
================================================================================

Create an infinite icon library by fine-tuning Stable Diffusion
===============================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

AI Engineer

*For part 2 of this blog post, on how we fine-tuned Flux.1-dev with the same dataset, see
[here](/blog/fine-tuning-flux-style-lora)

.*

Icon libraries provide a clean, consistent look for web interfaces.
Here at Modal, we mostly use
[Lucide](https://lucide.dev/)

.
We also like
[Heroicons](https://heroicons.com/)

, a set of freely-available icons
from the makers of
[Tailwind CSS](https://tailwindcss.com)

,
another open source library we use.

![Some example original Heroicons](https://modal-cdn.com/cdnbot/fine-tuning-stable-diffusion-original-heroicons.png)

Some examples icons from Heroicons:
`calendar-days`
,
`film`
, and
`users`
.

These icon libraries are incredibly useful.
But like libraries of books, icon libraries are limited.
If our app needs an icon for
`golden-retriever`
s or
`barack-obama`
,
we’re just out of luck.

But what if icon libraries were more like Borges’
[*Biblioteca de Babel*](https://en.wikipedia.org/wiki/The_Library_of_Babel)

:
an endless collection of everything we could possibly need?

Generative models like
[Stable Diffusion](https://huggingface.co/models?pipeline_tag=stable-diffusion)

hold this exact promise: once they have seen enough examples of some kind of data,
they learn to simulate the process by which that data is generated,
and can then generate more, endlessly.

So as an experiment, we took a Stable Diffusion model
and fine-tuned it on the Heroicons library.

Here’s an example icon it generated for
`barack-obama`
:

![An icon of Barack Obama's head](https://modal-cdn.com/cdnbot/fine-tuning-stable-diffusion-barack-obama.png)

Yes, we can fine-tune our own models.

You can play around with the fine-tuned model yourself
[here](https://modal-labs--heroicons.modal.run/)

.

We were able to create a number of delightful new black-and-white line icons, all in a rough imitation of the Heroicons style:

![Some example custom Heroicons](https://modal-cdn.com/cdnbot/fine-tuning-stable-diffusion-generated-heroicons.png)

Top row:
`apple-computer`
,
`bmw`
,
`castle`
.

Middle row:
`ebike`
,
`future-of-ai`
,
`golden-retriever`
.

Bottom row:
`jail`
,
`piano`
,
`snowflake`

The entire application, from downloading a pretrained model through fine-tuning and up to serving an interactive web UI,
is run on Modal.

Modal is a scalable, serverless cloud computing platform that abstracts away the complexities of infrastructure management.

With Modal, we can easily spin up powerful GPU instances, run the fine-tuning training script,
and deploy the fine-tuned model as an interactive web app, all with just a few lines of code.

In this blog post, we’ll show you how.

Table of contents
-----------------

* [Choosing a fine-tuning technique](#choosing-a-fine-tuning-technique)
* [Setting up accounts](#setting-up-accounts)
* [Preparing the dataset](#preparing-the-dataset)
* [Training on Modal](#training-on-modal)
* [Serving the fine-tuned model](#serving-the-fine-tuned-model)
* [Wrapping inference in a Gradio UI](#wrapping-inference-in-a-gradio-ui)
* [Parting thoughts](#parting-thoughts)

Choosing a fine-tuning technique
--------------------------------

Your first choice when fine-tuning a model is how you’re going to do it.

In
**full fine-tuning**
, the entire model is updated during training.
This is the most computationally expensive method. It is particularly costly
in terms of memory, because information that can be several times the size of the model
needs to be kept in memory.

In
**sequential adapter fine-tuning**
, new layers are appended to the model and trained.
This requires much less memory than full fine-tuning, because the number of new layers
is usually small — even just one.
However, it is unable to adjust the earliest layers of the model,
where critical aspects of the representation are formed,
and it increases the time required for inference.

In
**parallel adapter fine-tuning**
, new layers are inserted
“alongside” the existing layers of the model,
and their outputs superimposed on the outputs of the existing layers.
This approach takes excellent advantage of the parallel processing capabilities of GPUs
and the natural parallelism of linear algebra,
and it has become especially popular in the last few years,
in the form of techniques like LoRA (Low Rank Adaptation).

HuggingFace has pretty comprehensive documentation on all these techniques
[here](https://huggingface.co/docs/diffusers/main/en/training/overview)

.

For our use-case, we found that full fine-tuning worked best.
But parallel adapter fine-tuning methods, like LoRA, can also work well,
especially if you have a small dataset and want to fine-tune quickly.

Setting up accounts
-------------------

If you’re following along or using this blog post as a template for your own fine-tuning experiments,
make sure you have the following set up before continuing:

* A HuggingFace account (sign up
  [here](https://huggingface.co/join)

  if you don’t have one).
* A Modal account (sign up
  [here](https://modal.com/signup)

  if you don’t have one).

Preparing the Dataset
---------------------

The first step in fine-tuning Stable Diffusion for style is to prepare the dataset.

Most blog posts skip over this part, or give only a cursory overview.
This gives the false impression that dataset preparation is trivial
and that models, optimization algorithms, and infrastructure are the most important.

We found that handling the data was actually the most important and most difficult part of fine-tuning
— and just about all machine learning practitioners will tell you the same.

To use the Heroicons dataset, which consists of around 300 SVG icons, for fine-tuning, we need to:

1. Download the Heroicons from the
   [GitHub repo](https://github.com/tailwindlabs/heroicons)
2. Convert the SVGs to PNGs and add white backgrounds to the images

   Image models are trained on rasterized graphics, so we need to convert the icons.
3. Add white backgrounds to the PNGs

   We also need to add white backgrounds to the PNGs.
   This may seem trivial, but it is critically important - many models are incapable of outputting with transparency.
4. Generate captions for each image and create a
   `metadata.csv`
   file

   Since the Heroicon filenames match the concept they represent, we can parse them into captions.
   We also add a prefix to each caption:
   `“an icon of a <object>.”`

   We then create a
   `metadata.csv`
   file, where each row is an image file name with the associated caption.
   The
   `metadata.csv`
   file should be placed in the same directory as all the training images
   and contain a header row with the string
   `file_name,text`

   ```
   # tree heroicons_training_dir
   heroicons_training_dir/
    ├── arrow.png
    ├── bike.png
    ├── cruiseShip.png
    └── metadata.csv
   ```

   ```
   # metadata.csv

   file_name,text
   arrow.png,"an icon of a arrow"
   bike.png,"an icon of a bike"
   cruiseShip.png,"an icon of a cruise ship"
   ```

5. Upload the dataset to the HuggingFace Hub

   ```
   import os
   from datasets import load_dataset
   import huggingface_hub

   # login to huggingface
   hf_key = os.environ["HUGGINGFACE_TOKEN"]
   huggingface_hub.login(hf_key)

   dataset = load_dataset("imagefolder", data_dir="/lg_white_bg_heroicon_png_img", split="train")

   dataset.push_to_hub("yirenlu/heroicons", private=True)
   ```

You can see the post-processed dataset
[here](https://huggingface.co/datasets/yirenlu/heroicons)

.

Training on Modal
-----------------

### Setting up Diffusers dependencies on Modal

To fine-tune Stable Diffusion for style, we used the
[Diffusers library](https://github.com/huggingface/diffusers)

by HuggingFace.
Diffusers provides a set of easy-to-use scripts for fine-tuning these models on custom datasets.

You can see an up-to-date list of all their scripts in their
[`examples`
subdirectory](https://github.com/huggingface/diffusers/tree/main/examples)

.

For this fine-tuning task, we will be using the
[`train_text_to_image.py`](https://github.com/huggingface/diffusers/blob/abd922bd0c43a504e47eca2ed354c3634bd00834/examples/text_to_image/train_text_to_image.py)

script. This script does full fine-tuning.

When you run your code on Modal, it executes in
[a containerized environment](/docs/guide/images)

in the cloud, not on your machine.
This means that you need to set up any dependencies in that environment.

Modal provides a Pythonic API to define containerized environments
— the same power and flexibility as a Dockerfile, but without all the tears.

```
# fine-tune-stable-diffusion.py
import os
import sys
from dataclasses import dataclass
from pathlib import Path

from fastapi import FastAPI
from modal import Image, App, Volume, gpu, Secret

GIT_SHA = "abd922bd0c43a504e47eca2ed354c3634bd00834"  # specify the commit to fetch

image = (
    Image.debian_slim(python_version="3.10")
    .pip_install(
        "accelerate==0.27.2",
        "datasets~=2.19.1",
        "ftfy~=6.1.1",
        "gradio~=3.50.2",
        "smart_open~=6.4.0",
        "transformers~=4.38.1",
        "torch~=2.2.0",
        "torchvision~=0.16",
        "triton~=2.2.0",
        "peft==0.7.0",
        "wandb==0.16.3",
    )
    .apt_install("git")
    # Perform a shallow fetch of just the target `diffusers` commit, checking out
    # the commit in the container's current working directory, /root.
    .run_commands(
        "cd /root && git init .",
        "cd /root && git remote add origin https://github.com/huggingface/diffusers",
        f"cd /root && git fetch --depth=1 origin {GIT_SHA} && git checkout {GIT_SHA}",
        "cd /root && pip install -e .",
    )
)
```

### Setting up `Volume` for cloud storage of weights

Modal provides network file systems,
[Volumes](/docs/guide/volumes)

,
for writing information persistently from those cloud containers.

We use one to store the weights after we’re done training.
We then read the weights from it when it’s time to run inference and generate new icons.

```
# fine-tune-stable-diffusion.py

web_app = FastAPI()
app = App(name="example-diffusers-app")

MODEL_DIR = Path("/model")
model_volume = Volume.from_name("diffusers-model-volume", create_if_missing=True)

VOLUME_CONFIG = {
    "/model": model_volume,
}
```

### Setting up hyperparameter configs

We fine-tuned off the StableDiffusion v1.5 model, but you can easily also fine-tune off of other Stable Diffusion
versions by changing the config below. We used
`4000`
training steps, a learning rate of
`1e-5`
, and a batch size of
`1`
.

We set up one
`dataclass`
,
`TrainConfig`
, to hold all the training hyperparameters,
and another,
`AppConfig`
, to store all the inference hyperparameters.

```
# fine-tune-stable-diffusion.py

@dataclass
class TrainConfig:
    """Configuration for the finetuning training."""

    # identifier for pretrained model on Hugging Face
    model_name: str = "runwayml/stable-diffusion-v1-5"

    resume_from_checkpoint: str = "latest"
    # HuggingFace Hub dataset
    dataset_name = "yirenlu/heroicons"

    # Hyperparameters/constants from some of the Diffusers examples
    # You should modify these to match the hyperparameters of the script we are using.
    mixed_precision: str = "fp16"  # set the precision of floats during training, fp16 or less needs to be mixed with fp32 under the hood
    resolution: int = 128
    max_train_steps: int = (
        4000  # number of times to apply a gradient update during training
    )
    checkpointing_steps: int = (
        1000  # number of steps between model checkpoints, for resuming training
    )
    train_batch_size: int = (
        1  # how many images to process at once, limited by GPU VRAM
    )
    gradient_accumulation_steps: int = 1  # how many batches to process before updating the model, stabilizes training with large batch sizes
    learning_rate: float = 1e-05  # scaling factor on gradient updates, make this proportional to the batch size * accumulation steps
    lr_scheduler: str = (
        "constant"  # dynamic schedule for changes to the base learning_rate
    )
    max_grad_norm: int = 1  # value above which to clip gradients, stabilizes training
    caption_column: str = "text"  # name of the column in the dataset that contains the captions of the images
    validation_prompt: str = "an icon of a dragon creature"

@dataclass
class AppConfig:
    """Configuration information for inference."""

    num_inference_steps: int = 50 # How many steps to run the model for inference, the more the higher quality generally
    guidance_scale: float = 20 # How much the image should adhere to the text prompt
```

### Running fine-tuning

Now, finally, we’re ready to fine-tune.

We first need to decorate the
`train`
function with
`@app.function`
,
which tells Modal that the function should be launched in a cloud container on Modal.

Functions on Modal combine code and the infrastructure required to run it.
So the
`@app.function`
decorator takes several arguments that lets us specify
the type of GPU we want to use for training,
the Modal Volumes we want to mount to the container,
and any secret values (like the HuggingFace API key) that we want to pass to the container.

This training function does a bunch of preparatory things,
but the core of it is the
`notebook_launcher`
call that launches the actual Diffusers training script as a subprocess.
In particular, we are launching the script using the
[Accelerate](https://huggingface.co/docs/accelerate/en/index)

CLI command.
Accelerate is a Python library that makes it easy to leverage multiple GPUs for accelerated model training.

The training script saves checkpoint files every 1000 steps.
To make sure that those checkpoints are persisted,
we need to set
`_allow_background_volume_commits=True`
in the
`@app.function`
decorator.

```
# fine-tune-stable-diffusion.py

@app.function(
    image=image,
    gpu=gpu.A100(
        size="80GB"
    ),  # finetuning is VRAM hungry, so this should be an A100 or H100
    volumes=VOLUME_CONFIG,
    timeout=3600 * 2,  # multiple hours
    secrets=[Secret.from_name("huggingface-secret")],
    _allow_background_volume_commits=True
)
def train():
    import huggingface_hub
    from accelerate import notebook_launcher
    from accelerate.utils import write_basic_config

    # change this line to import the training script we want to use
    from examples.text_to_image.train_text_to_image import main
    from transformers import CLIPTokenizer

    # set up TrainConfig
    config = TrainConfig()

    # set up runner-local image and shared model weight directories
    os.makedirs(MODEL_DIR, exist_ok=True)

    # set up hugging face accelerate library for fast training
    write_basic_config(mixed_precision="fp16")

    # authenticate to hugging face so we can download the model weights
    hf_key = os.environ["HF_TOKEN"]
    huggingface_hub.login(hf_key)

    # check whether we can access the model repo
    try:
        CLIPTokenizer.from_pretrained(config.model_name, subfolder="tokenizer")
    except OSError as e:  # handle error raised when license is not accepted
        license_error_msg = f"Unable to load tokenizer. Access to this model requires acceptance of the license on Hugging Face here: https://huggingface.co/{config.model_name}."
        raise Exception(license_error_msg) from e

    def launch_training():
        sys.argv = [
            "examples/text_to_image/train_text_to_image.py",  # potentially modify
            f"--pretrained_model_name_or_path={config.model_name}",
            f"--dataset_name={config.dataset_name}",
            "--use_ema",
            f"--output_dir={MODEL_DIR}",
            f"--resolution={config.resolution}",
            "--center_crop",
            "--random_flip",
            f"--gradient_accumulation_steps={config.gradient_accumulation_steps}",
            "--gradient_checkpointing",
            f"--train_batch_size={config.train_batch_size}",
            f"--learning_rate={config.learning_rate}",
            f"--lr_scheduler={config.lr_scheduler}",
            f"--max_train_steps={config.max_train_steps}",
            f"--lr_warmup_steps={config.lr_warmup_steps}",
            f"--checkpointing_steps={config.checkpointing_steps}",
        ]

        main()

    # run training -- see huggingface accelerate docs for details
    print("launching fine-tuning training script")

    notebook_launcher(launch_training, num_processes=1)

@app.local_entrypoint()
def run():
    train.remote()
```

With that all in place, we can kick off a training run on Modal from anywhere
with a simple command:

```
modal run fine-tune-stable-diffusion.py
```

Serving the fine-tuned model
----------------------------

Once
`fine-tune-stable-diffusion.py`
has finished its training run, the fine-tuned model will be saved in the Volume.
We can then mount the volume to a new Modal
`inference`
function,
which we can then invoke from any Python code running anywhere.

```
# fine-tune-stable-diffusion.py

@app.cls(
    image=image,
    gpu="A10G", # inference requires less VRAM than training, so we can use a cheaper GPU
    volumes=VOLUME_CONFIG, # mount the location where your model weights were saved to
)
class Model:
    @enter()
    def load_model(self):

        import torch
        from diffusers import StableDiffusionPipeline

        # Reload the modal.Volume to ensure the latest state is accessible.
        app.model_volume.reload()

        # set up a hugging face inference pipeline using our model
        # potentially use different pipeline
        pipe = StableDiffusionPipeline.from_pretrained(
            MODEL_DIR,
            torch_dtype=torch.float16,
        ).to("cuda")

        pipe.enable_xformers_memory_efficient_attention()
        self.pipe = pipe

    @method()
    def inference(self, text, config):

        image = self.pipe(
            text,
            num_inference_steps=config.num_inference_steps,
            guidance_scale=config.guidance_scale,
        ).images[0]

        return image
```

Wrapping inference in a Gradio UI
---------------------------------

Finally, we set up a
[Gradio](https://www.gradio.app/)

UI that will allow us to interact with our icon generator.
That lets us build this entire app, from data prep to browser app, in Python.

Our Gradio app calls the
`Model.inference`
function we defined above.

We can do this from any Python code we want,
but we choose to also make this part of our Modal app,
because
[Modal makes it easy to host Python web apps](/docs/guide/webhooks)

.

```
# fine-tune-stable-diffusion.py

@app.function(
    image=image,
    max_containers=3,
)
@asgi_app()
def fastapi_app():
    import gradio as gr
    from gradio.routes import mount_gradio_app

    # Call to the GPU inference function on Modal.
    def go(text):
        return Model().inference.remote(text, config)

    # set up AppConfig
    config = AppConfig()

    prefix = "an icon of"

    example_prompts = [
        f"{prefix} a movie ticket",
        f"{prefix} campfire",
        f"{prefix} a castle",
        f"{prefix} a German Shepherd",
    ]

    description = f"""Describe a concept that you would like drawn as a [Heroicon](https://heroicons.com/). Try the examples below for inspiration.
    """

    # add a gradio UI around inference
    interface = gr.Interface(
        fn=go,
        inputs="text",
        outputs=gr.Image(shape=(512, 512)),
        title="Generate custom heroicons",
        examples=example_prompts,
        description=description,
        css="/assets/index.css",
        allow_flagging="never",
    )

    # mount for execution on Modal
    return mount_gradio_app(
        app=web_app,
        blocks=interface,
        path="/",
    )
```

Deployment on Modal is as simple as running one command:

```
modal deploy fine-tune-stable-diffusion.py
```

Parting thoughts
----------------

How does our fine-tuned model do as an infinite icon library?

![More generated Heroicons](https://modal-cdn.com/cdnbot/fine-tuning-stable-diffusion-generated-heroicons-more.png)

Top row:
`camera`
,
`chemistry`
,
`fountain-pen`
.

Middle row:
`german-shepherd`
,
`international-monetary-system`
,
`library`
.

Bottom row:
`skiing`
,
`snowman`
,
`water-bottle`

It’s certainly not perfect:

* The model sometimes outputs multiple objects when prompted for one (
  `water-bottle`
  ,
  `fountain-pen`
  ).
* Some icons have visual artifacts or strange shapes (
  `snowman`
  ).
* The outputs aren’t as simple as the real Heroicons (
  `camera`
  ,
  `german-shepherd`
  ).

Fine-tuning can be sensitive to the hyperparameters used,
including dataset size, number of training steps, learning rates, and resolution.

Because we defined our training to run on Modal, we can immediately scale it up into a massive grid search
— running tens or hundreds or thousands of copies of the training script at once,
each with different hyperparameters.

And it only takes a few lines of code to set up a grid search.
It might look like this:

```

RESOLUTIONS = [128, 512]
LEARNING_RATES = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
LEARNING_RATE_SCHEDULERS = ["constant", "cosine"]

@app.local_entrypoint()
def run():
    from uuid import uuid4

    configs = []
    for resolution in RESOLUTIONS:
        for learning_rate in LEARNING_RATES:
            for learning_rate_scheduler in LEARNING_RATE_SCHEDULERS:
                train.spawn(
                    {
                        "resolution": resolution,
                        "learning_rate": learning_rate,
                        "learning_rate_scheduler": learning_rate_scheduler,
                        "output_dir": uuid4(),
                    }
                )
```

Evaluation of which hyperparameter combinations are best will probably have to be done manually,
given how subjective style can be.

But that’s what makes machine learning
~~hard~~
fun!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/fine-tuning-vs-rag-article
================================================================================

Fine-tuning vs. RAG: Which approach is right for your use case?
===============================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

If you’re looking to use LLMs to build personalized chatbots or other AI applications, you’ve probably heard of fine-tuning and Retrieval Augmented Generation (RAG). These approaches allow organizations to tailor LLMs to specific domains or tasks, improving accuracy and relevance.

But when should you use fine-tuning versus RAG? This article explores the key differences and use cases for each method.

Fine-tuning LLMs
----------------

### What is fine-tuning?

Fine-tuning involves taking a pre-trained LLM and further training it on a smaller, specialized dataset. For example, you might take
[Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)

and fine-tune it on a dataset of medical articles to improve its understanding of medical terminology. This process adjusts the model’s parameters to better suit a specific task or domain.

### When to use fine-tuning

Fine-tuning is particularly useful in the following scenarios:

1. **Domain-specific tasks:**
   When you need the model to understand and generate content in a specialized field, such as legal, medical, or technical writing.
2. **Setting the style, tone, format, or other qualitative aspects:**
   For applications requiring a specific writing style or brand voice, fine-tuning can help maintain consistency.
3. **Improving reliability at producing a desired output:**
   Fine-tuning can significantly enhance performance on subjects not well-represented in the original training data.
4. **Handling many edge cases in specific ways:**
   Fine-tuning allows the model to learn from specific examples and adapt to unique scenarios.
5. **Cost sensitive:**
   While fine-tuning requires an upfront investment, it can lead to long-term cost savings and improved performance. Once a model has been fine-tuned, you may not need to provide as many examples in the prompt, resulting in cost savings and lower-latency requests.

### How to fine-tune

**Step 1: Prepare your custom data**

The first step is to gather a dataset that is relevant to your specific task or domain. This dataset should be large enough to provide meaningful training data for the model. Ensure that the data is high-quality, diverse, and representative of the task you want the model to perform. This includes considering the form of the data; for example, if you want the eventual fine-tuned outputs to be long, then your training data examples should be long. You may need to preprocess the data by tokenizing it, removing stop words, or performing other necessary steps to prepare it for fine-tuning. Additionally, the data will likely need to be prepared in a specific format, such as JSONL.

**Step 2: Select a base model**

Choose a pre-trained LLM that is suitable for your task. Consider factors such as the model’s architecture, size, and performance on similar tasks. Popular choices include models like
[Llama3](https://huggingface.co/models?search=Llama3)

and
[Mistral](https://huggingface.co/models?search=Mistral)

. You can find pre-trained models on platforms like
[Hugging Face’s Model Hub](https://huggingface.co/models)

.

**Step 3: Determine the required VRAM**

Estimate the amount of Video Random Access Memory (VRAM) needed to fine-tune your model. This is crucial to ensure that your computing resources can handle the model’s requirements. You can refer to our article on
[how much VRAM you need to fine-tune an LLM](/blog/how-much-vram-need-fine-tuning)

for guidance on estimating VRAM requirements.

**Step 4: Select a fine-tuning framework or library**

There are a number of
[libraries](/blog/fine-tuning-llms)

and frameworks that can help you fine-tune an LLM and abstract away some of the low-level details and provide built-in optimizations.

**Step 5: Select a serverless GPU provider for fine-tuning**

[Serverless GPU providers](/blog/serverless-gpu-article)

like
[Modal](https://modal.com/)

offer scalable GPU resources that simplify the fine-tuning process. You only pay for the GPUs when you actually run the fine-tunes and you don’t need to manage infrastructure.

Retrieval Augmented Generation (RAG)
------------------------------------

### What is RAG?

RAG, or Retrieval Augmented Generation, is a technique that enhances an LLM’s responses by incorporating external knowledge sources as part of the prompt context. Instead of relying solely on the model’s pre-trained knowledge, RAG allows the model to query and integrate information from external databases or documents in real-time.

### How does RAG work

**Step 1: Document Chunking**

The first step is to break down your text documents into smaller, manageable chunks. This process is called chunking. The goal is to divide the documents into sections that are meaningful and coherent, representing units of relevant context. For example, you might choose to chunk a lengthy article into paragraphs or sections based on thematic content. This ensures that each chunk retains enough context to be useful during the retrieval process, while also being small enough to allow for efficient searching and embedding.

**Step 2: Embedding and Database Storage**

Once the documents are chunked, the next step is to embed each chunk using an embedding model. An embedding model is a type of neural network that converts text into a dense vector representation, allowing the model to capture the semantic meaning of the text. The embeddings are then stored in a database, which can be queried later to retrieve relevant information.

**Step 3: Query Embedding and Similarity Search**

When a user submits a query to the LLM, the query is also embedded using the same embedding model as before. This embedded query is then used to search the database for the most similar embeddings. The similarity search is typically done using a vector similarity metric, such as cosine similarity or dot product. The goal is to find the embeddings that are closest to the query embedding, indicating that they contain relevant information.

**Step 4: Context Addition and LLM Query**

The final step in the RAG process is to take the contents of the most similar embedding(s) and add them as context to the original query. This enriched query is then passed to the LLM, which generates a response based on the original query and the additional context. The context provided by RAG helps the LLM to better understand the query and generate more accurate and informative responses.

### When to use RAG

RAG is particularly beneficial in these situations:

1. **Up-to-date information**
   : When your application requires access to the latest information that may not be present in the model’s training data.
2. **Factual accuracy**
   : RAG can improve the model’s ability to provide accurate, verifiable information by referencing external sources.
3. **Customizable knowledge base**
   : RAG allows you to easily update or modify the external knowledge source without retraining the entire model.

For instance, a customer support chatbot for a tech company could be fine-tuned on the company’s documentation and support tickets to understand product-specific terminology and common issues. RAG could then be used to incorporate the latest product updates or known issues in real-time.

RAG performance
---------------

In order to get RAG to perform how you want, there are a number of parameters that you will likely need to fiddle around with, including:

1. **Chunking strategy**
   : You may choose to chunk based on sentence boundaries, paragraphs, or even semantic meaning. Experimenting with different chunk sizes can help you find the optimal balance between context richness and retrieval efficiency. Smaller chunks may provide more precise information but can lead to a loss of context, while larger chunks may retain context but could introduce irrelevant information.
2. **Embedding model**
   : The choice of embedding model is crucial, as it determines how well the text is represented in vector space. Different models may capture different aspects of the text, so it’s important to select one that aligns with your specific use case. You can start by looking through the
   [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

   for the top embedding models, but remember that just because a model tops the leaderboard doesn’t necessarily mean that it will work best for your use case.
3. **Similarity metric**
   : The method you use to measure similarity between embeddings can greatly affect the retrieval results. Common metrics include cosine similarity, which measures the angle between two vectors, and dot product, which assesses the magnitude of the vectors. Depending on your application, you may want to experiment with different metrics to see which yields the best results for your queries. Additionally, consider implementing a hybrid approach that combines multiple metrics for improved accuracy.
4. **Retrieval threshold**
   : Setting a threshold for how similar an embedding must be to be considered relevant can help filter out noise. A lower threshold may retrieve more results, but could include less relevant information, while a higher threshold may yield fewer, but more accurate results. Tuning this parameter based on your specific needs can enhance the overall effectiveness of the RAG system.
5. **Context length**
   : The amount of context you provide to the LLM can also influence its performance. Too little context may lead to vague or irrelevant responses, while too much can overwhelm the model. Finding the right balance is key, and you may need to adjust the context length based on the complexity of the queries and the nature of the information being retrieved.

Conclusion
----------

Both fine-tuning and RAG offer powerful ways to enhance LLM performance for specific use cases. Fine-tuning excels in creating models with deep domain expertise and consistent output, while RAG provides flexibility and up-to-date information access. By understanding the strengths of each approach, you can choose the most appropriate method—or combination of methods—for your specific needs.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/flash-attention-article
================================================================================

What is Flash Attention?
========================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

[Flash Attention](https://github.com/Dao-AILab/flash-attention)

is an algorithm that speeds up the training and inference of transformer models.

How does Flash Attention work?
------------------------------

Many modern transformer models use a mechanism called “attention” to focus on important parts of their input. It’s like how humans pay attention to key words in a sentence. The problem, though, is that traditional attention computations are slow and memory-hungry, especially for long sequences of data (like long documents or high-resolution images).

Flash Attention rethinks how attention is computed on GPUs. It uses smart memory management techniques to do the same calculations much faster and with less memory. In particular, it carefully manages how data moves between different levels of memory on a GPU.

When should you use Flash Attention
-----------------------------------

You should consider using Flash Attention if:

* You’re working with large language models or any AI that uses attention mechanisms (like transformers) and you want to speed up training or inference.
* You have very long input sequences (thousands or tens of thousands of tokens) or large batch sizes
* Scenarios where GPU memory is a bottleneck

By using Flash Attention in these contexts, you can expect:

* Faster training and inference times
* Ability to handle longer sequences without running out of memory
* Potential to increase model size or batch size within the same memory constraints

Flash Attention Versions
------------------------

There have been several versions of Flash Attention. After the original Flash Attention, released in 2022,
[Flash Attention 2](https://arxiv.org/abs/2307.08691)

was released in early 2023. It included optimizations for memory access patterns and causal attention, achieving up to 2x speedup over its predecessor.

The latest iteration,
[Flash Attention 3](https://pytorch.org/blog/flashattention-3/)

, incorporates enhancements specifically designed for NVIDIA’s Hopper GPU architecture, (e.g. H100s) allowing for even greater efficiency and performance. This version leverages advanced techniques to maximize GPU utilization and further improve speed and memory efficiency.

How to use Flash Attention
--------------------------

The easiest way to use Flash Attention is to use a training or inference framework that has it integrated already. Below, we cover the most popular frameworks and the status of their integration with Flash Attention.

### PyTorch

PyTorch has
[native support](https://pytorch.org/blog/pytorch2-2/#bookmark=id.ok7v7pq0igzw)

for Flash Attention 2 as of version 2.2. You can use it directly in your PyTorch models. To enable Flash Attention in PyTorch, you typically need to select Flash Attention as
[the attention mechanism in the Scaled Dot Product Attention backend](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html#explicit-dispatcher-control)

.

### Hugging Face Transformers

The Transformers library supports Flash Attention for certain models. You can often enable it by setting the
[`attn_implementation="flash_attention_2"`](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#flashattention-2)

parameter when initializing a model. However, support may vary depending on the specific model architecture.

### vLLM

vLLM natively takes advantage of Flash Attention 2 as of v0.1.4. You don’t need to enable it separately.

Text Generation Inference (TGI)
-------------------------------

Flash Attention is enabled by default for TGI. However, its usage may vary depending on the specific models, even when compiled.

The system aims to utilize Flash Attention whenever possible due to its advantages, but it will revert to alternative methods if any issues arise.

### Separate Implementation

While these frameworks often include Flash Attention or similar optimizations, you can also install it using pip:

```
pip install flash-attn
```

or clone the repo and
[install it from source](/docs/examples/flux)

.

Make sure that you have its dependencies installed, including:

* **PyTorch**
  : Ensure you have PyTorch version 1.12 or above installed.
* **CUDA**
  : A compatible version of the CUDA toolkit is necessary for GPU support.
* **NVIDIA cuDNN**
  : This library is recommended for optimized performance on NVIDIA GPUs. For more information about the CUDA toolkit, refer to our
  [CUDA guide](/docs/guide/cuda)

  .

For a full example of how to run a transformers model on cloud compute with Flash Attention 3, you can refer to our Flux tutorial
[here](/docs/examples/flux)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/flux-3x-faster
================================================================================

Run FLUX.1-dev three times faster
=================================

![author](https://modal-cdn.com/will-shainin.jpg)

[Will Shainin

@Will\_Modal](https://twitter.com/Will_Modal)

ML Engineer

![author](https://modal-cdn.com/cdnbot/david_wang_headshotgylhrfss_85189999.webp)

[David Wang

@\_dcw02](https://twitter.com/_dcw02)

ML Perf Engineer

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

Developer Advocate

[

](https://modal-cdn.com/blog/videos/flux-3x-faster-baseline-animation.mp4)
[

](https://modal-cdn.com/blog/videos/flux-3x-faster-full-animation.mp4)

The era of “get your AI from an API” is rapidly coming to a close.

High-quality open weights models and high-performance open source software together mean that you can easily run your own API to generate
[images](/use-cases/image-video-3d)

or
[music](/use-cases/audio)

or
[text](/use-cases/language-models)

, with all the control and customization self-hosting affords.

But having the ability to run your own generative inference raises a bunch of questions: when does it make sense, how do you do it, and, importantly, how do you do it with the same performance and quality that proprietary generative APIs provide?

We recently shared
[our results and recommendations for running your own LLM inference](/llm-almanac/summary)

. But we also like media generative models, and optimizations look quite different. Where LLM inference is all about finding the right high-level framework and tuning the knobs, diffusion-based models of images require getting a lot closer to the metal.

In this blog post, we walk through how we made running the popular
[FLUX.1-dev model](https://huggingface.co/black-forest-labs/FLUX.1-dev)

by Black Forest Labs as an autoscaling service on Modal competitive with proprietary providers on speed and price by running the inference three times faster and speeding up cold boots. You can find the code
[here](https://github.com/modal-labs/modal-examples/blob/b5fbb047905382a611cb21d45aa6ddd631a1f15d/misc/flux_endpoint.py)

.

### tl;dr: 1.5x from optimizing compiler and hardware awareness, 2x from approximate caching

We determined that in order to be competitive with APIs serving FLUX.1-dev images, we needed to return results in under three seconds.

Applying the “standard” optimizations (running the Torch compiler, switching the data layout, and fusing the QKV calculation) got us halfway to the target.

Then we applied a fun, approximate activation caching technique, First Block Caching, and cut the latency in half again.

![](https://modal-cdn.com/blog/images/flux-3x-faster-full-results-1.webp)

Implement the baseline
----------------------

Before beginning to improve performance, you need to first measure the current performance cleanly.

We start with the standard Hugging Face
`diffusers`
library and create our
`FluxPipeline`
in 16bit precision.

```
self.pipe = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    torch_dtype=torch.bfloat16,
    use_safetensors=True,
)
```

Averaging across a variety of inputs, we find that we can generate a 1024x1024 image in ~6.75 seconds.

[

](https://modal-cdn.com/blog/videos/flux-3x-faster-baseline-animation.mp4)
![](https://modal-cdn.com/blog/images/flux-3x-faster-baseline-results.webp)

Apply standard optimizations for a 1.5x speedup
-----------------------------------------------

We started by applying a bunch of “standard” optimizations — using Torch’s optimizing compiler; fusing the query, key, and value computations in the Transformer attention; and using the “channels last” memory layout. These are nearly always a good idea.

### **Optimize the compute graph with the Torch compiler**

What is a PyTorch program really? By default, PyTorch constructs a compute graph of tensor operations dynamically in Python and runs it eagerly. This “virtual compute graph” is executed on the host/CPU and triggers execution of a “real compute graph” on the device/GPU. If you’re curious how this works, we recommend
[generating some PyTorch traces](/docs/examples/torch_profiling)

and examining them.

Graph representations of programs are really nice for program transformations. Back in the BC era (Before ChatGPT), most people used PyTorch to train their own neural networks, and the key program transformation was running the program backwards to figure out how to make it less wrong, aka “
[learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)

”.

Now that, like neural networks themselves, PyTorch is used more for inference, the key program transformation has changed to
*compilation*
. Compilation replaces the compute graph with an equivalent but faster one. If you’re familiar with database query compilers, think of logical-logical optimization transformations like predicate pushdown.

As with any respectable modern compiler, the Torch compiler operates as a series of lowerings into increasingly concrete intermediate representations.
[TorchDynamo](https://docs.pytorch.org/docs/stable/torch.compiler_dynamo_overview.html)

hooks the CPython frame interpreter, traces Python bytecode, and carves out stretches of Tensor operations into lowered
[“FX” graphs](https://docs.pytorch.org/docs/stable/fx.html#torch.fx.Graph)

. A backend compiler like
[TorchInductor](https://dev-discuss.pytorch.org/t/torchinductor-a-pytorch-native-compiler-with-define-by-run-ir-and-symbolic-shapes/747)

then takes these graphs and lowers them to a further optimized representation, like a
[Triton](https://github.com/triton-lang/triton)

kernel.

We separately compile the model’s two large subcomponents (the Transformer and the Variational Autoencoder). Our configuration settings appear in the code snippet below. We got many of them from
[this excellent guide](http://huggingface.co/docs/diffusers/en/tutorials/fast_diffusion)

on Hugging Face and did some light validation before adjusting other parameters.

The most notable choice is to use
`max-autotune`
, which incurs tens of minutes of compile-time cost but ensures optimal run-time performance. See the final section for details on how we cut that back down to minutes without losing Modal’s transparent auto-scaling.

```
class Flux:
    ...

    @modal.enter()
    def setup(self):
        self.pipe = FluxPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-dev",
            torch_dtype=torch.bfloat16,
            use_safetensors=True,
        )
        # torch.compile configuration
        config = torch._inductor.config
        config.conv_1x1_as_mm = True
        config.coordinate_descent_check_all_directions = True
        config.coordinate_descent_tuning = True
        config.disable_progress = False
        config.epilogue_fusion = False
        config.shape_padding = True

        # Mark layers for compilation with dynamic shapes enabled.
        self.pipe.transformer = torch.compile(
            self.pipe.transformer, mode="max-autotune-no-cudagraphs", dynamic=True
        )

        self.pipe.vae.decode = torch.compile(
            self.pipe.vae.decode, mode="max-autotune-no-cudagraphs", dynamic=True
        )
        # Trigger torch compile
        self.pipe("dummy prompt", height=1024, width=1024, num_images_per_prompt=1)
        ...
```

### Expose more parallelism to the GPU with fused QKV

FLUX includes a big Transformer model. The Transformer architecture’s signature component is the attention block, which transmits information between text and image and
[through internal circuits](https://transformer-circuits.pub/)

. They are usually written in terms of three separate matrix multiplications between the block’s input matrix
`X`
and its weight matrices
`W_q`
,
`W_k`
, and
`W_v`
.

If we concatenate the three weight matrices, we can perform the attention calculation as one large matrix multiplication: (
`QKV = X @ W_qkv`
). This exposes more of the parallelism in the operation to the lowered representations. In particular,
`X`
is the same matrix for the entire multiplication, not three variable references that
~~we totally promise~~
the Torch compiler must verify are to the exact same data.

```
class Flux:
    ...
    @modal.enter()
    def setup(self):
        ...
        self.pipe.transformer.fuse_qkv_projections()
        self.pipe.vae.fuse_qkv_projections()
        ...
```

### Improve data locality with channels-last memory layout

The last standard optimization we do is a common recommendation to improve data locality.

Tensors are spicy multi-dimensional arrays. Tensors representing images (or feature maps over images) have three dimensions: channel (color), height (y position) and width (x position). This three-dimensional array needs to be mapped onto linear computer memory.

By default, PyTorch orders image Tensors in memory by channel first, then by height, then by width (
`CHW`
or “Channels First”). Sequential accesses therefore read spatially nearby values from a single channel/color.

Let’s walk through an example. This image

```
// a 2x4 image with three channels of one byte each
┌────────┬────────┬────────┬────────┐
│ #888888│ #999999│ #AAAAAA│ #BBBBBB│
├────────┼────────┼────────┼────────┤
│ #CCCCCC│ #DDDDDD│ #EEEEEE│ #FFFFFF│
└────────┴────────┴────────┴────────┘
```

is represented in memory in
`CHW`
format as

```
// channels first
0x00 : 88 99 AA BB  CC DD EE FF   ← 🟥 red values
0x0F : 88 99 AA BB  CC DD EE FF   ← 🟩 green values
0x17 : 88 99 AA BB  CC DD EE FF   ← 🟦 blue values
```

But many operations in neural networks, like convolutions, are global across channels and local in space. That means we usually want to access all channels at a particular set of positions, and so we want channels to be
*last*
.

```
// channels last
0x00 : 88 88 88   🟥 🟩 🟦
0x03 : 99 99 99   🟥 🟩 🟦
0x06 : AA AA AA   🟥 🟩 🟦
0x09 : BB BB BB   🟥 🟩 🟦
0x0C : CC CC CC   🟥 🟩 🟦
0x0F : DD DD DD   🟥 🟩 🟦
0x12 : EE EE EE   🟥 🟩 🟦
0x15 : FF FF FF   🟥 🟩 🟦
```

You can convert PyTorch models into this format with the
`memory_format`
argument.

```
class Flux:
    ...
    @modal.enter()
    def setup(self):
        ...
        self.pipe.transformer.to(memory_format=torch.channels_last)
        self.pipe.vae.to(memory_format=torch.channels_last)
        ...
```

### Putting it all together, we get a 1.5x speedup

The performance improvement of these optimizations in aggregate is about 1.5x, driven mostly by the Torch compiler.

![flux-3x-faster-std-opt-results.png](https://modal-cdn.com/blog/images/flux-3x-faster-std-opt-results.webp)

This speedup is definitely respectable, as evidenced by the animation below, which shows the evolution of the image across denoising steps, rendered at the same rate those steps execute for the two methods.

[

](https://modal-cdn.com/blog/videos/flux-3x-faster-baseline-animation.mp4)
[

](https://modal-cdn.com/blog/videos/flux-3x-faster-std-opt-animation.mp4)

Apply
~~vibes-based~~
approximate caching for another 2x speedup
----------------------------------------------------------------

Applying the “standard” optimizations above is pretty straightforward and ends up being an appealing point on the engineering effort/performance curve. But we needed to go further on performance, so we needed to go deeper.

Diffusion models generate images iteratively, turning noise into art, one step at a time. That’s the process we’re showing in these animations. If you look closely you can see that during some steps, the image doesn’t change much at all.

[

](https://modal-cdn.com/blog/videos/flux-3x-faster-baseline-animation.mp4)

As it turns out, if you’re willing to tolerate some slight changes in the results, you can skip those steps entirely!

This is an important difference between neural networks and other programs. With neural networks, you can often remove chunks or skip steps, and the program still runs, and does “almost” the same thing. More like an analog computer than a digital one!

We used the “first block caching” technique and implementation from the
[ParaAttention repo](https://github.com/chengzeyi/ParaAttention)

, itself based on the approach from the
[TEACache paper](https://liewfeng.github.io/TeaCache/)

. The basic idea is to start running the model for a timestep. If, partway through the model’s forward pass (after the “first block”), it looks like there won’t be a large change, you skip the step.

```
class Flux:
    ...
    @modal.enter()
    def setup(self):
        ...
        from para_attn.first_block_cache.diffusers_adapters import apply_cache_on_pipe
        apply_cache_on_pipe(
            self.pipe,
            residual_diff_threshold=0.12,
            # quality degraded too much at higher thresholds
        )
```

The definition of “large” is a tunable parameter, where higher values lead to larger changes in model behavior but faster execution. This allows for a smoother tradeoff between performance improvement and quality degradation than other techniques that do the same, like quantization.

We got a 2x speedup with a threshold of
`0.12`
and images looked better than with the default of
`0.08`
, so we stuck with it.

![flux-3x-faster-full-results.png](https://modal-cdn.com/blog/images/flux-3x-faster-full-results-1.webp)

[

](https://modal-cdn.com/blog/videos/flux-3x-faster-std-opt-animation.mp4)
[

](https://modal-cdn.com/blog/videos/flux-3x-faster-full-animation.mp4)

Cut cold start latency by 30x with caches and snapshots
-------------------------------------------------------

As we optimized inference, we took an
*enormous*
hit on boot time — from seconds to tens of minutes.

Boot time matters for cost and speed as well. If boots are fast, you can run only as many replicas as you need to satisfy current demand and still hit your latency objectives.

This is something we think is very critical, and we spend a lot of time optimizing this at Modal! You can read more about why we think this is so important for generative applications in our
[GPU utilization](/blog/gpu-utilization-guide)

explainer and our
[case study with Suno](/blog/suno-case-study)

.

The primary culprit is the Torch compiler, and specifically
`max-autotune`
, which profiles multiple implementations at compile time to find the fastest one.

This is a classic use case for a cache — compute-intensive work that produces serializable artifacts. Torch Compile offers both piece-wise caching of smaller artifacts, like compiled Triton kernels, and a “megacache” that stores entire cached compute graphs. We used both, but the megacache didn’t offer a large speedup. It didn’t hurt either, and it’s a new feature we expect to improve over time, so we left it in. You can find the details
[here](https://github.com/modal-labs/modal-examples/blob/b5fbb047905382a611cb21d45aa6ddd631a1f15d/misc/flux_endpoint.py#L225-L230)

.

We also shave off a few seconds using Modal’s
[memory snapshots feature](/docs/guide/memory-snapshot)

, which lets us turn the many file reads and code execution in
`import torch`
and
`from_pretrained`
into a single file read (for every invocation after the first). Check out
[this blog post](/blog/mem-snapshots)

for a deep dive.

```
image = image.env(
	{
        "TORCHINDUCTOR_FX_GRAPH_CACHE": "1",
        "CUDA_CACHE_PATH": "/cache/.nv_cache",
        "TORCHINDUCTOR_CACHE_DIR": "/cache/.inductor_cache",
        "TRITON_CACHE_DIR": "/cache/.triton_cache",
	}
)

CACHE_VOLUME = modal.Volume.from_name("cache_volume", create_if_missing=True)

@app.cls(
    enable_memory_snapshot=True,
    volumes={"/cache": CACHE_VOLUME}
    ...
)
class Flux:
    @modal.enter(snap=True)
    def load(self):
        self.pipe = FluxPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-dev",
            torch_dtype=torch.bfloat16,
            use_safetensors=True,
        ).to("cpu")

    @modal.enter(snap=False)
    def setup(self):
        self.pipe.to("cuda")
        ... # rest of setup
```

Serve AI models at scale with Modal
-----------------------------------

Together, these optimizations cut FLUX.1-dev serving latency to match the performance of proprietary serving APIs. On Modal, that means you can match or beat providers on price too.

We didn’t talk too much about all the other problems that come up when building and serving a generative API — interactive development, handling bursty loads, and training/evaluating the next iteration of the service. If that’s interesting to you, check out the
[Modal serverless platform](https://modal.com)

, trusted to run generative inference at the scale of thousands of GPUs and tens of thousands of CPUs by customers from
[Suno](/blog/suno-case-study)

to
[Substack](/blog/substack-case-study)

to
[soccer teams](/blog/sports-case-study)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/flux-dev
================================================================================

What is Flux Dev?
=================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

[Flux Dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)

is a state-of-the-art 12 billion parameter text-to-image model developed by Black Forest Labs. Released in August 2024, it has rapidly gained momentum in just a few short months as the go-to choice for AI artists and developers seeking high-quality image generation capabilities.

Here are some examples of Flux Dev generated images we ran on Modal via our
[ComfyUI example](/docs/examples/comfyapp)

:

![cat](https://modal-cdn.com/flux-dev/cat.png)

A cat holding a sign that says "FLUX DEV"

![city](https://modal-cdn.com/flux-dev/city.png)

A dystopian cityscape with purplish atmosphere and dilapidated skyscrapers, depicted in a science fiction style

![beach](https://modal-cdn.com/flux-dev/beach.png)

Traditional chinese ink painting of a beautiful beach

Flux Dev is part of a suite of text-to-image models released by Black Forest Labs. This table outlines their core differences:

| Model | Description | Open weights | License required for commercial application |
| --- | --- | --- | --- |
| Flux Pro | Flagship, most detailed model | No | Yes |
| Flux Dev | Balanced model between detail and speed | Yes | Yes |
| Flux Schnell | Fastest model with less detail | Yes | No |

Is Flux Dev free to use?
------------------------

Yes, Flux Dev is free to use (inference and fine-tuning) for non-commercial use cases, which includes:

* Research and experimentation, including personal hobby projects
* Non-production use cases at for-profit companies
* Charitable organizations

If you intend to use Flux for revenue-generating, production use cases though, you need to either:

* Access the model through an
  [approved partner](https://blackforestlabs.ai/)
* Reach out to Black Forest Labs directly for a commercial license, which will be subject to some kind of fee or revenue sharing agreement

You can read the
[official license](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-dev)

on their Github.

Is Flux Pro free to use?
------------------------

No, Flux Pro is not free to use and completely closed-source. To access it, you need to
[create an account](https://docs.bfl.ml/quick_start/create_account/)

with Black Forest Labs directly or use one of their approved partners.

Is Flux Schnell free to use?
----------------------------

Yes, Flux Schnell is not only free to use but also free to use and fine-tune for commercial purposes under the Apache 2.0 license. You can read the
[official license](https://github.com/black-forest-labs/flux/blob/main/model_licenses/LICENSE-FLUX1-schnell)

on their Github.

How do you run Flux?
--------------------

You can run Flux in the following ways:

* ComfyUI: great for prototyping and those who prefer a more visual interface (see our
  [ComfyUI example](/docs/examples/comfyapp)

  )
* Diffusers: you can directly import
  `FluxPipeline`
  for convenient inference in your app (see our
  [Flux Schnell example](/docs/examples/flux)

  )
* Fine-tuning: you can fine-tune Flux for your specific use case (see our
  [Dreambooth pet art example](/docs/examples/dreambooth_app)

  )

Conclusion
----------

Flux Dev has effectively succeeded Stable Diffusion as the best open-weight diffusion model today for AI image generation. It’s a great choice for your image generation needs, but if you plan on serving or fine-tuning Flux Dev for revenue-generating, production use cases, you need to make sure to do it via an approved partner or have a direct agreement in place with Black Forest Labs. Alternatively, you can use Flux Schnell which has a very broad and generous license for commercial applications.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/google-cloud-function-pricing-guide
================================================================================

Google Cloud Run functions pricing: understanding costs and optimization
========================================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

[Google Cloud Run functions](https://cloud.google.com/functions)

offer a flexible and cost-effective serverless solution for running your code in the cloud. This guide breaks down the pricing model for both first and second generation functions, helping you understand and optimize your costs.

First gen vs. second gen functions
----------------------------------

Google Cloud Run functions come in two generations, each with its own pricing structure:

1. **First gen functions**
   : The original offering, priced based on invocations, compute time, and networking.
2. **Second gen functions**
   : Built on Cloud Run, offering more flexibility and features, with pricing based on vCPU, memory, and request count.

While Google recommends using second gen functions for new projects, both generations remain viable options. For a detailed comparison, see the
[Cloud Functions version comparison](https://cloud.google.com/functions/docs/concepts/version-comparison)

documentation.

First gen functions pricing
---------------------------

First gen functions are priced based on the following components:

1. **Compute Time**
   : $0.0000100 per GHz-second
2. **Memory**
   : $0.0000025 per GB-second
3. **Invocations**
   : $0.40 per million
4. **Networking**
   : $0.12 per GB of Internet data transfer out

The cost of the individual components are added together to get the total cost.

Google Cloud offers a generous free tier for first gen functions. For details on the free tier limits, refer to the
[official free tier documentation](https://cloud.google.com/functions/pricing-1stgen#free_tier)

.

For more details, refer to the
[official first gen pricing documentation](https://cloud.google.com/functions/pricing-1stgen)

.

### Understanding GHz-seconds and GB-seconds

For first gen functions, compute time is measured in GHz-seconds, while memory usage is measured in GB-seconds.

* GHz-seconds: The product of the CPU clock speed (in GHz) and the execution time (in seconds).
* GB-seconds: The product of the allocated memory (in GB) and the execution time (in seconds).

Examples:

* A 2.0 GHz CPU used for 1 second = 2 GHz-seconds
* A 2.4 GHz CPU used for 30 seconds = 72 GHz-seconds
* 256 MB (0.25 GB) of memory used for 1 second = 0.25 GB-seconds
* 1 GB of memory used for 30 seconds = 30 GB-seconds

### Pricing example

Let’s say your first gen function runs for 200ms on a 2.4 GHz CPU with 256 MB (0.25 GB) of allocated memory:

1. Compute Time: (2.4 GHz x 0.2 seconds) x ($0.0000100 / GHz-second) = $0.0000048
2. Memory: (0.25 GB x 0.2 seconds) x ($0.0000025 / GB-second) = $0.000000125
3. If this is the 2,000,001st invocation of the month, you’d also pay $0.0000004 for the invocation

Second gen functions pricing
----------------------------

Second gen functions are priced based on:

1. **vCPU usage**
   : $0.00002400 per vCPU-second
2. **Memory usage**
   : $0.0000025 per GB-second
3. **Invocations**
   : $0.40 per million
4. **Networking**
   : $0.12 per GB

### Free tier

Google Cloud offers a generous free tier for second gen functions. For details on the free tier limits, refer to the
[official free tier documentation](https://cloud.google.com/free/docs/free-cloud-features#cloud-functions)

.

For more details, refer to the
[official second gen pricing documentation](https://cloud.google.com/functions/pricing#gen2)

.

### Resource allocation

With second gen functions, you can specify both CPU and memory:

* **vCPU**
  : Choose from 0.08 to 8 vCPUs
* **Memory**
  : Allocate from 128MB to 32GB

This granular control allows you to optimize performance and cost for your specific workloads.

### Understanding vCPU-seconds

**vCPU-second**
:

A vCPU (virtual CPU) is a unit of computing power in cloud environments. In Google Cloud Run functions, a vCPU typically represents a hardware hyper-thread, which is approximately equivalent to half of a physical CPU core. This means that 2 vCPUs would roughly correspond to one full physical CPU core. The exact performance can vary depending on the underlying hardware and workload characteristics. vCPUs allow for flexible resource allocation and can be easily scaled up or down based on workload requirements, with Cloud Run functions offering options from 0.08 to 8 vCPUs per instance.

A vCPU-second measures the amount of virtual CPU time consumed by your function. It’s calculated by multiplying the number of vCPUs allocated to your function by the execution time.

Examples:

* 1 vCPU used for 1 second = 1 vCPU-second
* 2 vCPUs used for 30 seconds = 60 vCPU-seconds

### Pricing example

Let’s say your second gen function uses 1 vCPU and 2 GB of memory, and runs for 500ms:

1. vCPU cost: 1 vCPU x 0.5 seconds x $0.00002400 per vCPU-second = $0.000012
2. Memory cost: 2 GB x 0.5 seconds x $0.0000025 per GB-second = $0.0000025
3. Invocation cost: $0.40 / 1,000,000 = $0.0000004

Total cost for this execution: $0.0000149

Note: Prices may vary by region. Always check the
[official Google Cloud pricing page](https://cloud.google.com/functions/pricing)

for the most up-to-date information.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/google-cloud-run-vs-google-cloud-function-article
================================================================================

Google Cloud Run vs. Cloud Run Functions: understanding Google's serverless offerings
=====================================================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Introduction
------------

Google Cloud offers two primary serverless computing options:
[Cloud Run](https://cloud.google.com/run)

and
[Cloud Run Functions](https://cloud.google.com/functions)

. While their names are similar, these services cater to different use cases and offer distinct features. This article aims to clarify the differences between these two offerings and help you choose the right one for your project.

The evolution of Google’s serverless platform
---------------------------------------------

To understand the current landscape, it’s helpful to look at the evolution of Google’s serverless offerings:

1. **Cloud Functions (1st gen)**
   : Introduced in February 2016, this was Google’s initial serverless offering for simple, event-driven functions.
2. **Cloud Run**
   : Launched later as a more flexible, container-based serverless platform.
3. **Cloud Functions (2nd gen)**
   : Released in August 2022, bringing significant improvements over the first generation.
4. **Cloud Run Functions**
   : On August 21, 2024, Google rebranded Cloud Functions as Cloud Run Functions, merging the infrastructure with Cloud Run.

This evolution reflects Google’s effort to unify its serverless offerings while maintaining distinct services for different use cases.

Cloud Run: Container-based serverless computing
-----------------------------------------------

[Cloud Run](https://cloud.google.com/run/docs/overview/what-is-cloud-run)

is a fully managed serverless platform that allows developers to run stateless containers. It’s designed for deploying and scaling containerized applications with minimal infrastructure management.

### Key features of Cloud Run

1. **Container-based**
   : Supports any language, library, or binary that can be containerized.
2. **Flexible scaling**
   : Automatically scales based on incoming requests, including scaling to zero.
3. **Fine-grained billing**
   : Pay only for the exact resources used, billed to the nearest 100 milliseconds.
4. **HTTP/2 and HTTPS support**
   : Automatic SSL certificate provisioning and HTTP/2 support.
5. **Custom domains**
   : Ability to map custom domains to services.
6. **Concurrency**
   : Supports handling multiple requests within a single container instance.
7. **GPU support**
   : Access to GPUs for compute-intensive workloads like ML inference.
8. **VPC connectivity**
   : Secure access to resources in Virtual Private Cloud networks.
9. **Event-driven capabilities**
   : Integration with Eventarc for various event types.

Cloud Run Functions: Simplified function-as-a-service
-----------------------------------------------------

[Cloud Run Functions](https://cloud.google.com/functions/docs/concepts/overview)

, formerly known as Cloud Functions, is a serverless execution environment focused on single-purpose functions that respond to events or HTTP requests.

### Key features of Cloud Run Functions

1. **Function-centric**
   : Deploy individual functions written in supported languages.
2. **Event-driven**
   : Primarily designed for event-driven architectures.
3. **Automatic scaling**
   : Scales based on incoming events or requests.
4. **Simplified development**
   : Focus solely on writing function code without container management.

It’s worth noting that Cloud Run Functions inherits many capabilities from Cloud Run, including GPU access which is currently in preview. This inheritance allows developers to leverage powerful Cloud Run features within a more focused, function-based environment.

Key differences between Cloud Run and Cloud Run Functions
---------------------------------------------------------

While both services are part of Google’s serverless ecosystem, they have distinct characteristics:

1. **Deployment model**
   :

   * Cloud Run: Deploy entire containerized applications.
   * Cloud Run Functions: Deploy individual functions in
     [supported languages](https://cloud.google.com/functions/docs/concepts/exec#runtimes)

     .
2. **Abstraction level**
   :

   * Cloud Run: Lower level, offering more control over the runtime environment.
   * Cloud Run Functions: Higher level of abstraction, focusing on single-purpose functions.
3. **Container management**
   :

   * Cloud Run: Requires building and managing your own containers.
   * Cloud Run Functions: Handles container management automatically.
4. **Use cases**
   :

   * Cloud Run: Suitable for longer-running services, web applications, and more complex architectures.
   * Cloud Run Functions: Ideal for short-lived, event-based actions and simple API endpoints.
5. **Development experience**
   :

   * Cloud Run: More flexible, allowing use of any runtime or library that can be containerized.
   * Cloud Run Functions: Simpler for developers who want to focus solely on function code.
6. **Pricing model**
   :

   * Cloud Run: Billed based on container instance time and resource allocation.
   * Cloud Run Functions: Billed based on function execution time and memory usage.

Choosing between Cloud Run and Cloud Run Functions
--------------------------------------------------

### Use Cloud Run when:

* You need more control over the runtime environment or custom libraries.
* You’re deploying existing containerized applications.
* You require longer-running services or complex web applications.
* You want the flexibility to use any programming language or framework.

### Use Cloud Run Functions when:

* You need to quickly implement simple, event-driven code.
* Your team prefers focusing on code without managing infrastructure details.
* You’re working with short-lived, event-based actions triggered by cloud services.
* You want a higher level of abstraction and simplified deployment process.

Conclusion
----------

To get started with either service or to explore which option is best for your use case, visit the
[official Google Cloud documentation](https://cloud.google.com/run/docs/overview/what-is-cloud-run)

for Cloud Run and
[Cloud Run Functions](https://cloud.google.com/functions/docs/concepts/overview)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/gpu-types
================================================================================

A10 vs. A100 vs. H100 - Which one should you choose?
====================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

This article will guide you through the key differences between NVIDIA’s
[A10](https://www.nvidia.com/en-us/data-center/products/a10-gpu/)

,
[A100](https://www.nvidia.com/en-us/data-center/a100/)

, and
[H100](https://www.nvidia.com/en-us/data-center/h100/)

GPUs, helping you make an informed decision based on your specific needs and budget.

GPU comparison
--------------

Let’s start with a comparison of the GPUs available on Modal:

| GPU Type | VRAM (GiB) | Memory bandwidth (VRAM-to-SRAM, TB/s) | Price (on Modal, $ / hour) | Architecture |
| --- | --- | --- | --- | --- |
| H100 | 80 | 3.35 | 4.56 | Hopper |
| A100 (80GB) | 80 | 2 | 3.40 | Ampere |
| A100 (40GB) | 40 | 2 | 2.78 | Ampere |
| A10 | 24 | 0.6 | 1.10 | Ampere |
| L4 | 24 | 0.3 | 0.80 | Lovelace |
| T4 | 16 | 0.3 | 0.59 | Tesla |

* VRAM is high speed, byte-addressable memory located on your graphics card. It plays the same role in the GPU’s memory system as the RAM plays in your CPU’s. The more VRAM, the larger the models you can run.
* In the table above, we show the VRAM-to-SRAM memory bandwidth, which is the rate at which data can be transferred between the GPU’s main memory (VRAM, typically GDDR or HBM) and its on-chip cache memory (SRAM). This bandwidth is crucial for the GPU’s ability to quickly bring model parameters into the compute cores where activations and outputs are calculated.

### H100

* **Best for**
  : Training and inference for very large models (70B parameters or more), transformer-based architectures, low (8-bit) precision
* **Key features**
  :
  + Most powerful NVIDIA datacenter GPU that’s generally available at time of writing (2025)
  + ~2x faster than A100 for most workloads, but also harder to get (might have to queue), and more expensive
  + Optimized for large language model workloads. It offers over 3 TB/s of memory bandwidth, which is crucial for LLM inference workloads that require rapid data transfer between VRAM and compute cores.
  + Contains specialized compute units for lower precision (FP8) operations

### A100

* **Best for**
  : Training and inference for large models (7B-70B parameters)
* **Key features**
  :
  + NVIDIA’s workhorse GPU, meant for AI, data analytics, and HPC workloads
  + Available in 40GB and 80GB variants
  + Because memory bandwidth has scaled more slowly than arithmetic bandwidth, A100s can be more cost-effective than H100s for workloads that are memory-bound, like running large models on small batches

### A10

* **Best for**
  : Inference for small to medium models (7B parameters or less, like most diffusion-based image generation models), cost-effective, small-scale training for smaller models
* **Key features**
  :
  + Same architecture as A100, so most code that runs on A100 will run on A10
  + Good performance-to-cost ratio for smaller workloads

### L4

* **Best for**
  : Inference for small to medium size models (7B parameters or less, like most diffusion-based image generation models)
* **Key features**
  :

  + Cost-efficient GPU, but still very capable
  + L4 has the same amount of VRAM as A10, but only half the memory bandwidth
  + L4 offers 2x-4x better performance over and is newer than T4

### T4

* **Best for**
  :

  + Inference for small models
* **Key features**
  :

  + T4 is older and slower than L4
  + Offered for free with Google Colab, so good for small-scale experimentation and prototyping. For example, you can start with T4s on Colab, and run the same code in prod on L4s or A10s.

Choosing the right GPU
----------------------

When selecting a GPU for your machine learning, first gather the following information:

1. **Task Type**
   : Are you training, fine-tuning, or running inference?
2. **Model Size**
   : How many parameters does your model have?
3. **Memory Requirements**
   : How much VRAM does your model need?
4. **Budget**
   : What’s your cost constraint per hour of computation?
5. **Performance Needs**
   : Do you require the absolute fastest processing times?

Then follow this procedure to decide which GPU is the best fit:

1. Calculate the amount of memory that you need, depending on your use case and model size. Remember to take into account whether you are quantizing the models and/or using
   [techniques like LoRA or QLoRA](/blog/lora-qlora)

   . You can refer to our VRAM guides for more information on how to calculate the memory requirements:

   * [VRAM guide for inference](/blog/how-much-vram-need-inference)
   * [VRAM guide for training/fine-tuning](/blog/how-much-vram-need-fine-tuning)
2. Check against the table above for the most cost-effective GPU that the model will fit on
3. Start with the most cost-effective GPU to see whether the model runs/performs well and move to the more expensive ones if it doesn’t.

Advanced considerations
-----------------------

1. **Multi-GPU Setups**
   : For some super large models (greater than 100B parameters, like Llama3-405B), you may need to allocate more than a single even top-tier GPU. Modal’s platform makes it easy to
   [scale up your GPU resources as needed](/docs/guide/gpu#specifying-gpu-count)

   .

Conclusion
----------

At Modal, we offer flexible access to all these GPU types with a simple
`gpu="A100"`
or
`gpu="H100"`
flag in your code. This allows you to easily switch between GPUs based on your needs without worrying about hardware procurement or maintenance.

Ready to supercharge your AI workloads with the right GPU?
[Sign up for Modal](https://modal.com/signup)

today and experience the difference firsthand!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/gpu-utilization-guide
================================================================================

'I paid for the whole GPU, I am going to use the whole GPU': A high-level guide to GPU utilization
==================================================================================================

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

GPU Enjoyer

![A t-shirt that says 'I paid for the whole speedometer, I am going to use the whole speedometer'](https://modal-cdn.com/blog-gpu-utilization-whole-speedometer.webp)

Typical attire of a GPU utilization maximizer.

Graphics Processing Units, or GPUs, are the hottest mathematical co-processor since the
[FM synthesis chips that shaped the sounds of the late 1900s](https://old.reddit.com/r/chiptunes/comments/qc0zl5/why_did_fm_synthesis_take_so_long_to_take_off/hhemu2v/)

.

Like all co-processors, they are chosen when the performance of more flexible commodity hardware, like an x86 Central Processing Unit (CPU), is insufficient. GPUs are in particular
[designed for problems](/gpu-glossary/device-hardware/streaming-multiprocessor)

where CPUs cannot achieve the desired throughput of mathematical operations (in particular, matrix multiplications).

But GPUs are not cheap: high performance can command a high price.

Combined together, the high price, performance sensitivity, and throughput-orientation of GPU applications mean that a large number of engineers and technical leaders find themselves concerned with
*GPU utilization*
of some form or another — “we’re paying a lot, so we’d better be using what we’re paying for”.

At Modal, we have our own GPU utilization challenges to solve and we help our users solve theirs. We’ve noticed that the term “GPU utilization” gets used to mean very different things by people solving problems at different parts of the stack. So we put together this article to share our framework for thinking about GPU utilization across the stack and the tips and tricks we’ve learned along the way.

In particular, we’ll talk about three very different things that all get called “GPU utilization”:

* [GPU Allocation Utilization](#what-is-gpu-allocation-utilization)

  , the fraction of your GPUs that are running application code,
* [GPU Kernel Utilization](#what-is-gpu-kernel-utilization)

  , the fraction of time your application is running code on GPUs, and
* [Model FLOP/s Utilization](#what-is-model-flops-utilization-mfu)

  , the fraction of the GPUs’ theoretical arithmetic bandwidth your application is using to run models.

We’ll specifically focus on neural network inference workloads — neural networks because they are workload generating the most demand right now and inference because, unlike training, inference is a revenue center not a cost center. We’re betting on the revenue center.

What is utilization?
--------------------

### Utilization = Output achieved ÷ Capacity paid for

*Utilization*
relates the available capacity of a system to that system’s output.

In throughput-oriented systems like GPU applications, the capacity paid for is often a
*bandwidth*
(e.g. the arithmetic bandwidth) and the output achieved is then a
*throughput*
(e.g. floating point operations per second, FLOP/s).

Because it is a ratio, utilization is unitless. That means
**there are actually many GPU-related quantities you might call “GPU utilization”**
, leaving off the implicit units of the capacity and output. These different quantities range across orders of magnitude of time and across different organizational capacities (e.g. procurement, DevOps, and low-level performance engineering).

What is GPU Allocation Utilization?
-----------------------------------

### GPU Allocation Utilization = GPU-seconds running application code ÷ GPU-seconds paid for

First, consider the number of GPUs that you have allocated — whether that is fixed GPU capacity on-premise in your basement (or data center) or it is rented capacity in a cloud data center (or many people’s basements) — across a period of time.

We use the term
*GPU Allocation Utilization*
for the fraction of those GPU-seconds during which you were running application code. This is the highest-level notion of “GPU utilization”.

There are two key limits on GPU Allocation Utilization: economic and developer-operational.

The economic limits on GPU Allocation Utilization rise from combined technical and market limitations. Purchasing, commissioning, decomissioning, and selling GPUs cannot be done as quickly as the output demanded by the application changes (on the scale of seconds or minutes).

Of course, as for other hardware we are blessed with highly-virtualized data center platforms (“clouds”) where we can virtually allocate and de-allocate GPU capacity. Even there, however, existing pricing models and demand that exceeds supply leave providers dictating terms, like multi-month or multi-year commitments, which limit achievable utilization for a given quality-of-service.

### With a fixed, over-provisioned GPU allocation, utilization is low

Application Demand

Provisioned

Modal helps organizations solve this problem. We aggregate GPU demand across consumers and GPU supply across providers to improve GPU allocation efficiency.

But GPU Allocation Utilization isn’t just about the GPU-seconds paid for, it’s about the GPU-seconds spent running application code.

That’s where the DevOps limits on GPU Allocation Utilization come in. Even in a fully liquid GPU market, there is latency between the time at which a GPU is purchased or rented and the time at which the GPU is running useful work — time to configure operating systems, perform health checks, copy over application code, etc. Absent the ability to precisely predict future demand at timescales greater than that latency, this leads to reduced GPU Allocation Utilization, reduced quality-of-service, or both!

### If allocation is slow, utilization and QoS suffer

Application Demand

Provisioned

To achieve high GPU Allocation Utilization and meet quality-of-service goals, allocation and spin-up to application code needs to be fast enough to respond to increases in demand.

### With fast, automatic allocation, utilization and QoS can both be high

Application Demand

Provisioned

This is one of the core problems solved by Modal. We manage a large multi-cloud GPU fleet, benefitting from economies of scale to unlock better engineering solutions and concentration of measure to improve predictability of demand. We
[built a custom container stack (in Rust btw)](https://www.youtube.com/watch?v=3jJ1GhGkLY0)

to reduce the latency from non-application code and system configuration. And users’ workloads spin up faster because the serverless runtime for that container execution system frames user workloads in terms of application code, not virtual machine maintenance. That allows us to skip the repetitive, undifferentiated work required to create virtual machines. That unlocks novel engineering optimizations for us, like
[memory snapshotting and restoration](/blog/mem-snapshots)

, and it just-so-happens to make application engineering easier for our users.

What level of GPU Allocation Utilization can I expect to achieve?
-----------------------------------------------------------------

The existing numbers are sobering. According to the
[State of AI Infrastructure at Scale 2024 report](https://ai-infrastructure.org/the-state-of-ai-infrastructure-at-scale-2024/)

, the majority of organizations achieve less than 70% GPU Allocation Utilization
*when running at peak demand*
— to say nothing of aggregate utilization. This is true even of sophisticated players, like the former
[Banana serverless GPU platform](https://www.banana.dev/blog/sunset)

, which operated at an aggregate utilization of around 20%.

With Modal, users can achieve GPU Allocation Utilization in excess of 90% — in aggregate, not just at peak.

If that interests you, check out our
[docs](/docs)

and our
[pricing page](/pricing)

.

If it doesn’t, read on for more about the software engineering required to get the most out of your GPUs — on Modal or elsewhere.

What is GPU Kernel Utilization?
-------------------------------

### GPU Kernel Utilization = GPU-seconds running kernels ÷ GPU-seconds paid for

Just because an allocated GPU is running application code doesn’t mean it is running code
*on the GPU.*
The term of art for “code that runs on the GPU” in the popular
[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

for GPUs is “kernel”, and so we call the fraction of time we spend running code on the GPU the
*GPU Kernel Utilization*
.

This utilization metric is reported by, among others, the beloved
[`nvidia-smi`
command line tool](/gpu-glossary/host-software/nvidia-smi)

wrapping
[NVIDIA’s Management Library](/gpu-glossary/host-software/nvml)

for their GPU hardware, and so it is commonly checked and cited. We
[expose it to our users](/docs/guide/gpu-metrics)

under the name that library uses, “GPU utilization”. Note that this name can be slightly misleading, since this metric does not care whether the code we’re running on the GPU is exercising the hardware’s actual capacity.

An application that is achieving low GPU Allocation Utilization is necessarily going to achieve low GPU Kernel Utilization, so long as you consider all GPU-seconds being paid for: a unit not running application code can’t run kernels.

Why else might you achieve low GPU Kernel Utilization? In particular, what patterns will show up as low kernel utilization per GPU?

First, there might be lots of work to do that supports your application but doesn’t use the GPU, like moving input or output data via network or disk, downloading the many gigabytes of weights of a foundation model, or writing logs.

These tasks can be sped up by usual means — judicious application of lazy and eager loading, parallelization, increased bandwidth for non-GPU components like networks, and deleting more code
[YAGN](https://martinfowler.com/bliki/Yagni.html)

.

Second, the CPU might not be providing work to the GPU quickly enough. A typical GPU-accelerated program is, like a high-performance network application, a dance of concurrency between the CPU executing logic about what work must be done and specialized, but dumb, hardware that can actually do the work. For example, when multiplying two matrices, the popular PyTorch library needs to determine the shapes and types of those two matrices and then lookup the appropriate kernel — somewhat akin to a JIT database query optimizer selecting a physical operator mid-execution. If you are unable to complete this work before the GPU finishes its previous task, the GPU will idle. We’ll call this class of issue “host overhead”.

Often, resolving host overhead is a matter of re-writing the host logic — preventing slow host work (like logging in Python) from blocking the host work that drives the GPU. But at the scale of milliseconds per task step, Python starts to become incapable of keeping up, and at the scale of microseconds per task step, the latency required to schedule kernels onto the GPU via
[the CUDA C++ APIs and driver](/gpu-glossary/host-software/cuda-runtime-api)

begins to bottleneck.

In both cases, there are two basic optimizations. First, multiple kernels can be launched at once
[using CUDA Graphs](https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/)

, which essentially convert a sequence of kernel launches into a DAG that only needs to be launched once. Second, the application can aggregate more work for the GPU to complete for a given unit of host work — for example by
[batching](/docs/guide/dynamic-batching)

requests together — to improve utilization with a possible penalty to latency.

Code regions with low GPU Kernel Utilization can be identified from application traces, like those produced by the
[PyTorch Profiler](/docs/examples/torch_profiling)

. Specifically, any period of time where all CUDA streams are empty is a period of zero GPU Kernel Utilization, and so applications with low GPU Kernel Utilization have largely empty CUDA streams in their traces, like the one below. These periods of quiescence need to be correlated to activity on the host to determine which parts of the application code are leading to the bottleneck. GPU application profilers and trace viewers generally support this, e.g. by showing kernel launch dependencies, like the arrow in the trace below.

![A trace of a PyTorch application with low GPU Kernel Utilization](https://modal-public-assets.s3.amazonaws.com/tmpx_2c9bl5_c5aa7ab0.webp)

In traces of GPU applications, periods where no kernels are running appear as empty strips in the timelines of CUDA streams (e.g. Stream 7 7 in the trace above). For details, see
[our documentation](/docs/examples/torch_profiling)

.

What level of GPU Kernel Utilization can I hope to achieve?
-----------------------------------------------------------

GPU Kernel Utilization is the closest metric in this article to the better-known CPU utilization. CPU utilization tracks the fraction of CPU cycles during which instructions were being executed on behalf of your program (as opposed to the CPU idling or running other programs).

However, for CPU utilization, hitting 90%+ is often bad, even a trigger for alerts. But we want to and can achieve that level of GPU Kernel Utilization!

Fundamentally, this is downstream of the greater predictability of many GPU applications. Running a transactional database replica at 90% CPU utilization baseline risks degraded quality-of-service if query patterns or quantity change. Typical GPU applications have much less variability — for a database analogue, imagine repeatedly running only one basic sequential scan aggregation query, but with slightly different parameters each time — and so have more controllable quality-of-service.

What is Model FLOP/s Utilization (MFU)?
---------------------------------------

### Model FLOP/s Utilization = Model FLOP/s throughput achieved ÷ FLOP/s bandwidth paid for

At some galaxy-brained, CEO-math level, expenditures on GPUs are really expenditures on floating point operation bandwidth, and so the deepest and most fundamental utilization metric to measure is the ratio of that bandwidth to the throughput achieved.

This metric is known as
*MFU*
, which either means “Maximum” or “Model” FLOP/s Utilization, depending on who you ask. We go with “Model”, since it’s more common.

Instances that aren’t running application code or that aren’t running GPU kernels cannot achieve a high MFU, so low GPU Allocation Utilization or low GPU Kernel Utilization imply low Model FLOP/s Utilization.

However, high utilization at these more abstract levels does not imply high MFU.

First, as an implementation detail, communication between GPUs is frequently implemented via GPU kernels. This communication, like most communication in distributed systems, is subject to faults (hardware fault, programmer fault,
[shark attack fault](https://slate.com/technology/2014/08/shark-attacks-threaten-google-s-undersea-internet-cables-video.html)

), which frequently manifest as deadlock. From the perspective of GPU Kernel Utilization, a system that is deadlocked in the middle of running a communication kernel is fully utilized (!), but it is completing no useful work. We like to catch this particular issue by monitoring
[GPU power draw and heat](/docs/guide/gpu-metrics)

. More generally, optimizing communication is critical for achieving high MFU, especially for workloads that spread a single task across multiple nodes.

Second, floating point computation is just one of the things a GPU must do to complete a task. The most important other task is moving data. Computation can only occur on data stored inside of the
[register file](/gpu-glossary/device-hardware/register-file)

of the GPU’s
[streaming multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)

, which each store less than a megabyte, while foundation models are measured in gigabytes. The data to which a computation applies must generally be moved from a slower, larger area of the
[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

. The
[bandwidth of this memory](/gpu-glossary/device-hardware/gpu-ram)

is generally many times lower than the device’s FLOP/s bandwidth, especially in recent generations. The ratio of an algorithm’s FLOP/s throughput to its byte/s throughput is called the arithmetic intensity.

Bottlenecking on memory is a particular challenge in latency-sensitive foundation model inference workloads, where the arithmetic intensity is low (perhaps a few FLOPs per byte). Besides algorithmic rewrites to increase arithmetic intensity, like
[the online softmax in Flash Attention](https://arxiv.org/abs/2205.14135)

, the primary generic strategy is
[batching](/docs/guide/dynamic-batching)

more work together, which increases FLOPs executed more than memory bytes moved for most neural network inference workloads, but generally adds per-task latency.

Finally, GPU kernels must be carefully written to achieve high MFU.
[This public worklog by Si Boehm](https://siboehm.com/articles/22/CUDA-MMM)

gives a flavor for the effort required to reach state-of-the-art for a single kernel. Even that worklog stops short of truly maximizing MFU, since it tackles a problem that can’t make use of the fastest elements of contemporary GPUs, the
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

, and writing kernels that can saturate Tensor Cores is even more challenging — see
[this worklog from Pranjal Shankhdhar](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog)

. For this reason, most teams use high-quality open source kernels through libraries like CuBLAS or frameworks like PyTorch and vLLM.

The achieved FLOP/s and memory throughput of a GPU application can be monitored using the
[NVIDIA Data Center GPU Management tool](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/feature-overview.html#profiling-metrics)

,
`dcgm`
. The metrics prefixed with
`DCGM_FI_PROF`
are generally relevant. In particular, the
`DCGM_FI_PROF_DRAM_ACTIVE`
metric measures the utilization of the DRAM-to-SRAM memory bandwidth. The
`DCGM_FI_PROF_PIPE_TENSOR_ACTIVE`
metric measures the utilization of the Tensor Cores that provide the maximum FLOP/s bandwidth. This isn’t identical to MFU for subtle reasons covered well in Stas Bekman’s guide
[here](https://github.com/stas00/ml-engineering/blob/master/training/performance/README.md#mfu-vs-hfu)

.

What level of Model FLOP/s Utilization can I hope to achieve?
-------------------------------------------------------------

First, let’s note that measuring Model FLOP/s Utilization is tricky. The theoretical bandwidth can be read from manufacturer datasheets — but watch for asterisks like “with sparsity”. The achieved model throughput, on the other hand, can be hard to measure, in particular since some FLOPs might be spent on other computations, like activation recomputation in training. For that reason, it is often done based on pen-and-paper analysis of the algorithm and with approximate, “napkin” math.

The state-of-the-art for MFU in training is achieved by the foundation model teams at leading organizations like OpenAI, Google, and Meta. Of these, Meta is the most open and reports an MFU of 38 - 41% when training
[the LLaMA 3 405B model](https://arxiv.org/abs/2407.21783)

. The more recent DeepSeek-v3 training run by DeepSeek achieved around 20-30% MFU (there’s no official number)
[using GPUs with tighter communication bottlenecks](https://semianalysis.com/2025/01/31/deepseek-debates/)

.

Much of the shortfall is due to the need for inter-node communication in large training jobs, which creates bandwidth constraints that aren’t present in inference applications. For inference workloads, MFU might reach higher, closer to the
[70% - 80% MFU achieved by raw matrix multiplications](https://github.com/stas00/ml-engineering/tree/master/compute/accelerator#maximum-achievable-flops)

, but we aren’t aware of any published results from large-scale deployments. Let us know if we missed them!

For context, it’s also helpful to consider the equivalent of MFU for a job running on a CPU. For concreteness, consider the
[One Billion Row Challenge](https://github.com/gunnarmorling/1brc)

, which led teams around the world to competitively optimize a large-scale aggregation problem on CPUs. This problem requires three floating point operations per row on one billion rows, and so has a total FLOP count of 3 billion. The leading results finished in about one second, and so achieved a FLOP/s throughput of about 3 billion. If we assume that the hardware used for the challenge, eight cores out of a
[32 core AMD EPYC 7502P](https://www.hetzner.com/dedicated-rootserver/ax161)

machine which can run at 3.35 GHz, is capable of issuing one FLOP per cycle, then the FLOP/s bandwidth is ~26 billion, for an MFU of ~10%. However, that CPU has
[AVX2 SIMD vector instructions](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions)

with a lane width of 256 and so, assuming it can issue 16 FLOPs/cycle per core, the FLOP/s bandwidth is actually ~420 billion, leading to an MFU of under 1%.

How can I improve my GPU utilization?
-------------------------------------

If you’re not using
[Modal](/)

, that’s a great place to start! Especially for GPU Allocation Utilization.

Besides that, we recommend that if you want to improve your GPU utilization, you dive deeper into GPU-based computing.

We wrote a
[GPU Glossary](/gpu-glossary)

to collect together our definitions of the most important terms in one place, complete with links to some of our favorite resources for learning more. Try starting there!

Among those resources, a few stand out, like
[this talk by Horace He](https://www.youtube.com/watch?v=139UPjoq7Kw&t=1236s)

, of the PyTorch team, and
[this dense blog post](https://blog.codingconfessions.com/p/gpu-computing)

by Abhinav Upadhyay of Coding Confessions. We also highly recommend the
[ML Engineering Open Book](https://github.com/stas00/ml-engineering/)

by Stas Bekman for deep dives and useful snippets all across the stack.

*We’d like to thank
[Mark Saroufim](https://x.com/marksaroufim)

of
[PyTorch](https://x.com/pytorch)

& the
[GPU\_MODE Discord](https://x.com/GPU_MODE)

(join it!) and
[Erik Dunteman](https://x.com/erikdunteman)

of
[Pig](https://pig.dev)

for comments on a draft of this post.*

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-much-vram-need-fine-tuning
================================================================================

How much VRAM do I need for LLM model fine-tuning?
==================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Fine-tuning Large Language Models (LLMs) can be computationally intensive, with GPU memory (VRAM) often being the primary bottleneck. This is a guide to calculating the VRAM requirements for fine-tuning.

The rule of thumb
-----------------

For full fine-tuning of LLMs loaded in “half-precision” (16 bits), a quick rule of thumb is:

**16GB of GPU memory per 1B parameters in the model**

This is significantly higher than the
[2GB per 1B parameters needed for inference](/blog/how-much-vram-need-inference)

, due to the additional memory required for
[optimizer states, gradients, and other training-related data](https://fullstackdeeplearning.com/course/2022/lecture-2-development-infrastructure-and-tooling/#sharded-data-parallelism)

.

VRAM Requirements for fine-tuning a 7B model
--------------------------------------------

Let’s walk through a VRAM estimation for a 7B parameter model. The total VRAM requirements are the sum of the following individual components:

### 1. Model parameters

* Full precision (FP32): ~28GB (7B x 4 bytes)
* Half precision (FP16): ~14GB (7B x 2 bytes)
* Mixed precision: ~21GB (FP16 + partial FP32 copy)

### 2. Optimizer States

Using AdamW (most common optimizer):

* ~84GB (3 copies at 4 bytes/parameter)

Using 8-bit optimizers (e.g.,
[`bitsandbytes`](https://github.com/bitsandbytes-foundation/bitsandbytes)

):

* ~42GB (1 FP32 copy + 2 8-bit copies)

### 3. Gradients

* FP32: ~28GB
* FP16: ~14GB (often matches model weight precision)

### 4. Activations

Depends on batch size, sequence length, and model architecture. Can be reduced with
[activation checkpointing](https://medium.com/pytorch/how-activation-checkpointing-enables-scaling-up-training-deep-learning-models-7a93ae01ff2d)

. For most contemporary model types (transformers, diffusion models), activations don’t add nearly as much memory requirements as the other components above.

### Total VRAM Estimate

For full fine-tuning with half precision and using 8-bit optimizers, a 7B model might require 14+42+14 = ~70GB of VRAM.

Efficient fine-tuning: LoRA and QLoRA
-------------------------------------

Techniques like LoRA (Low-Rank Adaptation) and QLoRA (Quantized LoRA) significantly reduce VRAM requirements. You can read more about how in our
[LoRA vs. QLoRA article](/blog/lora-qlora)

.

VRAM requirements table
-----------------------

Here’s a comparison of VRAM requirements for different model sizes and fine-tuning techniques:

| Method | Precision | 7B | 13B | 30B | 70B | 110B |
| --- | --- | --- | --- | --- | --- | --- |
| Full | 16 | 67GB | 125GB | 288GB | 672GB | 1056GB |
| LoRA | 16 | 15GB | 28GB | 63GB | 146GB | 229GB |
| QLoRA | 8 | 9GB | 17GB | 38GB | 88GB | 138GB |
| QLoRA | 4 | 5GB | 9GB | 20GB | 46GB | 72GB |

Note: These are approximate values and actual usage may vary based on implementation details and additional memory needs during training.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-much-vram-need-inference
================================================================================

How much VRAM do I need for LLM inference?
==========================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

The rule of thumb
-----------------

A quick rule of thumb for LLM serving for models loaded in “half precision” - i.e. 16 bits, is approximately
**2GB of GPU memory per 1B parameters in the model**
.

Example
-------

Let’s calculate for Llama3-70B loaded in 16-bit precision:

`70B x 2GB/B = 140GB`

A single A100 80GB wouldn’t be enough, but 2x A100 80GB should suffice.

Impact of quantization
----------------------

You can decrease the amount of GPU memory needed by quantizing, essentially reducing the precision of the weights of the model. Common quantization levels include:

**16-bit:**
Also called “half-precision”, often used as the default, balancing precision and memory usage.

**8-bit:**
Generally achieves similar performance to 16-bit while halving memory requirements.

**4-bit:**
Significantly reduces memory needs but may noticeably impact model performance.

You can load
[HuggingFace models at half, 8-bit, or 4-bit precision](https://discuss.huggingface.co/t/loading-half-precision-pipeline/68975/2)

with simple parameter changes with the transformers library.

Precision matters
-----------------

To calculate the memory needed for a model with quantization, you can use the following formula:

`M = (P x (Q/8)) x 1.2`

Where:

`M`
: GPU memory (VRAM) expressed in gigabytes

`P`
: The number of parameters in the model (e.g., 70 for a 70B model)

`Q`
: The number of bits used for loading the model (e.g., 16, 8, or 4 bits)

`1.2`
: Represents a 20% overhead for additional tasks like
[key-value caching](https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/#key-value_caching)

, where you cache self-attention tensors for faster inference.

Example with quantization
-------------------------

Let’s consider 4-bit quantization of Llama3-70B:

`70 x (4/8) x 1.2 = 42GB`

This could run on 2x A10 24GB GPUs.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-deploy-aws-lambda
================================================================================

How to deploy code in AWS Lambda: the easy way for beginners
============================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

When you’re new to AWS Lambda, you might be tempted to follow
[the official documentation’s](https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html)

suggestion to create and deploy functions using the AWS Management Console’s graphical user interface (GUI). While this method is straightforward for simple functions, it makes it difficult to track changes and do version control. As your project grows, you’ll find it challenging to manage dependencies and scale your application.

Enter AWS Serverless Application Model (SAM)
--------------------------------------------

Given these limitations, we recommend instead using the
[AWS Serverless Application Model](https://aws.amazon.com/serverless/sam/)

(SAM) for deploying Lambda functions. AWS SAM is an open-source framework allows you to use a single YAML file to define your entire serverless stack, including Amazon API Gateway APIs, AWS Lambda functions, and Amazon DynamoDB tables.

SAM offers several advantages:

1. **Infrastructure as Code**
   : Your entire application, including Lambda functions, API Gateway, and other resources, can be defined in a single YAML file.
2. **Local Testing**
   : SAM allows you to test your functions locally before deploying.
3. **Simplified Deployment**
   : One command to package and deploy your entire application.
4. **Version Control Friendly**
   : SAM templates and function code can be easily version controlled.
5. **CI/CD Integration**
   : Easily integrate with CI/CD pipelines for automated testing and deployment.

Step 1: Install the AWS CLI
---------------------------

Before we start with SAM, you need to have the AWS CLI installed. This will allow you to interact with AWS services from your command line.

1. Install the AWS CLI by following the instructions for your operating system:
   [AWS CLI Installation Guide](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-install.html)
2. After installation, configure your AWS credentials:

   ```
   aws configure
   ```

   You’ll need to enter your AWS Access Key ID, Secret Access Key, default region, and output format.

Step 2: Install AWS SAM CLI
---------------------------

Next, we’ll install the AWS SAM CLI, which is the main tool we’ll use for deploying our Lambda function.

1. Follow the installation instructions for your operating system:
   [AWS SAM CLI Installation Guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/serverless-sam-cli-install.html)
2. Verify the installation by running:

   ```
   sam --version
   ```

Step 3: Initialize a SAM Project
--------------------------------

Now that we have SAM installed, let’s create a new project.

1. Open your terminal and navigate to a directory where you want to create your project.
2. Run the following command:

   ```
   sam init
   ```

3. You’ll be prompted with several options. Here’s a typical selection for a beginner:

   * Choose an AWS Quick Start Template
   * AWS Quick Start Template: Hello World Example
   * Use the most popular runtime and package type? (Python and zip): Y
   * Would you like to enable X-Ray tracing?: N
   * Would you like to enable monitoring?: Y
   * Project name: hello-world-sam (or any name you prefer)

This will create a new directory with your project name, containing a basic Lambda function and SAM template.

Step 4: Explore and Modify the Code
-----------------------------------

Let’s take a look at the generated code and make a small modification.

1. Navigate into your project directory:

   ```
   cd hello-world-sam
   ```

2. Open the
   `hello_world/app.py`
   file in your favorite text editor. You’ll see a basic Lambda function that returns a JSON response.
3. Let’s modify the message. Change the
   `message`
   variable to something like:

   ```
   message = f'Hello {event["name"]}! Welcome to AWS Lambda with SAM!'
   ```

4. Save the file.

Step 5: Deploy Your Function
----------------------------

Now it’s time to deploy your Lambda function to AWS.

1. In your project directory, run:

   ```
   sam deploy --guided
   ```

2. You’ll be prompted to provide some information:

   * Stack Name: Choose a name for your CloudFormation stack
   * AWS Region: Choose your preferred region
   * Confirm changes before deploy: Y
   * Allow SAM CLI IAM role creation: Y
   * Disable rollback: N
   * Save arguments to samconfig.toml: Y
3. SAM will now package and deploy your application. This process may take a few minutes.
4. Once completed, you’ll see outputs including the API Gateway endpoint URL where you can invoke your function.

Congratulations! You’ve just deployed your first Lambda function using AWS SAM.

Conclusion
----------

You’ve now successfully deployed a Lambda function using AWS SAM. This method provides a straightforward, repeatable process for deploying serverless applications. As you become more comfortable with SAM, you can explore more advanced features like local testing, adding more resources to your template, and setting up CI/CD pipelines.

Remember, while AWS Lambda is a powerful tool, you might find that Modal’s platform offers even simpler deployment processes with additional benefits. We encourage you to explore how Modal can further streamline your serverless development and deployment workflows.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-flux1-dev-on-modal
================================================================================

Flux.1-dev: Run a top text-to-image model on Modal
==================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What is Flux.1-dev?
-------------------

[Flux.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev)

is a powerful text-to-image model from Black Forest Labs.
[Together with Stable Diffusion 3.5](/blog/best-text-to-image-model-article)

, it is one of the
[best text-to-image models](/blog/best-text-to-image-model-article)

on the market.

Why should you run Flux.1-dev on Modal?
---------------------------------------

If you are looking to run Flux.1-dev, you will need access to a GPU in order for
the inference times to be fast. There are several ways to get a GPU, but the
easiest way is to use
[Modal](https://modal.com)

.

Modal is a cloud platform designed specifically for running machine learning
workloads. Unlike traditional cloud services that require complex infrastructure
management, Modal provides a serverless environment where you can deploy AI
models with just a few lines of Python code.

Example code for running the Flux.1-dev model on Modal
------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

Please note that this code is not optimized. For a more detailed example of how to run Flux fast with
`torch.compile`
, refer
[here](/docs/examples/flux)

.

```
import time
from io import BytesIO
from pathlib import Path

import modal

diffusers_commit_sha = "81cf3b2f155f1de322079af28f625349ee21ec6b"

cuda_dev_image = modal.Image.from_registry(
    "nvidia/cuda:12.4.0-devel-ubuntu22.04", add_python="3.11"
).entrypoint([])

flux_image = (
    cuda_dev_image.apt_install(
        "git",
        "libglib2.0-0",
        "libsm6",
        "libxrender1",
        "libxext6",
        "ffmpeg",
        "libgl1",
    )
    .pip_install(
        "invisible_watermark==0.2.0",
        "transformers==4.44.0",
        "huggingface_hub[hf_transfer]==0.26.2",
        "accelerate==0.33.0",
        "safetensors==0.4.4",
        "sentencepiece==0.2.0",
        "torch==2.5.0",
        f"git+https://github.com/huggingface/diffusers.git@{diffusers_commit_sha}",
        "numpy<2",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

app = modal.App("flux", image=flux_image)

with flux_image.imports():
    import diffusers
    import torch

@app.cls(gpu="H100", timeout=3600, secrets=[modal.Secret.from_name("huggingface")])
class Model:
    @modal.enter()
    def enter(self):
        self.pipe = diffusers.FluxPipeline.from_pretrained(
            "black-forest-labs/FLUX.1-dev", torch_dtype=torch.bfloat16
        ).to("cuda")

    @modal.method()
    def inference(self, prompt: str) -> bytes:
        print("Generating image...")
        image = self.pipe(
            prompt,
            output_type="pil",
            num_inference_steps=4,
        ).images[0]

        byte_stream = BytesIO()
        image.save(byte_stream, format="JPEG")
        return byte_stream.getvalue()

@app.local_entrypoint()
def main(prompt: str = "A majestic dragon soaring over snow-capped mountains"):
    output_dir = Path("/tmp/flux")
    output_dir.mkdir(exist_ok=True)

    t0 = time.time()
    image_bytes = Model().inference.remote(prompt)
    print(f"Generation time: {time.time() - t0:.2f} seconds")

    output_path = output_dir / "output.jpg"
    output_path.write_bytes(image_bytes)
    print(f"Saved to {output_path}")
```

Performance Considerations
--------------------------

The model generates images up to 1024x1024 resolution, with generation time
typically ranging from 2-4 seconds on an H100 GPU. Batch processing can
significantly improve throughput for bulk image generation tasks.

Additional resources
--------------------

* [Fine-tuning Flux.1-dev for style](/docs/examples/diffusers_lora_finetune)
* [Run Flux.1-dev fast on Modal](/docs/examples/flux)
* [Stable Diffusion 3.5 vs. Flux.1-dev](/blog/best-text-to-image-model-article)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-llama-3-1-70b-instruct-on-modal
================================================================================

How to deploy Llama 3.1 70B Instruct on Modal
=============================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Introduction to Llama 3.1 70B Instruct
--------------------------------------

The
[Meta Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)

collection of
multilingual large language models (LLMs) includes the Llama 3.1 70B model. With 70 billion parameters, it is powerful enough for a wide range
of tasks while being more accessible in terms of computational requirements
compared to larger models like the 405B.

This model is good for tasks like code execution, search, and complex reasoning.
Additionally, it includes a 128K token context window, making it well-suited for processing extended inputs, such as lengthy documents or comprehensive conversations.

Given that it is a larger model, it requires 2 H100 GPUs to run.
For a more memory efficient version of the same model, see the
[8B
variant](/blog/how-to-run-llama-3-1-8b-instruct-on-modal)

.

Why should you run Llama 3.1 70B Instruct on Modal?
---------------------------------------------------

[Modal](https://modal.com)

is the easiest way to access a
[GPU](/docs/guide/gpu)

to run machine learning workloads. With Modal, you can take your local function, decorate it with Modal decorators, and send it off to run in the cloud on a GPU.

Additionally, Modal supports various configurations, allowing you to
[customize
your environment](/docs/guide/images)

based on your specific needs, such as selecting the number of
GPUs and setting timeouts for your applications.

Example code for running the Llama 3.1 70B Instruct LLM on Modal
----------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

Please note that this code is not optimized for best performance. To run Llama 3.1 70B Instruct with a LLM serving framework like
[vLLM](https://github.com/vllm-project/vllm)

for better latency and throughput, refer to this more detailed example
[here](/docs/examples/vllm_inference)

. (You can modify the code in that example to run the 70B version instead of the 8B version.)

```
import modal

MODEL_ID = "NousResearch/Meta-Llama-3.1-70B-Instruct"
MODEL_REVISION = "d50656ee28e2c2906d317cbbb6fcb55eb4055a84"

image = modal.Image.debian_slim().pip_install("transformers", "torch", "accelerate")
app = modal.App("example-base-Meta-Llama-3-70B-Instruct", image=image)

GPU_CONFIG = "H100:2"

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.cls(
    gpu=GPU_CONFIG,
    volumes={CACHE_DIR: cache_vol},
    scaledown_window=60 * 10,
    timeout=60 * 60,
)
@modal.concurrent(max_inputs=15)
class Model:
    @modal.enter()
    def setup(self):
        import torch

        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

        from huggingface_hub import snapshot_download

        # Download the model to the cache directory
        model_path = snapshot_download(repo_id=MODEL_ID, cache_dir=CACHE_DIR)

        print(f"Model downloaded to: {model_path}")

        # Specify cache directory if needed
        model = AutoModelForCausalLM.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)

        self.pipeline = pipeline(
            "text-generation",
            model=model,
            revision=MODEL_REVISION,
            tokenizer=tokenizer,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device_map="auto",
        )

    @modal.method()
    def generate(self, input: str):
        messages = [
            {
                "role": "system",
                "content": "You are a helpful assistant.",
            },
            {"role": "user", "content": input},
        ]

        outputs = self.pipeline(
            messages,
            max_new_tokens=256,
        )

        return outputs[0]["generated_text"][-1]

# ## Run the model
@app.local_entrypoint()
def main(prompt: str = None):
    if prompt is None:
        prompt = "Please write a Python function to compute the Fibonacci numbers."
    print(Model().generate.remote(prompt))
```

Additional resources
--------------------

* [How to run Llama 3.1 8B Instruct on
  Modal](/blog/how-to-run-llama-3-1-8b-instruct-on-modal)
* [How to run Llama 3.1 405B Instruct on Modal](/blog/how_to_run_llama_405b_article)
* [vLLM Documentation](https://github.com/vllm-project/vllm)

  - Official documentation for vLLM, a framework for serving large language models.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-llama-3-1-8b-instruct-on-modal
================================================================================

How to run Llama 3.1 8B Instruct on Modal
=========================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Introduction to Llama 3.1 8B Instruct
-------------------------------------

[Meta Llama 3.1](https://ai.meta.com/blog/meta-llama-3-1/)

is a family of
open-source LLMs that includes various models of different
sizes - 8B, 70B, and 405B parameters.
While larger models like the 405B variant are designed to
deliver superior quality, they are also significantly more expensive to run.
Llama 3.1 8B is a good choice for many applications that require a
balance between quality and cost.

Licensing terms
---------------

When using the Llama 3.1 model, it’s important to be aware of the
[licensing terms](https://www.llama.com/llama3_1/license/)

set by Meta.

In particular, if you fine-tune your own model on top of Llama 3.1, you must
prominently include “Built with Llama” on your website or documentation. The
fine-tuned model’s name must also start with “Llama”.

GPU requirements
----------------

To run the Llama 3.1 model effectively, you will need access to a GPU due to its substantial computational requirements. The easiest way to
access a GPU is through
[Modal](https://modal.com)

, a cloud platform designed
for running machine learning workloads. Modal simplifies the process of
deploying AI models by automatically provisioning the necessary GPU resources,
allowing you to focus on your application without the hassle of managing
infrastructure.

Example code for running the Llama 3.1 8B Instruct LLM on Modal
---------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

Please note that this code is not optimized for best performance. To run Llama 3.1 8B Instruct with a LLM serving framework like
[vLLM](https://github.com/vllm-project/vllm)

for better latency and throughput, refer to this more detailed example
[here](/docs/examples/vllm_inference)

.

```
import modal

MODEL_ID = "NousResearch/Meta-Llama-3-8B"
MODEL_REVISION = "315b20096dc791d381d514deb5f8bd9c8d6d3061"

image = modal.Image.debian_slim().pip_install(
    "transformers==4.49.0", "torch==2.6.0", "accelerate==1.4.0"
)
app = modal.App("example-base-Meta-Llama-3-8B", image=image)

GPU_CONFIG = "H100:2"

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.cls(
    gpu=GPU_CONFIG,
    volumes={CACHE_DIR: cache_vol},
    scaledown_window=60 * 10,
    timeout=60 * 60,
)
@modal.concurrent(max_inputs=15)
class Model:
    @modal.enter()
    def setup(self):
        import torch
        from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

        from huggingface_hub import snapshot_download

        # Download the model to the cache directory
        model_path = snapshot_download(repo_id=MODEL_ID, cache_dir=CACHE_DIR)

        print(f"Model downloaded to: {model_path}")

        # Specify cache directory if needed
        model = AutoModelForCausalLM.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)
        tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=CACHE_DIR)

        self.pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            model_kwargs={"torch_dtype": torch.bfloat16},
            device_map="auto",
        )

    @modal.method()
    def generate(self, input: str):
        return self.pipeline(input)

# ## Run the model
@app.local_entrypoint()
def main(prompt: str = None):
    if prompt is None:
        prompt = "Please write a Python function to compute the Fibonacci numbers."
    print(Model().generate.remote(prompt))
```

Additional resources
--------------------

* [How to run Llama 3.1 8B on Modal with TensorRT-LLM](/docs/examples/trtllm_llama)
* [How to run LLama 3.1 70B on Modal](/blog/how-to-run-llama-3-1-70b-instruct-on-modal)
* [Llama 3.1 launch post](https://ai.meta.com/blog/meta-llama-3-1/)
* [Hugging Face model card for Llama 3.1](https://huggingface.co/NousResearch/Meta-Llama-3.1-8B)
* [vLLM GitHub Repository](https://github.com/vllm-project/vllm)

  - Repository
  for vLLM, a framework for serving large language models fast, including
  Llama models.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-nomic-embed-v1-5-on-modal
================================================================================

How to run Nomic Embed V1.5 on Modal
====================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What is Nomic Embed V1.5?
-------------------------

[Nomic Embed V1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)

is a powerful text embedding model that consistently ranks near the top of the
[MTEB embedding model leaderboard](/blog/mteb-leaderboard-article)

. The model excels at converting text into dense vector representations, making it particularly effective for semantic search, document clustering, and retrieval-augmented generation (RAG) applications.

What is Modal?
--------------

[Modal](https://modal.com)

is a cloud platform that provides the fastest and easiest way to access
[GPUs](/docs/guide/gpu)

for running inference on embedding models like Nomic Embed V1.5. Running
inference on a GPU is essential for embedding models because it significantly
accelerates the processing of large volumes of text, enabling real-time
applications. For more information on how to get started, visit the
[Modal documentation](/docs)

.

Performance considerations
--------------------------

The model delivers fast inference times on Modal’s H100 GPUs, typically processing text in milliseconds. For production deployments, consider implementing a caching layer for frequently embedded text to optimize costs and reduce latency. The model’s output vectors are suitable for direct use in vector databases like Pinecone or Weaviate.

Example code for running the Nomic Embed V1.5 embedding model on Modal
----------------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

```
import modal

MODEL_ID = "nomic-ai/nomic-embed-text-v1.5"
MODEL_REVISION = "d802ae16c9caed4d197895d27c6d529434cd8c6d"

image = modal.Image.debian_slim().pip_install(
    "torch==2.6.0", "sentence-transformers==3.4.1", "einops==0.8.1"
)
app = modal.App("example-base-nomic-embed", image=image)

GPU_CONFIG = "H100"

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.cls(
    gpu=GPU_CONFIG,
    volumes={CACHE_DIR: cache_vol},
    scaledown_window=60 * 10,
    timeout=60 * 60,
)
@modal.concurrent(max_inputs=15)
class Model:
    @modal.enter()
    def setup(self):
        from sentence_transformers import SentenceTransformer

        self.model = SentenceTransformer(
            MODEL_ID,
            revision=MODEL_REVISION,
            cache_folder=CACHE_DIR,
            trust_remote_code=True,
        )

    @modal.method()
    def embed(self, sentences: list):
        return self.model.encode(sentences)

# ## Run the model
@app.local_entrypoint()
def main():
    sentences = [
        "search_document: TSNE is a dimensionality reduction algorithm created by Laurens van Der Maaten"
    ]

    print(Model().embed.remote(sentences))
```

Additional resources
--------------------

* [Nomic AI Documentation](https://docs.nomic.ai/)

  - Official documentation and best practices
* [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

  - Benchmark comparisons
* [Vector Database Guide](https://www.pinecone.io/learn/vector-database/)

  - Understanding vector storage
* [RAG Architecture Patterns](https://www.datastax.com/guides/what-is-retrieval-augmented-generation)

  - Implementation strategies

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-stable-diffusion-3-5-large-on-modal
================================================================================

How to deploy Stable Diffusion 3.5 Large on Modal
=================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Introduction to Stable Diffusion 3.5 Large
------------------------------------------

Stable Diffusion 3.5 Large is the most powerful model in the
[Stable Diffusion
family](https://stability.ai/news/introducing-stable-diffusion-3-5)

. This model
is ideal for professional use cases, offering advanced capabilities for
generating high-quality images from textual descriptions. Along with Flux.1-dev,
it’s the premier open-source image generation model available today.

Our example code shows how
shows how to run a distilled version, Stable Diffusion 3.5 Large Turbo, which
generates high-quality images with exceptional prompt adherence in just 4 steps,
making it considerably faster than the original Stable Diffusion 3.5 Large.

Why should you run Stable Diffusion 3.5 Large on Modal?
-------------------------------------------------------

[Modal](https://modal.com)

is the best and easiest way to access a
[GPU](/docs/guide/gpu)

for running models like Stable Diffusion 3.5 Large. With
Modal, you just write a Python function, apply a decorator, and deploy.

This flexibility means that you can also easily fine-tune Stable Diffusion 3.5, store the LoRA weights, and serve them as a web service, all on Modal
infrastructure.

Example code for running Stable Diffusion 3.5 Large on Modal
------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

Please note that this code does not come with a UI. For a more detailed example of how to run Stable Diffusion 3.5 Large Turbo as a CLI, API, and UI, refer
[here](/docs/examples/text_to_image)

.

```
import io
import random
from pathlib import Path

import modal

app = modal.App("stable-diffusion-large-model-library")

image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "accelerate==0.33.0",
        "diffusers==0.31.0",
        "fastapi[standard]==0.115.4",
        "huggingface-hub[hf_transfer]==0.25.2",
        "sentencepiece==0.2.0",
        "torch==2.5.1",
        "torchvision==0.20.1",
        "transformers~=4.44.0",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

with image.imports():
    import diffusers
    import torch

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.cls(
    image=image,
    gpu="H100",
    volumes={CACHE_DIR: cache_vol},
    timeout=600,
)
class Inference:
    @modal.enter()
    def initialize(self):
        self.pipe = diffusers.StableDiffusion3Pipeline.from_pretrained(
            "adamo1139/stable-diffusion-3.5-large-turbo-ungated",
            revision="9ad870ac0b0e5e48ced156bb02f85d324b7275d2",
            cache_dir=CACHE_DIR,
            torch_dtype=torch.bfloat16,
        )

    @modal.enter()
    def move_to_gpu(self):
        self.pipe.to("cuda")

    @modal.method()
    def run(self, prompt: str, batch_size: int = 4, seed: int = None) -> list[bytes]:
        seed = seed if seed is not None else random.randint(0, 2**32 - 1)
        print("seeding RNG with", seed)
        torch.manual_seed(seed)

        images = self.pipe(
            prompt,
            num_images_per_prompt=batch_size,
            num_inference_steps=4,
            guidance_scale=0.0,
            max_sequence_length=512,
        ).images

        image_output = []
        for image in images:
            with io.BytesIO() as buf:
                image.save(buf, format="PNG")
                image_output.append(buf.getvalue())
        torch.cuda.empty_cache()
        return image_output

@app.local_entrypoint()
def main(prompt: str = "A princess riding on a pony"):
    output_dir = Path("/tmp/stable-diffusion")
    output_dir.mkdir(exist_ok=True)

    images = Inference().run.remote(prompt, batch_size=1)

    for i, image_bytes in enumerate(images):
        output_path = output_dir / f"output_{i:02d}.png"
        output_path.write_bytes(image_bytes)
        print(f"Saved {output_path}")
```

Tips and tricks for serving Stable Diffusion 3.5 Large fast
-----------------------------------------------------------

* **Use a powerful GPU**
  : Ensure you are using a
  [high-performance GPU](/docs/guide/gpu)

  like the H100 to maximize inference speed.
* **Optimize batch size**
  : Experiment with different batch sizes to find the optimal setting for your use case. Larger batch sizes can improve throughput but may require more memory.
* **Use caching**
  : Implement caching strategies for frequently requested images to speed up response times.

Additional resources
--------------------

* [Stable Diffusion 3.5 vs. Flux.1-dev](/blog/best-text-to-image-model-article)
* [Fine-tuning Stable Diffusion 3.5 for style](/blog/fine-tuning-stable-diffusion)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-stable-diffusion-3-5-medium-on-modal
================================================================================

How to run Stable Diffusion 3.5 Medium on Modal
===============================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Introduction to Stable Diffusion 3.5 Medium
-------------------------------------------

[Stable Diffusion 3.5
Medium](https://stability.ai/news/introducing-stable-diffusion-3-5)

is a
powerful image generation model with 2.5 billion parameters that is designed to
run “out of the box” on consumer hardware, striking a balance between quality
and ease of customization. It is trained on 1024x1024 images, and is most
suitable for generating images with those resolutions.

Why should you run Stable Diffusion 3.5 Medium on Modal?
--------------------------------------------------------

[Modal](https://modal.com/)

is the best and easiest way to access a
[GPU](/docs/guide/gpu)

for running models like Stable Diffusion
3.5 Medium. With Modal, you can quickly set up your environment without the
hassle of managing hardware or software dependencies. Additionally, Modal allows
you to
[fine-tune](/docs/examples/cloud_bucket_mount_loras)

Stable Diffusion
models and create style or character LoRAs.

Example code for running the Stable Diffusion 3.5 Medium image generation model on Modal
----------------------------------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

Please note that this code does not come with a UI. For a more detailed example of how to run Stable Diffusion 3.5 Medium as a CLI, API, and UI, refer
[here](/docs/examples/text_to_image)

. (You can modify the code in that example to run the medium version instead of the large version.)

```
import io
import random
from pathlib import Path

import modal

app = modal.App("stable-diffusion-medium-model-library")

image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "accelerate==0.33.0",
        "diffusers==0.31.0",
        "fastapi[standard]==0.115.4",
        "huggingface-hub[hf_transfer]==0.25.2",
        "sentencepiece==0.2.0",
        "torch==2.5.1",
        "torchvision==0.20.1",
        "transformers~=4.44.0",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

with image.imports():
    import diffusers
    import torch

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.cls(
    image=image,
    gpu="H100",
    volumes={CACHE_DIR: cache_vol},
    timeout=600,
)
class Inference:
    @modal.enter()
    def initialize(self):
        self.pipe = diffusers.StableDiffusion3Pipeline.from_pretrained(
            "adamo1139/stable-diffusion-3.5-medium-ungated",
            revision="1cb9becf522803da9d25c2713d1a82b41bc6198d",
            cache_dir=CACHE_DIR,
            torch_dtype=torch.bfloat16,
        )

    @modal.enter()
    def move_to_gpu(self):
        self.pipe.to("cuda")

    @modal.method()
    def run(self, prompt: str, batch_size: int = 4, seed: int = None) -> list[bytes]:
        seed = seed if seed is not None else random.randint(0, 2**32 - 1)
        print("seeding RNG with", seed)
        torch.manual_seed(seed)

        images = self.pipe(
            prompt,
            num_images_per_prompt=batch_size,
            num_inference_steps=4,
            guidance_scale=0.0,
            max_sequence_length=512,
        ).images

        image_output = []
        for image in images:
            with io.BytesIO() as buf:
                image.save(buf, format="PNG")
                image_output.append(buf.getvalue())
        torch.cuda.empty_cache()
        return image_output

@app.local_entrypoint()
def main(prompt: str = "A princess riding on a pony"):
    output_dir = Path("/tmp/stable-diffusion")
    output_dir.mkdir(exist_ok=True)

    images = Inference().run.remote(prompt, batch_size=1)

    for i, image_bytes in enumerate(images):
        output_path = output_dir / f"output_{i:02d}.png"
        output_path.write_bytes(image_bytes)
        print(f"Saved {output_path}")
```

Additional resources
--------------------

* [Stable Diffusion 3.5 vs. Flux.1-dev](/blog/best-text-to-image-model-article)
* [How to run Stable Diffusion 3.5 Large on
  Modal](/blog/how-to-run-stable-diffusion-3-5-large-on-modal)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-stable-diffusion-xl-on-modal
================================================================================

How to run Stable Diffusion XL on Modal
=======================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Introduction to Stable Diffusion XL
-----------------------------------

[Stable Diffusion XL
(SDXL)](https://stability.ai/news/introducing-stable-diffusion-xl)

generates
images of high quality in virtually any art style and is the best open model for
photorealism. It was trained on 1024x1024 images, and it is suitable for
generating images with those resolutions.

Why should you run Stable Diffusion XL on Modal?
------------------------------------------------

[Modal](https://modal.com)

is the best and easiest way to access a
[GPU](/docs/guide/gpu)

for running models like Stable Diffusion XL. With Modal,
you just write a Python function, apply a decorator, and deploy.

This flexibility means that you can also easily fine-tune Stable Diffusion XL,
store the LoRA weights, and serve them as a web service, all on Modal
infrastructure.

Example code for running the Stable Diffusion XL image generation model on Modal
--------------------------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

```
import io
import random
from pathlib import Path

import modal

app = modal.App("stable-diffusion-xl-model-library")

image = (
    modal.Image.debian_slim(python_version="3.12")
    .pip_install(
        "accelerate==0.33.0",
        "diffusers==0.31.0",
        "fastapi[standard]==0.115.4",
        "huggingface-hub[hf_transfer]==0.25.2",
        "sentencepiece==0.2.0",
        "torch==2.5.1",
        "torchvision==0.20.1",
        "transformers~=4.44.0",
    )
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
)

with image.imports():
    import torch
    import diffusers

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("hf-hub-cache", create_if_missing=True)

@app.cls(
    image=image,
    gpu="H100",
    volumes={CACHE_DIR: cache_vol},
    timeout=600,
)
class Inference:
    @modal.enter()
    def initialize(self):
        self.pipe = diffusers.DiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            cache_dir=CACHE_DIR,
            torch_dtype=torch.float16,
            use_safetensors=True,
            variant="fp16",
        )

    @modal.enter()
    def move_to_gpu(self):
        self.pipe.to("cuda")

    @modal.method()
    def run(self, prompt: str, batch_size: int = 4, seed: int = None) -> list[bytes]:
        seed = seed if seed is not None else random.randint(0, 2**32 - 1)
        print("seeding RNG with", seed)
        torch.manual_seed(seed)

        images = self.pipe(
            prompt,
        ).images

        image_output = []
        for image in images:
            with io.BytesIO() as buf:
                image.save(buf, format="PNG")
                image_output.append(buf.getvalue())
        torch.cuda.empty_cache()
        return image_output

@app.local_entrypoint()
def main(prompt: str = "A princess riding on a pony"):
    output_dir = Path("/tmp/stable-diffusion")
    output_dir.mkdir(exist_ok=True)

    images = Inference().run.remote(prompt, batch_size=1)

    for i, image_bytes in enumerate(images):
        output_path = output_dir / f"output_{i:02d}.png"
        output_path.write_bytes(image_bytes)
        print(f"Saved {output_path}")
```

Additional resources
--------------------

* [Serving LoRAs on top of Stable Diffusion
  XL](/docs/examples/cloud_bucket_mount_loras)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-whisper-large-v3-on-modal
================================================================================

How to deploy Whisper Large V3 on Modal
=======================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What is Whisper Large V3?
-------------------------

[Whisper Large V3](https://huggingface.co/openai/whisper-large-v3)

is OpenAI’s
most advanced open-source speech-to-text transcription model. It was trained on 1 million+ hours of diverse audio data and
is 1.54 billion parameters. Our sample code runs the
`large-v3-turbo`
version,
which is an efficiency-optimized variant that maintains high accuracy.

Why should you run Whisper Large V3 on Modal?
---------------------------------------------

[Modal](https://modal.com)

is the best and easiest way to access a
[GPU](/docs/guide/gpu)

for running
transformer models like Whisper Large V3.

Modal also makes it easy to run Whisper
[in parallel](/docs/examples/whisper-transcriber)

across many different
containers. This can be super useful for batch processing large audio datasets,
where you might want to transcribe hundreds of audio files in parallel.

Example code for running Whisper Large V3 on Modal
--------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Save the implementation code in
   `app.py`
5. Execute with
   `modal run app.py`

```
import modal

image = (
    modal.Image.debian_slim()
    .apt_install("ffmpeg")
    .pip_install("openai-whisper", "ffmpeg-python")
)
app = modal.App("example-base-whisper-large-v3-turbo", image=image)

GPU_CONFIG = "H100"

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("whisper-cache", create_if_missing=True)

@app.cls(
    gpu=GPU_CONFIG,
    volumes={CACHE_DIR: cache_vol},
    scaledown_window=60 * 10,
    timeout=60 * 60,
)
@modal.concurrent(max_inputs=15)
class Model:
    @modal.enter()
    def setup(self):
        import whisper

        self.model = whisper.load_model("large-v3", device="cuda", download_root=CACHE_DIR)

    @modal.method()
    def transcribe(self, audio_url: str):
        import requests

        response = requests.get(audio_url)

        # Save the audio file locally
        with open("downloaded_audio.wav", "wb") as audio_file:
            audio_file.write(response.content)

        result = self.model.transcribe("downloaded_audio.wav")
        return result["text"]

# ## Run the model
@app.local_entrypoint()
def main():
    url = "https://pub-ebe9e51393584bf5b5bea84a67b343c2.r2.dev/examples_english_english.wav"
    print(Model().transcribe.remote(url))
```

Related resources
-----------------

* [How to speed up Whisper](/blog/faster-transcription)
* [All the open-source Whisper variants](/blog/open-source-stt)
* [Run WhisperX on Modal](/blog/how-to-run-whisperx-on-modal)

  - WhisperX is a
  faster Whisper fork with some extra features.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how-to-run-whisperx-on-modal
================================================================================

How to run WhisperX on Modal
============================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What is WhisperX?
-----------------

[WhisperX](https://github.com/m-bain/whisperX)

extends OpenAI’s open-source Whisper model
with enhanced speaker diarization and more accurate timestamp alignment. It
uses Faster-Whisper under the hood, providing a 4x speed increase compared to the original Whisper.
This guide shows you how to deploy WhisperX on Modal for production-ready audio
transcription.

For more information on why you might want to use WhisperX, see our blog post on
[all the Whisper variants](/blog/open-source-stt)

.

Why should you run WhisperX on Modal?
-------------------------------------

[Modal](https://modal.com/)

is the best and easiest way to access
[GPU](/docs/guide/gpu)

resources for running advanced transformer models like WhisperX.

With Modal, you simply write a Python function, apply a decorator, and your
model is ready to run in the cloud on a GPU.

Modal also allows you to run WhisperX
[in
parallel](/docs/examples/whisper-transcriber)

across multiple containers, which
is useful for batch processing large audio datasets. This enables you to
transcribe hundreds of audio files simultaneously.

Example code for running the WhisperX speech recognition model on Modal
-----------------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal run app.py`

```
import modal

cuda_version = "12.4.0"  # should be no greater than host CUDA version
flavor = "devel"  #  includes full CUDA toolkit
operating_sys = "ubuntu22.04"
tag = f"{cuda_version}-{flavor}-{operating_sys}"

image = (
    modal.Image.from_registry(f"nvidia/cuda:{tag}", add_python="3.11")
    .apt_install(
        "git",
        "ffmpeg",
    )
    .pip_install(
        "torch==2.0.0",
        "torchaudio==2.0.0",
        "numpy<2.0",
        index_url="https://download.pytorch.org/whl/cu118",
    )
    .pip_install(
        "git+https://github.com/m-bain/whisperx.git@v3.2.0",
        "ffmpeg-python",
        "ctranslate2==4.4.0",
    )
)
app = modal.App("example-base-whisperx", image=image)

GPU_CONFIG = "H100"

CACHE_DIR = "/cache"
cache_vol = modal.Volume.from_name("whisper-cache", create_if_missing=True)

@app.cls(
    gpu=GPU_CONFIG,
    volumes={CACHE_DIR: cache_vol},
    scaledown_window=60 * 10,
    timeout=60 * 60,
)
@modal.concurrent(max_inputs=15)
class Model:
    @modal.enter()
    def setup(self):
        import whisperx

        device = "cuda"
        compute_type = (
            "float16"  # change to "int8" if low on GPU mem (may reduce accuracy)
        )

        # 1. Transcribe with original whisper (batched)
        self.model = whisperx.load_model("large-v2", device, compute_type=compute_type, download_root=CACHE_DIR)

    @modal.method()
    def transcribe(self, audio_url: str):
        import requests
        import whisperx

        batch_size = 16  # reduce if low on GPU mem

        response = requests.get(audio_url)
        # Save the audio file locally
        with open("downloaded_audio.wav", "wb") as audio_file:
            audio_file.write(response.content)

        audio = whisperx.load_audio("downloaded_audio.wav")

        result = self.model.transcribe(audio, batch_size=batch_size)
        return result["segments"]

# ## Run the model
@app.local_entrypoint()
def main():
    url = "https://pub-ebe9e51393584bf5b5bea84a67b343c2.r2.dev/examples_english_english.wav"

    print(Model().transcribe.remote(url))
```

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how_to_launch_a_jupyter_notebook_on_modal_programmatically_article
================================================================================

How to get GPUs with a Jupyter notebook on Modal
================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Over the years,
[Jupyter notebooks](https://jupyter.org/)

have evolved from simple coding environments to powerful platforms for complex data analysis, machine learning, and AI development. These applications often require significant computational power, making GPU acceleration a necessity.

However, the traditional approach of having dedicated GPU workstations or clusters comes with its own challenges:

1. High upfront costs
2. Underutilization during off-peak times
3. Difficulty in scaling resources up or down based on project needs
4. Maintenance and upgrade headaches

This is where the ability to spin up and down GPU resources on-demand becomes a game-changer.

In this guide, we show you how you can equip your Jupyter notebooks with flexible GPU resources provided by Modal’s serverless platform.

Enter Modal: On-Demand GPU-Powered Jupyter Notebooks
----------------------------------------------------

### Prerequisites

Before we begin, make sure you have the following:

1. An account at
   [modal.com](https://modal.com)
2. The Modal Python package installed (
   `pip install modal`
   )
3. Authenticated with Modal (run
   `modal setup`
   or
   `python -m modal setup`
   if the former doesn’t work)

### Instant GPU-Backed Notebooks

Launch a Jupyter notebook backed by Modal GPUs in seconds:

```
$ modal launch jupyter --gpu a10g
```

This single command provides you with a Jupyter instance backed by an NVIDIA A10G GPU, ready for your most demanding computations.

After running this command, you’ll see output similar to:

```
Jupyter on Modal, opening in browser...
    -> https://your-unique-url.modal.host/?token=your-secret-token
```

You can then open this URL in your web browser to access the Jupyter notebook.

For more advanced configurations, including other types of GPUs to attach to the notebook, you can refer to the Modal docs
[here](/docs/reference/cli/launch#modal-launch-jupyter)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how_to_run_chattts_article
================================================================================

ChatTTS: Running an open source text-to-speech model
====================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

[ChatTTS](https://github.com/2noise/ChatTTS)

is one of the
[best open-source text-to-speech libraries](/blog/open-source-tts)

available today. It offers high-quality voice synthesis and is particularly useful for developers looking to integrate advanced AI voice capabilities into their applications. In this guide, we’ll walk you through the process of running ChatTTS using Modal, a serverless cloud computing platform.

Prerequisites
-------------

Before we begin, make sure you have the following:

1. Create an account at
   [modal.com](https://modal.com)
2. Install the Modal Python package by running:

   ```
   pip install modal
   ```

3. Authenticate your Modal account by running:

   ```
   modal setup
   ```

   If this doesn’t work, try:

   ```
   python -m modal setup
   ```

Setting Up the Environment
--------------------------

We’ll be using a single Python file to run ChatTTS. Let’s call it
`chattts_modal.py`
. This file will contain all the necessary code to set up and run the text-to-speech service.

First, let’s import the required libraries and set up the Modal app:

```
import io
import modal

app = modal.App(name="tts")
```

Configuring the Image
---------------------

Next, we’ll configure an image with all the necessary dependencies:

```
tts_image = (
    modal.Image.debian_slim()
    .apt_install("git")
    .workdir("/app")
    .pip_install("git+https://github.com/2noise/ChatTTS.git@51ec0c784c2795b257d7a6b64274e7a36186b731")
    .pip_install("soundfile")
    .env({"TTS_HOME": "/tts"})
)

with tts_image.imports():
    import torch
    import torchaudio
    import ChatTTS
```

This image is based on Debian Slim and includes Git, the ChatTTS library, and the SoundFile library.

Creating the TTS Class
----------------------

Now, let’s create a
`TTS`
class that will handle the text-to-speech conversion:

```
@app.cls(
    image=tts_image,
    volumes={"/tts": modal.Volume.from_name("tts-cache", create_if_missing=True)},
    gpu="A10G",
    scaledown_window=300,
    timeout=180,
)
class TTS:
    def __init__(self, voice = "male"):
        voice_seeds = {
            "female": 28,
            "male": 34,
            "male_alt_1": 43,
        }
        print(f"Using voice {voice} with seed {voice_seeds[voice]}")
        self.voice_seed = voice_seeds[voice]

    @modal.enter()
    def load_model(self):
        import ChatTTS

        self.chat = ChatTTS.Chat()
        self.chat.load(compile=False)

        torch.manual_seed(self.voice_seed)
        self.rand_spk = self.chat.sample_random_speaker()

    @modal.method()
    def speak(self, text, temperature=0.18, top_p=0.9, top_k=20):
        text = text.strip()
        if not text:
            return

        params_infer_code = ChatTTS.Chat.InferCodeParams(
            spk_emb = self.rand_spk,
            temperature = temperature,
            top_P = top_p,
            top_K = top_k,
        )

        params_refine_text = ChatTTS.Chat.RefineTextParams(
            prompt='[oral_8][laugh_2][break_2]',
        )

        wavs = self.chat.infer(text, skip_refine_text=True, params_infer_code=params_infer_code, params_refine_text=params_refine_text)

        wav_file = io.BytesIO()
        torchaudio.save(wav_file, torch.from_numpy(wavs[0]).unsqueeze(0), 24000, format="wav", backend="soundfile")

        return wav_file
```

This class initializes the ChatTTS model, loads it into memory, and provides a
`speak`
method to convert text to speech.

Running the Text-to-Speech Conversion
-------------------------------------

Finally, let’s add a local entrypoint to run the text-to-speech conversion:

```
@app.local_entrypoint()
def tts_entrypoint(text: str):
    tts = TTS()
    wav = tts.speak.remote(text)
    with open(f"output.wav", "wb") as f:
        f.write(wav.getvalue())
```

This entrypoint creates a TTS instance, calls the
`speak`
method remotely, and saves the resulting audio as a WAV file.

Running the Script
------------------

To run the script, save all the code above in a file named
`chattts_modal.py`
. Then, you can run it using Modal:

```
modal run chattts_modal.py --text "Hello, this is a test of ChatTTS running on Modal."
```

This command will generate an
`output.wav`
file in your current directory with the synthesized speech.

Conclusion
----------

You’ve now learned how to run ChatTTS using Modal. This setup allows you to leverage the power of serverless computing for your text-to-speech needs, making it easy to scale and integrate into various applications.

For the full code and more details, you can check out the complete gist
[here](https://gist.github.com/erik-dunteman/24cc2619fb9d1caef3a2633f34c13a1e)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how_to_run_deepseek_r1_distilled_vllm
================================================================================

How to run DeepSeek-R1 Distilled Qwen-32B with vLLM on Modal
============================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What is DeepSeek-R1 Distilled Qwen-32B?
---------------------------------------

[DeepSeek-R1](https://www.deepseek.com/en/r1)

is an open-source state of the art “reasoning”
LLM that is competitive with gpt-o1 and other top closed LLMs.

There is a tremendous amount of excitement around DeepSeek-R1, and since its
weights are available on Hugging Face, theoretically, anyone can run it.

However, the full un-quantized DeepSeek-R1 model, at 671B parameters, is much too large to
run on consumer grade GPUs.

To address this, DeepSeek has created a series of distilled models, including DeepSeek-R1 Distilled Qwen-32B.

What are distilled models?
--------------------------

Distilled models are smaller, more efficient AI models that are trained to
replicate the behavior of larger “teacher” models while using fewer
computational resources.

In particular, the DeepSeek-R1 distilled models were created by fine-tuning
smaller base models (e.g., Alibaba’s Qwen, in the case of DeepSeek-R1 Distilled Qwen-32B) using samples of
reasoning data generated by DeepSeek-R1.

The expected performance of
[DeepSeek-R1 Distilled Qwen-32B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B)

is on par with
gpt-4o, gpt-o1-mini, and claude-3.5-sonnet according to reasoning, math, and coding
benchmarks.

What is vLLM?
-------------

If you are trying to serve DeepSeek in production, you will need to use
an inference server like
[vLLM](https://github.com/vllm-project/vllm)

,
[TensorRT](https://developer.nvidia.com/tensorrt)

, or
[Triton](https://developer.nvidia.com/triton-inference-server)

for better
latency and performance.

Why should you run DeepSeek-R1 Distilled on Modal?
--------------------------------------------------

Running even the distilled DeepSeek-R1 models requires GPUs, and
[Modal](https://modal.com/)

is the
easiest way to get a GPU.

Modal is a Python library that makes it painless for you to deploy your code in the cloud
and scale it to millions of requests, with or without GPUs.

You just write your Python function, attach a Modal decorator, and Modal handles the
rest.

Example code for running DeepSeek-R1 Distilled Qwen-32B on Modal
----------------------------------------------------------------

To run the following code, you will need to:

1. Create an account at
   [modal.com](https://modal.com)
2. Run
   `pip install modal`
   to install the modal Python package
3. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
4. Copy the code below into a file called
   `app.py`
5. Run
   `modal deploy app.py`

```
import os
import subprocess

from modal import Image, App, Secret, gpu, web_server

MODEL_DIR = "/model"
BASE_MODEL = "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"

# ## Define a container image
def download_model_to_folder():
    from huggingface_hub import snapshot_download
    from transformers.utils import move_cache

    os.makedirs(MODEL_DIR, exist_ok=True)

    snapshot_download(
        BASE_MODEL,
        local_dir=MODEL_DIR,
        ignore_patterns=["*.pt", "*.bin"],  # Using safetensors
    )
    move_cache()

# ### Image definition
# We'll start from a recommended Docker Hub image and install `vLLM`.
# Then we'll use `run_function` to run the function defined above to ensure the weights of
# the model are saved within the container image.
image = (
    Image.debian_slim(python_version="3.12")
    .pip_install("vllm==0.7.0", "fastapi[standard]==0.115.4")
    .pip_install(
        "huggingface_hub[hf_transfer]==0.26.2",
    )
    # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.
    .env({"HF_HUB_ENABLE_HF_TRANSFER": "1"})
    .run_function(
        download_model_to_folder,
        secrets=[Secret.from_name("huggingface")],
        timeout=60 * 20,
    )
)

app = App("vllm-inference-openai-compatible", image=image)

GPU_CONFIG = gpu.H100(count=2)  # 2x80GB H100

@app.function(
    gpu=GPU_CONFIG,
    scaledown_window=1200,
    min_containers=1,
)
@modal.concurrent(max_inputs=100)
@web_server(8000, startup_timeout=1200)
def openai_compatible_server():
    target = BASE_MODEL
    cmd = (
        f"python -m vllm.entrypoints.openai.api_server "
        f"--model {target} "
        f"--host 0.0.0.0 "
        f"--port 8000 "
        f"--tensor-parallel-size 2 "  # Enable tensor parallelism across 2 GPUs
        f"--max-model-len 32768 "  # Set maximum sequence length
        f"--enforce-eager"
    )
    subprocess.Popen(cmd, shell=True)
```

This will start up an OpenAI-compatible vLLM server.

Interact with the server
------------------------

Once it is deployed, you’ll see a URL appear in the command line, something like
[https://your-workspace-name—example-vllm-openai-compatible-serve.modal.run](https://your-workspace-name--example-vllm-openai-compatible-serve.modal.run)

.

You can then interact with the server using the Python
`openai`
library.

```
from openai import OpenAI

question = "What is the capital of the moon?"

client = OpenAI(
    base_url="<your-modal-url>/v1/",
    api_key="token-abc123",
)

response = client.chat.completions.create(
    model="deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
    messages=[
        {
            "role": "user",
            "content": question,
        }
    ],
)
```

Additional resources
--------------------

* [How to run DeepSeek-R1 Quantized on Modal (super-optimized)](/docs/examples/llama_cpp)
* [How to run an OpenAI-compatible vLLM server on Modal (super-optimized)](/docs/examples/vllm_inference)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how_to_run_gradio_on_modal_article
================================================================================

How to deploy a Gradio app
==========================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What Is Gradio?
---------------

[Gradio](https://www.gradio.app/)

is an open-source Python library that allows developers to create intuitive web interfaces for their ML models with just a few lines of Python code. Whether you’re working with text, images, audio, or video, Gradio simplifies the process of making your models interactive and accessible.

![gradio](/_app/immutable/assets/gradio-interface.DQdqY7Ep.jpg)

### Key Benefits of Gradio:

* Rapid prototyping of ML model interfaces
* Support for various input and output types
* Easy integration with popular ML frameworks
* Customizable UI components

Why Deploy Gradio in the Cloud?
-------------------------------

While Gradio is excellent for local development, if you want to share your demo with someone, you’ll probably need to deploy it in the cloud. Enter Modal!

Introducing Modal: Simplifying Cloud Deployment for Gradio
----------------------------------------------------------

[Modal](https://modal.com/)

provides a serverless platform that makes deploying Gradio interfaces in the cloud a breeze. With Modal, you can have your Gradio app up and running in minutes, without the hassle of managing infrastructure. Moreoever, you can also fine-tune your ML models and store their weights on Modal, making it a one-stop solution for all your ML deployment needs.

### Step-by-Step Guide to Deploying Gradio on Modal

Let’s walk through the process of deploying a Gradio interface on Modal.

#### Step 1: Create Your Gradio Application

First, create a file named
`gradio_app.py`
with the following code:

```
import modal

app = modal.App("gradio-app")
web_image = modal.Image.debian_slim(python_version="3.12").pip_install(
    "fastapi[standard]==0.115.4",
    "gradio~=5.7.1",
    "pillow~=10.2.0",
)

@app.function(
    image=web_image,
    min_containers=1,
    scaledown_window=60 * 20,
    # gradio requires sticky sessions
    # so we limit the number of concurrent containers to 1
    # and allow it to scale to 100 concurrent inputs
    max_containers=1,
)
@modal.concurrent(max_inputs=100)
@modal.asgi_app()
def ui():
    """A simple Gradio interface for a greeting function."""

    import gradio as gr
    from fastapi import FastAPI
    from gradio.routes import mount_gradio_app
    import time

    def greet(name, intensity):
        time.sleep(5)  # Simulating processing time
        return "Hello, " + name + "!" * int(intensity)

    demo = gr.Interface(
        fn=greet,
        inputs=["text", "slider"],
        outputs=["text"],
    )
    demo.queue(max_size=5)  # Enable queue for handling multiple request

    return mount_gradio_app(app=FastAPI(), blocks=demo, path="/")
```

This script creates a simple Gradio interface with a greeting function that takes a name and intensity as inputs.

#### Step 2: Deploy Your Gradio App on Modal

To deploy your Gradio app on Modal, simply run:

```
modal deploy gradio_app.py
```

Modal will process your deployment and provide you with a URL where your Gradio interface is now live and accessible to anyone.

Why Choose Modal for Your Gradio Deployment?
--------------------------------------------

Modal offers several advantages for deploying Gradio interfaces:

1. **Serverless Architecture**
   : No need to manage servers or worry about scaling.
2. **Rapid Deployment**
   : Get your demo online in minutes.
3. **Cost-Effective**
   : Pay only for usage - if no one uses your app, you don’t pay.
4. **Easy Updates**
   : Modify your Gradio interface and redeploy with a single command.
5. **Concurrent Request Handling**
   : The deployment script is configured to handle multiple requests efficiently.

Conclusion: Elevate Your ML Demos with Gradio and Modal
-------------------------------------------------------

By combining Gradio’s intuitive interface creation capabilities with Modal’s efficient cloud deployment solution, you can take your machine learning demonstrations to the next level.

Start deploying your Gradio interfaces with
[Modal](https://modal.com/)

today,
and experience the future of ML model demonstration and collaboration!

Other Resources
---------------

For a more detailed example of how to deploy a Gradio app on Modal that serves
as a demo frontend for a finetuned ML model, check out the
[Modal example](/docs/examples/cloud_bucket_mount_loras)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how_to_run_llama_405b_article
================================================================================

Llama3-405B: How to run an extra large open source LLM on Modal
===============================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Meta’s
[Llama3-405B](https://huggingface.co/meta-llama/Meta-Llama-3.1-405B)

represents a new frontier in open-source large language models, offering capabilities that rival top closed-source AI models. However, due to its size and computational requirements, it can be daunting to run.

This guide will walk you through the process of setting up and running Llama3-405B using
[vLLM](https://github.com/vllm-project/vllm)

on Modal, a serverless cloud computing platform.
**For the full code, you can view the
[gist](https://gist.github.com/charlesfrye/fd595d21e2d483cb71ace23bde6430c0)**
.

Memory Requirements and Optimization
------------------------------------

Running Llama3-405B is resource-intensive, but there are some optimizations to make it more accessible:

**VRAM Requirements:**
In the normal “half-precision” (FP16), the model would require over 800GB of VRAM just to load.

**8-bit Quantization:**
In our code, we are using the 8-bit quantized model, which significantly reduces the VRAM footprint.

**Multi-GPU Setup:**
In our example, we’re distributing the model across 8 A100 GPUs with 80GB each, totaling 640GB of VRAM.

**System Memory:**
The setup requires 336GB of system memory to handle data loading and processing.

Prerequisites
-------------

Before we begin, ensure you have the following:

1. Create an account at
   [modal.com](https://modal.com)
2. Install the Modal Python package by running:

   ```
   pip install modal
   ```

3. Authenticate your Modal account by running:

   ```
   modal setup
   ```

   If this doesn’t work, try:

   ```
   python -m modal setup
   ```

Running Llama3-405B
-------------------

To run Llama3-405B, you’ll need to use three separate files from the
[provided gist](https://gist.github.com/charlesfrye/fd595d21e2d483cb71ace23bde6430c0)

. Here’s how to use each one:

### 1. Downloading the Model (download.py)

First, you need to download the model weights to a Modal volume:

1. Save the
   `download.py`
   script from the gist to your local directory.
2. Run the command:
   `modal run download.py`

This process may take about 30 minutes. It downloads the model weights and stores them in a Modal volume for faster access in subsequent runs.

### 2. Setting Up the vLLM Server (api.py)

Once the model is downloaded, you need to set up the vLLM server:

1. Save the
   `api.py`
   script from the gist to your local directory.
2. Run the command:
   `modal deploy api.py`

This command deploys an OpenAI-compatible API server on Modal’s infrastructure. It sets up the necessary GPU resources and serves the model through an API.

### 3. Interacting with the Model (client.py)

Finally, you can interact with the model using the provided client script:

1. Save the
   `client.py`
   script from the gist to your local directory.
2. Run the script with:
   `python client.py`

This script allows you to send requests to the vLLM server and receive responses. It offers several options for customization:

* `--model`
  : Specify a model name (optional, defaults to the first available model)
* `--api-key`
  : Set the API key for authentication (default is “super-secret-token”)
* `--max-tokens`
  ,
  `--temperature`
  ,
  `--top-p`
  , etc.: Adjust various generation parameters
* `--prompt`
  : Provide a custom prompt (default is a limerick about baboons and raccoons)
* `--system-prompt`
  : Set a custom system prompt
* `--no-stream`
  : Disable streaming of response chunks
* `--chat`
  : Enable interactive chat mode

For example, to start an interactive chat session with a custom system prompt, you could use:

```
python client.py --chat --system-prompt "You are a helpful AI assistant."
```

Smaller versions of Llama3
--------------------------

If you want to run smaller versions of Llama3 on Modal, see:

* [How to run Llama3-8B on Modal](/blog/how-to-run-llama-3-1-8b-instruct-on-modal)
* [How to run Llama3-70B on Modal](/blog/how-to-run-llama-3-1-70b-instruct-on-modal)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how_to_run_ollama_article
================================================================================

How to run Ollama
=================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What is Ollama?
---------------

Ollama is an open-source project that simplifies the process of running and managing large language models. It has a bunch of nice features:

* install multiple models and switch between them on the fly, without requiring a daemon restart.
* comes with a powerful command-line interface, making it easy to integrate into your workflows. You can run commands like
  `ollama run <modelname> "Your request"`
  to quickly load a model and process your input.
* provides access to a wide range of pre-configured models. Simply running
  `ollama run <modelname>`
  will download and run the specified model if it’s not already available locally.

This guide will walk you through the process of running Ollama on Modal, a serverless cloud computing platform. This allows you to leverage Modal’s serverless GPU resources. The full code for this guide is
[here](https://github.com/irfansharif/ollama-modal/tree/master)

.

Prerequisites
-------------

Before we begin, make sure you have the following:

1. An account at
   [modal.com](https://modal.com)
2. The Modal Python package installed (
   `pip install modal`
   )
3. Modal CLI authenticated (run
   `modal setup`
   or
   `python -m modal setup`
   if the former doesn’t work)

Running Ollama on Modal
-----------------------

To run Ollama on Modal:

1. Clone
   [the project directory](https://github.com/irfansharif/ollama-modal/tree/master)

   containing the code.
2. Open a terminal and navigate to the project directory.
3. Run the following command:

```
modal run ollama-modal.py --text "Your question here"
```

This command will deploy the Ollama service on Modal and run an inference with your specified text.

Understanding the code
----------------------

### Service configuration

The
`ollama.service`
file contains a systemd service configuration for Ollama:

```
[Unit]
Description=Ollama Service
After=network-online.target
[Service]
ExecStart=/usr/bin/ollama serve
User=ollama
Group=ollama
Restart=always
RestartSec=3
[Install]
WantedBy=default.target
```

This configuration ensures that Ollama runs as a service, automatically starting after the network is online and restarting if it fails.

### Main application code

The
`ollama-modal.py`
file contains the main application code for running Ollama on Modal. Let’s examine its key components:

1. Importing necessary modules:

```
import modal
import os
import subprocess
import time
from modal import build, enter, method
```

These imports provide the required functionality for interacting with Modal and managing system processes.

2. Defining the model and pull function:

```
MODEL = os.environ.get("MODEL", "llama3:instruct")

def pull(model: str = MODEL):
    # ... (code for starting Ollama service and pulling the model)
```

This section sets up the default model and defines a function to start the Ollama service and pull the specified model.

3. Creating the Modal image:

```
image = (
    modal.Image
    .debian_slim()
    .apt_install("curl", "systemctl")
    .run_commands(
        # ... (commands to install Ollama)
    )
    .copy_local_file("ollama.service", "/etc/systemd/system/ollama.service")
    .pip_install("ollama")
    .run_function(pull)
)
```

This code creates a Modal image with Ollama installed and configured.

4. Defining the Ollama class:

```
@app.cls(gpu="a10g", region="us-east", scaledown_window=300)
class Ollama:
    @build()
    def pull(self):
        # ... (build step, currently empty)

    @enter()
    def load(self):
        subprocess.run(["systemctl", "start", "ollama"])

    @method()
    def infer(self, text: str):
        # ... (code for inference using Ollama)
```

This class encapsulates the Ollama functionality, including starting the service and performing inference.

5. Main entrypoint:

```
def main(text: str = "Why is the sky blue?", lookup: bool = False):
    if lookup:
        ollama = modal.Cls.from_name("ollama", "Ollama")
    else:
        ollama = Ollama()
    for chunk in ollama.infer.remote_gen(text):
        print(chunk, end='', flush=False)
```

This function provides a convenient way to run the Ollama inference from the command line.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/how_to_run_xtts_article
================================================================================

How to run XTTS
===============

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

How to Run XTTS: A Step-by-Step Guide
=====================================

[XTTS](https://huggingface.co/coqui/XTTS-v2)

is one of the
[best open-source text-to-speech](/blog/open-source-tts)

models available today. It offers high-quality, multilingual speech synthesis capabilities. In this guide, we’ll walk you through the process of running XTTS using Modal, a serverless cloud computing platform.

Prerequisites
-------------

Before we begin, make sure you have the following:

1. Create an account at
   [modal.com](https://modal.com)
2. Install the Modal Python package:

   ```
   pip install modal
   ```

3. Authenticate your Modal account:

   ```
   modal setup
   ```

   (If this doesn’t work, try
   `python -m modal setup`
   )

Setting Up the XTTS Environment
-------------------------------

We’ll be using a single Python file to set up and run XTTS. Let’s break down the code and explain each part:

First, we import the necessary libraries and set up the Modal app:

```
import io
import modal

app = modal.App(name="xtts")
```

Next, we define the image that will be used to run our XTTS model:

```
tts_image = (
    modal.Image.debian_slim(python_version="3.11.9")
    .apt_install("git")
    .run_commands("pip install git+https://github.com/coqui-ai/TTS@8c20a599d8d4eac32db2f7b8cd9f9b3d1190b73a")
    .env({"COQUI_TOS_AGREED": "1", "TTS_HOME": "/tts"})
)
```

This image is based on Debian Slim, installs Git, and sets up the TTS package from the Coqui repository. Note that we’re agreeing to Coqui’s terms of service by setting the
`COQUI_TOS_AGREED`
environment variable.

Implementing the XTTS Class
---------------------------

Now, let’s create the
`XTTS`
class that will handle the text-to-speech conversion:

```
with tts_image.imports():
    from TTS.api import TTS
    import torch

@app.cls(
    image=tts_image,
    volumes={"/tts": modal.Volume.from_name("tts-cache", create_if_missing=True)},
    gpu="A10G",
    scaledown_window=300,
    timeout=180,
)
class XTTS:
    def __init__(self):
        pass

    @modal.enter()
    def load_model(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to(self.device)
        print("Model Loaded")
        speakers = self.model.synthesizer.tts_model.speaker_manager.speakers.keys()
        print(f"Supported speakers: {speakers}")

    @modal.method()
    def speak(self, text, speaker="Kazuhiko Atallah", language="en"):
        wav_file = io.BytesIO()
        self.model.tts_to_file(
                text=text,
                file_path=wav_file,
                speaker=speaker,
                language=language,
        )
        return wav_file
```

This class does the following:

1. Loads the XTTS-v2 model when the container starts.
2. Provides a
   `speak`
   method that converts text to speech.

Running XTTS
------------

Finally, we define an entrypoint to run our XTTS model:

```
@app.local_entrypoint()
def tts_entrypoint(text: str):
    tts = XTTS()
    wav = tts.speak.remote(text)
    with open(f"output.wav", "wb") as f:
        f.write(wav.getvalue())
```

This entrypoint function takes a text input, runs the XTTS model, and saves the output as a WAV file.

How to Use the XTTS Script
--------------------------

To use this script:

1. Save the entire code into a file, for example,
   `xtts_modal.py`
   .
2. Run the script using Modal:

   ```
   modal run xtts_modal.py --text "Your text to be converted to speech"
   ```

This will generate an
`output.wav`
file in your current directory containing the synthesized speech.

Conclusion
----------

By following this guide, you’ve learned how to run XTTS using Modal. This setup allows you to leverage powerful GPU resources in the cloud for high-quality text-to-speech conversion. You can easily modify the script to support different languages or speakers.

For the full code and more details, you can check out the complete gist
[here](https://gist.github.com/erik-dunteman/a560198d1c57766bb536fb0e41b134ce)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/image-segmentation-article
================================================================================

Top image segmentation models
=============================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Image segmentation is a fundamental task in computer vision that involves partitioning an image into multiple segments or objects. This process allows machines to understand and analyze the content of images at a pixel level, enabling a wide range of applications from medical imaging to autonomous driving.

What is image segmentation?
---------------------------

![image-segmentation](/_app/immutable/assets/image-segmentation.U3DNXBYd.jpg)

Image segmentation is the process of dividing an image into multiple parts or regions, each of which corresponds to a different object or area of interest. The goal is to simplify the representation of an image into something more meaningful and easier to analyze.

For example, in a photo of a street scene, image segmentation might identify and separate areas corresponding to buildings, cars, pedestrians, and the road itself. Each of these segments can then be analyzed independently, allowing for more detailed and accurate image understanding.

What is mask generation?
------------------------

Mask generation is a specific output of image segmentation where the result is a binary mask for each identified object or region. A mask is essentially a black and white image where white pixels correspond to the object of interest, and black pixels represent the background or other objects.

These masks provide a precise outline of each segmented object, allowing for detailed analysis of shape, size, and position within the image.

How do image segmentation and mask generation differ?
-----------------------------------------------------

While closely related, image segmentation and mask generation have some key differences:

1. **Output format:**
   Image segmentation typically produces a labeled image where each pixel is assigned to a specific class or object. Mask generation, on the other hand, creates binary masks for each identified object.
2. **Granularity:**
   Image segmentation can be semantic (identifying broad categories) or instance-based (distinguishing individual objects within categories). Mask generation is usually associated with instance segmentation, providing a unique mask for each object instance.
3. **Application:**
   Image segmentation is often used for understanding the overall composition of an image, while mask generation is particularly useful for tasks that require precise object boundaries, such as image editing or medical image analysis.

Use cases for image segmentation and mask generation
----------------------------------------------------

Image segmentation and mask generation have numerous applications across various industries:

1. Medical imaging: Identifying tumors, measuring organ volumes, or planning radiation therapy.
2. Autonomous vehicles: Detecting road boundaries, other vehicles, pedestrians, and obstacles.
3. Satellite imagery: Land use classification, urban planning, and environmental monitoring.
4. Augmented reality: Separating foreground objects from backgrounds for realistic object placement.
5. Industrial inspection: Detecting defects or anomalies in manufacturing processes.
6. Face recognition: Isolating facial features for more accurate identification.
7. Content-based image retrieval: Improving search accuracy by understanding image content.

Top image segmentation models
-----------------------------

The top image segmentation/mask generation model that has emerged in the transformers era is Meta’s Segment Anything model:

### SAM2 (Segment Anything Model 2)

[SAM2](https://huggingface.co/facebook/sam2-hiera-large)

, developed by Meta, is an evolution of the original SAM model. Key features include:

* Ability to segment both images and videos
* Requires input in the form of bounding boxes or points to guide segmentation
* Improved efficiency and accuracy over the original SAM

SAM2 excels in tasks requiring flexible, user-guided segmentation. Its ability to work with minimal input makes it versatile for a wide range of applications.

You can try out SAM2 using Modal’s
[SAM2 example](/docs/examples/run_sam#run-facebooks-segment-anything-model-2-sam-2-on-modal)

, which provides a simple interface to experiment with the model.

### Language Segment-Anything

![image-mask](/_app/immutable/assets/image-mask.HYflkhq8.jpg)

[Language Segment-Anything](https://github.com/luca-medeiros/lang-segment-anything)

is a modification of SAM2 that allows the use of language prompts for segmentation instead of bounding boxes.

This tool, built on Meta’s Segment Anything Model 2 and the GroundingDINO detection model, simplifies object detection and image segmentation. Key features include:

* Text prompts for segment description
* Integration of language models with image segmentation
* Intuitive and flexible segmentation capabilities

LangSAM is especially beneficial in contexts where users describe objects in natural language, such as image editing or content moderation systems.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/introducing-b200-h200
================================================================================

Introducing: B200s and H200s on Modal
=====================================

It
[never](https://x.com/modal_labs/status/1592915154207133699)

[gets](/blog/introducing-l40s)

[old](/blog/introducing-h100)

: we are pleased to announce that the most powerful GPUs on the market are now available on Modal.

No “contact sales” button or asking for quota. B200s and H200s are accessible to anyone with a Modal account. It’s a one-liner to add them to any Modal Function.

```
import modal

app = modal.App()

@app.function(gpu="B200") # or "H200"
def run_big_model():
    # This will run on a Modal B200
```

B200s are priced at $6.25/hour, and H200s are priced at $4.54/hour.

What are the specs of the B200 and H200 GPUs?
---------------------------------------------

Comparing the B200s and H200s with H100s, we can see how much more powerful these new GPUs are.

|  | [B200](https://resources.nvidia.com/en-us-dgx-systems/dgx-b200-datasheet) | [H200](https://nvdam.widen.net/s/nb5zzzsjdf/hpc-datasheet-sc23-h200-datasheet-3002446) | [H100](https://resources.nvidia.com/en-us-hopper-architecture/nvidia-tensor-core-gpu-datasheet) |
| --- | --- | --- | --- |
| [Streaming Multiprocessor Architecture](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture) | Blackwell | Hopper | Hopper |
| [GPU RAM](https://modal.com/gpu-glossary/device-hardware/gpu-ram) | 180 GB | 141 GB | 80 GB |
| GPU RAM ↔ [Streaming Multiprocessor](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor)  Memory Bandwidth | 8 TB/s | 4.8 TB/s | 3.5 TB/s |
| FP4 [Tensor Core](https://modal.com/gpu-glossary/device-hardware/tensor-core)  Arithmetic Bandwidth | 9 PFLOP/s | n/a | n/a |
| FP8 Tensor Core Arithmetic Bandwidth | 5 PFLOP/s | 2 PFLOP/s | 2 PFLOP/s |

How do I interpret B200 and H200 GPU specs?
-------------------------------------------

Both B200s and H200s have more on-device memory than their predecessors. B200s have 180GB of HBM3e on-device memory (aka VRAM)—2.25x the storage of an H100. That leaves more space for the weights and caches of large Mixture-of-Experts (MoE) models like DeepSeek-R1, Qwen 3, and LLaMA 4. Some of these models are so large they can’t be deployed on a single node, even with 8 H100s, but they fit comfortably on 8 H200s or 8 B200s.

Note also that B200s have 8 TB/s of memory bandwidth to the Hopper GPUs’ ~4 TB/s. That means you can get bits into and out of their more capacious memory and into the compute units at twice the rate. Memory-bound workloads like chatbot inference will typically see a >2x latency decrease moving from H100s to B200s—without any changes to inference code, just infrastructure.

The new Blackwell architecture also introduces native support for 4-bit floating point operations in the Tensor Cores, resulting in a ~4x speedup over the native 8-bit floating point matrix math supported by Hopper GPUs. This simultaneously reduces contention for memory bandwidth (great for memory-bound workloads) and capacity while increasing arithmetic bandwidth (great for compute-bound workloads).

What are the performance gains for B200s over H200s and H100s?
--------------------------------------------------------------

Many workloads will see an immediate speedup between 2x and 4x when migrated from H100s to B200s. But the full performance benefits of B200s in particular will take some time to realize. For example, the popular open source LLM serving engine vLLM just stabilized support for Blackwell GPUs in
[version 0.9](https://github.com/vllm-project/vllm/releases/tag/v0.9.0)

, released on Tuesday. And achieving the peak performance promised by the hardware will take even more work.

Despite that, we’ve already seen big gains in our early testing for some workload types!

Below is one popular workload where our benchmarking showed significant gains for latency and throughput on B200s vs H200s. We used the latest version of vLLM to run the DeepSeek V3 Large Mixture-of-Experts Language Model in its native 8-bit precision to process 1000 tokens (about one page of text) as input and generate 128 tokens (e.g. a summary paragraph) as output. This model is too large to run on an 8xH100, but we benchmarked it on an 8xH200 and an 8xB200 on Modal.

* At 1 request per second, the median time-to-first-token is 2.5x faster on the B200s vs H200s.
* At a median time-to-first-token of 1 second, queries-per-second is 1.7x higher on the B200s vs H200s.

![benchmark](https://modal-cdn.com/blog/images/benchmarks-h200-b200.webp)

You can run vLLM on Modal B200s with
[this code sample](/docs/examples/vllm_inference)

.

As open-source engines release new optimizations, we expect to see further performance gains—and we’ll be here to
[explain](/docs/examples/trtllm_latency)

how you can make use of them on Modal!

Why Modal for B200s and H200s?
------------------------------

Modal is the easiest way to deploy code to GPUs with no reservations or commitments. Our custom infrastructure allows us to spin up GPU containers running your code in less than a second. We help you efficiently autoscale your workloads to hundreds of GPUs, and you only ever pay for what you use.

Modal also comes with $30/month in free compute, so you can try B200s or H200s for free right now!
[Sign up](/signup)

today to get started.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/introducing-client-1-0
================================================================================

Introducing: Modal 1.0
======================

Last week, we launched
[version 1.0 of the Modal client](https://pypi.org/project/modal/)

. This is a significant milestone for us because it marks a new level of maturity and stability for the Modal platform. We’ve worked to make the client API more robust and predictable, and we expect far fewer breaking changes moving forward.

So what do we mean by “Modal client”? If you’re new to Modal, you can think of Modal in two parts: our client, which is a Python SDK, and our managed cloud platform, which we use to run user workloads around the world.

The client gives developers the ability to access serverless cloud compute from their application code via decorators. Here’s a basic example:

```
import modal

app = modal.App()

@app.function()
def cloud_function():
	...

@app.local_entrypoint()
def main():
	cloud_function.remote()  # This runs in the cloud!
```

Developer ergonomics has been a core principle of Modal from the very beginning. Crafting a good developer experience requires making tricky tradeoffs along the way, however. Working towards 1.0 has given us a chance to reflect on our core design principles and some of the major decisions we’ve made along the way.

Design principles for 1.0
-------------------------

### 1. Avoiding unpredictable magic

We want using Modal to feel a little magical, but “magic” behavior can be a double-edged sword. With 1.0, we’ve intentionally moved away from some of these “magic” behaviors.

One example is our old “automounting” feature. If your App had imports of local packages, we’d automatically find and include them in your Modal deployment. While helpful on the surface, this became unpredictable in many cases. Users could end up with more packages than needed, making deployments slower. Users could also start expecting every file they read to be automatically included, even when that wasn’t the case.

With 1.0, these behaviors are now explicit. You’ll specify exactly what local files and packages are included in your container, apart from the Function’s own package. Your App definition might become more verbose, but the benefit is much more predictable and easy-to-debug behavior.

Before:

```
# Before: Modal will detect this local import and automount the package
from utils.llm import tokenize

app = modal.App()

@app.function()
def process():
    tokenize("some text")
```

After:

```
from utils.llm import tokenize

app = modal.App()

# Now: You explicitly add the local utils package to your Image
image = modal.Image.debian_slim().add_local_python_source("utils")

@app.function(image=image)
def process():
    tokenize("some text")
```

### 2. Separating core functionality from additional concepts

As the number of use cases powered by Modal grows, so does the number of concepts, and thus also the need for organizing them in a scalable way. We want to be able to add new features without making it harder to understand the basics.

After adding many new parameters to the
`@app.function`
decorator, we’ve recognized that trying to put every possible configuration into a single decorator quickly becomes unwieldy. Going forward, we’ll be clearly separating core functionality from additional concepts by splitting them up over separate decorators. An example of this is the new
`@modal.concurrent`
decorator, which lets you express concurrency patterns independently from the configuration in
`@app.function`
.

This pattern also allows us to add config options that are specific to each decorator, such as
`target_inputs`
in the code snippet below.

Before:

```
@app.function(allow_concurrent_inputs=1000)
def f(...):
    ...
```

After:

```
@app.function()
@modal.concurrent(max_inputs=1200, target_inputs=1000)  # Let concurrency spike during scaleup
def f(...):
    ...
```

### 3. One canonical path

We believe in providing clear paths for users to accomplish common tasks. This means reducing redundant methods for similar actions, simplifying the mental model for developers.

One example is how local files are brought into your Modal container. Previously, you could either specify them in your image setup or pass a
`modal.Mount`
directly in your Function configuration. This meant that two distinct object types were responsible for bringing local assets into the remote environment. We’ve now moved all of this functionality under the
`modal.Image`
class. This aligns with Modal’s core promise of abstracting the container environment: everything related to the container
*filesystem*
and its initial state is now managed explicitly and consistently within the
`Image`
definition. The
`Image`
class already had robust ways to bring over local data, so
[consolidating](/docs/guide/modal-1-0-migration#deprecating-mount-as-part-of-the-public-api)

here provides a more coherent experience.

```
# The below two Function definitions accomplished the same thing, so we're deprecating the former

mount = modal.Mount.from_local_dir("data").add_local_file("config.yaml")
@app.function(image=image, mount=mount)
def f():
    ...

image = image.add_local_dir("data").add_local_file("config.yaml")
@app.function(image=image)
def g():
    ...
```

A more predictable client release cycle
---------------------------------------

You’ll also notice a shift in our client release cycle. We’ll be batching updates together so it’s easier for you to quickly see what’s new and decide when to upgrade. This gives you more predictability and control over your development environment. You’ll always be able to see what changes we’ve made in our
[changelog](/docs/reference/changelog)

.

What’s next?
------------

Modal 1.0 is a significant milestone, but it’s just the beginning. There are a couple of particularly exciting directions we’re taking the client SDK. One is creating SDKs for other languages, which we’ve made
[good progress](/blog/sdk-javascript-go)

on already. The other is exploring the distinction between Modal as a software tool for humans versus a tool for agents. What does it mean to have good ergonomics in a world where development is mediated by AI tools? How can we adapt the platform to better serve the unique needs of AI-driven workflows?

We think you’ll appreciate the stability and clarity that Modal 1.0 brings, and we hope this release will make building with Modal even more productive. For detailed instructions on how to migrate to 1.0, check out the
[1.0 release notes](/docs/reference/changelog#100-2025-05-16)

and our
[migration guide](/docs/guide/modal-1-0-migration)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/introducing-h100
================================================================================

Introducing: H100s on Modal
===========================

NVIDIA’s H100 GPUs are the fastest machine learning accelerators on the market.
They have also been almost impossible to get a hold of.

But all Modal users can access them - starting now!

For the largest language models H100s boast up to 4x training speedups and up to
30x inference speedups compared to A100s, according to benchmarks by NVIDIA.

![A bar chart showing speedups on H100 GPUs](https://modal-cdn.com/cdnbot/h100-bar-chart.png)

Source:
<https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet>

Faster cards cost more to run, so we recommend exploring them on Modal where the
spend is justified. That could be a latency-sensitive application like
interactive LLM inference, where every millisecond counts, or a throughput-bound
job like fine-tuning a foundation model, where the price-to-performance ratio
can result in a
[lower total cost](https://www.databricks.com/blog/coreweave-nvidia-h100-part-1)

in some cases.

Our H100s have 80 GB of on-chip DRAM connected by high-bandwidth memory to
compute units capable of nearly two thousand teraFLOPS at 16 bit precision.

And for $7.65 per hour per GPU, you can run jobs that use up to 8 GPUs that
communicate via NVIDIA NVLink connections with 3.6 TB/s of bisectional
bandwidth.

On Modal, you only pay for what you use. Thanks to our robust autoscaling, you
can achieve significantly higher utilization and thus lower overall costs
compared to fixed GPU reservations.

Whether you’re responding to a burst of inference requests when your app hits
the top of HackerNews or launching one thousand ML experiments in parallel right
before the deadline, Modal is here to serve all your compute needs without
charging you after the job is done.

Getting started is simple: just set
`"H100"`
as the desired GPU type in the
Modal decorator of the function you want to run remotely.

```
import modal

app = modal.App()

@app.function(gpu="H100")
def run_gpt5():
    # This will run on Modal's H100s
```

If you have questions on our H100 support or want to share something incredible
you built that uses H100s, please reach out in our
[community Slack](https://modal.com/slack)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/introducing-l40s
================================================================================

Introducing: L40S GPUs on Modal
===============================

At Modal, we believe that
[AI inference has unique infrastructure needs](https://modal.com/blog/the-future-of-ai-needs-more-flexible-gpu-capacity)

.

So we’re thrilled to share that a new inference-focused accelerator is now available for all Modal users:
[NVIDIA L40S GPUs](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413)

, priced at $1.95/hr.

The L40S can offer substantial performance benefits over our current most popular inference-focused accelerator, the
[NVIDIA A10 GPU](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a10/pdf/a10-datasheet.pdf)

.

Run bigger models with the L40S
-------------------------------

At 48GB, the L40S has twice the on-device DDR6 random access memory of the A10.

That means you can run larger models on large inputs. For example,
[running Flux.1-schnell](https://modal.com/docs/examples/flux)

in 16bit precision consumes 24 GB of RAM just for model weights, so it cannot run on a single A10 GPU without a throughput-killing offload to CPU RAM. Trying to do so will trigger a dreaded CUDA OOM:

![flux a10](https://modal-cdn.com/cdnbot/tmp0d4e4yb7_a972858f.webp)

But the same workload fits very comfortably in a single L40S!

![flux l40s](https://modal-cdn.com/cdnbot/tmp2gewl_pt_3056472b.webp)

Run inference faster with the L40S
----------------------------------

The L40S is also faster than the A10, not just beefier.

Users can expect approximately a 40% speedup for memory-bound jobs like small-batch inference and well over a 100% speedup for compute-bound jobs using 16bit
[Tensor Cores](https://modal.com/docs/gpu-glossary/device-hardware/tensor-cores)

.
*Without any tuning*
, we were able to achieve a 20% speedup in a
[basic load test](https://github.com/modal-labs/modal-examples/blob/main/06_gpu_and_ml/llm-serving/openai_compatible/load_test.py)

for a chat-style workload (
[LLaMA 3.1 8B with vLLM](https://modal.com/docs/examples/vllm_inference)

).

![l40s benchmark](https://modal-cdn.com/l40s-benchmark.svg)

A10 vs L40S specs
-----------------

See the table below for a comparison of the features of the A10 and the L40S, adapted from the manufacturer datasheets. If any of the vocabulary is new to you, click the link to be taken to our new
[GPU Glossary](https://modal.com/gpu-glossary)

for an explanation.

|  | [A10](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a10/pdf/a10-datasheet.pdf) | [L40S](https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413?ncid=no-ncid) |
| --- | --- | --- |
| [Streaming Multiprocessor Architecture](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture) | Ampere | Ada Lovelace |
| [Compute Capability](https://modal.com/gpu-glossary/device-software/compute-capability) | 8.6 | 8.9 |
| [GPU RAM](https://modal.com/gpu-glossary/device-hardware/gpu-ram) | 24 GB DDR6 | 48 GB DDR6 |
| [GPU RAM](https://modal.com/gpu-glossary/device-hardware/gpu-ram)  ↔ [Streaming Multiprocessor](https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor)  Memory Bandwidth | 600 GB/s | 864 GB/s |
| FP16/BF16 [Tensor Core](https://modal.com/gpu-glossary/device-hardware/tensor-core)  Arithmetic Bandwidth | 125 TFLOP/s | 362 TFLOP/s |
| FP8 [Tensor Core](https://modal.com/gpu-glossary/device-hardware/tensor-core)  Arithmetic Bandwidth | N/A | 733 TFLOP/s |

Get started now
---------------

Modal is the easiest way to deploy code to GPUs. Our custom infrastructure allows us to spin up L40S (or other GPU) containers running your code in one second. We help you efficiently autoscale your workloads to hundreds of GPUs, and you only ever pay for what you use.

Modal also comes with $30/month in free compute, so you can try an L40S for free right now. Just
[sign up for Modal](https://modal.com/signup)

if you haven’t yet, install and authenticate with our
[Python SDK](https://modal.com/docs/guide)

, and then decorate a Python function with
`app.function(gpu="L40S")`
:

```
import modal

app = modal.App()

@app.function(gpu="L40S")
def run_flux_inference():
    # This will run on a Modal L40S
```

If you have questions on our L40S support or want to share something you’ve built, please reach out in our
[community Slack](https://modal.com/slack)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/jamsocket-is-joining-modal
================================================================================

Jamsocket is joining Modal
==========================

We’re excited to share that the
[Jamsocket](https://jamsocket.com/)

team is joining Modal! Jamsocket is a backend platform used to build sync engines that power collaborative apps. Jamsocket consists of multiple products:

* [Plane](https://github.com/jamsocket/plane)

  : the open-source orchestrator that powers Jamsocket
* [Y-Sweet](https://github.com/jamsocket/y-sweet)

  : a document store and realtime sync backend that runs on top of Jamsocket
* [ForeverVM](https://github.com/jamsocket/forevervm)

  : stateful Python REPLs as a service for AI code execution

![jamsocket photo](https://modal-cdn.com/blog/images/jamsocket-compressed.webp)

Paul Butler (Jamsocket co-founder), Akshat Bubna (Modal co-founder), and Taylor Baldwin (Jamsocket co-founder)

Jamsocket co-founders Paul Butler and Taylor Baldwin will be joining the Modal team as part of this acquisition. Paul is CEO of Jamsocket, with many years of experience as a software engineer and quant. Taylor is CTO, with a long background in building data visualizations and a fun stint on the New York Times’s R&D team.

We’ve known Paul and Taylor for several years now. They’re stalwarts in the NY tech scene and regularly host events for the developer community. We remember very early conversations where we swapped notes on gVisor, the secure container runtime that both Modal and Jamsocket are built on.

We’re excited to have Paul and Taylor on board to build Jamsocket and ForeverVM-like functionality on Modal’s platform. Jamsocket products will continue to operate normally while the core functionality is being migrated to Modal.

“

Increasingly, the applications people build on Jamsocket are AI-centric. Modal is a natural home for Jamsocket because these customers can deploy GPU inference, code sandboxes, and real-time backends all from one platform.

”

— Paul Butler,

Co-founder and CEO

“

What really sold me was seeing how much users love Modal. Even the ones writing in to support are saying things like, ‘I’m hitting this issue…but by the way, I love Modal.

”

— Taylor Baldwin,

Co-founder and CTO

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/jono-containers-talk
================================================================================

Fast, lazy container loading in Modal.com
=========================================

![author](https://modal-cdn.com/jonathon-belotti.png)

[Jonathan Belotti

@jonobelotti\_IO](https://twitter.com/jonobelotti_IO)

Software Engineer

Modal is a serverless GPU cloud platform in the same spirit as AWS Lambda but with a few differences: 1. we incorporate GPUs and 2. our approach to application development and deployment is designed to be both fast and developer-friendly.

In a previous article, we covered the generalities of how we launch containers fast in Modal. In this post, we will go into more depth on fast, lazy container loading.

The container loading problem
-----------------------------

Let’s look at a typical Python machine learning prediction inference function that leverages the BERT language model.

```
import torch
import transformers
def predict(prompt: str) -> list[dict]:
    device = "cuda" if torch.cuda.is_available() else "cpu"    model = transformers.pipeline(
        "fill-mask",
        model="bert-base-uncased",
        device=device
    )
    return model(prompt)
```

The function takes in a text prompt, processes it through the BERT model, and ultimately generates a text response accompanied by metadata.

It’s running on an approximately 8 GiB “fat container” running Python 3.11, and the BERT model itself is packaged as a 512 MiB
`.safetensors`
file.

When we initially started building Modal, loading this
`.safetensors`
file would have taken almost a full second. Considering that models have only gotten larger—now often in the tens of gigabytes—an 800 megabyte per second throughput wasn’t going to cut it.

Fortunately, we’ve made significant strides in improving performance. Our current throughput is about 2.5 gigabytes per second, allowing us to load the 512 MiB
`.safetensors`
file in just 200 milliseconds from disk cache and about 300 milliseconds from the network.

**In the rest of this blog post, we cover the specific improvements we made to accomplish this.**

The container loading problem
-----------------------------

In order to understand the container loading problem, you have to first understand the concept of the container file system. A container cannot operate without this file system, which is typically implemented as an overlay mount on the host system. The overlay filesystem uses some kernel functionality that allows for the management of changes made to a read-only lower directory, where the container image resides.

Here’s a breakdown of how it works:

```
overlay on /mnt/overlay type overlay (
    rw,
    relaltime,
    lowerdir=/tmp/tmph45cav46/lower,
    upperdir=/tmp/tmph45cav46/upper,
    workdir=/tmp/tmph45cav46/work
)
```

In this setup, the lower directory contains the immutable parts of the container image, and it must remain read-only. When you run the container, you can make changes—referred to as mutations—but if you stop and restart the container, those mutations shouldn’t affect the original image.

This is where the overlay filesystem shines: it allows these changes to happen in an upper directory, which you can discard after the container is done using them. Essentially, you keep the lower directory intact for reuse, maintaining your deployment model without the confusion of unwanted changes persisting across container restarts.

The container loading problem
-----------------------------

When we talk about container loading, there are two fundamental approaches:
**eager**
and
**lazy**
.

The
**eager approach**
is typical for tools like Docker and Kubernetes. It’s the go-to method when you’re getting started with containerization. This approach aims to load everything upfront, which, while reliable, can be less flexible and efficient, especially under certain pressures.

In contrast, the
**lazy approach**
, which is adopted by Modal and services like AWS Lambda, takes a different route. Instead of loading everything at once, it initializes components as they are needed. This can significantly improve efficiency and responsiveness, particularly in scenarios where resources are at a premium or need to be managed dynamically.

Docker/K8s loading approach
---------------------------

In the eager approach, cold-starting our fat Python container involves the following steps:

* The container image is a fat stack of N tarballs (gzipped).
* You will download N tarballs over the network concurrently at about 2 GiB/s.
* Decompression of the gzipped tarballs happens single-threaded at around 80 MiB/s.
* Finally, you need to unpack these into a root filesystem directory on the host.

On a Kubernetes node, this process reveals itself in the directory:

```
/var/lib/containerd/io.containerd.snapshotter.v1.overlayfs/
```

When you start up a fresh host, it lacks the necessary container image file system, resulting in a need to download and decompress these layers. While the download can be relatively quick with proper resource allocation, the decompression process becomes the bottleneck since it is single-threaded and slower than the download speed. For larger images—like an 8GB uncompressed image such as those provided by Nvidia—this can lead to about a minute spent on decompression.

Once decompressed, you still have to layout the host filesystem, write files, and set metadata, adding to the time it takes to start the container. You have to complete these unpacking steps before launching the container, as the unpacked layers become part of the lower directory in the overlay filesystem mount.

This loading mechanism suits typical Kubernetes clusters, where applications are less frequently started and stopped. In contrast, serverless multi-tenant platforms demand quick container startups and may experience more frequent node changes, resulting in a completely different cold-starting profile than what you see in a standard Kubernetes environment.

Lazy loading approach
---------------------

The lazy loading approach is very different. A container image serves as an index; essentially, it’s a data structure that holds all the files and metadata about those files. This index is around five megabytes and can be loaded in one to 100 milliseconds, depending on whether we’re grabbing it from memory, disk, or over the network. It can subsequently be FUSE mounted in about 2 milliseconds.

Since it’s a small amount of data, the index contains only pointers to the actual file data that must already exist somewhere in your container loading system—in the case of Modal, it’s typically content addressed.

This turns a 1 minute process into something that takes 100 ms or less.

Note: If you’re not familiar with FUSE, it’s a “Filesystem in Userspace.” This allows you to create a flexible file server that can respond to system calls made by a guest process on a host. These calls go through the kernel to be served by your FUSE server, which is free to operate however it wants, as long as it appropriately responds to the syscall. For example, if I showed you a Python program that wanted to open files, that open request would be routed into the kernel. The kernel then dispatches a FUSE message to the FUSE server, which takes care of handling that message.

How do we load the data?
------------------------

But note that we don’t load data, only metadata! And there’s no free lunch. So how do we load the file data?

Our architecture begins with a Python main container that interacts with a FUSE file server. The FUSE server plays a critical role as it connects to a page cache. When the required data isn’t in the page cache, the system checks the SSD storage, which is followed by the CDN (Content Delivery Network) for any necessary data. As a last resort, it reaches out to blob storage, but we aim to minimize hits to blob storage to just the rarest occasions. If we’re frequently accessing blob storage, it indicates that we’ve missed optimizing the process somewhere along the line.

![system-architecture](/_app/immutable/assets/modal-system-architecture.tMz0aWgc.jpeg)

The performance of lazy loading is closely tied to where we retrieve our file data from. Here’s a breakdown of the read latencies, throughputs, and costs associated with different caching tiers:

| System | Read Latency | Read Throughput | Cost ($USD/GiB/month) |
| --- | --- | --- | --- |
| Memory | 1-100 ns | 10-40 GiB/s | $2.0 |
| SSD | 100 μs | 4 GiB/s | $0.10 |
| AZ Cache Server | 1 ms | 10 GiB/s | $0.15\* |
| Regional CDN | 100 ms | 3-10 GiB/s | - |
| Blob Storage (e.g. S3) | 200 ms | 3-10 GiB/s | $0.015 |

When a read request comes in, we aim to serve it from the fastest available option. Ideally, this is memory, where latency can be as low as 100 nanoseconds and throughput can be high, though this comes at a significant operational cost. If the data isn’t found in memory—likely in the page cache—we may have to go to the SSD. In a fresh host scenario, the SSD might not even have the file, sending us to the next tier—an AZ cache server. This is a mere millisecond away with decent throughput, but if we continue down the hierarchy, we hit the regional CDN, where latencies jump to 100 milliseconds. At this point, we’re straying into problematic territory. The final fallback is blob storage, where you could be looking at latencies around 200 milliseconds. If you’re making read requests that all resolve to this tier, the time it takes for a Python process to spin up could stretch into hours.

While the situation may seem tough—especially as highlighted by the sluggish performance of certain tiers— there is a path forward.

Caching
-------

Caching is one solution. Many of our customers are using Modal for AI/ML stuff and there’s a lot of overlap.

In our lazy loading system, we effectively load Python 3.11 and the relevant libraries just once, making them accessible to multiple containers as they initialize. The first time you run a container, it accesses the necessary files from disk. Once that happens, you can store those files in your page cache, and from that point on, you’re in a good spot.

The diagram illustrates the overlap between different frameworks, such as PyTorch and DreamBooth, and shows that there are over 8,700 common files. This is a significant advantage since the overlap means we minimize redundant loads.

It’s also important to highlight that many of the files in a typical container image, especially larger ones—like those weighing in at around 8 GB when compressed—are essentially junk that your process will never read. Loading unnecessary files is a waste of resources. For example, the eagle loading system often pulls in locale data from the
`/etc`
directory, even when your application won’t utilize that information. This inefficiency can slow down processes and should be avoided whenever possible.

Use big hosts
-------------

Another thing we learned: don’t use small hosts. If you use a small host from a cloud provider, you’ll quickly find that you lack the necessary bandwidth in both your SSD and network connections to achieve peak performance.

To keep everything running smoothly, your network performance should well exceed your storage performance. Avoid network bottlenecks by aiming for at least 4 GB/s download speeds. Keep in mind that cloud providers often have limits on single flows – for instance, even if your instance has a maximum advertised speed of 20 gigabits down, they may throttle that to just 5 gigabits for a single flow. It’s crucial to use multiple flows (source IP, port, destination IP, protocol) to truly maximize your network throughput.

This leads to the question, “Why is it so slow?” It could be because all that network capacity is being squandered on a single flow. Stack several SSDs together using RAID 0 for better throughput. Remember, the SSD tiers should not be about durability; reserve that for the origin tier to prevent data loss.

As for our setup, we’ve found a specific AWS instance that we utilize quite a bit. It has eight GPU devices and can handle around 5 GB/s down speeds, along with ample SSD storage.

FUSE settings
-------------

Another area where we did a lot of optimization work was the FUSE settings. Starting out, we saw a peak read speed of only 800 MB/s, which was far below the potential of the NVMe SSD and network capabilities.

### Read-ahead settings

After extensive testing, we upped our read-ahead settings significantly. Read-ahead in the kernel allows it to request future pages when it gets a request for a specific page. By default, Linux sets this at 128 KB – which is merely a handful of 4 KB pages. For high-latency situations, this default is insufficient and could cause a fallback to the network. We’ve set our read-ahead to 32 MB, which has worked wonders for us.

However, be cautious. You can’t simply crank up this value indiscriminately. We once shipped a version with a read-ahead set at 1 GB, which resulted in disastrous latency issues in production. If the kernel is instructed to read a gigabyte of data on a single request, it doesn’t do well.

### FUSE request size

Another thing you can think about is FUSE request size. Each READ request from the kernel to the userspace FUSE server has a size. Our original setting was 128 KB. Our goal was to increase that to 1 MB for peak throughput—fewer, larger requests are more efficient. Unfortunately, we couldn’t push this higher without enabling direct IO.

It’s worth noting that enabling direct IO poses a challenge: it disables the page cache. Any write with direct IO goes straight past the page cache, and any read does the same. Personally, I appreciate the page cache—it works really well for coalescing reads and serving a larger amount of memory. It’s a good idea to keep that in play.

### Congestion threshold and background threads

The congestion and max background settings are some other knobs to tune.

The FUSE system in the kernel effectively processes requests using either a synchronous path or in the background. This dual approach is essential; some operations require immediate attention, while others can afford to be buffered. For standard read requests, we opt for buffering.

When configuring a FUSE server for a specific read and write pattern, one might not change the default configuration settings. However, since our system is a read-only file system and heavily read-oriented, we must maximize the queues in the background. This ensures that we don’t throttle the guest system, especially if they’re experiencing a backlog with their read requests. Under these circumstances, only synchronous requests will proceed smoothly. An example of a synchronous request could involve fetching file metadata.

Don’t blow up your heap!
------------------------

![fuse-heatmap](/_app/immutable/assets/fuse-heat-map.DzyPEVCM.jpeg)

We did some profiling work and found significant memory allocation issues. The flame graph indicated that about 50% of our samples were tied up in memory allocation, which is clearly not good for throughput. As it turned out, maintaining a large heap was detrimental; reducing the heap size while processing large files resulted in improved throughput.

The root cause of our heap expansion was related to how we handled reads over the network. Our goal was to cache data efficiently so that future reads would access the page cache and disk instead of hitting the network every time. However, this required us to store a substantial amount of file data in memory, ultimately leading to a bloated heap.

The solution we implemented involved deferring disk writes and streamlining memory usage. By keeping the heap small and streaming data directly from the network to the guest process without holding it in memory, we could significantly enhance performance. We estimated this adjustment increased throughput by around 100 to 200 MiB/s.

Summary
-------

To summarize, we took a focused approach on the performance of the FUSE file service, specifically targeting its read throughput when dealing with single files. Key strategies included going big on SSD and network capabilities, implementing techniques like read-ahead and prefetching, and balancing read and write I/O operations.

With these implemented changes and some additional tweaks we haven’t explored, we saw throughput go up to approximately 2.5 GiB/s, enabling us to load a 512 MiB
`.safetensors`
file in just 200 milliseconds from disk cache and about 300 milliseconds from the network.

**This article is adapted from a talk given by Jonathan Belotti at the NYC Systems meetup in August 2024. You can watch the full talk below.**

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/lemon-slice-case-study
================================================================================

How Lemon Slice built real-time generative video with Modal and Daily
=====================================================================

Video generation has been all the rage, and
[Lemon Slice](https://lemonslice.com/)

is on a mission to build the most expressive character generation models in the world. Lemon Slice has used Modal from the very beginning, and we’re excited to support their latest product: real-time video conversations with AI characters. Real-time is hard to accomplish, but with Modal and Daily’s low-latency primitives, it takes only seconds to get from user input to avatar response.

Prologue: scalable inference for a 1B parameter video model
-----------------------------------------------------------

Lemon Slice’s first
[viral product](https://news.ycombinator.com/item?id=41467704)

was a site where you could generate a video of a character speaking by inputting an image of the character plus text or audio.

![lemon slice product](https://modal-cdn.com/lemon-slice-product.gif)

See
<https://lemonslice.com/gallery>

for full quality examples with audio

From the get-go they knew they wanted to set up their production inference pipeline on Modal. The founders, who came from ML research backgrounds, had previously built out custom infrastructure on AWS and GCP. This was not something they were eager to repeat—from configuring instances to working across compute regions to building out scaling logic, it was going to be a big distraction from shipping new AI products to market.

![architecture without modal](https://modal-cdn.com/cdnbot/tmpfse31kxs_ebd718cf.webp)

Without Modal, the Lemon Slice team would have to set up and manage multiple infra services.

With Modal, they were able to scale to 10,000 requests per hour just by writing two Modal Functions in Python: one that was a REST endpoint for user input and another that ran inference on their video model. This solution:

* allowed them to avoid setting up an ECS cluster, job queue system, and load balancer.
* autoscaled efficiently, as Modal would spin GPU containers up and down for video inference based on request volume.
* came with performance optimizations like
  [memory snapshotting](/docs/guide/memory-snapshot)

  built in, cutting down container initialization time.

![architecture with modal](https://modal-cdn.com/cdnbot/tmp_ukux39z_f81a710f.webp)

With Modal, the Lemon Slice team writes Python functions and Modal manages the infrastructure components.

Modal’s autoscaling also sped up their model eval process. Every few hours when a new model checkpoint was created, 50+ sample videos could be generated in parallel, providing a quick slate of outputs for the team to assess.

“

Modal allowed us to go from an idea on Monday to something live on Tuesday. From 70% of an engineer’s time spent in infrastructure to like, less than 10%, that’s a huge delta.

”

— Lina Colucci,

CEO and Co-founder, Lemon Slice

Now let’s make it real-time
---------------------------

Lemon Slice’s newest product,
[Lemon Slice Live](https://lemonslice.com/live)

, lets users video chat with AI characters. The real-time component to the user interaction adds a new level of complexity. To tackle this, their architecture, from user input (audio) to AI output (video plus audio), is latency-optimized throughout:

1. When a user starts a video session, two Modal Functions are invoked. One starts a Pipecat server while the other loads up the video model for inference on a GPU. Modal autoscales the containers for these Functions based on how many user sessions are live.
2. When the user speaks, data starts flowing through the Pipecat pipeline.
   [Pipecat](https://www.pipecat.ai/)

   is an open-source orchestration framework that enables real-time, multi-modal AI services by chunking and processing data continuously.
3. The Pipecat pipeline calls Deepgram to turn user speech into text, Grok to get the LLM conversational response, ElevenLabs to convert that to speech, and finally the model running on the other Modal container to generate video.
4. To minimize latency between the Pipecat container and video inference container, the two containers a) communicate directly via our
   [Tunnel](/docs/guide/tunnels)

   feature, which uses TCP ports and b) are co-located using our
   [region selection](/docs/guide/region-selection)

   feature.
5. The video inference container sends frames to
   [Daily](https://www.daily.co/)

   , a global WebRTC infrastructure platform, which streams the final video and audio back to the user.

![live architecture](https://modal-cdn.com/cdnbot/tmpejx9y_k5_a57c8cf9.webp)

By combining the low latency features of Modal, Pipecat, and Daily, Lemon Slice has delivered a best-in-class character video generation product with an end-to-end video response latency of 3-6 seconds.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/livekit-modal
================================================================================

How to deploy LiveKit Agents on Modal
=====================================

![author](https://modal-cdn.com/ren-lu.jpg)

Yiren Lu

Solutions Engineer

If you are looking to build a real-time voice or video application, you can’t
just use HTTP. It’s too slow. Traditional HTTP is request-response based, creating overhead for
each interaction. Establishing new TCP connections and handshaking also creates
additional latency.

Instead, you should be using technologies like WebRTC.
[WebRTC](https://webrtc.org)

is purpose-built
for peer-to-peer audio/video streaming and data sharing without requiring
plugins or additional software.

But WebRTC is complex. It’s not easy to get right. You often have to write thousands of
lines of boilerplate code to handle connections, signalling, media
capture, peer connections, ICE candidates, STUN/TURN servers etc.

That’s why
[LiveKit](https://livekit.com)

has become so popular. LiveKit is an open-source library that abstracts away
the complexity of working with WebRTC. Rather than having to deal with all the
boilerplate yourself, you just use LiveKit’s SDK.

LiveKit Agents
--------------

Recently, LiveKit has launched a framework for building real-time voice
assistants, called
[LiveKit Agents](https://livekit.com/blog/introducing-livekit-agents/)

.

It allows you to define an AI agent that will join as a participant in a LiveKit
room.

This guide will walk you through
**deploying LiveKit agents on Modal**
using
Python. We’ll cover the
**LiveKit agent setup**
, the different configuration
options within LiveKit, and how to actually deploy on Modal.

LiveKit Agent Lifecycle
-----------------------

Here’s a high-level overview of the agent lifecycle:

* **Worker registration**
  : Your agent connects to the LiveKit server, registering as a “worker” via a WebSocket.
* **Agent dispatch**
  : When a user connects to a room, the LiveKit server selects an available worker, which then instantiates your program and joins the room. A worker can run multiple agent instances in separate processes.
* **Your program**
  : Here, you utilize the LiveKit Python SDK and can leverage plugins for processing voice and video data.
* **Room close**
  : The room closes automatically when the last non-agent
  participant leaves, and then disconnects remaining agents.

Why Deploy LiveKit Agents on Modal?
-----------------------------------

You can also deploy LiveKit Agents on
[Render](https://render.com)

,
[Kubernetes](https://kubernetes.io)

, and other cloud providers, but we think
that
[Modal](https://modal.com)

is the best option. Modal is a serverless cloud
platform and Python library. With Modal, you can write a Python function, add a
Modal decorator, and deploy your application in a container in the cloud in
seconds.

### ✅ **No Infrastructure Management**

Modal removes the complexity of managing Kubernetes clusters or provisioning cloud instances. Your LiveKit agents run in a fully managed environment with zero operational overhead.

### ✅ **Automatic Scaling**

With Modal, you can scale your LiveKit workloads dynamically based on demand. Modal’s serverless execution model ensures you only pay for what you use.

### ✅ **Optimized GPU Execution**

If your agent needs to run deep learning models, Modal supports running your workloads on GPUs like
**NVIDIA H100s**
.

Prerequisites
-------------

To run the following code, you will need:

1. A
   [LiveKit](https://livekit.com)

   account
2. A
   [Modal](https://modal.com)

   account
3. Accounts with the different AI API providers you want to use (
   [OpenAI](https://openai.com)

   ,
   [Cartesia](https://cartesia.com)

   ,
   [Deepgram](https://deepgram.com)

   , etc.), along with their API keys.
4. Run
   `pip install modal`
   to install the modal Python package
5. Run
   `modal setup`
   to authenticate (if this doesn’t work, try
   `python -m modal setup`
   )
6. Copy the code below into a file called
   `app.py`
7. Run
   `modal run app.py`

Setting Up LiveKit Agents on Modal
----------------------------------

### **Step 1: Adding Secrets in Modal Dashboard**

Before deploying your LiveKit agent, you need to add your API keys and secrets
to the
**Modal Dashboard**
to securely store and access them.

Navigate to the
**Secrets**
section in the Modal dashboard and add the following
secrets (for example purposes, we’re using OpenAI, Cartesia, and Deepgram):

![Modal Secrets](https://modal-cdn.com/cdnbot/modal-livekit-secretsndip6awa_78ed94b0.webp)

* `LIVEKIT_URL`
  - Your LiveKit WebRTC server URL
* `LIVEKIT_API_KEY`
  - API key for authenticating LiveKit requests
* `LIVEKIT_API_SECRET`
  - API secret for LiveKit authentication

You can find your LiveKit URL and API keys under
**Settings**
>
**Project**
and
**Settings**
>
**Keys**
in the LiveKit dashboard.

* `OPENAI_API_KEY`
  - API key for OpenAI’s GPT-based processing
* `CARTESIA_API_KEY`
  - API key for Cartesia’s TTS services
* `DEEPGRAM_API_KEY`
  - API key for Deepgram’s STT services

Once added, you can reference these secrets in your Modal functions.

### **Step 2: Define the Modal Application**

We define a Modal
**App**
with a lightweight Debian-based container image, then
install the necessary Python packages.

We also pre-import libraries that will be used by the functions we run on Modal in a given image using the
`with image.imports`
context manager.

```
from modal import App, Image, Secret, fastapi_endpoint, FunctionCall, Dict
import asyncio

image = Image.debian_slim().pip_install(
    "livekit>=0.19.1",
    "livekit-agents>=0.12.11",
    "livekit-plugins-openai>=0.10.17",
    "livekit-plugins-silero>=0.7.4",
    "livekit-plugins-cartesia==0.4.7",
    "livekit-plugins-deepgram==0.6.19",
    "python-dotenv~=1.0",
    "cartesia==2.0.0a0",
    "fastapi[standard]",
    "aiohttp",
)

app = App("livekit-example", image=image)

# Create a persisted dict - the data gets retained between app runs
room_dict = Dict.from_name("room-dict", create_if_missing=True)

with image.imports():
    from livekit import rtc
    from livekit.agents import AutoSubscribe, JobContext
    from livekit.agents.worker import Worker, WorkerOptions

    from livekit.agents import llm
    from livekit.agents.pipeline import VoicePipelineAgent
    from livekit.plugins import openai, deepgram, silero, cartesia
```

### **Step 3: LiveKit Agent Entrypoint**

Define the entrypoint function that connects the agent to a LiveKit room:

```
async def livekit_entrypoint(ctx: JobContext):
    print("Connecting to room", ctx.room.name)
    await ctx.connect(auto_subscribe=AutoSubscribe.AUDIO_ONLY)
    participant = await ctx.wait_for_participant()
    run_multimodal_agent(ctx, participant)
```

This function:

* Connects to a LiveKit room
* Subscribes to
  **audio-only**
  streams
* Waits for a participant to join
* Starts a multimodal AI-powered agent

### **Step 4: Running the Multimodal AI Agent**

Finally, we define a multimodal agent that uses Deepgram’s API for
**speech
recognition (STT)**
, OpenAI’s GPT-4o-mini for
**the large language model (LLM)**
,
and Cartesia’s TTS for
**text-to-speech (TTS)**
to process voice interactions.
You can also use
[other LLMs and TTS services](https://docs.livekit.io/agents/integrations/overview/)

- LiveKit supports a wide range of
plugins.

```
def run_multimodal_agent(ctx: JobContext, participant: rtc.RemoteParticipant):
    print("Starting multimodal agent")

    initial_ctx = llm.ChatContext().append(
        role="system",
        text="You are a voice assistant created by Modal. You answer questions and help with tasks."
    )

    agent = VoicePipelineAgent(
        vad=silero.VAD.load(),
        stt=deepgram.STT(model="nova-2-general"),
        llm=openai.LLM(model="gpt-4o-mini"),
        tts=cartesia.TTS(),
        chat_ctx=initial_ctx,
    )
    agent.start(ctx.room, participant)
```

### **Step 5: Handle LiveKit Web Events on room creation and deletion**

LiveKit can be
[configured to send webhooks](https://docs.livekit.io/home/server/webhooks/)

upon different events, like when a
room is started or finished.

To handle these events, we use Modal’s
`@fastapi_endpoint`
decorator to create a
FastAPI endpoint that listens for these events. Upon room creation, we spawn a
container in Modal to run the LiveKit worker. Upon room completion, the function
is cancelled and the Modal function is spun down. What this means is that you
are only charged for when a room is actually open and running.

```
@app.function(image=image)
@fastapi_endpoint(method="POST")
async def run_livekit_agent(request: dict):
    from aiohttp import web

    room_name = request["room"]["sid"]

    ## check whether the room is already in the room_dict
    if room_name in room_dict and request["event"] == "room_started":
        print(
            f"Received web event for room {room_name} that already has a worker running"
        )
        return web.Response(status=200)

    if request["event"] == "room_started":
        call = run_agent_worker.spawn(room_name)
        room_dict[room_name] = call.object_id
        print(f"Worker for room {room_name} spawned")

    elif request["event"] == "room_finished":
        if room_name in room_dict:
            function_call = FunctionCall.from_id(room_dict[room_name])
            # spin down the Modal function
            function_call.cancel()
            # delete the room from the room_dict
            del room_dict[room_name]
            print(f"Worker for room {room_name} spun down")

    return web.Response(status=200)
```

### **Step 6: Running the LiveKit Worker**

Next, we define a Modal
**function**
that runs the LiveKit worker. We specify
that we want to run this function (i.e. the LiveKit worker) with a GPU. We also
want to handle the case where the worker is cancelled, whereupon it will receive
a cancellation signal and clean up.

```
@app.function(
    gpu="A100", timeout=3000, secrets=[Secret.from_name("livekit-voice-agent")]
)
async def run_agent_worker(room_name: str):
    import os
    print("Running worker")

    worker = Worker(
        WorkerOptions(
            entrypoint_fnc=livekit_entrypoint,
            ws_url=os.environ.get("LIVEKIT_URL"),
        )
    )

    try:
        await worker.run()  # Wait for the worker to finish
    except asyncio.CancelledError:
        print(f"Worker for room {room_name} was cancelled. Cleaning up...")
        # Perform cleanup before termination

        await worker.drain()
        await worker.aclose()
        print(f"Worker for room {room_name} shutdown complete.")
        raise  # Re-raise to propagate the cancellation
    finally:
        await worker.drain()
        await worker.aclose()
```

### **Step 7: Deploy the Modal App**

With all this code in an
`app.py`
file, we can deploy both the Modal function
and the FastAPI endpoint by running
`modal deploy app.py`
.

In stdout, you’ll see the URL of the FastAPI endpoint, which you need to copy
and add to the LiveKit dashboard as the webhook URL.

![settings webhooks](https://modal-cdn.com/cdnbot/livekit-webhooksiceyins6_203427cc.webp)

### **Step 8: Spinning up a LiveKit frontend**

LiveKit provides a frontend Sandbox that you can use to test your agent.

![LiveKit Sandbox](https://modal-cdn.com/cdnbot/livekit-sandboxab5gr6cw_081613f6.webp)

Go to the
**LiveKit dashboard**
>
**Sandbox**
>
**Voice assistant**
. You should be able to
instantiate a voice assistant frontend sandbox. Since you have deployed your
agent with the appropriate
`LIVEKIT_URL`
, the frontend sandbox will automatically connect to
your agent.

Conclusion
----------

LiveKit Agents allows developers to build real-time voice assistants with
minimal effort.

And the best way to deploy is with Modal!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/llama-3-1-api
================================================================================

How to run Llama 3.1 as an API
==============================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

[Llama 3.1](https://www.llama.com/)

is Meta’s latest family of large language models (LLMs) that are quickly becoming the standard in the open-source LLM space. Llama 3.1
comes in three sizes (8B, 70B, 405B) as well as a fine-tuned Instruct version that is more optimized for instructions and dialogue.

Serving Llama 3.1 as an API requires significant compute, especially if you are using the 405B version. This guide will walk you through how to do this on
[Modal’s](https://modal.com)

serverless compute platform, giving you access to the latest GPUs (like A100s and H100s) while only paying for what you use.

How to serve Llama 3.1 8B as an API
-----------------------------------

Check out our
[detailed example](/docs/examples/vllm_inference)

which uses open-source serving framework
[vLLM](https://docs.vllm.ai/en/latest/)

to serve Llama 3.1 8B in OpenAI-compatible mode.

To run this example, you first need to
[create a Modal account](/docs/guide#getting-started)

and clone our
[examples repo](https://github.com/modal-labs/modal-examples)

.

How to serve Llama 3.1 70B as an API
------------------------------------

You can edit the linked example above to download the 70B model instead of the 8B. You will likely also have to boost your GPU VRAM settings; I’d recommend starting with 2 A100 80GB GPUs.

How to serve Llama 3.1 405B as an API
-------------------------------------

This is the largest Llama 3.1 model and requires even more VRAM. Check out our
[full guide](/blog/how_to_run_llama_405b_article)

and corresponding
[gist](https://gist.github.com/charlesfrye/fd595d21e2d483cb71ace23bde6430c0)

.

Pricing
-------

Modal’s
[pricing](https://modal.com/pricing)

is usage-based. For example, if you use two A100 80GB GPUs for 10 minutes, at a rate of $4.75/h that would cost you $4.75 • 2 gpus • 1/6 = $1.58.

In production, Modal containers automaticaly spin down when there is no usage and auto-scale when there is, giving you a nice balance between cost and performance.

Fine-tuning
-----------

The biggest reason to choose Llama 3.1 as your LLM is their generous
[community license](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE)

. You can fine-tune Llama 3.1 and serve your model as a commercial product.
Modal’s generic compute platform makes it a great choice for fine-tuning LLMs especially compared to other API providers that often offer very limited fine-tuning options. Check out our
[LLM fine-tuning example](/docs/examples/llm-finetuning)

to learn more.

Bottom line
-----------

For production-grade LLM inference, it’s hard to go wrong with Llama 3.1. Combined with open-source serving framework vLLM and Modal’s serverless compute platform, you can easily build a Llama 3.1 API to serve your LLM use cases at scale, all at a cost-effective price point.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/llama-human-eval
================================================================================

Beat GPT-4o at Python by searching with 100 dumb LLaMAs
=======================================================

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

AI Engineer

![author](https://modal-cdn.com/howard-halim.jpg)

[Howard Halim

@HowardHalim](https://twitter.com/HowardHalim)

Software Engineer

[View on GitHub](https://gist.github.com/charlesfrye/27f25188dbbcfdf20a83c0230020fe05)

> One thing that should be learned from the bitter lesson is the great
> power of general purpose methods, of methods that continue to scale
> with increased computation even as the available computation becomes
> very great. The two methods that seem to scale arbitrarily in this way
> are
> *search*
> and
> *learning*
> .
>
> Richard Sutton,
> *The Bitter Lesson*

The eponymously distasteful take-away of Richard Sutton’s
[essay](http://www.incompleteideas.net/IncIdeas/BitterLesson.html?ref=blog.heim.xyz)

has often been misconstrued: because scale is all you need, they say, smaller models are doomed to irrelevance. The rapid increase in model size above one trillion parameters and the technological limitations of GPU memory together seemed to foreclose on economical frontier intelligence anywhere except at an oligopoly of intelligence-as-a-service providers. Open models and self-serve inference were in retreat.

But as the quote above indicates, there are in fact two arrows in the scaling quiver: learning and search. Learning, as we do it now with neural networks, scales with
*memory*
at inference time — larger models perform better, ceteris paribus, because they can extract more data from their training set into more
[circuits](https://transformer-circuits.pub/)

and more
[templates](https://arxiv.org/abs/2305.18654)

. Search scales smoothly with
*compute*
at inference time — compute that can be spent on either producing higher quality candidates or on producing more candidates. In the ideal case, the scaling behavior can be predicted via so-called scaling laws.

Recent papers indicate that generative models like LLMs can be scaled up with search. The
[Large Language Monkeys](https://arxiv.org/abs/2407.21787)

paper, published on arXiv by Brown, Juravsky, and co-authors last week, includes several results in this vein and indicates that frontier-level intelligence in certain domains can be elicited from smaller models that can run on a single, past-generation GPU.
Further, they observed smooth, predictable improvement of performance with scale.

Put more simply: where before, it seemed frontier capabilities required
[one horse-sized duck](https://knowyourmeme.com/memes/horse-sized-duck)

, it is clear we can now alternatively get them with one hundred duck-sized horses (or, rather, LLaMAs).

This weekend, we set out to replicate this finding.

Scaling LLaMA 3.1 8B HumanEval on Modal
---------------------------------------

Running all of our experiments, including configuration and testing, cost well under $50.

You can find our code
[here](https://gist.github.com/charlesfrye/27f25188dbbcfdf20a83c0230020fe05)

. You can run it yourself without exceeding the $30/month in credits included in
[Modal’s free tier](/pricing)

.

### Metrics and data: HumanEval and pass@k

We picked a dataset that was not covered in the Large Language Monkeys paper:
[HumanEval](https://huggingface.co/datasets/openai/openai_humaneval)

, a somewhat misleadingly-named dataset of Python function specifications and their tests from OpenAI.

The existence of these tests is crucial to enabling search. Any candidate solution can be evaluated by running it against the tests — no humans are required to evaluate HumanEval. That means correctness can be assessed objectively, with none of the
[issues that bedevil LLM-as-judge approaches](https://arxiv.org/abs/2406.18403)

. The Large Language Monkeys paper further indicates that majority-voting and other techniques fall off their scaling laws quickly.

We set out to demonstrate that by running LLaMA 3.1 8B Instruct many times, we could match or exceed GPT-4o’s performance on HumanEval. Performance is measured via the “pass@k” metric: the chance that out of the k programs produced by the LLM, at least one will pass the tests. We consider also “fail@k”, the chance that no program will pass the tests, which is always 1 - pass@k.
[Result aggregator PapersWithCode reports](https://paperswithcode.com/sota/code-generation-on-humaneval)

GPT-4o’s pass@1 performance as 90.2% (0-shot, taken from Claude 3.5 Sonnet evals), so that was our target.

### Infrastructure: LLM inference

We ran our experiments with
[Modal’s serverless GPUs](/docs/guide/gpu)

. Smaller models are generally more compatible with a serverless approach because they can be more rapidly loaded from remote storage or disk. The arithmetic throughput of GPUs is many orders of magnitude greater than the read throughput of disks (
[H100 FLOP throughput is measured in PB/s](https://resources.nvidia.com/en-us-tensor-core/nvidia-tensor-core-gpu-datasheet?ncid=no-ncid)

), so it is in general a good idea to trade more computing time for less loading time. Of course, that means you want to make sure that setup time is as fast as possible, as we’ve done at Modal by
[rewriting the container stack](https://www.youtube.com/watch?v=3jJ1GhGkLY0)

.

Our experiments were enabled by the open source
[vLLM inference server software](https://docs.vllm.ai/en/latest/)

. Follow-up on promising
[initial research](https://arxiv.org/abs/2306.03081)

into scaling out search with LLMs last year was slowed by the need to implement performant caching mechanisms. These mechanisms are now a standard part of inference servers,
[pioneered by vLLM](https://charlesfrye.github.io/programming/2023/11/10/llms-systems.html#paged-memory)

. Caching ensures that token sequences that are repeatedly processed (like the prompt whose solution is being searched for) incur only constant cost with respect to search scale. Executing batch inference was as simple as changing the
`n`
parameter in our generic OpenAI-compatible client’s ChatCompletion requests to a vLLM server running in OpenAI-compatible mode on Modal. Check out
[this guide](/docs/examples/vllm_inference)

to running vLLM on Modal for more details.

We scaled up to ten A100-40GB GPUs and hit ~40,000 output tokens per second without particular attention to tuning — a decided benefit of vLLM over other (
[nominally more performant](https://bentoml.com/blog/benchmarking-llm-inference-backends)

) inference servers. This scale is compatible with
[Modal’s free tier](/pricing)

, but enterprises running on Modal can easily scale at least two orders of magnitude higher, or 4,000,000 output tokens/second. With our new
[reduced prices](https://modal.com/pricing)

, that’d cost roughly $0.25 per million output tokens, competitive with dedicated inference-as-a-service providers — plus greater control over your deployment.

### Infrastructure: Evaluation

Evaluating the model’s output requires executing arbitrary Python code, which means we need a technique for secure isolation. That would be a tricky proposition for a platform that offers inference-as-a-service or serverless GPUs alone. Good thing we have
[Modal Sandboxes](/blog/prompt-olympics-summer-2024)

! Sandboxes use the same fast-booting, secure containerization technology that powers the rest of Modal, but provide a simple interface for dynamic creation and teardown in the middle of program execution.

Again restricting ourselves to the concurrency limits of
[Modal’s free tier](/pricing)

, we were able to run ~3,000 tests in parallel (32 workers per node on 100 nodes). This was more than sufficient for our needs, so we didn’t press further on scaling evaluation.

Matching and exceeding GPT-4o’s performance
-------------------------------------------

We were able to replicate the core results of the
[Large Language Monkeys paper](https://arxiv.org/abs/2407.21787)

with a new model (they used the LLaMA 3 series, we used LLaMA 3.1) and a new dataset (they showed results for math datasets like GSM8K and for the software engineering dataset SWE-bench, we used HumanEval).

Specifically, we found that (with minimal prompt tuning and no tuning of other hyperparameters) we could boost the performance of LLaMA 3.1 8B from 66.4% with only one generation to comparable performance with GPT-4o with 100 generations (90.5% versus 90.2%) and clearly superior performance with 1000 (95.1%).

![Results for LLaMA 3.1 8B on HumanEval pass@k demonstrating better performance than GPT-4o pass@1 for 100 or more samples](https://modal-cdn.com/cdnbot/llama3-1-8b-pass-at-k.png)

LLaMA 3.1 8B pass@k on HumanEval, for k from 1 to 1000, versus GPT-4o's reported pass@1 performance.

We also found that our results on HumanEval were smoothly predictable (“enjoy scaling laws”) across three orders of magnitude. We prefer the following presentation, which inverts “pass@k” to “fail@k” and logarithmically transforms both axes.

![Results for LLaMA 3.1 8B on HumanEval fail@k demonstrating smooth log-linear scaling across three orders of magnitude](https://modal-cdn.com/cdnbot/llama3-1-8b-fail-at-k-scaling.png)

LLaMA 3.1 8B fail@k on HumanEval, for k from 1 to 1000, versus GPT-4o's reported fail@1 performance.

Both axes are log-transformed.

Note that these results are distinct from a “replication” of the original paper’s results in the strict sense. Instead, these are a replication of the core claim, which is that, when augmented with search, smaller models can outperform bigger ones in a predictable way. We consider that a stronger signal for the industrial relevance of the underlying work than replication sensu stricto.

What’s next?
------------

Search is a powerful technique for improving intelligent systems that has been relatively under-appreciated in this past decade focused on (deep) learning.

Search is powerful precisely because it can be transparently scaled. In the words of Richard Sutton, search “continue[s] to scale with increased computation even as the available computation becomes very great”. Modal is designed to make the available computation very great indeed. Search also shifts the balance of resource consumption from memory to compute, which has, due to semiconductor trends, historically been a winning move. Not coincidentally, it favors Modal’s
[serverless execution model](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf)

.

Search is enabled by high quality evaluation of outcomes. Impressive recent results in mathematics, like
[DeepMind’s AlphaProof and AlphaGeometry 2 getting a silver medal in the 2024 International Math Olympiad](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/)

, have been enabled by the translation of informal natural language mathematical problems into formal statements in Lean, which enables their detailed supervision by a proof verifier/compiler. The increased parallelization of mathematical work made possible by formalization also played a role in
[the recent verification that the fifth Busy Beaver number is 47,176,870](https://scottaaronson.blog/?p=8088)

.

By the Curry-Howard-Lambek correspondence, mathematical proofs can be identified with computer programs. We can expect similar gains in the use of generative models in programming by pairing them with compilers and test suites, as in our small experiment and in
[the original paper’s experiments on SWE-bench](https://arxiv.org/abs/2407.21787)

.

The extension of this technique to domains outside of mathematics and programming is not obvious — how do you effectively search over open-ended natural language responses to “write an email to my insurer contesting this claim denial” or “summarize this email”?. But we can loosely expect that generative models will see gains in performance in domains in proportion to those domains’ ability to precisely specify and speed up their measurement of outcomes and thence search. Agents in repeatable digital environments seem like a good frontier to target.

From this point of view, search is downstream of evaluation. Hence the
[claim](https://applied-llms.org/#build-llmops-but-build-it-for-the-right-reason-faster-iteration)

that
[many](https://hamel.dev/blog/posts/evals/)

[AI](https://youtu.be/RrDBV6odPKo)

[engineers](https://www.youtube.com/watch?v=2CIIQ5KZWUM)

[are](https://arjunbansal.substack.com/p/evolution-of-llm-agents)

[making](https://eugeneyan.com/writing/evals/)

: evaluation is the missing ingredient in the productionization and continual improvement of generative model applications.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/lora-qlora
================================================================================

LoRA vs. QLoRA: Efficient fine-tuning techniques for LLMs
=========================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Fine-tuning LLMs can be a computationally expensive and time-consuming process, especially when dealing with models containing billions of parameters. However, new fine-tuning techniques have made it possible to more efficiently fine-tune LLMs by reducing the number of parameters to update. In this blog post, we’ll explore two such techniques: LoRA and QLoRA and discuss their differences and pros and cons.

Table of contents
-----------------

* [Overview of LoRA and QLoRA](#overview-of-lora-and-qlora)
* [The problems with full fine-tuning](#the-problems-with-full-fine-tuning)
* [What is LoRA?](#what-is-lora)
* [What is QLoRA?](#what-is-qlora)
* [Which one should you use?](#which-one-should-you-use)

Overview of LoRA and QLoRA
--------------------------

|  | Full FT | LoRA | QLoRA |
| --- | --- | --- | --- |
| GB VRAM\* (Memory needed per 1GB model) | 16+ | 2+ | 0.5+ |
| % Params trained | ~100% | 0.5-5% | 0.5-5% |
| Speed | Slow | Fast | Slightly slower than LoRA |
| Quality | Can overfit | Stable and accurate | Can lose accuracy |

The problems with full fine-tuning
----------------------------------

Before diving into the efficient techniques, let’s briefly review the challenges of traditional full fine-tuning:

* Updates every single parameter in the base model
* Because of the large number of parameters that need to be updated, requires significant computational resources, typically 60GB+ of VRAM for a 7B parameter model
* Slow
* Prone to overfitting, especially when working with smaller datasets

What is LoRA?
-------------

LoRA, short for Low-Rank Adaptation, is a fine-tuning technique introduced by Microsoft researchers in their paper
[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)

.

The main idea behind LoRA is that instead of updating all the pre-trained weights, you freeze them and train smaller “adapter” matrices that represent the update to the base model.

In a standard neural network layer, we have:

```
Y = WX + b
```

Where:

* W is the weight matrix
* X is the input
* b is the bias
* Y is the output

LoRA modifies this as follows:

```
Y = (W + BA)X + b
```

Where:

* W is the frozen pre-trained weight matrix
* B and A are low-rank matrices, which means that they are “smaller” than the original W matrix and can be stored more efficiently
* BA is the product of these matrices, representing the update to W

### Upshot

* Only a small number of parameters (in A and B) need to be trained
* Uses way less VRAM, and most of the VRAM requirement is for loading the base model, not for training
* Can result in less overfitting compared to full fine-tuning
* Can be applied selectively to certain layers or components of the model
* Multiple LoRA modules can be trained for different tasks and swapped out as needed
* Can use a higher learning rate due to the smaller number of parameters

What is QLoRA?
--------------

QLoRA, or Quantized LoRA, is an extension of the LoRA technique that further reduces the memory footprint of fine-tuning by quantizing the low-rank matrices. Introduced in the paper
[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

, QLoRA applies post-training quantization to the A and B matrices, converting them from 32-bit floating-point numbers to lower-precision representations, such as 8-bit integers.

By quantizing the low-rank matrices, QLoRA achieves a 4x reduction in memory usage compared to standard LoRA, making it possible to fine-tune even larger models on resource-constrained devices.

### Upshot

* Further reduces the memory footprint of fine-tuning
* Can lead to a loss of knowledge and a lower-quality fine-tune, but not necessarily. Sometimes the quantization actually reduces overfitting.
* The loss of knowledge is also mitigated because the adapters are generally not quantized - it’s the base model that will suffer in performance

Which one should you use?
-------------------------

* If you have access to hardware with enough space,
  **use LoRA**
  . Refer the table below for a rough estimate of the memory requirements for different model sizes.

| Method | Bits | 7B | 13B | 30B | 70B | 110B | 8x7B | 8x22B |
| --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Full | Amp | 120GB | 240GB | 600GB | 1200GB | 2000GB | 900GB | 2400GB |
| Full | 16 | 60GB | 120GB | 300GB | 600GB | 900GB | 400GB | 1200GB |
| LoRA | 16 | 16GB | 32GB | 64GB | 160GB | 240GB | 120GB | 320GB |
| QLoRA | 8 | 10GB | 20GB | 40GB | 80GB | 140GB | 60GB | 160GB |
| QLoRA | 4 | 6GB | 12GB | 24GB | 48GB | 72GB | 30GB | 96GB |
| QLoRA | 2 | 4GB | 8GB | 16GB | 24GB | 48GB | 18GB | 48GB |

* If you don’t have enough space, for example, if you only have access to a free T4 on Google Colab,
  **try qLoRA**
  .

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/lovable-case-study
================================================================================

Powering 250,000 Lovable app creations in a weekend
===================================================

[Lovable](https://lovable.dev/)

is one of the fastest growing startups in history, having hit $75M ARR a mere 7 months after they launched. Lovable is on a mission to build the “last piece of software,” enabling anyone to create fully functional applications without writing code. Users enter a prompt describing the app they want, and Lovable generates the code, spins up the application, and enables real-time visual edits. Lovable uses
[Modal Sandboxes](/docs/guide/sandbox)

at massive scale to run LLM-generated code for thousands of apps in safe and isolated sandboxes.

![lovable](https://modal-cdn.com/blog/images/lovable.gif)

The problem: performance and reliability at scale
-------------------------------------------------

In June 2025, Lovable had a major promotional weekend event coming up, in collaboration with Anthropic, OpenAI, and Google. This event was set to dramatically increase user activity.

Unfortunately, Lovable’s existing code sandbox provider—a distributed cloud VM platform—raised concerns around scalability and operational risk. The team wasn’t confident the provider could scale fast enough to meet the expected demand, and with only this single vendor in place, Lovable was exposed to high risk of service disruption.

“

It was pretty clear when I joined that our biggest pain point was sandbox infrastructure. Any outage would essentially take us offline.

”

— Erik Lindblad,

Senior Infrastructure Engineer

Build vs. buy
-------------

Lovable explored building their own sandbox solution on AWS and Kubernetes. This direction presented its own challenges, from learning the intricacies of AWS identity management to needing to create their own pod provisioning system. While they eventually built a system that scaled up to meet existing demand, they knew that they would need to invest much more time to support the current customer growth trajectory.

With this in mind, they started evaluating other vendors, including Modal.

Proving out Modal’s reliability and speed
-----------------------------------------

Lovable’s key criteria were reliability, scalability, and fast sandbox startup times.

“

There’s tons of cheap providers for sandboxes. But as soon as you start scaling up, more and more of these vendors have trouble serving you with capacity. We wanted to find a vendor that could scale to where we needed to go for multiple years.

”

— Erik Lindblad

Lovable was able to get a proof of concept on Modal up and running in just days. They went from 15,000 lines of sandbox orchestration code with their previous provider to just 700 lines with Modal! This was partly enabled by Modal’s rich networking features, like
[Tunnels](/docs/guide/tunnels)

. Each of Lovable’s Sandboxes uses an encrypted Tunnel to allow communication with the server running inside the Sandbox. Behind the scenes, Modal operates a fleet of TCP relays around the globe to ensure low latency and reliability at petabyte scale.

Modal’s reliability and stability were fully proven out during the promotional weekend. Lovable saw concurrent sessions surge by 2.5-3x, successfully enabling users to build an estimated 250,000 applications in just 48 hours. Over the course of the event, Modal ran over 1 million sandboxes, powering up to 20,000 concurrent sandboxes at peak. Lovable’s platform team wasn’t paged even once during the whole weekend.

Modal Sandboxes are now used to serve every app generation session in Lovable. Below is an app built on Lovable—try it out yourself!

![lovable game](https://modal-cdn.com/blog/images/lovable-game-small.gif)

<https://lovable-modal-adventures.lovable.app/>

Unlocking more performance gains
--------------------------------

Now that baseline reliability is no longer a concern, Lovable plans to use more Modal Sandbox capabilities like
[snapshotting](/docs/guide/sandbox-snapshots)

to make the end user experience more performant. And of course, Modal will continue to support their viral growth as they push the limits of infrastructure at scale.

“

We've previously managed to break services like GitHub because of our load, so when Modal was able to handle the massive scale of our AI weekend event so smoothly, that meant a lot. We now trust Modal to keep up with our growth, and we're excited to build together in the long term.

”

— Anton Osika,

Founder and CEO

Interested in a deep dive on how Modal Sandboxes fit in to Lovable’s architecture? Stay tuned for Part 2!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/mem-snapshots
================================================================================

Memory Snapshots: Checkpoint/Restore for Sub-second Startup
===========================================================

![author](https://modal-cdn.com/jonathon-belotti.png)

[Jonathon Belotti

@jonobelotti\_IO](https://twitter.com/jonobelotti_IO)

Member of Technical Staff

Modal is a serverless GPU container runtime designed to scale from zero. We run our worker fleet and our users’ Functions lean. If there’s idle capacity we aim to cut it. This means that if additional load comes into a Function, we often need additional containers to start up and serve requests. Cold start latency occurs when a request is waiting for a container to start up, and our customers hate it.

We hate it too! Thankfully, with the introduction of
*memory snapshot*
restores, cold start latency on user Functions can be more than halved!

### import torch (eCDF)

import\_torch\_snapshot

import\_torch

What’s a memory snapshot?
-------------------------

A Modal memory snapshot is a couple of files that represent the entire state of a Linux container
*right before*
it was about to accept a request. We capture the container’s filesystem mutations and its entire process tree. Each process in that tree has state consisting of its memory mappings, file descriptor table, registers, environment variables, process ID, and more! It’s a party and everyone’s invited.

![Diagram of process component state that is saved to disk (credit Tristan Hume)](https://modal-cdn.com/cdnbot/modal-snapshot-process-diagramtq1ww54b_99ced4b9.webp)

All that captured state allows for the complete restore of a Python program, with the notable exception of live network connections and NVIDIA GPU state (discussed below).

Container snapshotting functionality is an outgrowth of the the ‘checkpoint/restore in userspace’ (CRIU) kernel technology. The gist is that you can checkpoint (save) a Linux container and restore it at a later time, even on a different computer. This functionality had existed for Linux
*VMs*
since at least 1999’s VMWare Workstation product, but as we know containers weren’t ‘a thing’ until around 2009. CRIU was first presented in 2011 by some
[“mad Russians”](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=099469502f62fbe0d7e4f0b83a2f22538367f734)

as a method for live migration of containers between physical servers, and has over time proved itself a
[handy technique](https://ipads.se.sjtu.edu.cn/_media/publications/catalyzer-asplos20.pdf)

for reducing serverless cold starts.

CRIU is developed for the
`runc`
container runtime, but for security reasons Modal uses the gVisor container runtime
`runsc`
(short for “run Sandboxed Container”). While
`runc`
has containers run on the host kernel, gVisor implements a userspace kernel and gives the guest container access to that. This has obvious implications for container snapshotting. While a
`runc`
snapshotting solution cooperates with the host Linux kernel to save container state—the kernel exposes, via /proc VFS, details about a process’s memory maps, open files, children processes— gVisor controls the userspace kernel serving a snapshotted container guest.

![Timeline showing when Linux, LXC, CRIU, and gVisor were released](https://modal-cdn.com/cdnbot/mem-snapshots-blog-post-289iuooft_6e5a069b.webp)

So while gVisor’s checkpoint/restore functionality was developed many years after CRIU, it’s actually quite like pre-CRIU solutions which involved customizing the Linux kernel (ie. ‘container restore in kernelspace’). gVisor’s core
`kernel.go`
file contains checkpoint/restore code and at least eighteen system components implement checkpoint/restore functionality in
`save_restore.go`
files. You can make a lot of checkpoint/restore specific kernel customizations if you reimplement the Linux kernel in userspace!

Performance: Checkpoint/restore vs. lazy loading
------------------------------------------------

It’s not obvious why restoring a container snapshot with gVisor’s
`runsc restore`
would be so much faster than a standard
`runsc run`
container startup.

The main reason is that Python’s import system is filesystem-based and needs to execute thousands of slow, sequential filesystem operations in order to become ready to perform useful work.

It’s
*thousands*
because even just importing
`torch`
in Python executes 26,000 syscalls! It’s slow because there’s a couple layers of indirection between the container’s filesystem and the actual file data. A Modal container filesystem is an
[OverlayFS](https://en.wikipedia.org/wiki/OverlayFS)

filesystem where the read-only lower is a FUSE-based lazy loading file server, which means that every file read incurs some overhead.

```
overlay on /mnt/overlay type overlay (
    rw, relaltime,
    upperdir=/tmp/tmph45cav46/upper,
    lowerdir=/tmp/tmph45cav46/imagefs_fuse,  << lazy-loading file server
    workdir=/tmp/tmph45cav46/work
)
```

The fastest code is the code that never runs, and so fast container startup is mostly about laziness, work avoidance. Basically, not doing work where it’s not needed. Almost every container on Modal has thousands of files in
`/usr/share/doc`
, but ~zero of our users’ programs actually need to read those files. So we don’t load them.

For more detail on our lazy-loading container filesystem, see
[Fast, lazy container loading in Modal.com](https://modal.com/blog/jono-containers-talk)

.

Despite cutting out heaps of eager file I/O with lazy loading, importing
`torch`
still executes 26,000 syscalls, context switching between the caller, the kernel, and the FUSE server. Python’s filesystem-based, syscall heavy module loading is just too slow. So we turn to checkpoint/restore, which turns thousands of syscalls into (roughly) a single file load, recreating the process’s memory mappings directly rather than re-running through the Python import system and all other application code executed during container startup.

A single file load?
-------------------

![Diagram showing the simplified restore problem, where Python process memory mappings are served from FUSE via gVisor.](https://modal-cdn.com/cdnbot/mem-snapshots-blog-post-4hth1rar3_a5e507bd.webp)

The container restore process is a frenetic process of summoning and ensemble choreography, but most of the performance is won or lost on how fast the ‘main’ process’s memory mappings can be brought into the operating system’s page cache. These memory mappings are typically 100MiB-10GiB and stored in a single snapshot ‘pages’ file, referring to the 4KiB page (or huge pages) of the virtual memory system.

When restoring, gVisor does not need to wait to read the entire ‘state’ file into memory before allowing the restored container to progress. Instead it can read the restored processes’ pages
[in the background](https://github.com/google/gvisor/commit/41f01d8f9c5aee4f7a31ec6183fb50bbc6f9b851)

, prioritizing those pages which a restored process blocks on.

This background-loaded pages file is made available to gVisor through the same distributed, FUSE-based file serving system used for our standard container loading. To ensure the FUSE system doesn’t keep gVisor waiting when it requests a page, we aggressively preload the entire pages file into page cache as early as we can. In the worst case, the restoring process page faults and gVisor finds that the FUSE file server doesn’t already have the page in-memory, nor is the page on host disk. Thus, the restoring process is blocked waiting for the FUSE server to complete a networked file read, which takes 10s of milliseconds.

Much has been ignored by focusing only on the pages file’s loading, but it’s here the 80 in the 80/20 rule. gVisor’s prioritized, background page loading and our FUSE filesystem’s aggressive preloading cooperate to minimize aggregate page fault latency in a restoring guest process.

Now, is all this fast enough?

Performance: 2.5x faster
------------------------

We’ve found that memory snapshot restore is about 2.5x faster than a standard container startup. A Stable Diffusion inference Function that takes around 13 seconds normally restores in
*only 3.5 seconds*
. A simple
`import torch`
example program which was noted as executing 26,000 syscalls takes normally around 5 seconds to cold start. With snapshot restore it’s around 1.05 seconds at p50 and 0.69 seconds at p0!

### import torch

import\_torch\_snapshot

import\_torch

### Stable Diffusion

stable\_diffusion\_snapshot

stable\_diffusion

While restore is already much, much faster than the status quo, we can make it better. The current restore implementation is performance-constrained by how fast the virtual memory of the main process, sometimes GiBs of data, can be loaded off disk (or over the network) and into memory.

We’re observing container restore incurring CPU pressure as it eagerly loads hundreds of thousands of 4KiB pages.
[CPU stalling](https://dx13.co.uk/articles/2023/07/27/cpu-stalls/)

is going as high as 900/ms/s, indicating our container cgroup resource management needs better tuning for aggressive resource usage at startup. This is not exactly as bad as leaving the handbrake on, but kind of like straining to get a fixie bike going uphill.

We also could optimize how we preload the guest’s virtual memory pages into the host page cache. Sometimes restore files are served over the network, and while we expect our hosts to drive around 2GiB/s of network download, in the tail we’re observing much less effective throughput.

So there’s still work to be done, and you should expect the restore line above to move more to the left and get straighter as we make optimizations!

Under the hood: snapshot lifecycle
----------------------------------

Deployed Functions with memory snapshots enabled only get snapshotted on-demand, not proactively. Also, a single deployed Function version will get snapshotted
*multiple times*
, for an interesting reason.

Snapshots are created on-demand when Modal’s scheduler finds that it does not have an existing active Function snapshot available for the Modal worker host onto which it intends to place that Function.

```
snapshot_info: Optional[api_pb2.SnapshotInfo] = None
if function_proto.checkpointing_enabled:
    snapshot_info = await get_or_create_checkpoint_for_worker_and_task(
        state, ephemeral_function_struct, worker_ephemeral_struct, task_id
    )
```

Having to match a Function snapshot with a particular worker is one of the reasons a single Function version will be snapshotted multiple times. For example, the AWS g6.12xlarge instance type does not support the
`pclmulqdq`
[Perform a Carry-Less Multiplication of Quadword](http://en.wikipedia.org/wiki/CLMUL_instruction_set)

instruction and so it cannot accept any snapshot created on a host which does. Our users didn’t sign up expecting to debug Invalid Opcode exceptions!

Beyond CPU featureset compatibility concerns, memory snapshots are also sensitive to changes in NVIDIA driver version and container runtime version. Such compatibility concerns are the major reason that Modal controls the snapshot lifecycle on behalf of users, ensuring that restores consistently succeed across a dynamic and evolving worker fleet.

Tradeoffs: or, is it really that simple?
----------------------------------------

Memory snapshots significantly reduce cold starts but they also decrease simplicity. Andrew Morton
[tried to warn us](https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=099469502f62fbe0d7e4f0b83a2f22538367f734)

, but we’ve sided with the Russians and gone a bit mad. We’re freezing containers mid-execution and serializing them to disk.

The main chore ‘checkpoint/restore in userspace’ tends to require is cooperation from the guest program. Programs need to understand that they will be paused for an indefinite amount of time and resumed on a different computer, possibly with a different IP address. They also need to understand that the state they establish prior to snapshotting will be
*reused*
over and over again—expectations of entropy may be violated.

The team has already worked hard to make the Modal
[client](https://github.com/modal-labs/modal-client)

cooperative with checkpoint/restore. When tricky cases cause restore to fail (and this does happen) we automatically fallback to a standard container startup. We recommend testing outside of production before deploying a Function to production with memory snapshots enabled.

For more information on managing sharp edges, see the
[known limitations](https://modal.com/docs/guide/memory-snapshot#known-limitations)

section of the memory snapshots guide.

Interface: adopting memory snapshot speedups
--------------------------------------------

Ok, so how do you use it?

Modal Functions can turn on snapshotting using the
`enable_memory_snapshot=True`
argument and
[lifecycle methods](/docs/guide/lifecycle-functions#container-lifecycle-hooks)

can opt-in to snapshotting using the
`snap=True`
argument.

Here’s a ‘hello world’ example that just imports
`torch`
.

```
import pathlib
import modal

image = modal.Image.debian_slim().pip_install("torch")
app = modal.App(name="demo-memory-snapshot")

pathlib.Path("./foo").write_text("disk mutation")

with image.imports():
    import torch  # 26k syscalls comin' right up!

@app.function(enable_memory_snapshot=True)
def f(x: int):
    print(f"Hello from torch {torch.__version__}. You gave me {x=}")
```

The above Modal Function
`f`
will be snapshotted once it has deployed and run a handful of times in production. (We currently create snapshots on-demand, not proactively.)

The snapshot has captured the
`import torch`
global import as well as anything else that executed in global scope, such as the rootfs disk mutation. It has essentially paused and saved
*right before*
it’s about to fetch a request and feed that request into
`f`
.

This allows us to jump straight back into
`f(x)`
on restore, as shown below.

![Diagram showing the snapshot restore process](https://modal-cdn.com/cdnbot/mem-snapshots-blog-post-5-ay7ar0i9_c042c9d0.webp)

Whenever Modal detects that a redeploy has invalidated an existing snapshot of
`f`
, new snapshots are created.

### Handling GPU state

It was said above that lifecycle functions can be snapshotted, and this is important for managing GPU state. Because memory snapshots do not yet support saving GPU state to file, GPU state must be created post-restore.

```
import time
import modal

image = (
    modal.Image.debian_slim()
    .pip_install(
        "transformers", "torch", "accelerate", "safetensors",
    )
)
app = modal.App("snap-demo", image=image)

@app.cls(gpu="a10g", enable_memory_snapshot=True)
class GPT2:
    @modal.enter(snap=True)
    def load(self):
        from transformers import AutoModelForCausalLM, AutoTokenizer
        self.model = AutoModelForCausalLM.from_pretrained("/root/cache", use_cache=True)
        self.tokenizer = AutoTokenizer.from_pretrained("gpt2", use_fast=True, use_cache=True)

    @modal.enter(snap=False)
    def setup(self):
        self.model.to("cuda")

    @modal.method()
    def run(self) -> str:
        input_ids = self.tokenizer.encode("What's up?", return_tensors="pt")
        out = self.model.generate(
            input_ids.to("cuda"),
            pad_token_id=self.tokenizer.eos_token_id
        )
        generated_text = self.tokenizer.decode(out[0], skip_special_tokens=True)
        return generated_text
```

The above GPT-2 inference Function has a
`snap=True`
lifecycle method which setups the model in CPU RAM for snapshotting, and a
`snap=False`
lifecycle method to move from CPU RAM to GPU vRAM right after restore.

This demo inference Function restores 2.5x faster using memory snapshots 🏎️.

Acknowledgements
----------------

Many thanks to the
[gVisor team](https://github.com/google/gvisor)

for creating gVisor and its
[Checkpoint/Restore](https://gvisor.dev/docs/user_guide/checkpoint_restore/)

functionality. Thanks also to Luis Capelo, Colin Weld, and Matt Nappo for their work and design discussions related to memory snapshots.

If you’re interested in building fast, reliable, and heavy-duty systems for the cloud,
[Modal is hiring](https://modal.com/company)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/modal-airflow
================================================================================

Run GPU jobs from Airflow with Modal
====================================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Data Engineer

Many teams use
[Airflow](https://airflow.apache.org)

to manage multi-stage workflows. However, when scaling workflows from local to production, Airflow relies on Celery or Kubernetes, which are difficult and time-consuming to set up, especially if you need to provision GPUs for AI / ML workflows.

Modal is a much simpler way to manage GPUs

and
[containerized environments](https://modal.com/docs/guide/images)

, making it ideal for AI / ML workflows. Modal can be triggered directly from an Airflow DAG and can serve as a replacement for your Celery or Kubernetes executor. You’d get the same scalability features from those backends with the ease of installation of the Local executor.

In this blog post, we’ll show you
**how to run Modal jobs from Airflow**
:

* Install Modal in your Airflow environment
* Set your
  [Modal token](https://modal.com/docs/reference/modal.config#modalconfig)

  ID and secret in your Airflow environment
* Option 1:
  [Deploy](https://modal.com/docs/guide/managing-deployments)

  your Modal Functions and call
  [`lookup`](https://modal.com/docs/guide/trigger-deployed-functions#invoking-deployed-functions)
* Option 2: Create a custom operator that uses
  [Modal Sandboxes](https://modal.com/docs/guide/sandbox)

We’ll go through each of these steps for a simple example: a two-step data pipeline pulling
[ELI5 questions](https://www.reddit.com/r/explainlikeimfive/)

from Reddit and answering them using an LLM.

Install Modal in your Airflow environment
-----------------------------------------

We recommend you use
[Astro CLI](https://www.astronomer.io/docs/astro/cli/local-airflow-overview)

to develop Airflow locally. Astro CLI is provided by the good folks at
[Astronomer](https://www.astronomer.io)

, a fully managed Airflow platform.

To install Modal into this Airflow environment, add
`modal`
to the
`requirements.txt`
file of your
[Astro project](https://www.astronomer.io/docs/astro/cli/develop-project#add-python-os-level-packages-and-airflow-providers)

. If you don’t have an Astro project,
[download the Astro CLI](https://www.astronomer.io/docs/astro/cli/install-cli)

and run
`astro dev init`
.

If you’re using
[Astro Hosted](https://www.astronomer.io/docs/astro/astro-architecture)

, these dependencies will be included in your deployment when you run
`astro deploy`
.

Set your Modal token
--------------------

Set the following environment variables in your Airflow environment:

* `MODAL_TOKEN_ID`
* `MODAL_TOKEN_SECRET`

If you already have Modal set up locally, you can find your token id and secret values by running
`cat ~/.modal.toml`
. You can also create new token credentials from your
[Modal Settings](https://modal.com/docs/guide/workspaces#create-a-token-for-a-workspace)

.

For local development, you can set these environment variables in
`.env`
in your Astro project. When you’re ready to deploy to production, you can sync these to your production deployment with
[these steps](https://www.astronomer.io/docs/astro/manage-env-vars#manage-environment-variables-locally)

.

Option 1: Deploy Modal Functions and call via
`lookup`
------------------------------------------------------

> **Good for**
> : Existing Modal users with deployed Functions, teams wanting separation of concerns between Airflow and Modal deploy process

Let’s assume we already have a Modal App called
`example-modal-airflow`
with two Functions:

* `fetch_reddit`
  : scrapes ELI5 questions from Reddit
* `answer_questions`
  : answers lists of questions using an LLM (requires GPU, see
  [this example](https://modal.com/docs/examples/trtllm_llama)

  )

If we deploy this App to our workspace with
`modal deploy`
, we can call it directly from Airflow with
`lookup`
and
`remote`
.

```
"""
Airflow DAG using Modal lookup
"""

from airflow.decorators import dag, task
from datetime import datetime
from modal import Dict, Function

@dag(
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
    doc_md=__doc__,
    tags=["example"],
)
def example_modal_airflow():
    # create dict for storing results
    d = Dict.from_name("reddit-question-answers", create_if_missing=True)

    @task()
    def fetch_reddit(**context) -> None:
        """
        This task gets the 100 newest ELI5 questions from Reddit
        """
        # look up function in our deployment
        f = Function.from_name("example-modal-airflow", "fetch_reddit")
        questions = f.remote()
        for q in questions:
            d[q] = None  # store questions first

    @task()
    def answer_questions(**context) -> None:
        """
        Uses LLM example to answer the questions
        """
        # look up inference function
        f = Function.from_name("example-modal-airflow", "answer_questions")
        questions = list(d.keys())
        answers = f.remote(questions)

        # update dict with answers
        for i in range(len(questions)):
            d[questions[i]] = answers[i]

    # define dependencies
    fetch_reddit() >> answer_questions()

# instantiate DAG
example_modal_airflow()
```

You can run
`astro run example_modal_airflow()`
from the terminal or go to the Airflow UI to trigger the workflow manually:

![airflow_ui](https://modal-cdn.com/cdnbot/airflow-ui.png)

If we go to our Modal dashboard, we can see the run logs for each of these invocations, including GPU utilization for the LLM task:

![gpu_modal_dashboard](https://modal-cdn.com/cdnbot/gpu_modal_dashboard.png)

We’re using a
[Modal Dict](https://modal.com/docs/reference/modal.Dict)

here as intermediate storage between tasks, which is also easy to inspect for debugging purposes. We can use it to look at an example output of our pipeline directly from any Python environment:

```
>>> import modal
>>> d = modal.Dict.from_name('reddit-question-answers')
>>> for item in d.items():
...   print(item[0])
...   print(item[1])
...   break
...
ELI5 Indian metro system

The Delhi Metro, also known as the DMRC (Delhi Metro Rail Corporation), is a rapid transit system serving the city of New Delhi and its surrounding areas in India. Here's an ELI5 explanation:

**What is it?**
The Delhi Metro is a train-based public transportation system that connects various parts of the city. It's like a big, underground highway for trains that takes people from one place to another.

**How does it work?**

1. **Trains:** The Delhi Metro has 8 lines with over 225 stations, which are connected by trains that run on tracks.
2. **Lines:** There are two types of lines: Phase I (Phase 1) and Phase II (Phase 2). Phase I has 6 lines, while Phase II has 3 more lines.
3. **Stations:** Each station has platforms where passengers can board or exit the train. Some stations have multiple platforms, so you might need to check the signs to find your platform number.
4. **Fares:** You can buy tickets at ticket counters or use your smart cards (like a special kind of debit card).
5. **Frequency:** Trains run frequently, usually every few minutes during peak hours and less often during off-peak hours
```

Here are some other options for passing data between tasks:

* [Pass the data](https://www.astronomer.io/docs/learn/airflow-passing-data-between-tasks)

  directly: this uses Airflow XComs, which in turn uses the metadata database in your Airflow deployment for storage. This approach is more limited in size of data you can transmit; if you’re using Postgres, that
  [limit is 1GB](https://www.astronomer.io/docs/learn/airflow-passing-data-between-tasks#when-to-use-xcoms)

  . Meanwhile, Modal Dicts have
  [a limit of 10GB](https://modal.com/docs/guide/dicts)

  .
* Mount a
  [Volume](https://modal.com/docs/guide/volumes)

  in your Modal function and store the data there: a lot of raw data is expressed in files (e.g.
  [NYC taxi trips](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)

  ), and Volumes are a more natural way to store files and directories.

Option 2: Create a custom Operator that uses Modal Sandboxes
------------------------------------------------------------

> **Good for**
> : Existing Airflow users who are looking for an easy way to access GPUs for a task directly in their DAG code

Alternatively, you can write a
[custom Operator](https://www.astronomer.io/docs/learn/airflow-importing-custom-hooks-operators)

that uses
[Modal Sandboxes](https://modal.com/docs/guide/sandbox)

to run Python code in a container defined at runtime.

Your directory structure will look something like this:

```
├── dags/
│   └── example_modal_operator.py # DAG that calls ModalOperator and passes in the function from scripts.py
│   └── utils/
│       └── scripts.py  # Python scripts we want to run inside a Modal Sandbox
├── include/
│   └── modal_operator.py # custom operator that defines how Python functions get run in Modal Sandboxes
```

Let’s start with
`modal_operator.py`
. In Airflow, an Operator is a Python class that gets instantiated as a task when you call it in a DAG. You may already be familiar with
[`BashOperator`](https://airflow.apache.org/docs/apache-airflow/stable/howto/operator/bash.html)

or
[`KubernetesPodOperator`](https://airflow.apache.org/docs/apache-airflow-providers-cncf-kubernetes/stable/operators.html)

. Custom Operators allow you to re-use code across tasks that call the same service:

Our Operator has three initialization parameters:

* `client`
  : a
  [modal.Client](https://modal.com/docs/reference/modal.Client)

  object that reads in our token environment variables
* `fn`
  : the Python function that we want to run in a sandbox
* `sandbox_config`
  : dictionary of Sandbox parameters (e.g. image, gpus)

```
# include/modal_operator.py

from airflow.models.baseoperator import BaseOperator
import inspect
import modal

class ModalOperator(BaseOperator):
    """
    Custom Airflow Operator for executing tasks on Modal.
    """

    def __init__(self, client, fn, sandbox_config, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.client = client
        self.fn = fn
        self.sandbox_config=sandbox_config

    def execute(self, context):
        # converts the Python function object into an executable string
        fn_lines = inspect.getsourcelines(self.fn)[0]
        fn_lines.append(f"{self.fn.__name__}()")
        fn_as_string = "".join(fn_lines)

        # runs the function in a Modal Sandbox with the provided config
        sb = modal.Sandbox.create(
            "python",
            "-c",
            fn_as_string,
            client=self.client,
            app=self.app,
            **self.sandbox_config
        )
        sb.wait()
        return sb.stdout.read()
```

Next, let’s define
`fetch_reddit`
within
`scripts.py`
:

```
# dags/utils/scripts.py

def fetch_reddit():
    # import task dependencies inside of functions, not global scope
    import os
    import praw

    # Reddit client secrets that are saved as Modal Secrets
    reddit = praw.Reddit(
        client_id=os.environ["CLIENT_ID"],
        client_secret=os.environ["CLIENT_SECRET"],
        user_agent="reddit-eli5-scraper",
    )
    subreddit = reddit.subreddit("explainlikeimfive")
    questions = [topic.title for topic in subreddit.new()]
    file_path = "/data/topics.txt"
    print(f"Writing data to {file_path}")
    with open(file_path, "w") as file:
        file.write("\n".join(questions))
```

Finally, let’s put this script and our new custom Operator together in a DAG:

```
# dags/example_modal_operator.py

"""
## ModalOperator + Sandboxes example

"""

from airflow.decorators import dag
from include.modal_operator import ModalOperator
from dags.utils.scripts import fetch_reddit
from datetime import datetime
import modal
import os

@dag(
    start_date=datetime(2024, 1, 1),
    schedule="@daily",
    catchup=False,
    doc_md=__doc__,
    tags=["example"],
)
def example_modal_operator():
    reddit = ModalOperator(
        task_id="fetch_reddit",

        # pass in your Modal token credentials from environment variables
        client=modal.Client.from_credentials(
            token_id=os.environ["MODAL_TOKEN_ID"],
            token_secret=os.environ["MODAL_TOKEN_SECRET"],
        ),

        # function we import from `scripts.py`
        fn=get_reddit_questions,
        sandbox_config={
            # define Python dependencies
            "image": modal.Image.debian_slim().pip_install(
                "praw"
            ),
            # attach Modal secret containing our Reddit API credentials
            "secrets": [
                modal.Secret.from_name("reddit-secret")
            ],
            # attach Volume, where the output of the script will be stored
            "volumes": {
                "/data": modal.Volume.from_name("airflow-sandbox-vol")
            },
        },
    )

    reddit

# instantiate the DAG
example_modal_operator()
```

This DAG imports the function in our script, instantiates a Modal Client, and launches the script in a Sandbox via our custom ModalOperator.

**Note**
: We are currently working on a Modal
[Airflow provider package](https://github.com/astronomer/airflow-provider-sample)

that would allow you to install the above
`ModalOperator`
and associated Modal Connection object directly into your Airflow project.

Conclusion: Airflow + Modal help each other
-------------------------------------------

The biggest benefit of using Modal with Airflow is that it easily allows you to
**isolate your task environment**
from your Airflow environment. The current solution for this today is to stand up a complicated deploy process building Docker images, publishing to a registry, and using the KubernetesPodOperator.

For Modal users, defining custom images or attaching GPUs is as simple as a
[function decorator](https://modal.com/docs/guide/gpu)

, while Airflow adds a single control pane to oversee the lifecycle of a multi-stage pipeline. Together you get the best of both worlds: full-featured data pipeline observability and easy GPU container lifecycle management.

Are you an Astronomer and Modal customer?
-----------------------------------------

We highly encourage you to try out Modal in your Astronomer workflows as we roll out a tighter integration. Please reach out to
[support@modal.com](mailto:support@modal.com)

or
[Astronomer support](https://www.astronomer.io/contact/)

if you have any feedback and/or are interested in being a design partner with us.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/modal-kestra-article
================================================================================

Build interactive workflows using Kestra and Modal
==================================================

![author](https://pbs.twimg.com/profile_images/1805968669228310528/cCC6Fy3M_400x400.jpg)

[Anna Geller

@anna\_\_geller](https://twitter.com/anna__geller)

Product Lead, Kestra

If you’ve ever needed to process a large dataset, you know how important it is to have sufficient compute resources at your disposal. Sometimes, you need more CPUs or a bigger disk, and other times, a GPU makes all the difference. Modal makes it effortless to provision compute resources on demand by defining the infrastructure requirements directly in your Python code.

With
[Kestra](https://github.com/kestra-io/kestra)

, you can easily configure and launch Modal functions directly from the UI, even when dealing with complex, dependent configurations. This allows you to adjust input parameters or resource allocations like GPU, CPU or memory dynamically at runtime, without needing to touch the underlying code.

In this post, we’ll create a forecasting workflow using Kestra and Modal:

* [Kestra](https://kestra.io/)

  for workflow orchestration, handling interactive inputs, conditional logic, managing output artifacts, and scheduling — all from an intuitive UI.
* [Modal](/)

  for serverless compute and dependency management, allowing you to run your Python code without having to worry about building Docker images or managing cloud servers.

Our workflow will use data stored as Parquet files on Hugging Face to train a predictive model for customer orders. We’ll output the model’s predictions as a Plotly chart and optionally trigger an alert in Slack.

![architecture diagram of forecasting app on Modal and Kestra](https://modal-cdn.com/article-assets/kestra-modal-forecast-app.png)

What are Kestra and Modal?
--------------------------

### Kestra

Kestra is an
[open-source](https://github.com/kestra-io/kestra)

orchestration platform that lets you create workflows from an easy-to-use UI while keeping
[everything as code](https://youtu.be/dU3p6Jf5fMw?si=0lO1sh6JLzLRrH6L)

under the hood. You can automate scheduled and event-driven data pipelines, infrastructure builds, human-in-the-loop business processes, and internal applications written in any language. You can create those workflows from the UI using an embedded code editor that provides syntax validation, autocompletion, and built-in docs.

What makes Kestra stand out:

* **Powerful UI**
  : manage workflows across teams with varying levels of engineering expertise — low-code UI forms for business users and a full code editor for developers.
* **Everything as Code**
  : define any workflow in a simple YAML configuration and deploy it from anywhere using Terraform, CI/CD, CLI, API or the Kestra UI.
* **Git integration**
  : version control integration and revision history make it easy to track changes and roll back if needed.
* **Highly customizable inputs**
  : add strongly typed inputs that can conditionally depend on each other — Kestra shows or hides downstream inputs based on what the user has entered previously.
* **Outputs & Artifacts**
  : store and track workflow artifacts and pass data across multiple tasks and flows.
* **Plugins**
  : use one of over 500 integrations to avoid writing code from scratch for common tasks such as extracting data from popular source systems, executing SQL queries within a database, reacting to events from external message brokers or triggering external jobs or API calls.

### Modal

Modal is a serverless platform that provides the compute resources needed for your Python apps without the pain of managing dependencies, containerization, or infrastructure. You can dynamically access GPUs or CPUs to run your code, and you only pay for what you use.

What makes Modal stand out:

* **Serverless compute**
  : spin up cloud resources instantly when you need them.
* **Cost-effective**
  : pay only for the time your resources are running, down to the second.
* **Pythonic**
  : add a few Python decorators to your code to offload compute to Modal — no need to maintain CI/CD pipelines, Kubernetes manifests, or Docker images.
* **Dependency management**
  : no need to worry about Dockerfiles or virtual environments — Modal handles all infrastructure-related processes as long as you define your dependencies directly in your Python code.

Now that you know a bit more about Kestra and Modal, let’s use them to build powerful interactive workflows.

Building a forecasting workflow with Kestra and Modal
-----------------------------------------------------

In this example, we’ll build a
**time-series forecast**
to predict the order volume based on historical data. This is a timely use case just ahead of Black Friday and the holiday season! We’ll use a SARIMA model to forecast the number of orders expected over the next 180 days and visualize the results.

This workflow will be
**interactive**
, allowing users to adjust parameters such as the dataset URL, S3 bucket path, the number of CPU cores, and memory.
[The code that generates the forecast](https://gist.github.com/anna-geller/8c37a868939ea94a6c91f069dc4c215c)

will run on Modal.

Here’s a
[short video](https://youtu.be/Wucyw4gRNiQ)

showing the final result:

Run Modal from Kestra
---------------------

Before diving into the full example, we first need to launch Kestra. Follow the
[Quickstart Guide](https://kestra.io/docs/getting-started/quickstart#start-kestra)

to get Kestra up and running in 60 seconds and execute your first workflow.

### “Hello World” in Kestra

Here’s a basic code scaffold to launch a “hello-world” flow in Kestra:

```
id: modal_hello_world
namespace: company.team

inputs:
  - id: my_first_input
    type: STRING
    defaults: World

tasks:
  - id: hello
    type: io.kestra.plugin.core.log.Log
    message: Hello {{ inputs.my_first_input }} 🚀
```

Go to the UI and click on the
`Create`
button. Paste the above code and
`Save`
the flow.

Then, click the
`Execute`
button to launch the flow, and soon after you should see the output in the logs:
`Hello World 🚀`
.

![hello world from Kestra](https://modal-cdn.com/article-assets/kestra-hello-world.png)

Each workflow in Kestra consists of three required components:

* a unique
  `id`
* a
  `namespace`
  used for organization and governance
* a list of
  `tasks`
  that define the workflow logic.

Optionally, you can also define
`inputs`
to allow users to dynamically execute the flow with different parameter values. Try that yourself by changing the value “World” to your name.

![defining inputs in Kestra](https://modal-cdn.com/article-assets/kestra-define-inputs.png)

### “Hello World” in Modal triggered from Kestra

Now, let’s add a Hello-World Modal example that we’ll trigger from Kestra. You can get your Modal token ID and secret by following the
[quickstart guide](/docs/guide)

.

```
id: modal_hello_world
namespace: company.team
tasks:
  - id: hello
    type: io.kestra.plugin.modal.cli.ModalCLI
    env:
      MODAL_TOKEN_ID: "your_modal_token_id"
      MODAL_TOKEN_SECRET: "your_modal_token_secret"
    commands:
      - modal run gpu.py
    inputFiles:
      gpu.py: |
        import modal

        app = modal.App(
            "example-gpu",
            image=modal.Image.debian_slim().pip_install(
                "torch", find_links="https://download.pytorch.org/whl/cu117"
            ),
        )

        @app.function(gpu="any")
        def print_gpu_info():
            import torch
            import subprocess

            subprocess.run(["nvidia-smi"])
            print("Torch version:", torch.__version__)
            print("CUDA available:", torch.cuda.is_available())
            print("CUDA device count:", torch.cuda.device_count())
            print("CUDA device name:", torch.cuda.get_device_name(0))
            print("CUDA device index:", torch.cuda.current_device())
```

When you point the cursor anywhere in the Modal plugin configuration and switch to the documentation tab, you will see the explanation of all Modal plugin properties and examples how to use it.

![running modal in a flow on kestra](https://modal-cdn.com/article-assets/kestra-modal-flow.png)

Interactive Workflows
---------------------

Let’s extend the previous code example by adding an input allowing to choose the compute type needed for the Modal task. The
`dependsOn`
property in the
`inputs`
section ensures that the GPU option is only shown if the user chooses to use GPU acceleration in the Modal function. When the GPU option is selected, the dropdown shows the list of available GPUs, allowing only valid values to be selected:

```
id: modal_hello_world
namespace: company.team

inputs:
  - id: compute_type
    displayName: CPU or GPU
    description: Whether to use CPU or GPU compute type
    type: SELECT
    values:
      - CPU
      - GPU
    defaults: CPU

  - id: gpu
    type: SELECT
    displayName: GPU request
    description: The GPU resources to allocate to the job
    defaults: "any"
    values: ["any", "t4", "l4", "a100", "h100", "a10g"]
    dependsOn:
      inputs:
        - compute_type
      condition: "{{ inputs.compute_type == 'GPU' }}"

  - id: cpu
    type: SELECT
    displayName: CPU request
    description: The number of CPU cores to allocate to the job
    defaults: "0.25"
    values: ["0.25", "0.5", "0.75", "1.0", "1.5", "2.0", "4.0", "8.0", "16.0"]
    dependsOn:
      inputs:
        - compute_type
      condition: "{{ inputs.compute_type == 'CPU' }}"

tasks:
  - id: run_modal
    type: io.kestra.plugin.modal.cli.ModalCLI
    env:
      MODAL_TOKEN_ID: "{{ kv('MODAL_TOKEN_ID') }}"
      MODAL_TOKEN_SECRET: "{{ kv('MODAL_TOKEN_SECRET') }}"
      GPU: "{{ inputs.gpu }}"
      CPU: "{{ inputs.cpu }}"
    commands:
      - modal run cpu_or_gpu.py --compute-type "{{ inputs.compute_type }}"
    inputFiles:
      cpu_or_gpu.py: |
        import os

        import modal

        app = modal.App(
            "example-cpu-gpu",
            secrets=[modal.Secret.from_local_environ(env_keys=["GPU", "CPU"])],
        )

        cpu_image = modal.Image.debian_slim().pip_install("torch", "psutil")
        gpu_image = modal.Image.debian_slim().pip_install(
            "torch", find_links="https://download.pytorch.org/whl/cu117"
        )

        @app.function(image=cpu_image, cpu=float(os.getenv("CPU", 0.25)))
        def print_cpu_info():
            import torch
            import platform
            import psutil

            print("Torch version:", torch.__version__)
            print("CUDA available:", torch.cuda.is_available())  # Should return False for CPU
            print("CPU count:", psutil.cpu_count(logical=True))
            print("CPU frequency:", psutil.cpu_freq().current, "MHz")
            print("CPU architecture:", platform.architecture()[0])
            print("Platform:", platform.system(), platform.release())
            print("Total memory (RAM):", psutil.virtual_memory().total // (1024**2), "MB")

        @app.function(image=gpu_image, gpu=os.getenv("GPU", "any"))
        def print_gpu_info():
            import torch
            import subprocess

            subprocess.run(["nvidia-smi"])
            print("Torch version:", torch.__version__)
            print("CUDA available:", torch.cuda.is_available())
            print("CUDA device count:", torch.cuda.device_count())
            print("CUDA device name:", torch.cuda.get_device_name(0))
            print("CUDA device index:", torch.cuda.current_device())

        @app.local_entrypoint()
        def main(compute_type: str = "CPU"):
            if compute_type == "GPU":
                print_gpu_info.remote()
            else:
                print_cpu_info.remote()
```

This example shows how to run Modal code as part of a Kestra workflow:

* Use the
  `commands`
  property in the
  `ModalCLI`
  plugin to run
  `modal`
  CLI commands (like
  `modal run cpu_or_gpu.py`
  )
* Use the
  `env`
  property to provide the necessary environment variables for authenticating with Modal and external services or to pass variables to Modal function decorators
* Set the
  `namespaceFiles.enabled`
  property to
  `true`
  if you want to store your Python code as a separate file in the built-in Code Editor rather than inline in YAML
* Override the
  `containerImage`
  property if you need to use a custom Modal version — the default is the
  `latest`
  version.

![pick Modal CPU or GPU compute on Kestra](https://modal-cdn.com/article-assets/kestra-pick-modal-gpu.png)

Adding Secrets
--------------

Now that we have the basic structure in place, let’s build out our order forecasting workflow.

To securely manage sensitive data such as Modal tokens or AWS credentials in Kestra, you can use
[Secrets](https://kestra.io/docs/concepts/secret)

. Adding secrets requires some additional setup, so to keep things simple for now, you can store them in the
[KV Store](https://kestra.io/docs/concepts/kv-store)

. Replace the placeholders with your actual credentials and execute the
`curl`
commands shown below (
*the double quotes are necessary*
). Alternatively, you can also add your KV pairs directly from the UI by navigating to the namespace
`company.team`
and adding the key-value pairs from the
`KV Store`
tab.

```
curl -X PUT -H "Content-Type: application/json" http://localhost:8080/api/v1/namespaces/company.team/kv/MODAL_TOKEN_ID -d '"your_credential"'
curl -X PUT -H "Content-Type: application/json" http://localhost:8080/api/v1/namespaces/company.team/kv/MODAL_TOKEN_SECRET -d '"your_credential"'
curl -X PUT -H "Content-Type: application/json" http://localhost:8080/api/v1/namespaces/company.team/kv/AWS_ACCESS_KEY_ID -d '"your_credential"'
curl -X PUT -H "Content-Type: application/json" http://localhost:8080/api/v1/namespaces/company.team/kv/AWS_SECRET_ACCESS_KEY -d '"your_credential"'
curl -X PUT -H "Content-Type: application/json" http://localhost:8080/api/v1/namespaces/company.team/kv/AWS_DEFAULT_REGION -d '"us-east-1"'
```

Now you can reference those values in your flow using the
`{{ kv('KEY_NAME') }}`
syntax.

Adding data and a model
-----------------------

At this point, we’ve got the entire skeleton in place. From here, every workflow built with Modal and Kestra together will look different: different data, different models, different actions.

[This GitHub Gist](https://gist.github.com/anna-geller/8c37a868939ea94a6c91f069dc4c215c)

includes the full workflow definition for our time-series forecasting use case if you’d like to try it for yourself. Simply copy the Gist’s raw content and paste it in Kestra UI when creating a new flow.

We just want to call out one last feature. The
`dependsOn`
property in the
`inputs`
is what lets us create interactive workflows that adjust based on previous user inputs. When you click on the
`Execute`
button in the Kestra UI, you’ll see the available input options allowing you to adjust whether or not you want to customize the forecast, the amount of CPU, memory, and more. Depending on those choices, you will see other inputs appear or disappear.

![executing the data forecast flow on Kestra](https://modal-cdn.com/article-assets/kestra-forecast-execute.png)

Run the above flow and navigate to the
`Outputs`
tab of the Execution page. From here, you’ll be able to
[download and view](https://youtu.be/Wucyw4gRNiQ)

the
`plotly`
report exported as an HTML file showing a forecasted order volume for each day of the forecast.

Automate Workflows with Triggers
--------------------------------

You can extend the workflow by adding a
`trigger`
. This way, you can automatically run the flow:

* on a schedule
* event-driven e.g. when a file is uploaded to an S3 bucket
* from an external application via a webhook.

Check Kestra’s
[triggers documentation](https://kestra.io/docs/workflow-components/triggers)

to learn more.

Next steps
----------

Using Kestra and Modal together allows you to create interactive data workflows that adapt to user’s inputs and to your compute needs.

Kestra is open-source, so if you enjoy the project, give us a
[GitHub star](https://github.com/kestra-io/kestra)

⭐️ and join
[our community](https://kestra.io/slack)

to ask questions or share feedback.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/modal-product-updates-jul-2025
================================================================================

Product updates: multi-node training clusters, B200 and H200s, and Client 1.0 release
=====================================================================================

🏋️ Multi-node training clusters in beta
---------------------------------------

![](https://modal-cdn.com/cdnbot/1id9la4d7_3aee338a.webp)

We’ve made multi-node training as easy as multi-GPU training on Modal. By simply adding the
`@clustered`
decorator, you can instantly tap into dozens of GPUs on multiple hosts, co-scheduled and co-located. And thanks to high-speed RDMA interconnect (3.2 Tbps Infiniband), those GPUs can communicate quickly enough to scale your training runs linearly with node count.

Check out the
[guide](https://modal.com/docs/guide/multi-node-training)

for more info and examples.
**We are actively looking for beta users, please
[reach out](https://modal.com/slack)

if interested!**

🚀 Introducing B200s and H200s on Modal
--------------------------------------

![](https://modal-cdn.com/cdnbot/2bi0jbx8r_1ec6fa68.webp)

Modal now offers NVIDIA B200 and H200 GPUs serverlessly. Add a one-liner to your Modal function, pay as you go ($6.25/hr for B200, $4.54/hr for H200), and see up to 2-4× faster LLM inference compared to H100s.

See the full announcement
[here](https://modal.com/blog/introducing-b200-h200)

.

🗞️ Modal Client 1.0 release
---------------------------

We recently launched
[version 1.0 of the Modal client](https://pypi.org/project/modal/)

, underscoring our commitment to API stability and giving users more clarity and predictability. For instructions on how to migrate to 1.0, check out our
[migration guide](https://modal.com/docs/guide/modal-1-0-migration)

.

For more insight into the design principles underlying the release, read our blog
[here](https://modal.com/blog/introducing-client-1-0)

.

👩‍💻 Client updates
------------------

Run
`pip install --upgrade modal`
to get the latest client updates.

* Added a
  [`modal.Volume.read_only`](https://modal.com/docs/reference/modal.Volume#read_only)

  method
  [(1.05)](https://modal.com/docs/reference/changelog#:~:text=Latest-,1.0.5%20(2025%2D06%2D27),-Added%20a%20modal)
* Added a
  `--secret`
  option to
  `modal shell`
  for including environment variables defined by named Secret(s) in the shell session
  [(1.04)](https://modal.com/docs/reference/changelog#:~:text=Added%20a%20%2D%2Dsecret%20option%20to%20modal%20shell%20for%20including%20environment%20variables%20defined%20by%20named%20Secret(s)%20in%20the%20shell%20session)
* Added support for specifying a timezone on
  `Cron`
  schedules
  [(1.03)](https://modal.com/docs/reference/changelog#:~:text=Added%20support%20for%20specifying%20a%20timezone%20on%20Cron%20schedules%2C%20which%20allows%20you%20to%20run%20a%20Function%20at%20a%20specific%20local%20time%20regardless%20of%20daylight%20savings%3A)
* Added a
  `--timestamps`
  flag to
  `modal app logs`
  that prepends a timestamp to each log line
  [(1.01)](https://modal.com/docs/reference/changelog#:~:text=Added%20a%20%2D%2Dtimestamps%20flag%20to%20modal%20app%20logs%20that%20prepends%20a%20timestamp%20to%20each%20log%20line.)

📊 LLM Engineer’s Almanac: SGLang or vLLM?
-----------------------------------------

![](https://modal-cdn.com/cdnbot/3c24nb034_08f150ed.webp)

In case you missed it: AI is actually open now, thanks to the teams releasing open models (LLaMA, ERNIE, DeepSeek) and engines (vLLM, SGLang).

But that just means more questions: which engine should you use? How much will it cost to self-host your RAG chatbot? We ran over a thousand LLM engine performance benchmarks and built an interactive tool to help you answer these questions. Check it out
[here](https://modal.com/llm-almanac/summary)

!

🖼️ FLUX.1 Kontext [dev] on Modal
--------------------------------

![](https://modal-cdn.com/cdnbot/4its4wyln_44782a6d.webp)

Black Forest Labs released weights for
[FLUX Kontext [dev]](https://huggingface.co/black-forest-labs/FLUX.1-Kontext-dev)

. It’s the new state-of-the-art open-weight image generation model, with super precise contextual editing abilities across success renderings. Our
[example](https://modal.com/docs/examples/image_to_image)

shows you how to run it on B200s.

🏎️ Run FLUX.1-dev 3x faster
---------------------------

![](https://modal-cdn.com/cdnbot/5euw8r4qy_40ad1a19.webp)

Speaking of FLUX: check out our blog post on how to make Flux [dev] run 3x faster with multiple optimizations. That means you have the flexibility of self-deployments while still getting the speed and price of API platforms! Read more
[here](https://modal.com/blog/flux-3x-faster)

.

❓Learn how Quora uses Modal Sandboxes at scale
----------------------------------------------

Quora uses Modal to power sandboxed code execution for Poe, their chatbot platform. Quora can create up to 1,000 Sandboxes per second with Modal, plus we’ve saved the team >15% engineering time compared to building their own sandbox solution! Read more
[here](https://modal.com/blog/quora-case-study)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/mteb-leaderboard-article
================================================================================

Top embedding models on the MTEB leaderboard
============================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

What is the MTEB leaderboard?
=============================

The
[MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard)

, hosted on Hugging Face, is a comprehensive benchmark for assessing the performance of embedding models across a wide range of tasks. It provides a standardized way to evaluate and compare different models.

The leaderboard encompasses various tasks, including:

1. Classification
2. Clustering
3. Pair classification
4. Reranking
5. Retrieval
6. Semantic textual similarity (STS)
7. Summarization

By evaluating models across these diverse tasks, the MTEB leaderboard offers a holistic view of embedding model capabilities.

Beyond the rankings: choosing the right model for your use case
---------------------------------------------------------------

While the MTEB leaderboard provides valuable information about model performance, it’s essential to understand that a high ranking doesn’t necessarily mean a model is the best fit for your specific use case. Several factors should be considered when selecting an embedding model:

1. **Task-specific performance**
   : Some models may excel in certain tasks but underperform in others. Analyze the breakdown of scores across different tasks to identify models that perform well in areas relevant to your project.
2. **Computational requirements**
   : Larger embedding models often require more computational resources. Consider your hardware and cost limitations and inference speed requirements when choosing a model.
3. **Domain relevance**
   : The MTEB benchmark uses general-purpose datasets. If your application focuses on a specific domain (e.g., medical, legal, or financial), a domain-specific model might outperform general models.

To make an informed decision, it’s crucial to run thorough evaluations of potential models using datasets and tasks that closely resemble your specific use case. This approach ensures that you select the most suitable embedding model for your project’s unique requirements.

Top 5 models on the MTEB leaderboard
------------------------------------

As of 2025, here are some of the top models on the MTEB leaderboard and their backgrounds:

1. **[NV-Embed-v2](https://huggingface.co/nvidia/NV-Embed-v2)**
   : Developed by NVIDIA,
   `NV-Embed-v2`
   is a generalist embedding model that fine-tunes a base LLM (Mistral 7B) to provide text embeddings.
2. **[Nomic-Embed-Text-v1.5](https://huggingface.co/nomic-ai/nomic-embed-text-v1.5)**
   :
   This multimodal model is developed by Nomic, and is a generalist embedding
   model. For a guide on how to run this model on Modal, see
   [here](/blog/how-to-run-nomic-embed-v1-5-on-modal)

   .
3. **[bge-en-icl](https://huggingface.co/BAAI/bge-en-icl)**
   : This model is developed by the Beijing Academy of Artificial Intelligence (BAAI). It’s part of the BAAI General Embedding (BGE) family, which includes a range of embedding models for both English and Chinese.
4. **[stella\_en\_1.5B\_v5](https://huggingface.co/dunzhang/stella_en_1.5B_v5)**
   : This model is built on top of the
   `Alibaba-NLP/gte-large-en-v1.5`
   and
   `Alibaba-NLP/gte-Qwen2-1.5B-instruct`
   models. At 1.5B parameters, this is around 5x smaller than the other top 5 models, most of which are ~7B parameters.
5. **[SFR-Embedding-2\_R](https://huggingface.co/Salesforce/SFR-Embedding-2_R)**
   : The
   `Salesforce/SFR-Embedding-2_R`
   model is developed by the Salesforce AI Research team. It builds upon their previous work on the SFR-Embedding-Mistral model, which was trained on large datasets to improve text retrieval and semantic search capabilities.
6. **[gte-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)**
   : gte-Qwen2-7B-instruct is the latest model in the gte (General Text Embedding) model family. It’s made by Alibaba.

Domain-specific embedding models
--------------------------------

While general-purpose models dominate the MTEB leaderboard, domain-specific embedding models can offer superior performance for specialized applications. Here are some examples of embedding models fine-tuned for specific domains:

1. **Medicine**
   :
   [PubMedBERT](https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext)

   is fine-tuned on medical literature and clinical notes, making it well-suited for tasks in healthcare and biomedical research. Additionally,
   [BioLORD](https://huggingface.co/FremyCompany/BioLORD-2023)

   is another model tailored for similar applications.
2. **Finance**
   :
   [Finance Embeddings from Investopedia](https://huggingface.co/FinLang/finance-embeddings-investopedia)

   ,
   [Voyage Finance](https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/)

   , and
   [BGE Base Financial Matryoshka](https://huggingface.co/philschmid/bge-base-financial-matryoshka)

   are examples of models fine-tuned on financial datasets, offering improved performance for tasks such as sentiment analysis of financial news or SEC filings.
3. **Law**
   : For legal applications, consider exploring the
   [Domain-Specific Embeddings and Retrieval: Legal Edition](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/)

   , which discusses models fine-tuned on legal documents, enhancing their utility for legal research, contract analysis, and other law-related NLP tasks.
4. **Code**
   :
   [CodeBERT](https://huggingface.co/microsoft/codebert-base)

   and
   [GraphCodeBERT](https://huggingface.co/microsoft/graphcodebert-base)

   are designed specifically for programming language understanding, making them useful for code search, code completion, and bug detection tasks.
5. **Math**
   :
   [Math Similarity Model](https://huggingface.co/math-similarity/Bert-MLM_arXiv-MP-class_zbMath)

   is tailored for mathematical tasks.
6. **Other languages**
   :

* **Japanese**
  :
  [RoSEtta-base-ja](https://huggingface.co/pkshatech/RoSEtta-base-ja)
* **Korean**
  :
  [KoSimCSE-roberta](https://huggingface.co/BM-K/KoSimCSE-roberta)
* **Chinese**
  :
  [GTE-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct)
* **French**
  :
  [Sentence-Camembert-large](https://huggingface.co/dangvantuan/sentence-camembert-large)
* **Arabic**
  :
  [Arabic-STS-Matryoshka](https://huggingface.co/omarelshehy/Arabic-STS-Matryoshka)

These domain-specific models demonstrate the potential for tailored embedding solutions in specialized fields. When working with domain-specific tasks, it’s worth exploring these models alongside the top performers on the MTEB leaderboard to find the best fit for your particular use case.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/nvidia-a100-price-article
================================================================================

How much is an Nvidia A100?
===========================

![author](https://modal-cdn.com/margaret-shen.jpeg)

Margaret Shen

Head of Business Operations

Nvidia A100 GPUs were launched in 2020, and while they are no longer the most bleeding edge GPUs on the market, they still offer great performance for training and deploying LLMs and diffusion models.

A100 Configurations and Pricing
-------------------------------

The Nvidia A100 price varies based on configuration. The two primary factors influencing the price are GPU memory (40GB vs 80GB) and form factor (PCIe vs SXM).

### 40GB vs. 80GB

This refers to the amount of VRAM on the GPU, and determines how large of a model you can run/fine-tune. Please refer to our articles on how much VRAM you need to
[fine-tune](https://modal.com/blog/how-much-vram-need-fine-tuning)

or
[serve](https://modal.com/blog/how-much-vram-need-inference)

transformers models for an estimate on how much bang for your buck you would get.

### PCIe vs SXM

Generally, the SXM version is more expensive than the PCIe version.

That’s because SXM GPUs are directly socketed onto the motherboard, enabling more direct and high-bandwidth connections. If your primary goal is to effectively run larger transformer models, the SXM version is the better choice due to its enhanced performance capabilities.

Direct Purchase Price from Nvidia
---------------------------------

The current market prices for the Nvidia A100 vary from $8,000 to $10,000 for the 40GB PCIe model to $18,000 to $20,000 for a 80GB SXM model.

Alternatives to Direct Purchase: A100s in the Cloud
---------------------------------------------------

### Traditional Cloud Platforms - AWS, GCP, OCI

For most companies, the high upfront costs of setting up their own data centers lead them to seek cloud platforms instead. Traditionally, companies have made GPU reservations lasting 1-3 years, with Amazon, Google, and Oracle dominating this market.

AWS (Amazon) and GCP (Google) also provide flexible purchase models for A100s, including spot and on-demand options. The on-demand model allows users to pay for A100s by the second, but this flexibility comes at a higher per-hour price compared to reservations. Spot pricing enables users to utilize unused capacity and is also billed by the second, although it does not guarantee that workloads won’t be pre-empted, making it generally cheaper than on-demand options.

Below are comparison tables of current A100 GPU per-hour list prices across these platforms.

| **GPU type** | **Purchase model** | **AWS\*** | **GCP\*** | **OCI** |
| --- | --- | --- | --- | --- |
| A100 40GB SXM | 1 year reservation | $2.52 | $2.31 | $3.05 |
|  | 3 year reservation | $1.56 | $1.29 | n/a |
|  | On-demand | $4.10 | $3.67 | n/a |
|  | Spot | $1.15 | $1.17 | n/a |
| A100 80GB SXM | 1 year reservation | $3.15 | n/a | $4.00 |
|  | 3 year reservation | $1.95 | n/a | n/a |
|  | On-demand | $5.12 | $5.12 | n/a |
|  | Spot | n/a | $1.57 | n/a |

\*
*Prices based on us-east-1 for AWS and us-central1 for GCP. Note that prices will vary based on region.*

AWS also offers a Savings Plan that provides discounts from on-demand pricing in exchange for a 1 or 3-year commitment to a baseline $/hr usage. This plan is more flexible than a reservation but typically comes with a higher price.

Finding information on A100s for these cloud providers can be challenging due to differing naming conventions. Below is a guide on the instance type names associated with A100s for each provider:

| **GPU type** | **AWS** | **GCP** | **OCI** |
| --- | --- | --- | --- |
| A100 40GB SXM | p4d.24xlarge | a2-highgpu-\*   a2-megagpu-\* | bm-gpu4.8 |
| A100 80GB SXM | p4de.24xlarge | a2-ultragpu-\* | bm.gpu.a100-v2.8 |

Note that AWS and OCI only offer A100s in configurations of 8 GPUs, while GCP provides configurations ranging from 1 to 16.

### Serverless Compute Startups

Many companies are exploring alternatives to the hyperscalers for several reasons:

* Inflexibility of GPU configurations
* Lack of availability without making a big commitment
* Slow and manual provisioning of resources
* Overhead of configuring and managing infrastructure

Emerging GPU platforms are addressing these challenges by offering a more flexible model for accessing and scaling resources. By adopting a
[serverless approach](/blog/serverless-gpu-article)

, these platforms can spin up GPUs for users only when needed, simplifying the management of the underlying infrastructure. Below is a comparison of A100 per-hour prices for the most popular serverless GPU providers.

| **GPU Type** | [**Modal**](https://modal.com/) | [**Lambda Labs**](https://lambdalabs.com/) | [**Runpod**](https://www.runpod.io/) | [**Baseten**](https://www.baseten.co/) |
| --- | --- | --- | --- | --- |
| A100 40GB SXM | $2.78 | $1.29 | n/a | n/a |
| A100 80GB SXM | $3.40 | $1.79 | $2.72 | $6.144 |

While these prices may be slightly higher than the spot or reservation prices of the hyperscalers, they do not reflect the full picture. Serverless options can quickly autoscale GPUs and charge based on usage, leading to significantly higher utilization and lower overall costs for workloads with unpredictable volume.

Interested in exploring the Nvidia A100 price options? On Modal, you can deploy a function with an A100 attached in just a few minutes.
[Try it out](https://modal.com/signup)

!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/nvidia-b200-pricing
================================================================================

How much does it cost to run NVIDIA B200 GPUs in 2025?
======================================================

Researchers and engineers can now get their hands on the first NVIDIA
[Blackwell GPU](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)

, the
[B200](https://www.nvidia.com/en-us/data-center/dgx-b200/)

. With 192 GB of ultra-fast HBM3e and a second-generation Transformer Engine that introduces FP4 arithmetic, a single card delivers up to 20 petaFLOPS of sparse-FP4 AI compute.

Blackwell B200 specs & performance
----------------------------------

The
[B200](/blog/introducing-b200-h200)

represents a massive leap in GPU capabilities: built on TSMC’s 4NP process, it packs 208 billion transistors across a dual-die design, enabling both the new FP4 Tensor Cores and an on-package NVSwitch.

* **Process**
  : TSMC 4NP with 208 billion transistors (dual-die)
* **Memory**
  : 192 GB HBM3e (cloud consoles expose 180 GB usable) — 2.4x H100 capacity
* **Bandwidth**
  : 8 TB/s memory bandwidth, doubling Hopper’s throughput
* **Peak compute**
  : 20 PFLOPS FP4 with 2:1 sparsity — ~5x H100 inference throughput
* **Interconnect**
  : NVLink 5 at 1.8 TB/s bidirectional, removing PCIe bottlenecks

This huge transistor budget allows you to fit models that would require complex parallelism on H100s, while delivering inference throughput that makes real-time AI applications viable.

NVIDIA B200 cloud pricing
-------------------------

Here’s per-GPU pricing for B200s across major providers, from most to least flexible purchase options (July 2025):

| **Provider & SKU** | **Serverless** | **Spot** | **On‑demand** | **Capacity block** | **1‑yr reservation** | **3‑yr reservation** | **Pricing sources** |
| --- | --- | --- | --- | --- | --- | --- | --- |
| **Modal** | $6.25/hr | n/a | n/a | n/a | n/a | n/a | [Modal pricing](https://modal.com/pricing) |
| **Baseten** | $9.98/hr | n/a | n/a | n/a | n/a | n/a | [Baseten pricing](https://www.baseten.co/pricing/) |
| **RunPod** | n/a | n/a | $5.99/hr | n/a | ~$5.09 hr | n/a | [Runpod pricing](https://www.runpod.io/gpu-models/b200) |
| **Lambda Labs** | n/a | n/a | $3.79/hr | n/a | $3.49/ hr | $2.99/hr | [Lambda Labs pricing](https://modal.com/pricing) |
| **AWS** | n/a | n/a | $14.24/hr | $8.14/hr | ~$12.50/hr | n/a | [Vantage](https://instances.vantage.sh/aws/ec2p6-b200.48xlarge)  , [AWS Saving Plan](https://aws.amazon.com/blogs/aws/announcing-up-to-45-price-reduction-for-amazon-ec2-nvidia-gpu-accelerated-instances/#:~:text=We%20are%20also%20now%20delivering,%E2%80%94%20Channy)  , [AWS Capacity Block pricing](https://aws.amazon.com/ec2/capacityblocks/pricing) |
| **GCP** | n/a | $8.06/hr | $18.53/hr | n/a | $11.12/hr | $7.09/hr | [Vertex pricing](https://cloud.google.com/vertex-ai/pricing)  , [Google Cloud pricing](https://cloud.google.com/compute/all-pricing?hl=en#section-5)  , [Spot pricing](https://cloud.google.com/spot-vms/pricing?hl=en#section-2) |

*Note that B200s are only available in instances of 8 GPUs on AWS and GCP*

Choosing the right provider
---------------------------

Different scenarios call for different providers:

| **Scenario** | **Best fit** | **Rationale** |
| --- | --- | --- |
| Bursty AI inference traffic | Modal | Per-second billing and sub-second cold starts keep effective cost lowest |
| Static, predictable AI inference traffic | AWS, GCP | Most reliable option that offers reservation-based discounts |
| Multi-week training runs | Lambda Labs | Cheapest reservation prices |

Serverless options like Modal automatically scale up and down from 0 so you only pay for compute you actually use. Reserved-capacity options provide a fixed block of resources; this is acceptable for static workloads but for variable workloads results in increased latency when demand is high and wasted money when demand is low.

On‑premise options: buy a B200 or DGX B200?
-------------------------------------------

For those considering ownership:

* **Standalone B200 SXM module**
  :
  [$30,000 - $40,000](https://www.tomshardware.com/pc-components/gpus/nvidias-jensen-huang-says-blackwell-gpu-to-cost-dollar30000-dollar40000-later-clarifies-that-pricing-will-vary-as-they-wont-sell-just-the-chip)

  (one 700W GPU board)
* **Grace-Blackwell GB200 Superchip**
  :
  [$60,000 - $70,000](https://www.tweaktown.com/news/98292/nvidias-new-gb200-superchip-costs-up-to-70-000-full-b200-nvl72-ai-server-3-million/index.html)

  (1x Grace CPU + 2x B200)
* **NVIDIA DGX B200**
  :
  [~$515,000](https://wccftech.com/nvidia-blackwell-dgx-b200-price-half-a-million-dollars-top-of-the-line-ai-hardware/)

  (8x B200, 1.44 TB GPU RAM, 72 PFLOPS FP8)

At $30k per card, breakeven against $6-8/hour cloud rates happens at ~60% utilization over 18 months (excluding electricity and cooling). Factor in datacenter space (~14 kW per DGX B200) and staff before buying.

B200 vs. H100/H200: is the upgrade worth it?
--------------------------------------------

The
[B200](/blog/introducing-b200-h200)

offers compelling advantages for specific workloads:

1. **Memory headroom**
   - 192 GB HBM3e lets you serve GPT-4-class 400B parameter models on one card instead of 2-way sharding
2. **FP4 Transformer Engine**
   - 5x higher inference throughput; MLPerf Llama-2-70B results show 2-3x tokens/second on identical node counts
3. **Fifth-gen NVLink**
   - 1.8 TB/s cuts all-reduce time ~40% in 8-GPU training replicas
4. **Better real-world latency**
   - Modal’s benchmarks show 2.5x lower TTFB versus H200 for MoE models

Quick‑start guide: run code on a cloud B200 in under 5 minutes
--------------------------------------------------------------

Modal’s serverless platform lets you run and deploy code on a B200 without having to manage cloud resources. To get started, simply
[sign up](/signup)

and run the code snippet below:

```
import modal

app = modal.App()

@app.function(gpu="B200")
def run_big_model():
    # This will run on a B200 on Modal
```

At $0.001736/second, you can benchmark for pennies, then scale to thousands of ephemeral workers without touching an instance planner.

Get started with B200s today
----------------------------

Modal serverless B200s at $6.25/hour is the most cost-effective option for bursty workloads.

If your H100s are out of memory or your user-visible latency targets are slipping, Blackwell’s 192 GB HBM3e and FP4 Tensor Cores are the most cost-effective escape hatch available today.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/nvidia-h100-price-article
================================================================================

How much is an Nvidia H100?
===========================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Direct purchase price from Nvidia
---------------------------------

When purchasing directly from Nvidia, the H100 GPU is estimated to cost around $25,000 per GPU. However, it’s important to note that these prices can vary based on factors such as volume discounts and specific configurations.

For example, a full H100 GPU system, which includes multiple H100 chips, can cost up to $400,000.

Alternatives to direct purchase: GPU-on-demand platforms
--------------------------------------------------------

Given the high cost and limited availability of H100 GPUs, many companies are exploring alternatives through GPU-on-demand platforms. These services offer flexible access to high-performance GPUs without the need for significant upfront investment. Here are some of the top platforms:

1. [Modal](/docs/guide/gpu)
2. [Lambda](https://lambdalabs.com/)
3. [Runpod](https://www.runpod.io/)
4. [Baseten](https://www.baseten.co/)

Here’s a comparison table of H100 GPU prices across these platforms:

| Platform | H100 Price (per hour) |
| --- | --- |
| Modal | $4.56 |
| Lambda Labs | $2.99 |
| Runpod | $5.59 |
| Baseten | $9.984 |

Note: Prices are approximate and may vary based on region, availability, and specific configurations. Always check the official pricing pages for the most up-to-date information.

Pricing parameters
------------------

When considering the cost of using H100 GPUs on cloud platforms, it’s important to understand that the total price of a job depends on more than just the per-hour rate. Several factors contribute to the overall runtime and, consequently, the cost. This includes:

1. **Cold start time:**
   This refers to the time it takes for a new instance of your application to start up and become ready to handle requests. In serverless environments, cold starts can occur when a new container or runtime environment needs to be initialized. For GPU workloads, this includes the time to allocate and initialize the GPU, load any necessary drivers or libraries, and set up the CUDA environment.
2. **Model loading time:**
   This includes the time it takes to load your code, dependencies, and any large models into GPU memory. For large AI models, this can be significant. You should aim to do this as infrequently as possible: for example, load the model once and reuse it for multiple inferences, amortizing this cost over many requests.
3. **Inference speed:**
   The speed of inference depends largely on the framework you use. For example, using optimized inference engines like NVIDIA TensorRT or vLLM can significantly speed up inference compared to standard PyTorch or TensorFlow implementations.
4. **Input/Output operations:**
   If your job involves heavy I/O, such as downloading large datasets or models, or reading large files or writing extensive outputs, this can add to the overall runtime.

Depending on the platform you use, how much time each of these factors takes, and thus the amount of time you are billed for, can vary significantly.

Conclusion
----------

While H100 GPUs offer unparalleled performance for AI and machine learning tasks, their high direct purchase cost can be prohibitive for many organizations.
[Serverless GPU](/blog/serverless-gpu-article)

platforms provide a more accessible and flexible alternative, allowing users to leverage the power of H100s without the hefty upfront investment.

Ready to experience the performance of H100 GPUs with the flexibility of serverless computing?
[Sign up for Modal](https://modal.com/signup)

today and start building your AI applications with ease!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/nvidia-h200-price-article
================================================================================

How much is an Nvidia H200?
===========================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

The
[Nvidia H200](https://www.nvidia.com/en-us/data-center/h200/)

, which began delivery in late 2024, is Nvidia’s latest and most
powerful GPU for AI workloads, featuring significantly more memory than its
predecessor, the
[H100](https://www.nvidia.com/en-us/data-center/h100/)

. It’s particularly well-suited for running the latest
large language models like DeepSeek.

How is the H200 different from the H100?
----------------------------------------

The key differentiator of the H200 is its massive 141GB of memory, which is nearly double the capacity of the H100.

The H200 also offers:

* Higher memory bandwidth (4.8 TB/s vs 3.35 TB/s on H100)
* Up to 1.6 times higher inference performance for LLMs like GPT-3 and Llama-70B in specific scenarios

This additional VRAM and bandwidth makes the H200 particularly well-suited for:

1. Running larger (100+B parameter) AI models that won’t fit in H100 memory
2. Handling longer context windows in LLMs
3. Processing larger batch sizes for improved throughput

Recommended Hardware for Modern AI Models
-----------------------------------------

The H200’s expanded memory capacity makes it the ideal choice for running the latest generation of large language models. For example:

* **DeepSeek Models**
  : You can run the full DeepSeek-R1 671B model on 8xH200s.
  You can run distilled versions of the model on a single H200.
* **Multi-Modal Models**
  : Models that process both text and images require significant VRAM, making the H200 particularly valuable
* **Fine-tuning**
  : The additional memory allows for fine-tuning larger models or
  using larger batch sizes.

Direct Purchase Price
---------------------

The H200 GPU costs
[~$30,000](https://viperatech.com/shop/nvidia-h200-nvl-graphic-card-141-gb-passive-pcie-900-21010-0040-000/)

per chip if you are purchasing it directly from a hardware vendor.

However, it’s important to note that organizations typically aren’t buying just a single chip; they may be investing in configurations like a
[Nvidia DGX H200 supercomputer](https://www.nvidia.com/en-us/data-center/dgx-h200/)

with 8 H200s for ~$300k.

Alternatives to Direct Purchase: GPU-on-demand Platforms
--------------------------------------------------------

Given the substantial cost and limited availability of H200 GPUs, most
organizations will probably access H200 GPUs via cloud service providers and AI
infrastructure companies.

Cloud service providers with partnerships with Nvidia include:

* [Amazon Web Services
  (AWS)](https://aws.amazon.com/blogs/aws/new-amazon-ec2-p5en-instances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking/)

  + P5en instances with 8xH200s cost $84.8/hr
* [Google Cloud](https://cloud.google.com/compute/docs/gpus#h200-gpus)

  + A3 Ultra VMs are available through
    [Google Kubernetes Engine (GKE)](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus)
* [Microsoft
  Azure](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/nd-h200-v5-series?tabs=sizebasic)

  + ND v5 H200 series VM
* [Oracle Cloud
  Infrastructure](https://blogs.oracle.com/cloud-infrastructure/post/now-ga-largest-ai-supercomputer-oci-nvidia-h200)

The cost of an H200 GPU across the major cloud providers is roughly $10/GPU/hour.

There’s also a newer generation of GPU-on-demand platforms that are just
beginning to offer (often limited) H200 access, including
[Modal](https://modal.com/)

,
[RunPod](https://runpod.io/)

,
[CoreWeave](https://coreweave.com/)

, and
[Lambda Labs](https://lambdalabs.com/)

.

H100 vs. H200
-------------

Given that availability of H200s is still fairly limited,
and they are generally priced at a premium (they cost ~$10/GPU/hour, compared
to ~$5/GPU/hour for
[H100s](/blog/nvidia-h100-price-article)

), are they worth it?

The answer is that if you are training or running inference on larger models (>
70B parameters), then the H200 probably makes sense.

For example, running Llama-70B on a single H200 at 8-bit precision is 1.9x more performant than the
H100.

On the other hand, for most inference workloads, there will
be little to no performance gain over H100s.

H100s are at this point widely available, and are still a great, cost-effective
choice for most inference and fine-tuning workloads. Try
[Modal](https://modal.com/signup)

to get started
with them today!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/open-ai-agents
================================================================================

Open-source AI agents
=====================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

What are AI agents?
-------------------

AI agents are generative AI tools that can autonomously perform tasks given natural language prompting.

The most well-known AI agent is
[Devin](https://x.com/cognition_labs/status/1767548763134964000)

, a software engineering AI agent that shocked the developer world with its slick UI and sophisticated reasoning. Given a task, Devin develops a plan of action, reads docs on the Internet, adds debug statements, and deploys its code to a website, much like a professional software engineer would do.

Devin has even been spotted asking questions in our
[Modal Slack community](https://x.com/raunakdoesdev/status/1769066769786757375)

. Devin also occasionally makes contributions to our
[examples repo](https://github.com/modal-labs/modal-examples/commits?author=devin-ai-integration%5Bbot%5D)

.

Devin is proprietary software built by
[Cognition AI](https://www.cognition.ai/)

, so naturally many developers have started building and exploring open-source alternatives. In this guide, we’ll highlight a few of the most popular open-source AI agents.

Open-source AI agents
---------------------

#### [OpenHands](https://github.com/All-Hands-AI/OpenHands)

Formerly known as OpenDevin, OpenHands is arguably the most popular open-source AI agent today. They provide a convenient Docker command to try it out locally as well a headless mode for scripting. The experience overall feels very Devin-like.

Here’s a screenshot of me running OpenHands locally and asking it to build me a simple calculator app:

![openhands-example](https://modal-cdn.com/open-devin/openhands-example.png)

OpenHands runs all generated code in an isolated
[Docker container](https://docs.all-hands.dev/modules/usage/architecture/runtime)

. They recently just raised a
[seed round](https://www.all-hands.dev/blog/press-release-all-hands-announces-5m-to-scale-ai-agent-for-software-development)

, presumably to help them build out their hosted solution, which is currently in
[waitlist](https://www.all-hands.dev/join-waitlist)

.

#### [SWE-agent](https://github.com/princeton-nlp/SWE-agent)

SWE-agent is a product of the Princeton NLP department and the authors of
[SWE-bench](https://www.swebench.com/)

, an AI agent evaluation framework based on resolving Github issues from 12 popular Python repositories. SWE-agent is their attempt at building an AI agent that can solve the benchmark.

Here’s a screenshot of me running the SWE-agent locally and pointing it at a simple Github issue to fix (resolving a missing colon):
![swe-agent](https://modal-cdn.com/open-devin/swe-agent.png)

Compared to OpenHands, SWE-agent is less a general AI agent and more specifically optimized to resolving specific Github issues; it doesn’t appear to accept generic prompts. At this point in time, the project overall seems to be more research-focused, and it’s unclear if companies are successfully using SWE-agent in production.

#### [Aider](https://github.com/paul-gauthier/aider)

Aider’s AI agent works directly from the terminal, unlike the other examples that have a frontend with IDE, browser, and terminal. As a result, it’s a lot easier to install and get started with; you just run
`pip install aider-chat`
, add your LLM API keys, and start chatting with the LLM right away.

Here’s an example of me asking Aider to build a simple calculator app:
![aider-example](https://modal-cdn.com/open-devin/aider-example.png)

One downside of Aider is it executes code directly on your computer. Generated code has no safety guarantees, so you’d ideally want to run it in an isolated sandbox environment.

#### [smol-developer](https://github.com/smol-ai/developer)

smol developer is one of the first open-source AI agents that created a lot of buzz around May 2023. Similar to Aider, it functions as a code generation tool you trigger from the terminal, but it doesn’t actually execute any of the code automatically. A “human in the loop” is still required to debug and run the generatd code.

We built a tool called
[devlooper](https://github.com/modal-labs/devlooper)

which extends smol developer by iteratively generating and testing code safely in our
[Sandbox](https://modal.com/docs/guide/sandbox)

primitive.

In this GIF, devlooper takes 11 iterations to create a Python library that generates voronoi diagrams:
![devlooper-example](https://modal-cdn.com/open-devin/devlooper-example.gif)

#### [Devika](https://github.com/stitionai/devika)

Devika is another popular open-source alternative, with its express purpose stated as an “open-source alternative to Devin”. Compared to the other AI agents, I personally found this a bit more difficult to set up and seems to be in more of an experimental development stage.

Conclusion
----------

Open-source has quickly been catching up to Devin since the excitement around its release as developers explore alternatives. OpenHands seems to be the most promising to check out, but there are many to consider.

Arguably the biggest challenge with running open-source AI agents is finding the right isolated runtime to safely run generated code. This is a great use case for our
[Sandbox](/docs/guide/sandbox)

primitive that allows you to define containers at runtime and run arbitrary code inside of them. We’re following the AI agent space closely, so let us know in our
[Slack](https://modal.com/slack)

which AI agents you’ve been trying out!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/open-source-gpu-glossary
================================================================================

We open sourced the GPU Glossary
================================

Two months ago, we released the
[GPU Glossary](https://modal.com/gpu-glossary)

, an interlinked compendium of short documents about GPU programming topics.

Today, we’re announcing that the GPU Glossary is now available “open source” —
[on GitHub](https://github.com/modal-labs/gpu-glossary)

under a Creative Commons BY 4.0 License.

Why write a GPU Glossary?
-------------------------

Programming GPUs is hard.

In some way’s that’s fundamental, because GPUs are used for high-performance computing and high-performance computing is hard — performance breaks abstractions, performance is hard to debug, performance is relative and competitive.

But it’s also harder than it needs to be, because there isn’t the same depth and quality of reference and educational material available online. In particular, there aren’t many resources that cross the entire stack, weaving a coherent picture, the way a high-quality textbook or course would.

We wrote the
[GPU Glossary](https://modal.com/gpu-glossary)

to help address this gap — to provide “GPU documentation for humans”.

Why make it open source?
------------------------

When we shared the GPU Glossary, we observed a ton of excitement and enthusiasm from the community, from the
[GPU MODE Discord](https://www.youtube.com/watch?v=qmpGv72qPCE)

to
[Hacker News](https://news.ycombinator.com/item?id=42675529)

and
[Twitter](https://x.com/KhonaMikail/status/1867315316516380723)

.

We also received multiple requests to contribute — typo fixes, small corrections, entire new sections, compilation to eBook format, you name it. We accepted those we could (and thanked the contributors
[here](https://modal.com/gpu-glossary/contributors)

), but our throughput over email and social media was limited.

That’s why we’re releasing the raw material
[on GitHub](https://github.com/modal-labs/gpu-glossary)

and under a
[permissive license](https://github.com/modal-labs/gpu-glossary)

that encourages others to contribute and makes it easier for us to handle contributions. It also allows you to fork it for your own purposes. You just need to link back to us — a bit of credit goes a long way in the attention economy!

How can I contribute?
---------------------

We’ve pre-populated the
[GitHub Issues](https://github.com/modal-labs/gpu-glossary/issues)

for the repo with some things we’d like to do to extend the glossary — like adding
[two sample matmul kernels](https://github.com/modal-labs/gpu-glossary/issues/10)

,
[writing about Thread Block Clusters](https://github.com/modal-labs/gpu-glossary/issues/1)

, and including a
[script that outputs the glossary a single Markdown file](https://github.com/modal-labs/gpu-glossary/issues/2)

. We’d be happy to accept your contribution!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/open-source-stt
================================================================================

All the open-source Whisper variations
======================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Why can’t you just use Whisper?
-------------------------------

When OpenAI open-sourced
[Whisper](https://openai.com/index/whisper/)

, it gave the world a great speech-to-text model. Whisper is accurate, reasonably efficient, and free (even for commercial use cases). But it’s missing some key features that developers are often looking for, including:

* **Speaker Diarization**
  : Distinguishing between different speakers in an audio file.
* **Word-level timestamps**
  : Whisper has segment level timestamps but not word-level timestamps.
* **Streaming**
  : Whisper processes audio in chunks of 30 seconds and does not
  support real-time or streaming speech-to-text conversion.

(For a guide on how to run vanilla Whisper on Modal, see
[here](/blog/how-to-run-whisper-large-v3-on-modal)

.)

Whisper variations
------------------

This means that if you are building a performant app that needs transcription, you shouldn’t use the Whisper library directly. Instead, you should use a variant that fills in some of the missing functionality, or provides speedups.

In this article, we’ll explore the various open-source Whisper variations, their unique features, and help you decide which one best suits your specific needs.

Table of contents
-----------------

* [Takeaways](#takeaways)
* [WhisperX](#whisperx)
* [Whisper JAX](#whisper-jax)
* [Whisper.cpp](#whispercpp)
* [Distil-Whisper](#distil-whisper)
* [Whisper Streaming](#whisper-streaming)
* [Running Whisper on Modal](#running-whisper-on-modal)

Takeaways
---------

* Overall best:
  [WhisperX](https://github.com/m-bain/whisperX)
* If you need to recognize multiple speakers:
  [WhisperX](https://github.com/m-bain/whisperX)
* If you need real-time transcription:
  [Whisper Streaming](https://github.com/ufal/whisper_streaming)
* If you want word-level timestamps:
  [WhisperX](https://github.com/m-bain/whisperX)
* If you need to run it on a phone or laptop:
  [Whisper.cpp](https://github.com/ggerganov/whisper.cpp)

WhisperX
--------

[WhisperX](https://github.com/m-bain/whisperX)

stands out as the most versatile and feature-rich Whisper variation. Here’s why it’s our top pick:

1. **Fast automatic speaker recognition**
   : WhisperX adds word-level timestamps and speaker diarization, making it ideal for multi-speaker transcriptions.
2. **Speed**
   : It uses
   [Faster-Whisper](https://github.com/guillaumekln/faster-whisper)

   under the hood, providing a 4x speed increase compared to the original Whisper.
3. **Language support**
   : While not universal, WhisperX supports a wide range of languages including English, Spanish, French, German, Italian, Portuguese, Dutch, Russian, Mandarin Chinese, and Japanese.

If your project requires accurate transcription with speaker identification and
precise timing, WhisperX is likely your best choice.

For a guide on how to deploy WhisperX on Modal, see
[here](/blog/how-to-run-whisper-large-v3-on-modal)

.

Whisper JAX
-----------

[Whisper JAX](https://github.com/sanchit-gandhi/whisper-jax)

offers unparalleled speed for those with access to TPU v4 hardware:

1. **JAX implementation**
   : Built on the Hugging Face implementation of Whisper but written in JAX instead of PyTorch.
2. **Extreme speed-up**
   : Achieves 10-15x speed increase on TPU v4 hardware, with potential for up to 70-100x faster performance than the original OpenAI implementation.
3. **Scalability**
   : Particularly effective with large batch sizes for long audio files.

If you have access to TPU v4 and need to process large volumes of audio quickly, Whisper JAX is the winner.

Whisper.cpp
-----------

[Whisper.cpp](https://github.com/ggerganov/whisper.cpp)

brings Whisper’s capabilities to edge devices:

1. **C++ implementation**
   : Super lightweight implementation in pure C/C++ that allows for on-device usage, including laptops and phones.
2. **Quick startup**
   : Fast boot-up time makes it ideal for applications requiring immediate transcription.
3. **Efficient processing**
   : Can transcribe 1 hour of audio in approximately 8.5 minutes on standard hardware.

Choose Whisper.cpp when you need offline processing or want to run transcriptions directly on user devices.

Distil-Whisper
--------------

[Distil-Whisper](https://github.com/huggingface/distil-whisper)

, from the HuggingFace team, is a lightweight and efficient Whisper variation:

1. **Smaller and faster**
   : A distilled version of Whisper that is 4x smaller and 6x faster than the original model.

Whisper Streaming
-----------------

[Whisper Streaming](https://github.com/ufal/whisper_streaming)

is a real-time Whisper implementation that supports streaming audio:

1. **Real-time transcription**
   : Provides streaming capabilities for live audio transcription.
2. **Support for different backends**
   : Several alternative backends are integrated. The most recommended one is faster-whisper with GPU support.

Running Whisper on Modal
------------------------

Once you’ve picked your Whisper model, you will need somewhere to deploy it. Modal offers a serverless compute platform for AI and data teams. When you deploy Whisper (or a variant) of it on Modal, you are only charged when the model is running, and you don’t have to worry about infrastructure management.

We have a guide for how to get this set up
[here](/docs/examples/whisper-transcriber)

.

Conclusion
----------

In the vast majority of use cases, our recommendation is to use WhisperX
(preferably
[deployed on Modal](/blog/how-to-run-whisperx-on-modal)

) for the best balance of ease-of-use, performance,
and feature completion.

For more tips and tricks on how to get faster transcription, see our post on
[performance optimizations for running Whisper](/blog/faster-transcription)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/open-source-tts
================================================================================

Top open-source text-to-speech models in 2025
=============================================

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

*Updated: 2025-05-29*

The text-to-speech (TTS) landscape is changing rapidly, with new state-of-the-art
models launching every month, many of them open-source.

This article explores the top open-source TTS models, based on Hugging Face’s trending models and insights from our
[developer community](https://modal.com/slack)

.

Table of Contents
-----------------

| Model | Parameters | Created by | Released | License |
| --- | --- | --- | --- | --- |
| [Chatterbox](#chatterbox)  ( [deploy on Modal](/docs/examples/chatterbox_tts)  ) | Not specified | Resemble AI | May 28 2025 | MIT |
| [Dia](#dia)  ( [deploy on Modal](https://github.com/waydegilliam/dia-modal)  ) | 1.6B | Nari Labs | Apr 21 2025 | Apache 2.0 |
| [Kokoro](#kokoro) | 82M | Hexgrad | Jan 10 2025 | Apache 2.0 |
| [Sesame CSM](#sesame-csm) | 1B | Sesame | Feb 26 2025 | Apache 2.0 |
| [Orpheus](#orpheus) | 3B/1B/400M/150M | Canopy Labs | Mar 7 2025 | Apache 2.0 |

Chatterbox
----------

[Chatterbox](https://github.com/resemble-ai/chatterbox)

is a small, fast, and easy-to-use TTS built on 0.5B Llama. At the time of this writing, it’s the #1 trending TTS model on Hugging Face.

Here’s Chatterbox generated audio for “Have you heard about Modal Labs? They’re transforming cloud computing for AI and machine learning workloads.”

Your browser does not support the audio element.

If you’re just getting started with open-source TTS models, we recommend Chatterbox. You can deploy it on Modal today using
[our example](/docs/examples/chatterbox_tts)

.

Dia
---

[Dia](https://github.com/nari-labs/dia)

is a 1.6B parameter TTS model that generates highly realistic sounding dialogue. At this time, Dia only supports English.

Your browser does not support the audio element.

This generated audio sounds quite human, albeit a little manic (not to mention the creepy laughter they insert everywhere).

The creators of Dia, Nari Labs, are still in early stages of development. However, we’ve seen lots of excitement within our community around the Dia model, even a user-submitted
[how to deploy Dia on Modal](https://github.com/waydegilliam/dia-modal)

repo.

Kokoro
------

[Kokoro](https://github.com/hexgrad/kokoro)

is an 82M parameter TTS model. At 82M parameters, it’s less than 10% the size of Dia. This means that it’s much faster and cheaper to run, though arguably at the cost of some quality.

Your browser does not support the audio element.

This generated audio sounds more artificial and Siri-like than other models, but it’s probably the cleanest output among all our examples, especially its pronunciation of “Modal Labs”.

Sesame CSM
----------

[Sesame CSM](https://github.com/SesameAILabs/csm)

(Conversational Speech Model) is a 1B parameter TTS model built on Llama. It’s particularly well-suited for conversational use cases where you have two different speakers.

Your browser does not support the audio element.

This generated audio sounds the most unnatural out of our examples, though there are parameters you can tweak to improve quality like adding context.

Orpheus
-------

[Orpheus](https://github.com/canopyai/Orpheus-TTS)

is a Llama-based TTS model that comes with 3B, 1B, 400M, and 150M parameter versions. It was trained on over 100k hours of English speech data.

It’s optimized for natural, human-like speech and also supports zero-shot voice cloning, guided emotion, and realtime streaming. They also have a family of
[multi-lingual models](https://huggingface.co/collections/canopylabs/orpheus-multilingual-research-release-67f5894cd16794db163786ba)

that includes Chinese, Hindi, Korean, and Spanish as well as fine-tuning scripts.

While the quality of the demos is impressive, we had trouble getting it to run (including their examples), so take caution if trying to deploy this yourself.

Conclusion
----------

Text to speech is an exploding use case for many AI-first companies, and the quality of open-source models continues to improve every month. Deploying open-source TTS on Modal could give you the best of both worlds: higher quality models at a fraction of the cost compared to other closed source providers.

To get started, check out our
[Chatterbox TTS example](/docs/examples/chatterbox_tts)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/prompt-olympics-summer-2024
================================================================================

Competitive prompt engineering
==============================

What is the skill ceiling for prompting? The team at
[Basis](https://www.getbasis.ai/)

has been examining automatic prompt optimization tools and seeing promising results. But even when automatically-discovered prompts outperformed their hand-designed prompts, they faced that nagging question.

To find out, they organized the Prompt Olympics, an event that brought the culture of competitive programming to prompt engineering. What better way to find the peaks of human performance than some friendly competition — and a $5,000 prize?

At Modal, we love programming competitions (we hired IOI medalists before it was cool), so we jumped at the chance to provide them the infrastructure they needed to run the Prompt Olympics.

In this article, we walk through two components of the competition, explaining both the prompting challenges and how Basis deployed them on Modal.

Challenge: Writing a prompt to write code
-----------------------------------------

![Coding challenge interface](https://modal-cdn.com/cdnbot/prompt-olympics-summer-2024-coding-challenge-ui.png)

In one challenge, participants were asked to do something familiar to any competitive programmer — write code that passes tests. But instead of writing it themselves, they had to write prompts to control language models writing code.

The underlying LLM for this challenge, LLaMA 3 70B, was deployed on Modal, which provides serverless GPU acceleration and simple web deployment for Python programs. If you want to deploy your own LLM on Modal, check out
[this guide](/docs/examples/trtllm_llama)

.

To grade this challenge, the LLM-produced code needed to be executed and tested. But this is a competitive environment and the code is untrusted. How do you handle a foolish LLM (or an angry player) generating code that runs
`sudo rm -rf /`
or unzips a zip bomb?

Modal provides just the right tool for this: a Modal
[Sandbox](/docs/guide/sandbox#dynamic-sandboxes)

is a dynamically allocated secure container with all the features of normal Modal Functions, from controlled access to Secrets and custom environments to GPU acceleration. For more on running untrusted LLM code with Modal Sandboxes, see
[this video](https://www.youtube.com/watch?v=X3yzWtAkaeo)

from LangChain.

Finale: Playing Crafter
-----------------------

[

](https://modal-cdn.com/crafter_best_a1749f51.mp4)

The winner of the event was decided by playing a video game. But instead of controlling the character directly, participants had to construct a prompt for a language model that controlled the character. The video game chosen was
[Crafter](https://github.com/danijar/crafter)

, a simplified two-dimensional version of Minecraft designed for use in agent research. The video above shows the best run, where an agent successfully creates a pickaxe and mines coal while dodging enemies.

This challenge resembled a different type of competitive programming. In some competitions, participants are challenged not to pass test cases or minimize latency but to write programs that control virtual agents, like simulated delivery drones or tanks.

For this challenge, the team at Basis found that even the most capable open source models, like LLaMA 3 70B, were difficult to prompt effectively — though they were able to run those models on Modal. So they switched instead to a proprietary model API service for the LLM component.

But they still used Modal! In addition to running a client for the external modeling service, the Basis team ran the Crafter game on Modal. Modal provides general purpose computing infrastructure with serverless semantics and a easy-to-use Pythonic SDK, not just the tools you need for LLM inference.

---

You can read more about the competition, including the results and the winning prompts, on the
[Basis blog](https://www.getbasis.ai/blogs/prompt-olympics-summer-2024---recap)

.

We were incredibly excited to bring our infrastructure to support the Basis team in bringing the spirit of competitive programming to prompting. Much of the team at Modal learned to program through competitions. If you did too, know that we’re
[always hiring](https://jobs.ashbyhq.com/modal)

!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/qart-codes-evals
================================================================================

How we used evals and inference-time compute scaling to generate beautiful QR codes that actually work
======================================================================================================

![author](https://modal-cdn.com/charles-frye.jpg)

[Charles Frye

@charles\_irl](https://twitter.com/charles_irl)

Developer Advocate

![author](https://modal-cdn.com/will-shainin.jpg)

[Will Shainin

@Will\_Modal](https://twitter.com/Will_Modal)

ML Engineer

![author](https://modal-cdn.com/erik-dunteman.jpg)

[Erik Dunteman

@erikdunteman](https://twitter.com/erikdunteman)

Founder, pig.dev

Two techniques dominate discussion of the engineering of language model applications and artificial intelligence:
[evals](https://x.com/leonardtang_/status/1919147635794977207)

and
[inference-time compute scaling](/blog/llama-human-eval)

. These techniques are general, far beyond topics du jour like RAG chatbots and code completion and AI boyfriends. In fact, they are fundamental to any application of foundation generative models.

In this blog post, we’ll walk through how we used evals and inference-time scaling to improve the quality for our implementation of one of the first viral applications of image generation models: encoding QR data inside a generated image (aka
*QArt codes*
).

Here are some samples from our system, without any per-prompt cherry-picking:

![Neon green cubes, rendered in blender…](https://modal-cdn.com/blog/images/qart-code-green-cubes.webp)
![A dense jungle scene filled with towering trees, intertwining vines, and an abundance of foliage…](https://modal-cdn.com/blog/images/qart-code-jungle.webp)
![Penguins have a picnic on the Savannah…](https://modal-cdn.com/blog/images/qart-code-penguins.webp)
![A close-up view of a vibrant graffiti-covered urban wall…](https://modal-cdn.com/blog/images/qart-code-graffiti.webp)

We were able to

* **boost QR code scan rate**
  to our
  **service-level objective of ninety-five percent**
* while
  **improving aesthetic quality**
* and returning codes to users
  **in under 20s**
  , p95.

We use codes like these in all the places we might use QR codes — like guerrilla marketing and credit giveaways.

You can try the system for yourself at
<https://qart.codes>

, check out the code
[here](https://github.com/charlesfrye/qart-codes)

, or read on for more about how we did it — and learn some ideas for engineering your own generative model applications.

Generative models always start with a cool demo, like QR codes that look beautiful.
-----------------------------------------------------------------------------------

The same basic story has played itself out over and over again in this early era of “generative AI”. A new foundation model or technique for some domain trends on social media and there’s a gold rush as a million hackers simultaneously build zero-to-one proofs of concept in a day. Whether it’s making pictures of cats or booking a flight, it works well enough to cut a demo video and maybe snag some of the least intelligent VC money.

One of the first instances of this pattern kicked off on June 5, 2023, when Reddit user nhciao
[posted](https://reddit.com/r/StableDiffusion/comments/141hg9x/controlnet_for_qr_code/)

“ControlNet for QR Code” on the Stable Diffusion subreddit, demonstrating that diffusion models with ControlNet steering could create images that were also QR codes.

![Screenshot of a Reddit post by u/nhciao demonstrating QR code generation with diffusion models](https://modal-cdn.com/blog/images/qart-codes-nhciao-post.webp)

ControlNets add the ability to “control” the images made by diffusion models like Stable Diffusion or Flux using some other image. You might, for instance, use this simple line drawing of a turtle to control pose and position and size — all generations match the line drawing on those factors. This can be much more expressive than prompting! “The turtle is looking up and to the left with a look of resignation on its face, taking up the bottom two-thirds of the image…”

![A line drawing of a turtle is converted into images of turtles with the same structure](https://modal-cdn.com/blog/images/qart-codes-controlnet-sample.webp)

Separately, QR (Quick Response) codes are
[designed](https://perthirtysix.com/how-the-heck-do-qr-codes-work)

to transmit short snippets of information between machines, just like other protocols you may be more familiar with (HTTP/IP/Ethernet). But they transmit via a chaotic and unpredictable medium: bright and dark patches of color that modulate light in open air, at human scales, and without the protective shielding of a fiber optic cable. And because QR code readers have to be robust to interference in that medium, like
[the piece of paper they are printed on being torn](https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/QR_Code_Damaged.jpg/1200px-QR_Code_Damaged.jpg)

, the QR code protocol comes with sophisticated error correction.

The insight from nhciao was that ControlNets based on brightness patterns or edges, like the one demonstrated above, could be used to produce images with the brightness and darkness patterns of a QR code. And thanks to the error correction, the model can exercise a bit of creative license and “corrupt” the pattern to better match the prompt while still communicating the encoded message.

Like many others, some of us jumped on these “QArt codes” immediately — all the way back in the summer of 2023.

[![https://x.com/charles_irl/status/1669842036860788736](https://modal-cdn.com/blog/images/qart-codes-launch-2023.webp)](https://x.com/charles_irl/status/1669842036860788736)

Then comes the disappointment, like QR codes that don’t scan.
-------------------------------------------------------------

So much for the party, now for the hangover. The problem that came next, and the reason you haven’t seen gorgeous QR codes replace the standard black-and-white squares, was the same problem that bedevils demos of coding agents and artificial lawyers and self-driving cars — “it worked on my
~~machine~~
prompt”. It’s hard to consistently create QR codes that are both gorgeous and scannable!

Consider, for instance, the image below, which we generated using a web app created by nhciao. This image may look a bit like a QR code, especially if you squint, but we haven’t been able to get it to scan. The model’s attempt to build the required Finder Pattern (concentric squares of alternating black and white) in the bottom-left out of a dark skyscraper and some reflections appears to have failed.

![A city skyline with fireworks with some elements of a QR code included](https://modal-cdn.com/blog/images/qart-codes-sample-failure.webp)

Not quite a QR code!

This is a show-stopping bug. A putative QR code that readers can’t scan isn’t a QR code, it’s just a picture with a weird aesthetic. Similarly, failures for coding agents that neglect to implement features or chatbots that offer off-policy refunds limit their applicability.

So in building the initial version of
[qart.codes](https://qart.codes)

, we focused first and foremost on this problem — “scannability”.

The result was a system that produced scannable QR codes with a reasonable probability (as in, it didn’t make us want to tear our hair out), so long as we only prompted with textures or styles, like “computer circuitry, splashes of color in gunmetal gray”, and not with complex scenes, like “a patient orangutan explains string theory to an anime waifu”.

We started using it in a few places where we were previously using QR codes — and even in places where we weren’t using QR codes previously because they’d ruin the vibe.

[![A slide showing a link displayed via a QR code](https://modal-cdn.com/blog/images/qart-codes-sample-usage-irl.webp)](https://www.youtube.com/watch?v=N7lJY5IKVLE&t=2364s)

A QArt code in the slides for a talk by one of the authors.

With some creativity and effort on post-processing, you could even achieve some pretty cool effects!

![A QArt code created by superimposing an image (the logo for Lovers in a Dangerous Spacetime) over a generated QArt code.](https://modal-cdn.com/blog/images/qart-codes-sample-composite.webp)

A QArt code created by superimposing an image (the logo for Lovers in a Dangerous Spacetime) over a generated QArt code.

But it was far from the promise of the original demo.

We solved this quality gap by developing good evals, following the standard ML engineering playbook: collect human judgments, align to them, then scale.
--------------------------------------------------------------------------------------------------------------------------------------------------------

The problem here is the problem of
*evaluation*
— determining the value or quality of the system. In traditional software, most evaluation can be done objectively. It can even be done “rationally” (as opposed to empirically). That is, you only need a single demonstration or logical argument for correctness or incorrectness. Think of simple repros and compiler guarantees.

But neural networks are less like the digital computers we are used to programming and more like analog computers. They are a sloppy heap of floating point numbers, not an elegant stack of integers. Correctness and quality are measured and defined for a specific use case and up to a tolerance — if they are measured and defined at all!

The tools we use to assess quality for these systems are called
*evals*
. They combine a procedure for producing inputs (which might just be a database or files!) with a procedure for evaluating outputs (which might be human labelers or might be a computer program). To engineer these procedures, you need to understand the goals of your system.

Our system has two core goals:

1. produce an image that a QR reader can scan
2. produce an aesthetically appealing image.

One servant cannot serve two masters, so we needed to determine how to trade off between these goals.

Aesthetic quality is

* primarily the domain of the underlying foundation models,
* highly subjective and so variable across users, and
* difficult to measure well.

Scanning, on the other hand, is

* firmly in our application domain and maps onto correctness,
* has minimal variability across users, and
* easy to measure objectively.

So we decided to make scan rate the key target for eval development.

Next, the goals need to be operationalized, mapped onto concrete processes and measurements.

We first operationalized our two goals as “can be scanned by a human wielding an iPhone pointed at a screen in our office” and “gets a thumbs up from the prompter”.

These both involve a lot of human labor, so we next came up with automated processes to replace them. We found the
[QReader library](https://github.com/Eric-Canas/qreader)

for QR code scanning, which uses a YOLO model to detect QR codes and then applies a number of transformations, like blurring and thresholding, before attempting to interpret the image as a code. We also found a simple aesthetic rating predictor from the Stable Diffusion community — a quick-and-dirty machine learning model that could predict human aesthetic ratings.

But those systems are also heuristic and might themselves make mistakes we can’t predict. So we needed to be sure they were aligned with our trusted human process, and we couldn’t just rely on tests and logic. We needed to run experiments to know that they were good.

We needed evals for our evals.

We built our evals iteratively. We vibe-checked them manually at first and then with results on 100 prompt-URL pairs (first image below). Finally, we scaled up our alignment check for our evals to results on about two thousand prompt-URL pairs.

That meant we needed to attempt to scan thousands of QR codes by hand while writing down the results and our aesthetic judgment. Sometimes, as an engineer, you’ve just go to roll your sleeves up and do some manual labor. Major props to Will Shainin on the team for taking the plunge (it’s always the mechanical/electrical engineers, not the PhDs).

![A photo of a post-it note attached to a computer monitor indicating four options: scans/not scans, good/not good. The computer monitor shows a simple notebook interface for evaluating QArt codes.](https://modal-cdn.com/blog/images/qart-codes-evals-apparatus.webp)
![Two of the authors manually check whether a QArt code scans.](https://modal-cdn.com/blog/images/qart-codes-charles-dunteman-evals.webp)

![A screenshot of a mobile phone displaying a link decoded from a QArt code.](https://modal-cdn.com/blog/images/qart-codes-evals-at-home.webp)

You may not like it, but this is what peak ML engineering performance looks like.

As you can see, the “apparatus” here can be pretty low-tech and low-effort. Tools of the trade include post-it notes, whiteboards, spreadsheets, and Jupyter notebooks — in roughly that order! Sticking to simple techniques keeps the focus where it belongs: on the data and the problem.

One piece where we did find it helpful to use an external tool was experiment management and data visualization, which crosses from ML software to data visualization to team coordination.

We used
[Weights & Biases’ Weave](https://github.com/wandb/weave)

for this, since one of our team members had some experience with it (conflict of interest note:
[he is now fully divested](https://x.com/charles_irl/status/1925569091781448004)

). We were able to log raw data from our experiments to
`wandb`
and then construct structured charts and analyses on the fly, all shareable across the team.

![A screenshot of the Weave user interface showing sample QArt codes and derived metrics.](https://modal-cdn.com/blog/images/qart-codes-weave-plot.webp)

An example W&B Weave chart grouping generations by the system version (v1/v2) and whether they scanned (True/False) and displaying samples along with a distribution of aesthetic scores.

FYI: while the details of the approach for this project are idiosyncratic, the approach itself is not at all! See, for instance, Hamel Husain’s blog post
[“Creating a LLM-as-a-Judge That Drives Business Results”](https://hamel.dev/blog/posts/llm-judge/)

.

The final piece (not the first) is scaling up offline compute to select system parameters.
------------------------------------------------------------------------------------------

Putting it all together, our final engineering goal was to

* achieve a ninety-five percent scan rate, as measured by QReader, and
* not regress on aesthetic quality, as measured by the aesthetic rating predictor

on a set of prompts and URLs.

We manually iterated on code, checking evals ad hoc as we went, until we’d narrowed down the outlines of our system. We developed new prompting strategies, confirmed our model choice, and determined which parameters we needed to vary and over which ranges (tl;dr - long prompts, stick with SD1.5 for now, and focus on ControlNet start time).

This part is still pretty vibes-based, and it’s where the great ML engineers, the ones who can 10x a project and
*avoid*
a million dollar training run, are separated from the merely good.

![Two photographs, juxtaposed: one of the authors evaluating QArt codes manually, his pose matching that of the man in the other photograph. That man is Rick Rubin, intently producing music.](https://modal-cdn.com/blog/images/qart-codes-ml-vibe-engineering.webp)

Yeah man, we need the guidance to come in on the track a bit earlier — and can somebody turn down the diffusion noise?

Once we were ready to sweep over key configuration parameters, it was time to scale up. Thanks to our trusty evals, we didn’t need ten engineer-seconds per image anymore, just a computer-second or two. So we could scale this up into the tens of thousands of images and calculate our figures of merit in minutes. (BTW
[Modal’s serverless platform](/)

makes this trivial, just a few lines of code).

But even though we had built an eval system we trusted, we wanted to continue to look at the data to check whether our evals still aligned with our heuristic judgments.

For this, we developed a visualization we called the “toast plot”, after a famous bit of Internet culture.

![A grid of pieces of toast, arranged in rows by the amount of time they were toasted and arranged in columns by the temperature they were toasted at.](https://modal-cdn.com/blog/images/qart-codes-toast-plot.webp)

POV: you are an ML engineer making breakfast.

Two parameters of interest vary along two axes — how long is the bread cooked for & at what temperature; how long is the ControlNet guidance in place & at what strength. The results for a single case are below.

![A grid of QArt codes, arranged in rows by the amount of time they were subjected to ControlNet guidance and arranged in columns by the strength of the ControlNet guidance.](https://modal-cdn.com/blog/images/qart-codes-toast-plot-sample.webp)

An instance of the “toast plot” for one prompt.

We selected the configuration one row and one column from the top-left corner, which produced images that had much better aesthetics than the existing system.

But it didn’t hit our objective of ninety-five percent scan rate — in fact, it actually had a lower scan rate than our existing systems.

But we hadn’t given up on our goals. We just used another technique:
*inference-time compute scaling*
.

Our evals were good enough to move them to production — so we could solve all of our problems by scaling compute!
-----------------------------------------------------------------------------------------------------------------

Let’s step back for a second. Our evals are how we measure our system’s performance offline. We automated them, so now they are fast. Why not run our evals in production so that we can improve our system’s performance
*online*
?

The easiest move is to create multiple outputs in parallel and then compute evaluation metrics on all of them. These can either be used on the backend (e.g. to retry) or delivered to the frontend. We chose the latter. Our frontend then ranks the QR codes — first by whether they scan, then by their aesthetic score — and selects the top four for display.

Scaling up parallel generations maps rather nicely onto the hardware used for inference: GPUs. The
[one thing every engineer needs to know about GPUs](https://www.youtube.com/watch?v=ch2ODgbJjlA)

is that they are designed to achieve high throughput through massive concurrency and parallelism.

Producing multiple images for a single prompt exposes more parallelism for the GPU to exploit. In this case, we get exponential improvement in scan probability (
`p^n`
) for sublinear latency cost. That’s what engineers call “a big win”. For a walkthrough of this same basic idea applied to LLMs, see
[our blog post on the “Large Language Monkeys” paper](/blog/llama-human-eval)

.

The usual blocker is that even once automated, the evals are too slow to run in production. Even though our evals do require running neural networks, these networks are much lighter weight than the generator itself and so contribute far less to latency.

By creating eight generations per request, we were able to hit our ninety-five percent scan rate SLO (measured per batch) while staying within an acceptable latency (at time of writing, under 20s p95 end-to-end) while improving aesthetic quality (just look at them!).

![A QArt code in the style of Piet Mondrian.](https://modal-cdn.com/blog/images/qart-code-mondrian.webp)
![A QArt code displaying a bouquet of flowers.](https://modal-cdn.com/blog/images/qart-code-floral.webp)
![A QArt code depicting a fantastical mushroom forest.](https://modal-cdn.com/blog/images/qart-code-mushrooms.webp)
![A QArt code showing a grim, dark industrial scene.](https://modal-cdn.com/blog/images/qart-code-industrial.webp)

You can try the service for yourself by clicking this link:
<https://qart.codes>

— or just scan one of the codes above.

If you’d like to go “beyond the demo” and build robust applications of generative models, try Modal.
----------------------------------------------------------------------------------------------------

The present of generative models and artificial intelligence is hype and vaporware that promises the stars but is at best just a start.

We expect this technology’s future, and perhaps a noticeable percentage of the future world economy, to look rather different: taking hard problems and constructing automated evaluations for them. Building hills for engineers and their machines to climb. For more on this vision, see the work of
[Andon Labs](https://andonlabs.com/)

, who partnered with Anthropic on
[Claude Vend](https://www.anthropic.com/research/project-vend-1)

.

If you’re interested in building that future, we think Modal, the serverless platform we are building and which we used to do this work, is a great tool. It provides high-performance, flexible compute that can scale up your evals and your inference with a Python SDK loved by ML researchers and production engineers alike. Check out
[our marketing site](/)

for details.

*The authors would like to thank
[u/nhciao](https://reddit.com/u/nhciao)

for sharing the initial QArt codes work,
[Illyasviel](https://huggingface.co/lllyasviel/ControlNet)

and
[monster-labs](https://huggingface.co/monster-labs)

for training useful ControlNets for QArt code generation, and
[Axel Setyanto](https://www.linkedin.com/in/axelsetyanto/)

& Teddy Frye for work on the frontend.*

*We’d also like to thank the following reviewers: Colin Weld & Peyton Walters of Modal and
[Hamel Husain](https://twitter.com/hamelhusain)

(the
[evals guy](https://bit.ly/evals-ai)

).*

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/quora-case-study
================================================================================

How Quora uses Modal to run thousands of Python sandboxes simultaneously
========================================================================

[Quora](https://www.quora.com/)

is a Q&A platform where users can ask, answer, and peruse questions on a variety of topics. With 400 million monthly unique visitors, it’s an invaluable contributor to the world’s knowledge-sharing. Quora uses
[Modal Sandboxes](/docs/guide/sandbox)

to securely execute LLM-generated code in Poe, their AI chatbot platform. The team shipped months earlier using Modal rather than building in-house. They’re also saving an ongoing 2 engineers’ worth of infrastructure maintenance time!

Hello, Poe
----------

In 2023, Quora launched
[Poe](https://poe.com/)

, an AI chatbot platform where anyone can deploy a public chatbot. With millions of monthly active users, Poe is the default destination for many AI builders to experiment with different models. Quora has since raised $75M to keep expanding Poe.

A code interpreter for Poe
--------------------------

Many of the LLM bots in Poe can generate code, and users expected to run that code in Poe rather than copy-pasting it to their editors. The Quora team needed a way to safely execute code in Poe in a completely isolated way, keeping that code separate from both the main Quora infrastructure and any other user’s session.

![poe gif](https://modal-cdn.com/blog/images/poe.gif)

In-chat Python execution in a Poe chatbot

There were three key requirements for this feature.

1. Security, since LLM-generated code can’t be trusted by default.
2. Low latency, since chatbot responses need to be fast in order to feel conversational.
3. Reliability, since the product has millions of users and is expected to be polished.

Low latency, reliable systems are effortful to construct. While a basic sandbox prototype would have been easy to build, the Quora team knew that orchestrating a fast-scaling system for millions of users would have taken months. And that was just considering the core code execution primitive. If they wanted security features like outbound networking restrictions or debugging features like container-level observability, that would have taken even longer.

Modal’s Sandbox product was fully featured and scalable right out of the box. Quora was already familiar with Modal and, due to Modal’s superior reliability over alternatives, had it recommended as the default deployment solution for users publishing their own Poe bots. This gave Quora the confidence to expand their usage into Modal Sandboxes.

“

There would be a lot of edge cases and unknowns if we built code sandboxes ourselves: dealing with setting separate environments, minimizing risk areas—this is not just for set-up but needs continuous consideration. We offloaded this to Modal and are actively saving 2 engineers' worth of ongoing engineering time.

”

— Hwan Seung Yeo,

Director of Engineering

A Modal Function by any other name
----------------------------------

[Modal Sandboxes](/docs/guide/sandbox)

are really just our core primitive—Modal Functions—minus our client running inside of them. This means that Quora got a battle-tested and continuously improving product right out of the box.

✅ Modal’s custom container stack, which we have invested years into making robust and secure, is already built on gVisor for enterprise-grade container isolation.

✅ Fast scalability is built in. Quora stress-tested Sandbox creation throughput to 1000 Sandboxes per second with no issue, allowing them to support thousands of users who might be generating code at any given point in time.

✅ Powerful
[networking primitives](/docs/guide/sandbox-networking)

like Tunnels and IP allowlisting come for free, too, allowing Quora to have full customizability and control over Sandbox communications.

This is just the beginning. The Poe team is working on an under-the-wraps new product that will also leverage Modal Sandboxes for code execution. We’re looking forward to sharing more once they launch!

Build fast like Quora
---------------------

Want to ship LLM coding features in days rather than months? Get started today with
[Modal Sandboxes](/docs/guide/sandbox)

.

1. Install Modal:
   `pip install modal`
2. Create an account:
   `python -m modal setup`
3. Run:

```
import modal
app = modal.App.lookup("sandbox-manager", create_if_missing=True)
sb = modal.Sandbox.create(app=app)

p = sb.exec("python", "-c", "print('hello')")
print(p.stdout.read())
sb.terminate()
```

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/rabbitmq-vs-kafka-article
================================================================================

RabbitMQ vs. Kafka: choosing the right messaging system
=======================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

[RabbitMQ](https://www.rabbitmq.com/)

and
[Apache Kafka](https://kafka.apache.org/)

are both popular open-source distributed messaging systems, but they generally excel in different scenarios. Understanding the differences between these two technologies is crucial for developers and architects when choosing the right tool for their specific use cases.

What is RabbitMQ?
-----------------

RabbitMQ is a traditional message-oriented middleware (MOM) that implements the
[Advanced Message Queuing Protocol (AMQP)](https://www.rabbitmq.com/tutorials/amqp-concepts)

. Developed in 2007 and written in Erlang, RabbitMQ is designed for low-latency message queuing and routing.

### Key features of RabbitMQ:

1. Flexible routing: RabbitMQ uses exchanges to route messages to queues based on various criteria.
2. Multiple protocols: Supports AMQP, MQTT, STOMP, and more.
3. Push model: Delivers messages to consumers as soon as they’re available.
4. Message acknowledgment: Ensures reliable delivery of messages.

What is Apache Kafka?
---------------------

Apache Kafka, on the other hand, is a distributed event streaming platform. Developed by
[LinkedIn](https://engineering.linkedin.com/kafka/kafka-linkedin-current-and-future)

in 2011 and written in Scala and Java, Kafka is designed for high-throughput, fault-tolerant, and scalable messaging.

### Key features of Kafka:

1. Distributed commit log: Messages are stored in a distributed, append-only log.
2. Scalability: Easily scales horizontally across multiple servers.
3. Stream processing: Supports real-time data processing with
   [Kafka Streams](https://kafka.apache.org/documentation/streams/)

   .
4. Long-term storage: Can retain messages for extended periods.

Use cases
---------

### When to use RabbitMQ

RabbitMQ is well-suited for:

1. Complex routing scenarios: When you need to route messages based on various criteria.
2. Traditional publish-subscribe messaging: For applications that require classic message queue patterns.
3. Low-latency messaging: When you need immediate message delivery.
4. Microservices communication: For decoupling services in a microservices architecture.

Taking the Uber app as an example, RabbitMQ might be used for:

* Real-time driver-passenger matching: When a ride request comes in, RabbitMQ could quickly route the message to the most appropriate driver based on location, vehicle type, and other factors.
* In-app notifications: For sending immediate push notifications to drivers or riders about trip updates, promotions, or account-related messages.
* Payment processing: To handle individual payment transactions in real-time, ensuring quick and reliable processing of each ride payment.

### When to use Kafka

Kafka is ideal for:

1. High-throughput event streaming: When dealing with large volumes of real-time data.
2. Log aggregation: Collecting and processing logs from multiple sources.
3. Stream processing: For applications that need to process and analyze data streams in real-time.
4. Event sourcing: When you need to maintain a complete history of events.

Taking the Uber app as an example, Kafka might be employed for:

* Trip tracking: To continuously ingest and process GPS data from millions of active drivers, allowing for real-time tracking and ETAs.
* Surge pricing calculations: To analyze real-time demand and supply data across different areas, enabling dynamic pricing adjustments.
* Analytics and reporting: To collect and process vast amounts of trip data, user behavior, and app usage for business intelligence and improving services.
* Fraud detection: To analyze patterns in real-time across millions of trips and transactions, identifying potential fraudulent activities.

Performance and scalability comparison
--------------------------------------

### Throughput

Kafka excels in high-throughput scenarios, reliably handling millions of messages per second. This makes it ideal for large-scale data streaming applications. RabbitMQ, while theoretically capable of similar throughput, requires more brokers to achieve it and is optimized for lower throughputs (thousands to tens of thousands of messages per second).

### Latency

Both Kafka and RabbitMQ offer very low latency in the millisecond range. However, RabbitMQ’s latency tends to increase under high-throughput workloads, while Kafka maintains consistent low latency even at scale.

### Scalability

Kafka is designed for massive horizontal scalability, capable of handling petabytes of data and trillions of messages per day across hundreds or even thousands of brokers. RabbitMQ can be scaled horizontally as well, but not to the same extent as Kafka.

### Fault tolerance and availability

Both systems offer robust fault tolerance and high availability:

* Kafka replicates data across multiple nodes and supports geo-replication across different datacenters and regions.
* RabbitMQ uses quorum queues and streams for data replication across nodes, and federations of clusters for moving messages between geographically distributed brokers.

While both are reliable solutions, Kafka has proven its capabilities in hyper-scale scenarios at companies like LinkedIn, Twitter, and Netflix, providing lower latencies at higher throughput.

Message persistence and durability
----------------------------------

Kafka and RabbitMQ offer different approaches to message persistence and durability:

### Kafka

* Stores messages on disk by default
* Configurable retention periods
* Facilitates easy message replay and data reprocessing from any point in time
* Designed for high-throughput, long-term storage of messages

### RabbitMQ

* Offers flexible options for message persistence:
  1. Durable queues and messages survive server restarts
  2. Persistent delivery mode increases message survival chances
  3. Publisher confirms ensure messages are received by the broker
  4. Consumer acknowledgements prevent message loss during processing
* Manages disk space, blocking producers when space is low
* Persistence features can be fine-tuned but may impact performance due to increased I/O

Both systems provide robust message durability, with Kafka optimized for long-term storage and high-throughput scenarios, while RabbitMQ offers more granular control over persistence settings.

Conclusion
----------

Choosing between RabbitMQ and Kafka depends on your specific use case:

* If you need complex routing, low-latency messaging, or traditional publish-subscribe patterns, RabbitMQ might be the better choice.
* If you’re dealing with high-throughput event streaming, need long-term storage of messages, or want to process large-scale data streams, Kafka is likely the more suitable option.

To dive deeper into the differences between RabbitMQ and Kafka, you can refer to the
[official RabbitMQ documentation](https://www.rabbitmq.com/documentation.html)

and
[Apache Kafka documentation](https://kafka.apache.org/documentation/)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/ramp-case-study
================================================================================

How Ramp automated receipt processing with fine-tuned LLMs
==========================================================

![Ramp logo](https://modal-cdn.com/cdnbot/rampqk0qx0or_29b0f860.webp)

[Ramp](https://ramp.com/)

uses Modal to fine-tune their LLMs and scale batch
processing. With Modal, Ramp was able to accelerate development of their
text-to-structured-JSON model for receipt management, driving down receipts
requiring manual intervention by 34%.

About Ramp
==========

Ramp is rebuilding the CFO suite. It combines corporate cards and expense
management, vendor management and price intelligence, procurement, bill
payments, and accounting integrations into a unified platform designed to save
time and money with every click. Businesses use Ramp as their primary spend
management solution to fully automate non-payroll spend and streamline their
financial operations. Time consuming tasks for finance teams like uploading
receipts, paying vendors, and tracking spend are managed seamlessly in Ramp’s
user-friendly interface.

The problem: Fine-tuning with custom code is a pain
---------------------------------------------------

One of Ramp’s flagship products is its intelligent receipt submissions flow,
which uses an LLM to transform OCR data to structured JSON.

Ramp initially tried LLM providers like OpenAI but were not able to get the
customizability they wanted and were also concerned about cost, reliability, and
security. They then considered using a fine-tuning API provider on open-source
models, but quickly realized this black box approach lacked customizability.

“

Finetuning on Modal allows us to implement custom logic and preprocessing. Additionally, being able to train hundreds of models in parallel helps us accelerate our training iteration and tuning.

”

— Rahul Sengottuvelu,

Head of Applied AI at Ramp

Ramp realized they needed a platform that would grant them the flexibility to
control each step of their fine-tuning workflow.

The solution: Improve model accuracy while saving cost
------------------------------------------------------

By adopting Modal, Ramp was able to quickly and confidently drive down receipts
requiring manual intervention by 34% on infrastructure that was an estimated 79%
cheaper than other major LLM providers like OpenAI.

![Diagram of Ramp's receipt processing workflow](https://modal-cdn.com/cdnbot/ramp-processing-diagramwtc95g7f_af06c6a3.webp)

As a generalized platform for running Python functions in the cloud, Modal gave
Ramp the flexibility needed to create a custom experimentation framework. They
set up Modal functions to:

* Train many candidate models in parallel
* Persist the weights from different fine-tuning runs into
  [Modal volumes](https://modal.com/docs/reference/modal.Volume)
* Serve an inference
  [endpoint](https://modal.com/docs/guide/webhooks)

  that
  could spin up the different models as needed based on a parametrized input

These critical use cases allowed the team to quickly evaluate performance across
multiple model designs.

Modal was able to support this workflow by:

* **Easily orchestrating infrastructure**
  : Modal automatically handles scaling
  up and down GPUs, which traditionally would be a huge pain with major cloud
  providers
* **Standardizing experiment environments**
  : Modal allows users to easily define
  a
  [containerized environment](https://modal.com/docs/guide/images)

  that can be attached to any Modal function

Bonus: Speeding up LLM batch processing
---------------------------------------

Outside of fine-tuning, the Ramp team also opportunistically found other use
cases for Modal.

“

Just use Modal. You can get an application up in 5 minutes.

”

— Chris Nguyen,

Software Engineer at Ramp

For instance, one engineer was faced with the daunting task of using an LLM to
strip out PII on 25,000 invoices. A script that would’ve taken 3 days to
complete the task locally was ported to a Modal function and parallelized on 256
cloud workers, which allowed the task to be completed in a mere 20 minutes at a
cost of $100.

Companies are quickly recognizing that using ergonomic infrastructure for
data-intensive applications can double developer productivity. With Modal’s
serverless platform as a critical part of their data processing stack, Ramp is
well-equipped to ship their AI features faster than ever before.

“

It’s a good feedback loop. It’s pretty easy with Modal.

”

— Gennady Korotkevich,

Software Engineer at Ramp

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/region-selection-launch
================================================================================

Introducing: Region selection
=============================

Modal now allows you to select which cloud region you’d like your Functions to run in! As a reminder, Modal is a serverless compute platform that makes it easy for developers to run cloud workloads without managing the underlying infrastructure.

![Modal region map](https://modal-cdn.com/tmpngp1l_r9_a05062c3.jpg)

You can specify regions from around the world to run your Modal Functions in.

How to specify region(s)
------------------------

Cloud region selection is available on the
[Team plan](/pricing)

(for limited CPU use cases) and the
[Enterprise plan](/pricing)

. Please email
[support@modal.com](mailto:support@modal.com)

to get access to the feature. Once you’ve been granted access, specifying the cloud region for a Function requires just a line of code!

```
import os
import modal

app = modal.App("...")

@app.function(region="us-east") # also supports a list of options, for ex. region=["us-central", "us-east"]
def function():
    print(f"running in {os.environ['MODAL_REGION']}") # us-east-1, us-east-2, us-ashburn-1, etc.
```

In the example above,
`function`
will run on instances located in
`"us-east"`
.

Region options
--------------

You can specify your desired region using varying levels of granularity. At the most granular level, you can directly specify the actual underlying region, like
`region=["eu-west-1", "eu-paris-1"]`
. We recommend using broader levels when possible, however, as this increases the pool of possible resources your Function can be assigned to and makes for faster cold starts.

Sampling of possible region strings below. Please see our
[docs](/docs/guide/region-selection)

for the full list.

```
  Broad              ->             Specific
 =====================================================
  "us"          "us-east"           "us-east-1"
                "us-central"        "us-east-2"
                "us-west"           "us-central1"
                                    "us-chicago-1"
                                    "us-west-1"
                                    ...
 -----------------------------------------------------
  "eu"          "eu-west"           "eu-central-1"
                "eu-north"          "eu-west-1"
                                    "eu-north-1"
                                    ...
 -----------------------------------------------------
  "ap"          "ap-northeast"      "asia-northeast3"
                "ap-southeast"      "asia-southeast1"
                "ap-melbourne"      "ap-southeast-3"
                                    "ap-melbourne-1"
                                    ...
```

Specifying regions also works when specifying the underlying cloud. For example, if you’re interested in running in AWS specifically,
`cloud="aws", region="us-east"`
would automatically filter for just
`"us-east-1"`
or
`"us-east-2"`
.

Use cases
---------

There are a few reasons why you might want to specify a region for your Modal Functions:

1. **Regulatory requirements**
   : your company may be subject to local regulation around where user data can be processed. GDPR, for example, places strict guardrails on when user data can leave the EU. Even if you can prove that you and your sub-processors provide an adequate level of data protection, some of your customers may still insist that their data never leaves a certain region. Specifying
   `region="eu"`
   will ensure that Functions run within EU datacenters.
2. **Reducing egress fees:**
   if your Function needs to read data from a dependency like S3, you may be charged egress fees by the provider hosting that service if the data has to move across regions. This could add up to $100+/TB depending on which region your dependency is in!
3. **Reducing latency:**
   if your applications are latency sensitive (e.g. real-time inference), you might want your app’s endpoints to be running near your users and/or near an external DB. In these cases, you can set your Functions to run in a region that is geographically proximate to your users and your existing control plane.

Have questions on region selection? Please reach out in our
[community Slack](https://modal.com/slack)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/run-docker-image
================================================================================

The easiest way to run a Docker image in the cloud
==================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Docker has revolutionized the way developers build, package, and distribute
applications. By containerizing an application, you ensure that it runs
consistently across different environments, reducing the “works on my machine”
problem. But while running a Docker image locally is straightforward (you can
use
[Docker Desktop](https://www.docker.com/products/docker-desktop/)

or
[Docker Engine](https://docs.docker.com/engine/)

), deploying and managing it in the cloud introduces
complexities — networking, scaling, and infrastructure overhead, to name a few.

This is where
[Modal](https://modal.com)

comes in.

[Modal](https://modal.com)

is a Python library that lets you run code in
containers in the cloud - and it’s the easiest way to run a Docker image in the
cloud.

Modal allows you to specify custom images for those containers, including images
from public registries like Docker Hub as well as
[private images](https://modal.com/docs/guide/private-registries#private-registries)

from AWS ECR
and GCP Artifact Registry.

In this guide, we’ll cover how you can run both public and private images, as
well as images defined in a
Dockerfile, on Modal, in less than
five minutes.

Why use Modal to run Docker images?
-----------------------------------

Modal provides the
**fastest, easiest, and most developer-friendly**
way to run Docker containers in the cloud. Here’s why it stands out:

### ✅ **No infrastructure management**

With Modal, you don’t need to provision VMs, manage Kubernetes clusters, or worry about networking. Just define your container, and Modal handles the rest.

### ✅ **Automatic scaling**

Modal seamlessly scales your workloads up or down based on demand, eliminating the need for manual tuning or auto-scaling configurations.

### ✅ **Simple API integration**

Unlike traditional cloud providers that require complex CLI tools and configurations, Modal lets you run Docker images using a simple Python-based API.

### ✅ **Cost efficiency**

Modal only charges for compute time, so your containers shut down when they’re
not in use—perfect for intermittent workloads like batch jobs or model
inference.

Prerequisites
-------------

To run a Docker image on Modal, you will need to:

* Create an account at
  [modal.com](https://modal.com)
* Run
  `pip install modal`
  to install the modal Python package
* Run
  `modal setup`
  to authenticate (if this doesn’t work, try
  `python -m modal setup`
  )
* Copy the code below into a file called
  `app.py`
* Run
  `modal deploy app.py`
  to deploy your function

Running an arbitrary public image
---------------------------------

Public registries like
[Docker Hub](https://hub.docker.com/)

have many pre-built
container images for common software packages. You can specify public images for your Modal function using
[`Image.from_registry`](/docs/guide/images#use-an-existing-container-image-with-from_registry)

.

In the example below, we run an official CUDA image from Docker Hub that is a
requirement for running
`cupy`
, a CUDA replacement for
`numpy`
.

```
import modal

# 1) use officially supported CUDA image
# 2) pip install cupy, a CUDA replacement for numpy
image = modal.Image.from_registry("nvidia/cuda:12.4.0-devel-ubuntu22.04", add_python="3.11").pip_install("cupy-cuda12x")

app = modal.App("example-gpu", image=image)

@app.function(gpu="A10G")  # 3) attach a GPU to your function
def square(x=2):
    import cupy as cp

    print(f"The square of {x} is {cp.square(x)}")
```

Running a private registry image
--------------------------------

Private Docker Hub, AWS ECR, and GCP Artifact Registry images are
[also supported](/docs/guide/private-registries#private-registries)

.

Running a custom Docker image from a Dockerfile
-----------------------------------------------

Sometimes, you might be working in a setting where the environment is already defined as a container image in the form of a Dockerfile. Modal supports defining a container image directly from a Dockerfile via the
[`Image.from_dockerfile`](/docs/reference/modal.Image#from_dockerfile)

function. It takes a path to an existing Dockerfile.

For instance, we might write a Dockerfile based on the official Python image and add
`scikit-learn`
:

```
FROM python:3.11

RUN pip install sklearn
```

We can then define a Modal image from this Dockerfile:

```
import modal

dockerfile_image = modal.Image.from_dockerfile("Dockerfile")

@app.function(image=dockerfile_image)
def fit():
    import sklearn
    ...
```

Conclusion
----------

Running a Docker image in the cloud doesn’t have to be complicated. With Modal, you can go from a local containerized app to a fully managed cloud deployment in just a few lines of Python code.

Whether you’re deploying web applications, machine learning models, or batch
processing workloads, Modal offers the easiest and most scalable way to run
Docker images and more in the cloud.

Try
[Modal](https://modal.com)

today and simplify your cloud container deployments!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/sandbox-launch
================================================================================

Modal Sandboxes are generally available
=======================================

Sandboxes are the Modal primitive for safely running untrusted code, whether that code comes from LLMs, users, or other third-party sources. We’ve been honing Sandboxes in beta for the past year, and today we’re excited to announce they’re generally available!

Why we built Sandboxes
----------------------

We built Modal
[Functions](https://modal.com/docs/reference/modal.Function)

to run code written by you, the user. Your Functions can interact with your Modal workspace - they can mount Secrets, create Volumes, call other Functions, and more. This model works because you know you can trust the code you deploy directly.

But agentic systems need to execute code without human supervision. Your agent may make a destructive mistake, or a malicious user may prompt your agent in a dangerous direction! In either case, you can’t trust an LLM with your resources the same way that you can trust yourself. LLM-generated code should run in an isolated environment where its blast radius is limited.

These concerns extend to your users as well. When executing user-written code, you need to ensure that an attacker can’t damage your environment or extract sensitive data.

We built Sandboxes to solve for these concerns. Sandboxes give you a dynamic environment to run code in an arbitrary language, safely isolated from the rest of your Modal resources.

Enough talk, let’s see the code
-------------------------------

[Sandboxes](https://modal.com/docs/guide/sandbox)

provide a simple
`exec`
API for executing code:

```
import modal
app = modal.App.lookup("sandbox-manager", create_if_missing=True)
sb = modal.Sandbox.create(app=app)

p = sb.exec("python", "-c", "print('hello')")
print(p.stdout.read())
sb.terminate()
```

LLMs may specify dependencies or need to execute code in other languages. Sandboxes let you configure the execution environment at runtime, using the same
[Image](https://modal.com/docs/guide/images)

API and infrastructure as Functions:

```
# Get requested dependencies from LLM and use them to
# dynamically build the Sandbox image.
llm_output = '{ "requested_packages": ["nodejs", "php"] }'
packages = json.loads(llm_output)["requested_packages"]
image = modal.Image.debian_slim().apt_install(*packages)

# Test that our languages work!
sb = modal.Sandbox.create(image=image, app=app)
p = sb.exec("node", "-e", 'console.log("hello from nodejs")')
print(p.stdout.read())
p = sb.exec("php", "-r", "echo 'hello from php';")
print(p.stdout.read())
```

You can even
[snapshot your filesystem](https://modal.com/docs/guide/sandbox-snapshots#filesystem-snapshots)

for persistence and to fan out search over many Sandboxes:

```
sb = modal.Sandbox.create(app=app)
sb.exec("bash", "-c", "echo 'data_file' > /data").wait()
snap = sb.snapshot_filesystem()

# These sandboxes will all have /data present and can fan out to
# run tests over many different states
sb2 = modal.Sandbox.create(image=snap, app=app)
sb3 = modal.Sandbox.create(image=snap, app=app)
p2 = sb2.exec("pytest", "tests/unit")
p3 = sb3.exec("pytest", "tests/integration")
print(p2.stdout.read())
print(p3.stdout.read())
```

This is just a taste of the Sandbox feature set. Check out the
[Sandbox docs](https://modal.com/docs/guide/sandbox)

for details on how to
[forward ports](https://modal.com/docs/guide/sandbox-networking#forwarding-ports)

,
[restrict network access](https://modal.com/docs/guide/sandbox-networking#networking)

,
[access files](https://modal.com/docs/guide/sandbox-files)

and more.

Why use Modal Sandboxes
-----------------------

Sandboxes run on the same underlying infrastructure as Functions, meaning you get all the benefits you’re used to with Modal Functions. This means blazing fast cold starts, access to the latest GPUs, global region selection, and more are all available in Sandboxes. As we make our core platform more powerful and reliable, those improvements will play out in both Functions and Sandboxes.

The tight integration in our platform also means it’s simple to quickly build features that use both Sandboxes and Functions!

Customer stories
----------------

We’re proud of the applications our customers are building across a variety of use cases that require secure and scalable code execution.

### Accelerating agent benchmarks with SWE-bench

[SWE-bench](https://github.com/swe-bench/SWE-bench)

is the highest profile benchmark for testing coding agents. It runs LLM-generated code against actual GitHub pull requests to measure model performance at fixing bugs, implementing features, and more.

We’ve upstreamed Modal support into SWE-bench for blazing fast evaluations. By adding a simple
`--modal`
flag to their run command, researchers can now:

* Run evaluations entirely in the cloud without any infrastructure setup
* Execute tests in parallel across hundreds of containers
* **Run the
  [Verified benchmark](https://openai.com/index/introducing-swe-bench-verified/)

  (500 tasks) in only 7 minutes**

Modal’s built-in image caching follows the same layered approach as SWE-bench’s existing Docker images, allowing for a simple integration process. These evaluation runs could take hours previously; the Modal integration enables a much tighter feedback loop.

[

](https://modal-cdn.com/swe_bench_vid.mp4)

### Secure code execution at Quora

[Quora](https://www.quora.com/)

uses Modal Sandboxes to power the code execution in Poe, their AI chat platform. When you ask Poe’s AI-powered bots to write and run code, that code executes safely in Modal Sandboxes. They’re completely isolated, meaning the code is kept separate from both the main Quora infrastructure and any other user’s code.

This integration allows Poe to offer interactive coding features while still maintaining strict security. You can experiment with code that the AI suggests without worrying about damaging the platform or exposing any sensitive data.

![Screenshot of Poe executing code to run a Caesar cipher](https://modal-cdn.com/cdnbot/quora_poe_chatc16fpv5o_3be7782a.webp)

We’ve done extensive performance testing to make sure we’re future-proofed at any scale that Quora may burst up to.
**We’ve tested Sandbox creation throughput up to 1000 Sandboxes per second**
- if you need to rapidly scale out code execution, let us know!

### Large-scale refactors with Codegen

[Codegen](https://codegen.com)

is building an AI system for performing large-scale codebase refactors. Their approach involves building a massive in-memory index of the target codebase and giving AI models the ability to execute “codemods” - automated code transformations that implement the desired changes. For example, our own
[modal-client](https://github.com/modal-labs/modal-client)

repository looks like this in Codegen:

![Visualization of the modal-client repository using Codegen](https://modal-cdn.com/cdnbot/codegen_visualizationtb2yqp1y_d1a8c5bc.webp)

Modal Sandboxes provide Codegen with two critical capabilities:

1. A reliable environment for building and maintaining their in-memory codebase representations
2. A secure execution environment for running AI-generated codemods with strict isolation

This combination of performance and security enables Codegen to confidently apply AI-driven refactoring at scale.

### AI workforce automation with Relevance AI

[Relevance AI](https://relevanceai.com)

is building an AI workforce platform that uses agents to automate complex tasks. They leverage Modal Sandboxes in two key ways:

1. Providing a secure environment for their AI agents to run dynamically generated code
2. Powering their notebook/builder feature where users can write and execute code in a serverless environment

![Screenshot of Relevance's notebook feature](https://modal-cdn.com/cdnbot/relevance_notebookejxosnq0_5458ea4b.webp)

Modal Sandboxes were a perfect fit for Relevance AI because they offer:

* Flexibility to install any package on demand
* Full customization of runtime commands
* Fast cold-boot times for responsive execution
* Support for any programming language their agents need

This combination lets Relevance AI’s agents tackle a wide range of automation tasks while maintaining strict security boundaries between executions.

Get started today
-----------------

Modal Sandboxes are available to all users. Whether you’re building an AI coding assistant, running untrusted user code, or just need a secure environment for code execution, Sandboxes provide the tools you need.

To get started:

1. Install Modal:
   `pip install modal`
2. Create an account:
   `python -m modal setup`
3. Check out our
   [Sandbox documentation](https://modal.com/docs/guide/sandbox)

We can’t wait to see what you build!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/scaling-comfyui
================================================================================

Scaling ComfyUI
===============

![author](https://modal-cdn.com/kenny-ning.jpg)

[Kenny Ning

@kenny\_ning](https://twitter.com/kenny_ning)

Growth Engineer

In our
[ComfyUI example](https://modal.com/docs/examples/comfyapp)

, we demonstrate how to run a ComfyUI workflow with arbitrary custom models and nodes as an API.

But does it scale?

Generally, any code run on Modal leverages our serverless
[autoscaling](https://modal.com/docs/guide/concurrent-inputs#how-does-autoscaling-work-on-modal)

behavior:

* One container per input (default behavior) i.e. if a live container is busy processing an input, a new container will spin up
* Option to have one container handle more than one concurrent input with
  `@modal.concurrent`
* Option to keep some containers always live with
  `min_containers`

In this post, we’ll load test our ComfyUI API endpoint across these options and compare how these different approaches affect latency and cost.

Load testing with Locust
------------------------

First, we’ll serve the existing ComfyUI example on an A10G:

```
@app.cls(gpu="A10G")
```

Then we’ll use Python load testing library
[Locust](https://locust.io/)

to simulate web traffic to our hosted ComfyUI endpoint.

```
# locustfile.py
import locust
from locust import HttpUser, task

class FakeUser(HttpUser):
    wait_time = locust.between(1, 5)

    prompts = [
        "a regal phoenix with feathers that ignite with rebirth",
        "a majestic peacock with secret patterns in its feathers",
        "a majestic bald eagle with hidden constellations in its plumage",
        "a graceful swan with feathers that purify_water"
        # 100 other AI-generated prompts to choose from random
    ]

    @task
    def generate_image(self):
        import random

        self.client.post(
            "/",
            json={"prompt": random.choice(self.prompts)},
        )
```

Running
`locust`
in the same directory as the above
`locustfile.py`
opens an interactive UI, where we can specify how many concurrent users we want to simulate. Each “fake” user will select a random prompt from our list and submit a POST request to our ComfyUI API endpoint, then wait 1-5 seconds between subsequent requests.

Each request triggers our ComfyUI workflow to draw the given prompt into a background image like so:
![comfy-collage](https://modal-cdn.com/comfy-collage.png)

Option 1: Run one container per input
-------------------------------------

* Median response time: 4.4s
* Estimated cost per minute: $0.18 (10 A10Gs at
  [$1.10/h](https://modal.com/pricing)

  )

By default, a Modal web endpoint will spin up a new container per input unless there’s one sitting ready. Let’s simulate 10 concurrent users using the Locust web UI:
![comfy_load_test_1](https://modal-cdn.com/comfy-load-test-1.png)

We can see in our Modal dashboard that a new container spins up to process each individual request:
![10-containers](https://modal-cdn.com/modal-dash-10-containers.png)

This is a good default autoscaling option. However, the first few inputs need to wait for a container to start from scratch — a “cold start”. This cost is noticeable in the first data point of the yellow line in the Locust report.

In our ComfyUI application, this time is mostly spent launching the ComfyUI server in the background, as indicated in the
`@enter`
function of our ComfyUI class:

```
class ComfyUI:
    @modal.enter()
    def launch_comfy_background(self):
        cmd = "comfy launch --background"
        subprocess.run(cmd, shell=True, check=True)
```

Note that cold start here does not include downloading models or custom nodes, all of which is done once at build time. However, loading said models and custom nodes into memory is done at container start time. Long start times are most often driven by running lots of custom nodes.

Option 2: Run multiple inputs on one container
----------------------------------------------

* Median response time: 32s
* Estimated cost per minute: $0.02 (1 A10G)

One down side of Option 1 is cost; you have to pay for usage on each of the 10 GPUs you launched. Let’s try running all requests on a single container / GPU with
`@modal.concurrent`
.

```
@app.cls(gpu="A10G")
@modal.concurrent(max_inputs=10)
```

When we run the same Locust load test, only one container is provisioned:
![modal-dash-1-container](https://modal-cdn.com/modal-dash-1-container.png)

This setting is 10x cheaper, but nearly 10x slower. This is because ComfyUI is
[single threaded](https://github.com/comfyanonymous/ComfyUI/issues/4331)

and can only process one input at a time. The chart below shows the elevated response time in Option 2 (right side), compared to Option 1 (left side).

![comfy_load_test_2](https://modal-cdn.com/comfy-load-test-2.png)

Option 3: Maintain a warm pool with
`min_containers`
----------------------------------------------------

Option 1 eventually stabilizes to ~4s response time, but the first request can be upwards of ~20s because it has to wait for the container to warm up. To drive these first request response times down, we can specify a minimum number of containers to have always running with
`min_containers`
. To demonstrate, let’s set up a warm pool of 5 containers on our endpoint:

```
@app.cls(min_containers=5, gpu="A10G")
```

After a few seconds, we have 5 containers ready to accept inputs:
![modal-dash-5-containers](https://modal-cdn.com/modal-dash-5-containers.png)

Let’s run the Option 1 load test again (10 concurrent users, 1 container per input), once without
`min_containers`
(left) and once with
`min_containers`
(right):

![load-test-3](https://modal-cdn.com/comfy-keep-warm-test.png)

This reduced the first request response time by ~10s (compare the first point of the yellow line charts). However unlike the previous options, you will always have a minimum of 5 containers live costing you $0.09 per minute. That’s equivalent to ~$130 per day, so be sure to use this option with caution.

A more economical complement to
`min_containers`
is
`scaledown_window`
, which specifies how many seconds a container should wait after processing a request before spinning down. By default, it is one minute. By extending this timeout to, say, five minutes, we increase the chance we can re-use this container for the next request and save some cold start time.

Scaling to 100 concurrent users
-------------------------------

Now let’s run another load test with our Option 1 settings (one container per input, no keep warm) starting with 10 concurrent users, then scaling up to 100. This graph tells a good story of Modal’s container lifecycle:

![modal-scaling-locust](https://modal-cdn.com/comfy-scaling-test.png)

At a high level:

* The first few requests will take some time while containers start up (~20s)
* System eventually stabilizes at ~5s per request (mostly raw ComfyUI workflow execution time)
* As users scale to 100, response time increases temporariliy while new containers spin up to help work through the demand spike
* System goes back to normal after ~1 minute

At peak load, Modal scaled up to
**62 concurrent GPUs**
:

![gpu-modal-spike](https://modal-cdn.com/modal-62-gpu.png)

Note that only Enterprise customers can scale this high. Team workspaces are limited to 30 concurrent GPUs and Starter workspaces are limited to 10.

Conclusion
----------

Yes, ComfyUI as an API does scale well with serverless! However, you need to think about how to balance inference speed with cost and a lot of this also depends on your specific ComfyUI application.

1. **Have a lot of custom nodes?**
   Your container cold start might be longer than in this experiment and you might see better performance running concurrent requests on fewer containers i.e. raising
   `max_inputs`
   in
   `@modal.concurrent`
   .
2. **“Bursty” traffic?**
   If you expect clients to use the API multiple times in short succession, increase
   `scaledown_window`
   and/or set a small warm pool to increase the chance of re-using live containers across sessions.

The only way to know the right balance is to run similar experiments on your deployment and see what works best for you.

Coda: Deploying with Comfy Deploy
---------------------------------

We’re proud to be the underlying serverless provider of
[Comfy Deploy](https://www.comfydeploy.com/)

, the easiest way to take a local workflow and deploy it to production with a rich UI, team collaboration features, and development environments. Because Comfy Deploy uses Modal under the hood, the same scaling principles mentioned here also apply to workflows deployed with them.

Thanks to the team at Comfy Deploy for inspiring this blog post and providing feedback!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/sdk-javascript-go
================================================================================

Modal SDKs for JavaScript and Go (alpha)
========================================

![author](https://modal-cdn.com/eric-zhang.jpg)

[Eric Zhang

@ekzhang1](https://twitter.com/ekzhang1)

Founding Engineer

Here at Modal, we make it easy for developers to bring powerful, on-demand cloud infrastructure into their own stacks. Many of you have been asking for a way to use Modal from other languages, even reverse-engineering our APIs to do so!

Today, we’re excited to launch an alpha of our
**official
[Modal SDKs for JavaScript and Go](https://github.com/modal-labs/libmodal)**
.

With these new SDKs, you can now
**call deployed Modal Functions**
and
**create isolated sandboxes**
from any server-side JavaScript or Go code (e.g., your
**Node backend**
, from a
**Next.js server handler**
, or within a
**Go microservice**
) — all without leaving your language of choice.

What can you do with the SDK?
-----------------------------

These new client libraries let you:

* 🔁
  **Call deployed Modal Functions**
  from anywhere — think image processing, LLM inference, audio transcription, and long-running batch jobs.
* 🧪
  **Create and interact with sandboxes**
  — isolated, secure VMs where you can:
  + Execute code generated by a language model
  + Run untrusted scripts in a controlled environment
  + Check out a Git repo and run tests, linters, or setup commands
  + Launch a container with arbitrary dependencies and run one-off jobs

However, they don’t support deploying Modal Functions — those still need to be written in Python!

> **⚠️ Note:**
> Only a limited subset of Modal functionality is currently available. Functions can only be called with JSON-like or
> `bytes`
> data types. Also, advanced function calling modes like
> `map()`
> and
> `spawn()`
> , sandbox tunnels, and features like volumes and container lifecycle management are coming soon. Stay tuned.

How to get started
------------------

First, authenticate with Modal. You can either:

* Run
  `pip install modal && modal setup`
  to log in via the CLI, or
* Set your credentials as environment variables:

  ```
  export MODAL_TOKEN_ID=ak-YOUR_TOKEN_ID
  export MODAL_TOKEN_SECRET=as-YOUR_TOKEN_SECRET
  ```

Then, install the SDK for your language.

### JavaScript (Node, Deno, Bun, etc.)

```
npm install modal
```

### Go

```
go get -u github.com/modal-labs/libmodal/modal-go
```

Example: JavaScript SDK in action
---------------------------------

### 1. Call a deployed Modal Function

```
import { Function_ } from "modal";

const echo = await Function_.lookup("my-deployed-app", "echo_string");

// Call the function with args.
let ret = await echo.remote(["Hello world!"]);
console.log(ret);

// Call the function with kwargs.
ret = await echo.remote([], { s: "Hello world!" });
console.log(ret);
```

### 2. Start a sandbox and run a command

```
import { App } from "modal";

const app = await App.lookup("libmodal-example", { createIfMissing: true });
const image = await app.imageFromRegistry("python:3.13-slim");

const sb = await app.createSandbox(image);
console.log("Started sandbox:", sb.sandboxId);

const p = sb.exec(["python", "-c", "print(3 + 4)"]);
await p.stdout.readText(); // => "7"
await p.wait(); // => exit code: 0

await sb.terminate();
```

These examples are just the beginning — the SDKs are designed to be lightweight, consistent, and easy to use from any JavaScript or Go codebase.

Why this matters
----------------

Modal is a great fit for compute-heavy tasks, dynamic workloads, and anything you don’t want to run on your own servers.
Until now, accessing Modal meant writing Python. That’s no longer the case.

With the Modal SDKs for JavaScript and Go, you can:

* Call cloud functions from your existing infra
* Spin up secure, dynamic sandboxes from anywhere
* Run ephemeral workloads without provisioning machines

…and integrate with Modal from your language of choice — all without learning a new stack.

---

Documentation and examples are available in the GitHub repository at
**[modal-labs/libmodal](https://github.com/modal-labs/libmodal)**
.

Remember that this is an
*alpha release*
. We’re just getting started, so APIs and naming conventions are subject to change, and newer features may be missing or incomplete. We’ve been rigorously testing these new SDKs though, and we can easily run 10,000 sandboxes in either Go or JavaScript with no issues.

Got questions? Come hang out in our
[community Slack](/slack)

— we’d love to hear what you’re building. And keep an eye out: we’ll be announcing new features for these SDKs soon.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/serverless-gpu-article
================================================================================

Top 5 serverless GPU providers
==============================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Serverless GPUs refer to a type of cloud computing service that allows you to run GPU-accelerated workloads that automatically scale up and down based on demand. You pay only for the compute time you use, and offload the management of the underlying hardware or software.

In the last few years, a number of new serverless GPU providers have emerged. This article will explain what differentiates them from one another.

What are serverless GPUs good for?
----------------------------------

Serverless GPUs can be a good fit for:

1. **Model Serving**
   : Deploying and running AI models for inference.
2. **Model Fine-tuning**
   : Fine-tuning AI models on custom datasets.
3. **Video and image processing**
   : Speeding up video and image processing tasks.
4. **CI/CD**
   : Running GPU-accelerated CI/CD pipelines.

Top serverless GPU providers
----------------------------

[Modal](https://modal.com)
--------------------------

Modal offers a Python SDK that makes it easy to deploy and run GPU-accelerated functions.

For example, if you want to run a function that requires
`torch`
and uses a GPU, you can define the following:

```
import modal

image = modal.Image.debian_slim().pip_install("torch")

app = modal.App("gpu-example")

@app.function(gpu="A100")
def gpu_task():
    import torch
    return torch.cuda.get_device_name(0)
```

Modal is probably the most flexible of the new serverless GPU providers: it lets you run arbitrary Python code in the cloud, attaching GPUs if you want, making Modal suitable for a wide range of use cases - not just model serving, fine-tuning, and training, but also other potentially GPU-accelerated workflows like CI/CD.

For more detailed examples and documentation, visit the
[Modal docs](https://modal.com/docs/examples)

.

[RunPod](https://www.runpod.io/)
--------------------------------

Runpod’s serverless GPU offering is called
[RunPod Serverless](https://docs.runpod.io/serverless/overview)

.

RunPod Serverless lets you deploy custom endpoints with your choice of GPU via a couple different modalities:

1. Quick Deploy: Pre-built custom endpoints for popular AI models.
2. Handler Functions: Bring your own functions to run in the cloud.
3. vLLM Endpoint: Specify and run a Hugging Face model in the cloud.

RunPod Serverless allows you to deploy custom endpoints with GPU support through their web console. The process involves logging into the RunPod Serverless console, creating a new endpoint, and configuring various parameters such as the endpoint name, GPU specifications, worker count, and Docker image details. Optional features like FlashBoot can be enabled for faster startup times. Once configured, you can deploy your endpoint with a single click, making it ready for use in GPU-accelerated tasks.

Once deployed, you can interact with your RunPod Serverless endpoint using the provided Endpoint URL. This allows you to send requests to your deployed model or application for inference or other GPU-accelerated tasks.

Note that RunPod also has a non-serverless GPU offering, called
[RunPod Pods](https://docs.runpod.io/pods/overview)

, which are virtual machines with GPUs.

[Baseten](https://www.baseten.co/)
----------------------------------

Baseten is a serverless platform that is highly focused on model serving and inference.

They offer a unique framework called
[Truss](https://docs.baseten.co/truss-reference/overview)

, with an associated CLI, for configuration and deployment of models. To deploy a model on Baseten backed by a GPU, you specify the resources you need in a
`config.yaml`
file.

Here’s an example of how to ask for A10G GPUs in order to run Stable Diffusion XL.

```
resources:
  accelerator: A10G
  cpu: "4"
  memory: 16Gi
  use_gpu: true
```

You can also configure other aspects of the deployment, such as the number of replicas (for scaling) and whether you want Baseten to auto-scale.

You can then deploy your model with a
`truss push`
command. This creates a Docker image and pushes it to Baseten, where it can be deployed and run. It also automatically creates an API that you can use to send requests to your deployed model.

Baseten isn’t as suitable if you have use cases outside of model serving and inference.

[Replicate](https://replicate.com/)
-----------------------------------

Replicate offers serverless GPU-powered inference for a wide range of pre-trained models, as well as the ability to deploy custom models on GPUs behind a serverless endpoint.

### Pre-trained Models

For most users, the main benefit of Replicate is the extensive library of pre-trained models that are ready to use. The details of which GPU resources are needed for each model are generally abstracted away from the user, who can simply specify the model name.

### Custom Models

Replicate also allows users to
[deploy custom models](https://replicate.com/docs/guides/deploy-a-custom-model)

. In the context of Replicate, a “model” refers to a trained, packaged, and published software program that accepts inputs and returns outputs.

To create and deploy a custom model on Replicate, you create a model in the Replicate web UI and train the model using the Replicate training API. You can then create a deployment for your model, which will provide a private, fixed API endpoint, and configure it to use certain GPUs/hardware.

[Fal](https://www.fal.ai/)
--------------------------

Fal is a newer player in the serverless GPU space. It is focused on the out-of-the-box deployment and serving of media generation models like Flux and SDXL and offers ready-made endpoints for the most popular models that users can call via API.

Fal offers
[private serverless models](https://fal.ai/docs/private-serverless-models)

as an enterprise feature. To use Fal for private serverless models, similar to with Modal, you can:

1. Decorate your Python code with Fal-specific decorators.
2. Specify the GPU you want to use (e.g., “GPU-A100”) as a parameter to the decorator.
3. Deploy your code using the Fal CLI.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/serverless-http
================================================================================

Lambda on hard mode: Inside Modal's web infrastructure
======================================================

![author](https://modal-cdn.com/eric-zhang.jpg)

[Eric Zhang

@ekzhang1](https://twitter.com/ekzhang1)

Founding Engineer

At Modal, we built an HTTP and WebSocket stack on our platform. In other words,
your serverless functions can take web requests.

This was tricky! HTTP has quite a few edge cases, so we used Rust for its speed
and to help manage the complexity. But even so, it took a while to get right. We
recently wrapped up this feature by introducing
[full WebSocket support](/blog/websocket-launch)

(real-time bidirectional
messaging).

We call this service
`modal-http`
, and it sits between the Web and our core
runtime.

![Simple schematic with modal-http at the center](https://modal-cdn.com/cdnbot/modal-http-20.png)

You can deploy a simple
[web endpoint](/docs/guide/webhooks)

to a
`*.modal.run`
URL by running some Python code:

```
from modal import App, web_endpoint

app = App(name="small-app")

@app.function()
@web_endpoint(method="GET")
def my_handler():
    return {
        "status": "success",
        "data": "Hello, world!",
    }
```

(
*This takes
**0.747 seconds**
to deploy today.*
)

But you can also run a much larger compute workload. For example, to set up a
data-intensive video processing endpoint:

```
from modal import App, web_endpoint
from .my_video_library import Video, do_expensive_processing

app = App(name="big-app")

# 30 minutes, 8 CPUs, 32 GB of memory
@app.function(timeout=1800, cpu=8, memory=32 * 1024)
@web_endpoint(method="POST")
def my_handler(video_data: Video):
    # Process the video
    edited_video = do_expensive_processing(video_data)

    # Return it as a response
    return edited_video
```

This post is about the behind-the-scenes of serving a web endpoint on Modal. How
does your web request get translated into an autoscaling serverless invocation?

What makes our HTTP/WebSocket implementation particularly interesting is its
lack of limits. Serverless computing is
[traditionally understood](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2019/EECS-2019-3.pdf)

to prioritize small, lightweight tasks, but Modal can’t compromise on speed or
compute capacity.

When resource limits are removed, handling web requests gets proportionally more
difficult. Users may ask to upload a gigabyte of video to their machine learning
model or data pipeline, and we want to help them do that! We can’t just say,
“sorry, either make your video 200x smaller or split it up yourself.” So we had
a bit of a challenge on our hands.

“Lambda on hard mode”
---------------------

Serverless function platforms have constraints. A lot of them, too!

* Functions on
  [AWS Lambda](https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html)

  are limited to 15-minute runs and 50 MB images. As of 2024, they can only use
  3 CPUs (6 threads) and 10 GB of memory. Response bandwidth is 16 Mbps.
* [Google Cloud Run](https://cloud.google.com/run/quotas)

  is a bit better, with
  4 CPUs and 32 GB of memory, plus 75 Mbps bandwidth.
* [Cloudflare Workers](https://developers.cloudflare.com/workers/platform/limits/)

  are the most restricted. Their images can only be 10 MB in size and have 6
  HTTP connections. Execution is limited to 30 seconds of CPU time, 128 MB of
  memory.

But modern compute workloads can be
[much more demanding](/examples)

: training
neural networks, rendering graphics, simulating physics, running data pipelines,
and so on.

Modal containers can each use up to
**64 CPUs**
,
**336 GB of memory**
, and
**8
Nvidia H100 GPUs**
. And they may need to download up to
**hundreds of
gigabytes**
of model weights and image data on container startup. As a result,
we care about having them spin up and shut down quickly, since having any idle
time is expensive. We scale to zero and bill
[by the second](/pricing)

.

As a user, this is freeing. I often get questions like, “does Modal have enough
compute to run my
[fancy bread-baking simulation](/docs/examples/blender_video)

”
— and I tell them, are you kidding? You can spin up dozens of 64-CPU containers
at a snap of your fingers. Simulate your whole bakery!

In summary: Modal containers are potentially long-running and compute-heavy,
with big inputs and outputs. This is the opposite of what “serverless” is
usually good at. How can we ensure quick and reliable delivery of HTTP requests
under these conditions?

### A distributed operating system

Let’s take a step back and review the concept of serverless computing. Run code
in containers. Increase the number of containers when there’s work to be done,
and then decrease it when there’s less work. You can imagine a factory that
makes cars: when there are many orders, the factory operates more machines, and
when there are fewer orders, the factory shifts its focus. (Except in computers,
everything happens faster than in a car factory, since they’re processing
thousands of requests per second.)

This isn’t unique to serverless computing; it’s how most applications scale
today. If you deploy a web server, chances are you’d use a PaaS to manage
replicas and scaling, or an orchestrator like Kubernetes. Each of these
offerings can be conceptualized by a two-part schematic:

1. **Autoscaling:**
   Write code in a stateless way, replicate it, then track how
   much work needs to be done via latency, CPU, and memory metrics.
2. **Load balancing:**
   Distribute work across many machines and route traffic to
   them.

Together autoscaling and load balancing constitute a kind of analogue to an
*operating system*
in the distributed services world: something that manages
compute resources and provides a common execution environment, allowing software
to be run.

Although a unified goal, there are many approaches. (A lot of
[ink](https://research.google/pubs/maglev-a-fast-and-reliable-software-network-load-balancer/)

[has](https://aosabook.org/en/v2/nginx.html)

[been](https://www.eecs.harvard.edu/~michaelm/postscripts/mythesis.pdf)

[spilled](https://github.com/tangchq74/papers/blob/fad260ab66567e843e5ad6e238f7051ffe384e8a/XFaaS-SOSP23-Final.pdf)

on load balancing in particular.) Here’s a brief summary to illustrate how this
schematic maps onto a few popular deployment systems. We’re in good company!

| System (release date) | Autoscaling | Load balancing |
| --- | --- | --- |
| *Heroku (2009)* | By p95 latency | HTTP reverse proxy |
| *Kubernetes (2014)* | Resource metrics (custom) | Custom reverse proxy and/or load balancer |
| *AWS Lambda (2014)* | Traffic | HTTP reverse proxy |
| *Azure Functions (2016)* | Traffic | HTTP reverse proxy |
| *AWS Fargate (2017)* | Resource metrics (custom) | HTTP/TCP/UDP load balancer |
| *Render (2019)* | CPU/memory target | HTTP reverse proxy |
| *Google Cloud Run (2019)* | Traffic and CPU target | HTTP reverse proxy |
| *Fly.io (2020)* | Traffic (custom) | HTTP/TCP/TLS proxy, by distance and load |
| *Modal (2023)* | Traffic (custom) | **Translate HTTP to function calls** |

So… I spot a difference there. Hang on a second. I want to talk about Modal’s
HTTP ingress.

### Translating HTTP to function calls

You might notice that setting up an HTTP reverse proxy in front of serverless
functions is a popular option. This means that you scale up your container, and
some service in front handles TLS termination and directly forwards traffic to a
backend server. For most of these platforms, HTTP is the main way you can talk
to these serverless functions, as a network service.

But for Modal, we’re focused on building a platform based on the idea that
serverless functions are just
*ordinary functions*
that you can call. If you
want to define a function on Modal, that should be easy! You don’t need to set
up a REST API. Just call it directly with
`.remote()`
.

```
from modal import App
from PIL import Image

app = App()

@app.function()
def compute_embeddings(image: Image) -> list[int]:
    return my_ml_model.run(image)

@app.function()
def run_batch_job(image_names: list[str]) -> None:
    for name in image_names:
        image = fetch_image(name)
        vec = compute_embeddings.remote(image)  # invoke remote function
        print(vec)
```

Since
`run_batch_job()`
can be invoked in any region, and
`compute_embeddings()`
can be called remotely from it, we needed to build generic high-performance
infrastructure for serverless
*function calls*
. Like, actually “calling a
function.” Not wrapping it in some REST API.

Calling a function is a bit different from handling an HTTP request. There’s a
mismatch if you try to conflate them! By supporting both of these workloads, we
can:

* Use a faster, optimized path (for calls between functions) that can be
  location and data cache-aware, rather than relying on the same HTTP protocol.
* Fully support real-time streaming in network requests, rather than limiting it
  to fit the use case of a typical function call.
* Offer first-class support for complex heterogeneous workloads on CPU and GPU.

Modal’s bread and butter is systems engineering for heavy-duty function calls.
We’re already focused on making that fast and reliable. As a result, we decided
to handle web requests by translating them into function calls, which gives us a
foundation of shared infrastructure to build upon.

Understanding the HTTP protocol
-------------------------------

To understand how HTTP gets turned into a function calls, first we need to
understand HTTP. HTTP follows a request-response model. Here’s what a typical
flow looks like. On the top, you can see a standard
`GET`
request with no body,
and on the bottom is a
`POST`
request with body.

***Note:**
HTTP GET requests can technically have bodies too, though they should
be ignored. Also, a less-known fact is that request and response bodies can be
interleaved,
[sometimes even in HTTP/1.1](https://datatracker.ietf.org/doc/html/rfc6202)

!*

![Diagram of two requests, HTTP GET on top and HTTP POST on the bottom](https://modal-cdn.com/cdnbot/modal-http-10.png)

The client sends some headers to the server, followed by an optional body. Once
the server receives the request, it does some processing, then responds in turn
with a set of a headers and its own response body.

Both the client and server directions are sent over a specific wire protocol,
which varies between HTTP versions. For example, HTTP/1.0 uses a TCP stream for
each request, HTTP/1.1 added keepalive support, HTTP/2 has concurrent stream
multiplexing over a single TCP stream, and HTTP/3 uses QUIC (UDP) instead of
TCP. They’re all unified by this request-response model.

Here’s what an HTTP/1.1 GET looks like, as displayed by
`curl`
in verbose mode.
The
`>`
lines are request headers, the
`<`
lines are response headers, and the
response body is at the end:

```
$ curl -v http://example.com
*   Trying 93.184.216.34:80...
* TCP_NODELAY set
* Connected to example.com (93.184.216.34) port 80 (#0)
> GET / HTTP/1.1
> Host: example.com
> User-Agent: curl/7.68.0
> Accept: */*
>
* Mark bundle as not supporting multiuse
< HTTP/1.1 200 OK
< Accept-Ranges: bytes
< Age: 521695
< Cache-Control: max-age=604800
< Content-Type: text/html; charset=UTF-8
< Date: Fri, 23 Feb 2024 17:22:54 GMT
< Etag: "3147526947+gzip"
< Expires: Fri, 01 Mar 2024 17:22:54 GMT
< Last-Modified: Thu, 17 Oct 2019 07:18:26 GMT
< Server: ECS (cha/8169)
< Vary: Accept-Encoding
< X-Cache: HIT
< Content-Length: 1256
<
<!doctype html>
<html>
<head>
    <title>Example Domain</title>
    <!-- note: head contents omitted for brevity -->
</head>

<body>
<div>
    <h1>Example Domain</h1>
    <p>This domain is for use in illustrative examples in documents. You may use this
    domain in literature without prior coordination or asking for permission.</p>
    <p><a href="https://www.iana.org/domains/example">More information...</a></p>
</div>
</body>
</html>
* Connection #0 to host example.com left intact
```

To iron out the differences between HTTP protocol versions, we needed a backend
data representation for the request. In a reverse proxy, the backend protocol
would just be HTTP/1.1, but in our case that would add additional complexity for
reliably reconnecting TCP streams and parsing the wire format. We instead
decided to base our protocol on a stream of
*events*
.

Luckily, there was already a well-specified protocol for representing HTTP as
event data:
[ASGI](https://github.com/django/asgiref)

, typically used as a
standard interface for web frameworks in Python.

***Note:**
ASGI was made for a different purpose! Usually the web server and
ASGI application run on the same machine. Here we’re using it as the internal
communication language for a distributed runtime. So we adjusted the protocol to
our use case by serializing events as binary Protocol Buffers.*

ASGI doesn’t support every internal detail of HTTP (e.g., gRPC servers need
access to HTTP/2 stream IDs), but it’s a common denominator that’s enough for
web apps built with all the popular Python web frameworks: Flask, Django,
FastAPI, and more. That’s a lot of web applications, and the benefit of this
maturity is that it lets us greatly simplify our model of HTTP serving.

Here’s what a POST request looks like in ASGI. The blue arrows represent client
events, while the green arrows are events sent from the server.

![Diagram of an HTTP POST request with events marked](https://modal-cdn.com/cdnbot/modal-http-11.png)

1. At the start of a request, when headers are received, we begin by parsing the
   headers to generate a
   *function input*
   via the
   `http`
   request scope. This
   triggers a new function call, which is scheduled on a running task according
   to availability and locality.
2. Then, the request body is streamed in, and we begin reading it in chunks to
   produce real-time
   `http.request`
   events that are sent to the serverless
   function call. If the server falls behind, backpressure is propagated to the
   client via TCP (for HTTP/1.1) or HTTP/2 flow control.
3. The function starts executing immediately after getting the request headers,
   then begins reading the request body. It sends back its own headers and
   status code, followed by the response body in chunks.
4. The request-response cycle finishes, optionally with HTTP trailers.

In this way, we’re able to send an entire HTTP request and response over a
generic serverless function call. And it’s efficient too, with proper batching
and backpressure. We don’t need to establish a single TCP stream or anything; we
can use reliable, low-latency message queues to send the events.

Unlike AWS Lambda’s 6 MB limit for request and response bodies, this
architecture lets us support request bodies of up to 4 GiB (682x bigger), and
streaming response bodies of unlimited size.

Of course, although conceptually simple, it’s still a pretty tricky thing to
implement correctly since there are a lot of concurrent moving parts. Our
implementation is in Rust, based on the
[hyper](https://hyper.rs/)

HTTP server
library and
[Tokio](https://tokio.rs/)

async runtime. Here’s a snippet of the
code that buffers the request body in chunks of up to 1 MiB in size, or waits
for 2 milliseconds of duration.

```
/// Stream an HTTP request body into the `data_in` channel for a web
/// endpoint. This function also sends `http.disconnect` when the request
/// finishes, or the HTTP client disconnects.
async fn stream_http_request_body(
    &self,
    function_call_id: &str,
    mut body: hyper::Body,
    disconnect_rx: oneshot::Receiver<()>,
) -> Result<()> {
    let asgi_body = |body, more_body| Asgi {
        r#type: Some(asgi::Type::HttpRequest(asgi::HttpRequest {
            body,
            more_body,
        })),
    };
    let asgi_disconnect = Asgi {
        r#type: Some(asgi::Type::HttpDisconnect(asgi::HttpDisconnect {})),
    };

    let (tx, mut rx) = mpsc::channel(16); // Send at most 16 chunks at a time.

    tokio::spawn(async move {
        let body_buffer_time = Duration::from_millis(2);
        let body_buffer_size = 1 << 20; // 1 MiB

        let mut last_put = Instant::now();
        let mut current_segments = Vec::new();
        let mut current_size = 0;

        while let Some(result) = body.next().await {
            let Ok(buf) = result else {
                // If the request fails, send a disconnection immediately.
                tx.send(asgi_disconnect).await?;
                return Ok(());
            };
            if buf.is_empty() {
                continue;
            }

            current_size += buf.len();
            current_segments.push(buf);

            if current_size > body_buffer_size || last_put.elapsed() > body_buffer_time {
                let message = asgi_body(Bytes::from(current_segments.concat()), true);
                current_segments.clear();
                current_size = 0;
                tx.send(message).await?;
                last_put = Instant::now();
            }
        }

        // Final message, possibly empty.
        let message = asgi_body(Bytes::from(current_segments.concat()), false);
        tx.send(message).await?;

        // Wait for a client disconnect signal (or for the response to finish sending),
        // then forward that to the data channel.
        match disconnect_rx.await {
            Ok(()) => {}
            _ => tx.send(asgi_disconnect).await?, // => RecvError
        };

        anyhow::Ok(())
    });

    let mut index = 1;
    let mut messages = Vec::new();
    while rx.recv_many(&mut messages, 16).await != 0 {
        self.put_data_in(function_call_id, &mut index, &messages)
            .await?;
        messages.clear();
    }

    anyhow::Ok(())
}
```

You might have noticed the
`disconnect_rx`
channel used in the snippet above.
This hints at one of the realities of making reliable distributed systems that
we glossed over: needing to thoroughly handle failure cases everywhere, all the
time.

### Edge cases and errors

First, if a client sends an HTTP request but exits in the middle of sending the
body, then we propagate that disconnection to the serverless function.

![Diagram of a disconnected HTTP request](https://modal-cdn.com/cdnbot/modal-http-12.png)

We reify this using an ASGI
`http.disconnect`
event, which allows the user’s
code to stop executing gracefully. Otherwise, we might have a function call
that’s still running even after the user has canceled their request.

Another issue is if the server has a failure. It might throw an exception, crash
due to running out of memory, hit a user-defined timeout, be preempted if on a
spot instance, and so on. If a malicious user is on the system, they also might
send malformed response events, or events in the wrong order!

We keep track of any violations and display an error message to the user. Rust’s
pattern matching and ownership help with managing the casework.

### Dealing with HTTP idle timeouts

Okay, so if we had been a standard runtime, we would be done with HTTP now. But
we’re still not done! There’s one more thing to consider: long-running requests.

If you make an HTTP request and the server doesn’t respond for 300 seconds, then
Chrome cancels the request and gives you an error. This is not configurable.
Other browsers and pieces of web infrastructure have varying timeouts. Our users
often end up running expensive models that take longer than 5 minutes, so we
need a way to support long-running requests.

Luckily, there’s a solution. After 150 seconds (2.5 minutes), we send a
temporary “303 See Other” redirect to the browser, pointing them to an
alternative URL with an ID for this specific request. The browser or HTTP client
will follow this redirect, ending their current stream and starting a new one.

Browsers will follow up to 20 redirects for a link, so this effectively
[increases the idle timeout to 50 minutes](https://modal.com/docs/guide/webhook-timeouts)

.
An example of this in action is shown below, with a single redirect.

![Diagram of a long-running request with one 303 See Other response](https://modal-cdn.com/cdnbot/modal-http-13.png)

Is this behavior a little strange? Yes. But it just works “out-of-the-box” for a
lot of people who have web endpoints that might execute for a long time. And if
your function finishes processing and begins its response in less than 2.5
minutes, you’ll never notice a difference anyway.

For people who need to have very long-running web requests, Modal
*just works*
.

### WebSocket connections

That’s it for HTTP. What if a user makes a WebSocket connection? Well, the
WebSocket protocol works by starting an HTTP/1.1 connection, then establishing a
*handshake*
via HTTP’s connection upgrade mechanism. The
[handshake](https://datatracker.ietf.org/doc/html/rfc6455#section-1.2)

looks
something like this:

```
> GET /ws HTTP/1.1
> Host: my-endpoint.modal.run
> Upgrade: websocket
> Connection: Upgrade
> Sec-WebSocket-Key: dGhlIHNhbXBsZSBub25jZQ==
> Sec-WebSocket-Version: 13
>
< HTTP/1.1 101 Switching Protocols
< Upgrade: websocket
< Connection: Upgrade
< Sec-WebSocket-Accept: s3pPLMBiTxaQ9kYGzzhZRbK+xOo=
```

***Note:**
There is also another version of the WebSocket protocol that
bootstraps from HTTP/2, but it’s not supported by many web servers yet. For now,
you need a dedicated TCP connection.*

The
`Sec-WebSocket-Key`
header is random, while
`Sec-WebSocket-Accept`
is
derived from an arbitrary hash function on the key. (This is just some protocol
junk that we had to implement, see
[RFC 6455](https://datatracker.ietf.org/doc/html/rfc6455)

.) ASGI has a separate
[WebSocket interface](https://asgi.readthedocs.io/en/latest/specs/www.html#websocket)

that encodes this handshake into a pair of
`websocket.connect`
and
`websocket.accept`
events, so we translated our incoming request into those
events.

After the handshake, all of the infrastructure is already in place, and we
transmit messages between
`modal-http`
and the serverless function via data
channels in the same way as we did for HTTP.

![Diagram of a WebSocket connection](https://modal-cdn.com/cdnbot/modal-http-14.png)

Our server-side Rust implementation is based on hyper as before, but it upgrades
the connection to an asynchronous
[tokio-tungstenite](https://github.com/snapview/tokio-tungstenite)

stream once
the handshake is accepted.

Building on open-source infrastructure
--------------------------------------

We’ve built a lot of infrastructure to support HTTP and WebSocket connections,
but we didn’t start from scratch. The Rust ecosystem was invaluable to making
this custom network service, which needed to be high-performance and correct.

But while we’ve talked a lot about the serverless backend and design choices
made to support heavy workloads, we haven’t talked yet about how requests
actually
*get*
to
`modal-http`
. For this part, we relied on boring, mature
open-source cloud infrastructure pieces.

Let’s still take a look though. Modal web endpoints run on the wildcard domain
`*.modal.run`
, as well as on
[custom domains](/docs/guide/webhook-urls#custom-domains)

as assigned by users
via a CNAME record to
`cname.modal.domains`
. The most basic way you’d deploy a
Rust service like
`modal-http`
is by pointing a
[DNS record](https://en.wikipedia.org/wiki/Domain_Name_System)

at a running
server, which has the compiled binary listen on a port.

![A browser sends a request to modal-http](https://modal-cdn.com/cdnbot/modal-http-0.png)

Rust is pretty fast, so this is a reasonable design for most real-world
services. A single node nevertheless doesn’t scale well to the traffic of a
cloud platform. We wanted:

* **Multiple replicas.**
  Replication of the service provides fault tolerance and
  eases the process of rolling deployments. When we rollout a new version, old
  replicas need a gradual timeout.
* **Encryption.**
  Support for TLS is missing here. We
  *could*
  handle it in the
  server directly, but rather than reinventing the wheel, it’s easier and safer
  to rely on well-vetted software for TLS termination. (We also need to allocate
  [on-demand certificates](https://caddyserver.com/docs/automatic-https#on-demand-tls)

  for custom domains.)

So, rather than the simplified flow above, our actual ingress architecture to
`modal-http`
looks like this. We placed a TCP network load balancer in front of
a
[Kubernetes](https://kubernetes.io/)

cluster, which runs a
[Caddy](https://caddyserver.com/docs/)

deployment, as well as a separate
deployment for
`modal-http`
itself.

![Full path of a request through L4 NLB and Caddy](https://modal-cdn.com/cdnbot/modal-http-1.png)

Note that none of our
*serverless functions*
run in this Kubernetes cluster.
Kubernetes isn’t well-suited for the workloads we described, so we wrote our own
high-performance serverless runtime based on
[gVisor](https://gvisor.dev/)

, our
own file system, and our own job scheduler — which we’ll talk about another
time!

But Kubernetes is still a rock-solid tool for the more traditional parts of our
cloud infrastructure, and we’re happy to use it here.

### Caveat: Multi-region request handling

It’s a fact of life that light takes time to travel through fiber-optic cables
and routers. Ideally,
`modal-http`
should run on the edge in geographically
distributed data center regions, and requests should be routed to the nearest
replica. This is important to minimize baseline latency for web serving.

We’re not there yet though. It’s early days! While our serverless functions are
already running in many different clouds and data centers based on
*compute
availability*
, since GPUs are scarce, our actual servers only run in Ashburn,
Virginia for now.

This is a bit of a tradeoff for us, but it’s not a fundamental one. It gives us
more flexibility at the moment, although
`modal-http`
will be deployed to more
regions in the future for latency reasons. Right now heavyweight workloads on
Modal probably aren’t affected, but for very latency-sensitive workloads (under
100 ms), you’ll likely want to specify your container to run in Ashburn.

Lessons learned
---------------

So, there you have it. Serverless functions are traditionally limited to a
request-response model, but Modal just released full support for WebSockets,
with GPUs and fully managed autoscaling. And we did this by translating web
requests into function calls.

Our service,
`modal-http`
, is written in Rust and based on several components
that let us handle HTTP and WebSocket requests at scale. We’ve placed it behind
infrastructure to handle the ingress of requests, and we’re planning to expand
to more regions in the future.

Some may wonder: If Modal translates HTTP to this message format, wouldn’t that
stop people from being able to use the traditional container model of
[`EXPOSE`](https://docs.docker.com/reference/dockerfile/#expose)

-ing TCP ports?
This is a good question, but it’s not a fundamental limitation. The events can
be losslessly translated back to HTTP on the other end! We
[wrote examples](/docs/examples/comfyapp)

of this for systems like ComfyUI, and
we’re
[building it into the runtime](https://github.com/modal-labs/modal-client/pull/1513)

with just a bit of added code.

We’ve already been running Rust to power our serverless runtime for the past two
years, but
`modal-http`
gives us more confidence to run standard Rust services
in production. Just for comparison, when we first introduced this system to
replace our previous Python-based ingress, the number of
`502 Bad Gateway`
errors in production decreased by 99.7%, due to clearer error handling and
tracking of request lifetimes. And it laid the groundwork for WebSocket support
without fundamental changes.

Today, web endpoints and remote function calls on Modal use a common system.
Having uniformity allows us to focus on impactful work that makes our cloud
runtime faster and lower-priced, while improving security and reliability over
time.

Acknowledgements
----------------

Thanks to the Modal team for their feedback on this post. Special thanks to
Jonathon Belotti, Erik Bernhardsson, Akshat Bubna, Richard Gong, and Daniel
Norberg for their work and design discussions related to
`modal-http`
.

If you’re interested in fast, reliable, and heavy-duty systems for the cloud,
[Modal is hiring](/company)

.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/serverless-inference-article
================================================================================

Best practices for serverless inference
=======================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Serverless inference is a cloud computing model that allows you to deploy and serve machine learning models without managing the underlying infrastructure. Notable characteristics of a serverless model include:

* No server management required
* Automatic scaling to handle varying loads
* Pay-per-use pricing model
* Low operational overhead

Why use serverless inference?
-----------------------------

Serverless inference offers several advantages, particularly for deploying and managing expensive transformer-based models. Here’s why it’s beneficial:

1. Cost-efficiency: Serverless inference eliminates idle GPU time costs. You only pay for the compute resources used during actual inference, making it ideal for models with variable or “bursty” traffic patterns.
2. Scalability: It automatically scales to handle varying loads, from sporadic requests to sudden traffic spikes, without manual intervention.
3. Reduced operational overhead: There’s no need to manage servers or worry about capacity planning. The cloud provider handles infrastructure management, allowing you to focus on model development and optimization.
4. Flexibility: Serverless inference adapts to your needs, whether you’re serving a single model or multiple models with different resource requirements.

While serverless inference may appear more expensive on a “per-minute” basis compared to traditional server-based deployments, it eliminates the need to provision for maximum capacity scenarios. This can lead to significant cost savings, especially for workloads with variable demand.

It’s worth noting that even if you anticipate running GPUs around the clock, actual utilization rarely matches this expectation. Serverless inference helps optimize resource usage and costs in these scenarios.

Top serverless inference providers
----------------------------------

In recent years, a number of companies have emerged to offer serverless capabilities for running inference workloads. These include:

* [Modal](https://modal.com/)
* [RunPod](https://www.runpod.io/)
* [Lambda Labs](https://lambdalabs.com/)
* [Replicate](https://replicate.com/)

Note that while GCP, Azure, and AWS each offer their own serverless cloud platforms, only GCP Cloud Run Functions supports running GPUs, and this is currently still in preview.

For more details on the providers above, check out our
[comparison article](/blog/serverless-gpu-article)

.

Best practices for serverless inference
---------------------------------------

To optimize your serverless inference deployments:

1. Leverage GPU acceleration: For compute-intensive models, utilize GPU resources effectively:

   * Choose
     [the appropriate GPU type and memory](/blog/how-much-vram-need-inference)

     for your model to ensure efficient resource utilization.
   * Consult your provider’s documentation on how to specify GPU requirements for your functions.
2. Minimize cold starts:
   [Cold starts](/docs/guide/cold-start)

   (the time it takes to spin up a new container with your model in it) can significantly impact latency for serverless functions. Consider these techniques:

   * Maintain a
     [pool of warm instances](/docs/guide/cold-start#run-more-warm-containers)

     that are always up and running.
   * Adjust container idle timeouts to keep containers warm for longer periods, if supported.
3. Optimize model loading and initialization:

   * Utilize lifecycle methods or initialization hooks provided by your serverless platform to load models during container warm-up rather than on first invocation.
   * Move large file downloads (e.g. model weights) to the build or deployment phase when possible, so that they are downloaded only once.
   * Take advantage of pre-built images or layers which come with optimized dependencies for common ML frameworks.
   * Consider model quantization or pruning techniques to reduce the size of the model that needs to be loaded without significantly impacting performance.
   * Use persistent storage options to cache model weights, reducing load times on subsequent invocations.
4. Implement efficient batching:

   * Utilize
     [batching mechanisms](/docs/guide/dynamic-batching)

     provided by your serverless platform to automatically batch incoming requests, improving throughput.
   * Implement custom batching logic within your inference function for fine-grained control over batch size and processing.

Conclusion
----------

Serverless inference offers a powerful way to deploy machine learning models with minimal operational overhead. By understanding the concepts and following best practices, you can leverage serverless platforms to efficiently serve your AI models at scale.

To get started with serverless inference, check out the
[Modal documentation](/docs)

or explore other cloud providers’ offerings.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/soc2type2
================================================================================

Modal is SOC 2 Type II Compliant
================================

As a cloud compute platform, Modal is committed to customer security and privacy as top priorities. Our product is secure by design, and we take measures from development to deployment to mitigate risk and earn the trust of our users. Last year we achieved SOC 2 Type I compliance, and earlier this year we announced support for HIPAA compliant workloads. Today, we’re excited to announce that we’ve successfully completed our SOC 2 Type II audit.

SOC 2 is one of the most recognizable and established standards for information security compliance. In order to achieve compliance, an independent third-party auditor assesses the security posture of a company and determines if it meets key criteria around security, availability, and confidentiality. SOC 2 Type II is more rigorous than Type I, since it assesses a company’s controls over a window of time rather than at a single point. No deviations were found in our audit.

As our customer base grows, we’re committed to providing stronger and stronger reassurances around the security of our platform. We will be renewing our SOC 2 audits annually alongside continual improvements to our product and processes.

You can read more about our security program
[here](https://modal.com/docs/guide/security)

, or reach out to us at
[security@modal.com](mailto:security@modal.com)

for a copy of the report.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/speeding-up-container-launches
================================================================================

How Modal speeds up container launches in the cloud
===================================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

At Modal, one of our goals is to make running code in the cloud as intuitive and easy as running code locally.

To do this, we’ve had to architect a system that spins up cloud-based containers (with your code in them) as fast as possible, ideally under 1 second.

In this article, we will cover some of the techniques around how we did this.

Understanding containers
------------------------

Let’s start with what a container is.

Think of a container as a lightweight, stand-alone, executable package of software that includes everything needed to run it, isolated from the rest of the system. This includes the code, runtime, system tools, libraries, and settings.

At the heart of a container is a Linux root file system that replicates a traditional Linux environment with directories like
`usr`
,
`etc`
, and
`lib`
. Containers also include various features for resource management and security.

Containers ensure resources are isolated, allowing applications to run effectively without affecting one another.

Shortening image pulls
----------------------

While containers solve many problems related to software consistency and isolation, running them efficiently in the cloud presents new challenges.

One of the most significant is the time it takes to pull a container image from a remote repository.

Container images can be large, often weighing in at several hundred megabytes or even gigabytes. For example, a standard Docker image might be around 1 GB, and more complex applications that rely on frameworks like CUDA or TensorFlow can easily exceed 10 GB. Pulling such large images from a remote repository can take several minutes.

This slow download time can be a significant bottleneck. The infamous Docker progress bar, slowly filling as the image downloads, is a common sight for anyone who has worked with containers in the cloud.

Reducing image bloat
--------------------

One of the primary reasons for the slow download times is that many container images are unnecessarily bloated. A typical container might contain thousands of files, many of which are not even used by the application.

Some developers attempt to address this issue by optimizing their Dockerfiles, which is a good start, but we wanted to go even deeper.

We noticed that when running a Python application, the system might make thousands of file system calls, but only access a small fraction of the files in the container image.

For example, executing the command:

```
$ python3 -c 'import sklearn'
```

results in:

* **3,043 calls to
  `stat`**
* **1,073 calls to
  `openat`**

but only actually accesses 1,000 unique files. This means that a vast majority of the files in the container image are not even being used.

By identifying and focusing on the essential files needed to run an application, it’s possible to significantly reduce the size of the container image and, consequently, the time it takes to pull and start the container.

Avoiding Docker
---------------

While Docker itself is a powerful tool for managing containers, it comes with some overhead that can slow down the process of launching containers, especially in the cloud. To streamline the process, it’s possible to bypass Docker entirely and use a lightweight container runtime like
[**runc**](https://www.docker.com/blog/runc/)

or
[**gVisor**](https://gvisor.dev/)

.

These runtimes won’t manage images or containers; instead, they simply point to a root filesystem and takes a JSON configuration to execute a container.

This opens up a way for us to start the container without having to pull images at all.

After constructing the image, Modal transfers it to network storage. Following this, when it comes time to run the container, Modal deploys
`gVisor`
, providing it with a root filesystem that’s stored on the network.

Rather than wasting time waiting for an image to download, the container can launch almost immediately since necessary files are already on the network share.

(Note: a further reason we use
`gVisor`
and not regular containers is that it is more secure. Regular containers share the host system’s kernel. This means that if a vulnerability is discovered in the kernel, it could potentially affect all containers running on that host. Conversely, a malicious container might be able to exploit kernel vulnerabilities to break out of its container and access the host system or other containers.
`gVisor`
works by intercepting application system calls and acts as a guest kernel, limiting the surface area for potential attacks.)

Caching frequently accessed files locally
-----------------------------------------

Even with optimizations like avoiding Docker and reducing image size, there’s still the issue of file system latency. For instance, executing heavy imports leads to a significant number of file operations. Even with NFS latency around 2 milliseconds, if you have around 4,000 file accesses, this can lead to an wait time of approximately 8 seconds.

The solution we landed on is to cache frequently accessed files locally. By storing these files on a local SSD or in memory, it’s possible to reduce access times dramatically. Local SSDs have latencies in the range of 100 microseconds. Caching files in the Linux page cache can bring latencies down even further.

Caching is especially effective when running the same container image multiple times, as many of the files accessed will be the same. Even when running different images, there’s often a significant overlap in the files they access.

Content-addressed caching
-------------------------

To effectively cache files, we can deploy a technique called content-addressing. This method involves hashing the contents of each file, then leveraging that hash to define the storage location for each file.

When
`gVisor`
tries to access a file, it first checks the cache to see if the file is already available locally. If it is, the file is returned from the cache, bypassing the need to access the network or disk.

If the file isn’t in the cache, it can be fetched from the network and stored locally for future use. This approach ensures that the most frequently accessed files are always available quickly, significantly improving the performance of container workloads.

To do this, we had to set up a simple filesystem in FUSE (Filesystem in Userspace). Contrary to popular belief, building filesystems isn’t prohibitively complex. You can even do this in Python!

Conclusion
----------

By focusing on what’s essential for running an application and avoiding unnecessary overhead, we’ve developed a system that significantly reduces the time it takes to start containers, making it easier for developers to deploy and scale their applications in the cloud. To see this for yourself, you can get started with Modal
[here](/docs)

.

**This article is adapted from Erik Bernhardsson’s 2023 talk at Data Council.**

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/sports-case-study
================================================================================

How a top tier European soccer team sped up their data processing and reduced costs by 50%
==========================================================================================

Since the advent of
[Moneyball](https://en.wikipedia.org/wiki/Moneyball:_The_Art_of_Winning_an_Unfair_Game)

,
sports teams around the world have incorporated data analysis into their decision making. At Modal, we’re fortunate to
partner with one of the world’s best soccer teams in their quest to win their championship. To honor their request for anonymity,
we will be referring to them as
[AFC Richmond](https://ted-lasso.fandom.com/wiki/AFC_Richmond)

.

![Computer Vision Soccer](https://modal-cdn.com/cdnbot/soccer-cv0vsttzny_8f133f35.webp)

Image taken from
[Amritangshu Mukherjee’s medium post on tracking soccer players](https://medium.com/@amritangshu.mukherjee/tracking-football-players-with-yolov5-bytetrack-efa317c9aaa4)

The problem: Processing spatio-temporal match data efficiently
--------------------------------------------------------------

In every match, computer vision systems are deployed to produce large amounts of tracking data for every player. Typical tracking data contains x/y positions
for each of the 22 players and the ball at 25 frames-per-second, resulting in ~3.5 million observations per game. AFC Richmond was looking for a solution to ingest
their tracking data for each frame of a match, run inference on it, and write the results to cloud storage. AFC Richmond uses a custom transformer-based model that takes
as input the unstructured spatio-temporal data from sequences of play, and produces structured outputs and high-dimensional embeddings. These outputs and embeddings are
used for analyzing the performance of the players in different situations: was it the right time to take a shot? How effective was the positioning of the players during a
particular moment? How do other teams handle such situations?

Before Modal, AFC Richmond tried using a GPU cluster on a major cloud provider, but it was not well set up for this workflow and required them to choose from a
limited set of instance types. This limitation meant that AFC Richmond had to pay for larger and more powerful configurations than they needed. Furthermore, long
cluster warmup times (6-8 minutes) added to their costs and made horizontal scaling trickier than they had hoped.

Modal’s solution: serverless batch processing on GPUs
-----------------------------------------------------

![Workflow diagram](https://modal-cdn.com/cdnbot/soccer-workflow5azt_811_d8df8d28.webp)

AFC Richmond decided to switch over to Modal so that their infrastructure would be more flexible to build on. They didn’t have to worry about underutilization, and containers started up
in a matter of seconds. The usage-based pricing and serverless nature of the product resulted in a 50% cost reduction for processing a full season of games.

Modal is also well set up to scale automatically based on the volume of data inputs. Using Airflow on Modal, AFC Richmond was able to achieve high parallelization,
processing data for games in a matter of minutes rather than hours.

Furthermore, the team loved the smooth developer experience; they were able to get set up and run their first job within hours:

“

Modal made it easy to install a minimal set of libraries needed for the specific workflow and provided an easy way to read/write from cloud storage.
The ability to switch quickly between CPUs and different GPU types made testing and iterating incredibly straightforward, and the smooth web interface made it easy
for our team to share logs and debug together.

”

— Led Tasso,

Data Scientist at AFC Richmond

Bonus: Semantic search with embeddings
--------------------------------------

AFC Richmond also built a lightweight in-memory vector DB on top of Modal, as this turned out to be cheaper than using managed vector DB solutions. This allowed them to make queries
based on the semantic similarity of embeddings generated in the previous steps. As an example: coaching staff can take a particular moment of a match and query for similar situations
that showed up in a different match to determine the best course of action for the players.

We’re excited to partner with AFC Richmond to develop more use cases and are honored that we can indirectly deliver joy to millions of soccer fans around the world.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/substack-case-study
================================================================================

Why Substack moved their AI and ML pipelines to Modal
=====================================================

![Substack logo](https://modal-cdn.com/tmphcchgran_7a8ab121.jpg)

[Substack](https://www.substack.com)

is a popular online platform for writers to publish newsletters, with over 17k writers and $300M in paid subscription volume.

Substack employs ML for various purposes, including spam detection, newsletter recommendations, audio transcription, sentiment analysis, and image generation. For nearly all these models Substack has moved both training and deployment from AWS SageMaker to Modal.

The challenges deploying AI and ML before Modal
-----------------------------------------------

Previously, Substack’s training and deployment pipelines were built on AWS SageMaker and orchestrated with Airflow. Adding or updating models was a slow and painful process for a few reasons.

One, the developer experience on SageMaker was convoluted. Engineers had to navigate to the SageMaker product, create a notebook, specify machine requirements, and wait for that machine to turn on, all before a single line of code could be written. Not to mention the difficulty of juggling multiple remote environments—from the Jupyter notebooks to the SageMaker training machines to the final production infra.

Two, collaborating was difficult. Mike Cohen, Head of AI and MLE at Substack, found that his team often had to duplicate code across various notebooks due to the way SageMaker would package code before sending it off to training machines. It was hard to share components across similar projects.

Three, containers took forever to start up. Engineers had to wait 5+ minutes for training machines to spin up, impeding their ability to iterate quickly and incurring unnecessary costs. They tried AWS’s Serverless Inference product thinking it’d be faster, but it lacked GPU support and still proved to be too slow.

“

Using SageMaker just felt like a very convoluted process. It felt very removed from a normal engineering workflow. With Modal, it’s a lot faster. You don’t feel like you have to wait for notebooks to spin up or remember to turn them off or any of that type of stuff.

”

— Mike Cohen,

Head of AI & ML Engineering

Making everything faster on Modal
---------------------------------

Mike first tried Modal to run a transcription model and quickly realized how much more natural it felt to iterate on Modal. All development and testing was done in the most obvious places—his code editor and the command line. Getting an inference endpoint up and running took just an hour to implement, and Modal’s container autoscaling helped them parallelize transcription workloads out of the box.

![Substack workflow](https://modal-cdn.com/tmpu1284bf9_e7632e36.png)

Substack decided to migrate over training and deployment for existing models as well. They implemented a full fine-tune → validation → deployment workflow by leveraging Modal’s native storage primitives. In this workflow, they:

1. Fetched training data from Snowflake, wrote it to S3, then mounted the S3 bucket to Modal.
2. Ran the model fitting with Modal functions.
3. Saved the weights to a
   [Modal network volume](/docs/guide/volumes)

   .
4. Ran model validation with another Modal function using those weights.
5. Deployed the new model on Modal and updated a
   [Modal key-value store](/docs/guide/dicts)

   to track which model version was in production.

“

We want to invest more in our recommendations system and models in general. My teammate was able to spin up a new recommendation model with Modal very quickly in a matter of days, whereas that probably would have taken quite a bit of time in SageMaker land.

”

— Mike Cohen

By moving to Modal, Substack has been able to develop and deploy ML workflows with greater speed and flexibility than ever before.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/suno-case-study
================================================================================

How Suno shaved 4 months off their launch timeline with Modal
=============================================================

![Suno logo](https://modal-cdn.com/cdnbot/suno-headerdkjin7qt_49c562b1.webp)

[Suno](https://www.suno.ai/)

uses Modal to scale inference and batch
pre-processing to thousands of GPUs. With Modal, Suno was able to bring a
state-of-the-art music generation model to market four months early instead of
hiring a team of engineers to build and maintain infrastructure.

About Suno
==========

Suno is a music generation app that can make any song you describe. Enter a
simple text description—like “a deep house song about serverless infra”—and Suno
makes you a song complete with vocals in seconds. Suno’s users include
Grammy-winning artists, but the core user base is people experiencing making
music for the first time. Microsoft recently
[announced](https://blogs.bing.com/search/december-2023/Turn-your-ideas-into-songs-with-Suno-on-Microsoft)

they’ve partnered with Suno to bring song generation capabilities to Copilot,
their AI chatbot!

Avoiding past infrastructure pain
---------------------------------

Prior to starting Suno, all four founders worked at Kensho, an AI tech startup
for financial data. They had personally spent significant amounts of time
setting up and managing Kubernetes clusters to support their data-heavy
workloads—so when they started working on Suno, they knew exactly what they did
not want:

* They did not want to manage their own clusters. They knew this would only
  become more complex over time in order to handle scaling, redundancy, and load
  balancing.
* They did not want to divert engineering resources and delay time-to-market in
  a rapidly evolving industry.
* They did not want to commit to 3-year-long GPU reservations to secure
  reasonable prices.

Georg, co-founder and CTO of Suno, gave Modal a try after a friend’s
recommendation. He was intrigued by how easy it was to deploy code in the cloud.

An easy setup
-------------

Suno began by running their batch pre-processing on Modal, allowing Modal to
dynamically manage the compute needed by these workflows. Not a single config
file was used—all they needed was a few short Python scripts running in Modal:

“

Modal reminded me of the difference between PyTorch and TensorFlow, where
Torch catered more to the ML crowd and was okay deviating from some CS
principles. That’s the beauty of Modal. You don’t have to understand much
about containers; all you need to know is that you can scale your function
calls in the cloud with a few lines of Python.

”

— Georg Kucsko,

Co-founder and CTO, Suno

Suno then expanded their use of Modal to model deployment. As a general purpose
platform, Modal offered many features that Suno could leverage, like the ability
to:

* Expose functions directly as web endpoints
* Chain together inputs and outputs of inference functions to create end-to-end
  sequences across multiple models and containers

…all defined programmatically in Python.

The Modal team worked closely with Suno as they transitioned from prototypes to
production. Georg remarked, “It’s almost like we’re on the same team; us
flagging something and you guys immediately working on it is awesome.”

(Auto)scaling to 1000 GPUs
--------------------------

![Suno GPU usage chart](https://modal-cdn.com/cdnbot/suno-bar-graph50ir9ze9_70e0512f.webp)

Suno’s GPU usage on Modal is variable and peaks on holidays

As Suno’s popularity grew, the feature they found most valuable was Modal’s
ability to auto-scale up or down thousands of GPUs to efficiently match demand.
During holidays like Christmas and Valentine’s Day, request volume would shoot
up as users created more songs to share with friends and family.

“

What kills you is this peak demand, right? Like you just can’t afford to be
buying machines for steady demand and then also have two people for
six months do nothing other than building inference that can handle scaling
down and up from that.

”

— Georg Kucsko

Aside from saving developer time, Suno also did not need to commit financially
to a large amount of GPUs, with the challenges that this typically
entails—either low utilization or a degraded user experience.

Modal looks forward to supporting Suno as their compute needs grow!

*p.s. check out this theme song we made with Suno!*

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/sync-labs-case-study
================================================================================

How sync. uses Modal to lipsync 100 hours of video a day
========================================================

“

With Modal, we went from a research project to a revenue-generating business almost overnight. Leveraging Modal allowed us to focus on what we do best – training state of the art video generation models – without worrying about MLops and infrastructure.

”

— Prady Modukuru,

CEO

About sync.
-----------

sync. is a research lab training foundational models to understand and manipulate humans in video. They released a state-of-the-art lipsyncing model that lets you reanimate the speech of any human in live-action, animated, or AI generated video. They are the original team behind wav2lip, and are used by thousands of developers, marketing teams, and creatives around the world.

Check out how their latest zero-shot lipsyncing model preserves the original speaking style of Nicolas Cage even when he’s translated into other languages.

But they didn’t start here. Check out the viral demo that brought them to the forefront of AI video: Lex Fridman and Mark Zuckerberg talking about Elon Musk in Hindi.

The Challenge: Graduating from Google Colab
-------------------------------------------

The viral tweet above was powered by a Google Colab notebook. This was great for a prototype and to get research out quickly, but impossible to scale into a business.

Like many, they started with AWS Lambda and quickly ran into headaches with their deployment process, autoscaling policies, and lack of GPU support. At this time, they were only a team of five and wanted to focus on training new models and iterating on product, not MLops infrastructure.

Then they checked out AI-as-an-API providers like Replicate. While Replicate was great for serving off-the-shelf models, the sync. team quickly became frustrated deploying their custom model via Replicate’s Cog framework. Every code update required a redeploy and rebuild of the container, which took
**10-15 minutes**
. Successful startups need to be able to iterate in seconds, not tens of minutes.

Finding Modal: The Right Fit for AI-Driven Video Editing
--------------------------------------------------------

Through connections at Founders Inc and Y Combinator, sync. discovered Modal’s startup credits program and was successfully awarded $25K in credits. Unlike other solutions, Modal just worked. No need to log in to a separate platform to write code; sync. could easily decorate their existing code, run
`modal serve`
to see code changes live, and deploy worry-free knowing that Modal would handle autoscaling.

This fast iterative loop meant sync. could deploy up to
**95 times a day**
testing many small iterations and updates while managing a production workload. In one year, they were able to ship 10 major model variants to production and 1000 iterations in between.

Deep dive: lipsyncing over 100 hours of video a day
---------------------------------------------------

Let’s look at a specific example of how Sync Labs uses parallel execution on Modal to lipsync over 100 hours of video a day.

Short videos are processed in a single batch. Longer videos are broken down systematically:

1. Splice the video into n scenes
2. For n scenes, run face detection and translation across n T4 GPU containers
3. Pass each scene to proprietary LipSync GenAI model on more powerful A100s
4. Stitch dubbed scenes back together

![sync diagram](https://modal-cdn.com/cdnbot/sync-diagramva5adbna_2b2d3f05.webp)

This is a great example of how Modal’s simple
[parallel execution model](/docs/guide/scale)

can efficiently process what would have been hours-long, compute-intensive workloads.

What’s next for sync. and Modal?
--------------------------------

sync. wants to put the power of a VFX studio in everyone’s pocket. Translation and lipsyncing is just the beginning of AI-enabled editing; the team is hard at work on emotion editing, pose adjustment, and even changing physical characteristics of the subject.

It’s incredible how much a small team can accomplish these days. sync. can focus entirely on R&D and let Modal handle the platform. This is the kind of modern startup organization that Modal was built to support; people at the cutting edge of AI research who want to ship code fast.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/text-to-video-ai-article
================================================================================

Top open-source text-to-video AI models
=======================================

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

*Updated: 2025-02-28*

Open-source text-to-video AI models are rapidly approaching the quality of leading closed-source models like Kling or OpenAI’s Sora.

Here’s a quick look at some of the top
[currently trending](https://huggingface.co/models?pipeline_tag=text-to-video&sort=trending)

(as of this writing) open-source text-to-video models:

| Model | Parameters | Created by | Released |
| --- | --- | --- | --- |
| HunyuanVideo | 13 billion | Tencent | Dec 03 2024 |
| Mochi ( [deploy on Modal](/docs/examples/mochi)  ) | 10 billion | Genmo | Oct 22 2024 |
| Wan2.1 | 14 billion | Alibaba | Feb 25 2024 |

And here’s a comparison of their video generation quality for the sample prompt: “A white dove is flapping its wings, flying freely in the sky, in anime style.” (prompt taken from
[Penguin Video Benchmark](https://github.com/Tencent/HunyuanVideo/blob/main/assets/PenguinVideoBenchmark.csv#L12)

)

[

](https://modal-cdn.com/text-to-video/hunyuan_dove.mp4)

Hunyuan

[

](https://modal-cdn.com/text-to-video/mochi-dove.mp4)

Mochi

[

](https://modal-cdn.com/text-to-video/wan-dove.mp4)

Wan2.1

### HunyuanVideo

* Released: Dec 3, 2024
* Creator: Tencent

[Hunyuan](https://huggingface.co/tencent/HunyuanVideo)

(roughly pronounced “hwen-yoo-en” in English) is the leading open-source text to video AI model. It is consistently at or near the top of HuggingFace’s trending models and by far the most discussed model in our
[community Slack](https://modal.com/slack)

.

Key features:

* Over 13 billion parameters
* Diffusers integration
* FP8 model weights to save GPU memory
* Several popular fine-tunes e.g.
  [SkyReels V1](https://huggingface.co/Skywork/SkyReels-V1-Hunyuan-T2V)

  which is fine-tuned on 10s of millions of human-centric film and television clips

Example videos:

[

](https://modal-cdn.com/text-to-video/hunyuan-alien.mp4)

Ultra-realistic, intricate textures. Panspermia Extraterrestrial life, Fermi paradox, swirling dust particles, Unreal engine 5 render

[

](https://modal-cdn.com/text-to-video/hunyuan-astro.mp4)

An astronaut flying in space by Hokusai, in the style of Ukiyochi

[

](https://modal-cdn.com/text-to-video/hunyuan-golden.mp4)

A few golden retrievers playing in the snow

These videos demonstrate Hunyuan’s high quality and realistic generation capabilities, though the astronaut video does not really adhere to the style prompt.

### Mochi

* Released: Oct 22, 2024
* Creator: Genmo

[Mochi](https://github.com/genmoai/mochi)

is a popular high quality text-to-video model. Mochi ranks similarly to Hunyuan on crowd-sourced
[leaderboards](https://artificialanalysis.ai/text-to-video/arena?tab=Leaderboard)

.

Key features:

* 10 billion parameters
* Easy to
  [deploy on Modal](/docs/examples/mochi)
* Support for LoRA fine-tuning
* Native
  [ComfyUI](/docs/examples/comfyapp)

  integration

Examples:

[

](https://modal-cdn.com/text-to-video/mochi-alien.mp4)

Ultra-realistic, intricate textures. Panspermia Extraterrestrial life, Fermi paradox, swirling dust particles, Unreal engine 5 render

[

](https://modal-cdn.com/text-to-video/mochi-astro.mp4)

An astronaut flying in space by Hokusai, in the style of Ukiyochi

[

](https://modal-cdn.com/text-to-video/mochi-golden.mp4)

A few golden retrievers playing in the snow

In these examples, Mochi’s quality is generally a little worse compared to Hunyuan, though the first example is arguably my favorite in the entire series of videos in this article.

### Wan2.1

* Released: Feb 25, 2024
* Creator: Alibaba

[Wan2.1](https://github.com/Wan-Video/Wan2.1)

is the most recent model in this series and is positioning itself as the newest state-of-the-art.

Key features:

* 14 billion parameters
* Smaller 1.3 billion parameter version also available
* ComfyUI integration

Example videos:

[

](https://modal-cdn.com/text-to-video/wan-alien.mp4)

Ultra-realistic, intricate textures. Panspermia Extraterrestrial life, Fermi paradox, swirling dust particles, Unreal engine 5 render

[

](https://modal-cdn.com/text-to-video/wan-astro.mp4)

An astronaut flying in space by Hokusai, in the style of Ukiyochi

[

](https://modal-cdn.com/text-to-video/wan-golden.mp4)

A few golden retrievers playing in the snow

The overall quality of Wan2.1 is maybe slightly worse than Hunyuan, but it does the best job adhering to the style instructions of the astronaut prompt.

Notable mentions:
-----------------

* [Step-Video-T2V](https://huggingface.co/stepfun-ai/stepvideo-t2v)

  : Relatively unknown AI startup Stepfun’s 30B(!) parameter model.
* [AnimateDiff-Lightning](https://huggingface.co/ByteDance/AnimateDiff-Lightning)

  : Bytedance’s faster version of the popular
  [AnimateDiff](https://github.com/guoyww/AnimateDiff)

  (especially among ComfyUI users). This isn’t a standalone text-to-video model, but rather a video adapter for an existing text-to-image base model like Stable Diffusion.

Running Text-to-Video AI Models
-------------------------------

Text-to-video AI models are inherently difficult to run due to their large parameter sizes and the complexity of video generation tasks.

A good rule of thumb for selecting a model is to pick one with a diffusers or ComfyUI integration. This is a good indicator of model maturity and will make your deploment process much easier.

Conclusion
----------

The text-to-video space is moving at a very fast clip, with new models claiming “state-of-the-art” being released every few weeks.

As GPUs become easier and cheaper to access, deploying open-source models like Hunyuan, Mochi, and Wan2.1 are becoming even more attractive options. At Modal, this is as simple as running our end-to-end
[Mochi example](/docs/examples/mochi)

, but you can run any code on Modal in a cost-effective and developer-friendly way.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/the-future-of-ai-needs-more-flexible-gpu-capacity
================================================================================

The future of AI needs more flexible GPU capacity
=================================================

![author](https://modal-cdn.com/erik-bernhardsson.jpg)

[Erik Bernhardsson

@bernhardsson](https://twitter.com/bernhardsson)

CEO and Founder

The last couple of years of Gen AI frenzy have brought us some undeniably cool new products, like Copilot and Suno.
One thing they all have in common is they demand
*lots*
of compute – in particular GPUs.
But the supply of GPUs is constrained.
This supply-demand imbalance has caused the market for cloud GPUs to behave very differently than other cloud markets.

Why is that?
What should we do?
And what can we expect going forward?
Should startups keep buying long-term GPU reservations from cloud vendors?
Or will there be other options in the future?

How do you make money from AI models?
-------------------------------------

AI is powered by GPUs, and most of the GPU demand today goes towards training large generative models.

But training is a cost center and eventually you need to recoup that cost through real revenue.
How do you do that?
Enter inference – the less sexy but money-making sibling of training.

So why is most GPU demand driven by training even though inference is where you make the money?
I think a lot of it reflects where we are in the cycle – there’s an
*expectation*
that the revenue potential for inference is big, but in order to get this, you have to spend a lot of money on training.

The economics of this – high upfront capital, but large potential – is something VCs understand quite well, so I think this explains why model builders have had no challenges raising lots of dollars.
But for the economics of this to make sense
*eventually*
, we need to see a much larger % of GPU spend going towards inference.

So let’s talk about inference for a second – how is it different than training?

Inference workloads are volatile
--------------------------------

Let’s say you expect a bunch of users to use your service for inference and you want to get a bunch of GPUs to handle that.
Here’s an interesting problem though: inference is
*volatile*
.

Consider a service that gets 1 req/s on average.
The number of requests each minute will be a Poisson process that looks something like this:

![static day](https://modal-cdn.com/cdnbot/static-day.jpg)

Let’s say every request takes 10 sec to handle.
How many GPUs do you need? If you’re running at 100% utilization, then you need exactly 10.
But the noise makes this impossible.
In practice, something like 12-15 GPUs running at 60-75% utilization is the right range for this.
We simply need a bit of padding in order to handle the noise.

It gets more complicated – the 24h cycle
----------------------------------------

Let’s make the model a bit more complex and assume you have a night-and-day cycle.
Now the request rate looks more like this:

![sine wave day](https://modal-cdn.com/cdnbot/sine-wave-day.jpg)

Using fixed capacity for this causes utilization to drop further, because we have to provision for the
*peak*
load.
This will push the utilization below 50%.

It’s actually a lot worse than that!
------------------------------------

Let’s add more sources of volatility you might encounter, like

* The 7 day week
* Events you can’t forecast – a tweet goes viral! someone posts a link to your service on Reddit!
* Trends in your usage (big growth, periods of decline, etc)
* Needs for internal bursty stuff (backfilling etc)

This means in reality your usage volume over a month will look like this:

![dynamic month](https://modal-cdn.com/cdnbot/dynamic-month.jpg)

This looking at it over a month. Seen over a day, it’s super noisy:

![dynamic day](https://modal-cdn.com/cdnbot/dynamic-day.jpg)

How do you pick a number of GPUs now that balance utilization and latency? How do you forecast usage 3 years out based on this? Do you want to tie up a lot of your venture capital dollars in these long-term commitments?

These are hard questions to answer, especially for startups that want to ship cool stuff and not worry so much. It’s not maybe surprising that this is one of the top AI infrastructure concerns among companies in a recent survey:

![gpu scarcity](https://modal-cdn.com/cdnbot/gpu-scarcity.png)

This is from
[The State of AI Infrastructure at Scale 2024](https://ai-infrastructure.org/the-state-of-ai-infrastructure-at-scale-2024/)

which features many other gems – go check it out.

Training is also volatile!
--------------------------

I just talked a lot about inference being volatile. But going back to training, training can be volatile too!

Of course, training tends to be much less latency sensitive. But training
*demand*
at a company probably varies quite a lot in reality. Sometimes you have lots of demand for very important jobs, sometimes it’s just long shot experimental R&D. Sometimes a developer is actively iterating and would really benefit from getting a 100 extra GPUs for a few hours.

There’s many other volatile workloads!
--------------------------------------

The same goes with many other types of things. Batch jobs (including batch inference) for instance. Small training jobs (including fine-tuning) is another example. While inference is inherently volatile and unpredictable, most other workloads also benefit from more flexible GPU consumption.

From our conversations and other people’s experience, the real world utilization of large GPU cluster
[is often sub 50%](https://www.photoroom.com/inside-photoroom/so-you-want-to-rent-an-nvidia-h100-cluster-2024-consumer-guide?slug=inside-photoroom/so-you-want-to-rent-an-nvidia-h100-cluster-2024-consumer-guide&_storyblok_published=511470179&)

!

How can GPU providers provide on-demand GPUs?
---------------------------------------------

So far, I’ve presented some arguments for why GPU
*demand*
is quite unpredictable and doesn’t fit the long-term-fixed-size-reservation model.
But could GPU
*supply*
be flexible?

I think the answer is that it can be, to a much larger extent than today – much like the CPU market where flexible consumption is the default.
Supporting this for GPUs is not an easy thing to build, but there’s a whole range of things we can bet on:

### Demand pooling

Pooling lots of users into the same underlying pool of compute can improve utilization drastically.
It reduces amount of capacity that has to be reserved in aggregate.
Instead of provisioning for the sum of the peaks, you can provision for the peak of the sum. This is a much smaller number!

![pooling](https://modal-cdn.com/cdnbot/pooling.jpg)

The chart above shows a simulation with 5 users. Because their peaks don’t coincide, we can get dramatically better utilization by pooling all their usage.
This requires multi-tenancy, meaning we want to run many different users on the same underlying pool of GPUs.

### Supply pooling

It’s also possible to pool the supply of GPUs to increase the capacity.
There are a few different strategies:

1. Use several regions.
   Many models (like Stable Diffusion) take a second or two to run.
   It’s often possible to send the request halfway across the world and back with minimal impact on latency.
   This is obviously less ideal for latency-sensitive tasks.
2. Pool different GPU types together and fall over between them (and use previous-generation GPUs when possible – the ones often left behind by the training crowd)
3. Aggregate several cloud vendors, in particular ones with on-demand GPU availability.

It should be mentioned that Modal uses all of these things and have invested a very substantial amount in resource pool scaling and the “bin packing” of jobs.
We actually solve a mixed-integer programming problem every minute to maximize our cloud utilization.

### Multi-tenancy requires fast scaling

With a multi-tenant pool of compute, and with large variance in demand, it’s critical that we can scale up and down very quickly.
In particular, booting up instances and provisioning them is incredibly slow, especially for inference workloads.
We want to start containers in seconds, not minutes.
This also improves utilization drastically, since hardware is being spent actually crunching numbers, not starting or stopping.

Taking this to its most extreme form, you end up with “serverless” infrastructure.
The idea is to let the users write application code, but let the infrastructure handle the container lifecycle management, request routing, and everything else.
It’s no secret we are hardcore believers of this at Modal!

Fast initialization of models is a hard problem.
A typical workload needs to fire up a Python interpreter with a lot of modules, and load gigabytes of model weights onto the GPU.
Doing this
*fast*
(as in, seconds or less) takes a lot of low-level work.
At Modal we built a file-system purpose made for this, and are spending a lot of time on ways to snapshot CPU and GPU memory for fast initialization.

### Demand smoothing

Another option for reducing the variance and improving utilization over time is to shift latency-insensitive demand from periods of high demand to low demand.
You could imagine giving discounts for jobs with high turnaround time, or scaling up training jobs overnight.
We are thinking a lot about these types of features at Modal!

The future of GPU consumption
-----------------------------

To summarize, some trends I expect to be true:

* Future GPU consumption will skew much heavier towards inference vs today
* There will be a much larger market for on-demand GPUs
* A substantial fraction of inference workloads will be powered by on-demand GPUs due to its unpredictable nature
* A meaningful fraction of small and medium size training workloads will shift to on-demand GPUs, because of the flexibility and faster feedback loops
* People will still make long-term GPU reservations to get the lowest possible price. But this will not be the default way to get capacity.

Modal is heavily investing in this.
We’re big believers in a future of flexible GPU consumption and have been working on this for several years.
We let you run big bursty jobs with hundreds of CPU, or deploy GPU-based cloud functions that can scale up and down instantaneously (including to zero).
If you’re interested, please
[try it out](https://modal.com/signup)

!

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/top-code-agent-sandbox-products
================================================================================

Top AI Code Sandbox Products in 2025
====================================

Large language model (LLM) applications, and the “software agents” they power, are generating more and more
**new code on‑the‑fly**
. Cursor is writing
[almost 1 billion lines of accepted code](https://x.com/amanrsanger/status/1916968123535880684)

each day.

Executing that code directly on your application servers is a security and reliability risk: it can expose secrets, overwhelm resources, or even escape the container. AI‑first sandboxes solve three problems at once:

1. **Security isolation**
   – containers or micro‑VMs cut the blast radius of malicious or buggy code.
2. **Ephemeral scale**
   – thousands of developer‑agent sessions can be spun up and torn down in seconds without leaving idle VMs running.
3. **Observability & networking guardrails**
   – a good provider exposes granular process logs & metrics and throttles egress

When evaluating a provider, look for (i) start‑up latency, (ii) language/runtime flexibility, (iii) per‑sandbox networking controls, (iv) price and autoscaling limits, and (v) an SDK that fits your stack.

1. Modal Sandboxes
------------------

**How it works**
–
[Modal](/docs/guide/sandbox)

lets you define a sandbox with one line of Python and then
`exec`
arbitrary commands inside. Sandboxes inherit Modal’s serverless container fabric, so they autoscale from zero to
**10,000+ concurrent units**
and back with sub‑second cold starts.

**Strengths**

* **Scale & reliability:**
  Production users such as
  [Lovable](/blog/lovable-case-study)

  and
  [Quora](/blog/quora-case-study)

  run millions of untrusted code snippets a day without pre‑provisioning capacity.
* **Flexibility:**
  Sandbox images can be dynamically defined at runtime via Modal’s Python SDK.
* **Robust networking features:**
  Built‑in
  [tunnelling](/docs/guide/sandbox-networking#forwarding-ports)

  for direct external connections and
  [granular egress policies](https://modal.com/docs/guide/sandbox-networking#networking)

  to lock down outbound networking.
* **Code‑first DX:**
  [Python/TS/Go SDKs](/blog/sdk-javascript-go)

  , no YAML, and snapshot/volume primitives that feel native to developer‑agent workflows.

**Weaknesses**
- on‑prem deployment is not an option today.

---

2. E2B
------

An
**open‑source runtime**
purpose‑built for AI “developer agents”. A sandbox boots in less than a second and can be orchestrated from Python or JavaScript.

**Strengths**
– OSS licence (plus a hosted SaaS option), bring‑your‑own cloud, fine‑grained filesystem API.

**Weaknesses**
– you manage the cluster; scaling past a few hundred sandboxes means running the E2B control‑plane yourself. No built‑in outbound‑network policies or IP filtering. You have to craft & push a Docker image for every custom environment.

---

3. Together Code Sandbox
------------------------

Together AI extends its GPU cloud with sandboxes that start a full VM from snapshot in
**500 ms**
(2.7 s cold) and resume with memory already loaded—great for heavy IDE‑style developer agents.

* **Strengths**
  – hot‑swappable VM sizes (2‑64 vCPU), Git‑versioned storage, live preview hosts.
* **Weaknesses**
  – Docker‑based Dev‑Container images limit on-the-fly environments. No first‑class tunnels. Pricing is VM‑style (per vCPU & GiB‑RAM per minute); less attractive for bursty, sub‑minute jobs.

---

4. Fly Machines (DIY)
---------------------

Fly.io’s Machines API spins up a micro‑VM in less than a second and exposes a REST interface. Developers often script Machines as an
**ad‑hoc sandbox**
for user code.

* **Strengths**
  – global edge regions, persistent VM option, straightforward CLI.
* **Weaknesses**
  – no sandbox‑specific features (tunnels, snapshots, per‑process logs). Networking controls and
  [secrets management](https://community.fly.io/t/fly-io-secret-create-api/22185/2)

  have to be layered on.

---

5. Daytona
----------

Targets
**AI agents and eval pipelines**
with 90 ms sandbox creation, built‑in Git and LSP support, and a Python SDK.

* **Strengths**
  – low latency, live stream of stdout/stderr, file upload helpers.
* **Weaknesses**
  – young ecosystem; feature parity with Modal’s tunnels or Together’s snapshots still evolving.

---

6. Roll‑your‑own on Kubernetes
------------------------------

You can assemble a sandbox layer with
**Kubernetes +
[gVisor](https://zesty.co/finops-glossary/gvisor-in-kubernetes/)

, Kata Containers, or Firecracker micro‑VMs**
.

* **Strengths**
  – full control, no vendor lock‑in.
* **Weaknesses**
  – steep ops burden: patch vulnerabilities, handle image caching, and wire up per‑sandbox network policies yourself. A misconfigured pod can
  [expose the entire cluster](https://blog.sighup.io/how-to-run-untrusted-containers-in-kubernetes)

  .
* Example:
  [Using Firecracker and Go to run short, untrusted code execution jobs](https://stanislas.blog/2021/08/firecracker/)

Comparative snapshot
--------------------

| Provider | Autoscale ceiling | Snapshots | SDKs | Cold‑start P95 | Pricing\* | Sources |
| --- | --- | --- | --- | --- | --- | --- |
| **Modal** | 20k+ containers | FS + Memory | Py / JS / Go | Sub-second | $0.0000131/CPU/s, with $30 credits/mo | [Modal docs](https://modal.com/docs/guide/sandbox#sandboxes)  , [Modal pricing](https://modal.com/pricing) |
| **E2B** | Depends on your infra (OSS version) | FS + Process State | Py / JS | Sub-second | Hosted version: $0.000028/CPU/s, with $100 one-time credits | [E2B docs](https://e2b.dev/docs)  , [E2B pricing](https://e2b.dev/pricing) |
| **Together** | Limited | FS + Memory | REST / CLI | 2.7 s (500 ms resume) | $0.0000248/CPU/s | [Together](https://www.together.ai/code-sandbox)  , [Together Code Sandbox pricing](https://docs.together.ai/docs/together-code-sandbox#pricing) |
| **Fly** | Limited | Memory | REST / CLI | Sub-second | $0.000000529/CPU/s | [Fly machines](https://fly.io/blog/fly-machines/)  , [Fly machine pricing](https://fly.io/docs/about/pricing/) |
| **Daytona** | Warm pool scaling | FS | Py | 90 ms | $0.000028/CPU/s | [Daytona](https://www.daytona.io/)  , [Daytona pricing](https://www.daytona.io/pricing) |
| **DIY K8s** | Depends on your infra | Your choice | Any | Highly variable | You pay the underlying Infra + ops |  |

*\*Prices normalized to cost per physical CPU core (=2 vCPUs) per second. Note that some providers bundle in memory while others charge for it separately.*

Launch a Modal Sandbox in a few lines of code
---------------------------------------------

```
import modal
app = modal.App.lookup("sandbox-manager", create_if_missing=True)
sb = modal.Sandbox.create(app=app)

p = sb.exec("python", "-c", "print('hello')")
print(p.stdout.read())
sb.terminate()
```

The
[Modal Sandbox](/docs/guide/sandbox)

shuts down automatically when your developer‑agent finishes. No YAML, no VM lifecycle headaches—just clean, scalable isolation.

Using Modal Sandboxes for your software agents
----------------------------------------------

If your roadmap involves
**software agents**
that write or modify code, investing in a purpose‑built sandbox saves months of security engineering. Modal offers scaling to handle millions of executions daily, with sub‑second starts that keep your agents responsive. The built‑in networking tunnels and per‑sandbox egress policies mean your agents can safely interact with databases and APIs without exposing your entire infrastructure. Plus, the code‑first SDK integrates seamlessly into existing Python workflows—no Kubernetes manifests or VM provisioning required.

Start with
[Modal’s free tier](/pricing)

and scale to tens of thousands of concurrent sandboxes when your agent platform takes off.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/vllm-vs-tgi-article
================================================================================

vLLM vs. TGI
============

![author](https://modal-cdn.com/ren-lu.jpg)

[Yiren Lu

@YirenLu](https://twitter.com/YirenLu)

Solutions Engineer

Why use an inference framework?
-------------------------------

Why can’t developers simply use a library like Transformers to serve their models?

While libraries like Transformers are excellent for training and basic inference, they have limitations when it comes to large-scale deployment and serving of LLMs:

1. **Memory efficiency**
   : LLMs require significant memory resources. General-purpose libraries may not optimize memory usage, leading to inefficient resource allocation. For more information about the VRAM requirements for serving LLMs, read
   [here](/blog/how-much-vram-need-inference)

   .
2. **Inference speed**
   : Standard libraries often lack optimizations specific to inference, resulting in slower processing times for large models.
3. **Batching and queueing**
   : Handling multiple requests efficiently requires sophisticated batching and queueing mechanisms, which are not typically included in training-focused libraries.
4. **Scalability**
   : Serving LLMs at scale requires careful management of computational resources, which is beyond the scope of most general-purpose libraries.

Instead, for most production model serving, to maximize throughput and minimize latency, you should be using an inference server. Two of the most popular inference servers for LLM use cases are vLLM and TGI.

What are vLLM and TGI?
----------------------

### vLLM

[vLLM](https://github.com/vllm-project/vllm)

is an open-source library designed for fast LLM inference and serving. Developed by researchers at UC Berkeley, it utilizes
[PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)

, a new attention algorithm that effectively manages attention keys and values. vLLM delivers up to 24x higher throughput than Hugging Face Transformers, without requiring any model architecture changes.

Key features of vLLM include:

* Efficient memory management
* Continuous batching
* Optimized kernel implementations
* Support for various model architectures

### TGI (Text Generation Inference)

[TGI](https://huggingface.co/docs/text-generation-inference/en/index)

, short for Text Generation Inference, is a toolkit for deploying and serving Large Language Models (LLMs). Developed by Hugging Face, TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and more. It focuses on providing a production-ready solution for deploying and serving large language models, with a particular emphasis on text generation tasks.

Performance comparison: Which one is faster?
--------------------------------------------

When it comes to performance, both vLLM and TGI offer significant improvements over baseline implementations. However, determining which one is faster is not straightforward, as performance can vary depending on the specific use case, model architecture, and hardware configuration.

1. **Throughput**
   : vLLM often demonstrates higher throughput, especially for larger batch sizes, due to its PagedAttention mechanism and continuous batching optimizations.
2. **Memory efficiency**
   : vLLM’s PagedAttention technique allows for more efficient memory usage, potentially enabling higher concurrency on the same hardware.
3. **Ease-of-use**
   : Since TGI is made by Hugging Face,
   [serving any Hugging Face model (including private/gates ones) with TGI](https://huggingface.co/docs/text-generation-inference/en/quicktour)

   is relatively straightforward. The default way of running TGI, via an official Docker container, also brings up an API endpoint.
4. **Production-readiness**
   : TGI offers built-in telemetry via OpenTelemetry and Prometheus metrics. vLLM has fewer “production-ready” bells and whisles.

**We would generally recommend using vLLM, which provides a nice balance of speed, support for distributed inference (needed for large models), and ease of installation.**

How to deploy models with vLLM
------------------------------

Modal is a serverless cloud computing platform that makes it easy for you to deploy open-source models with frameworks like vLLM and TGI. To get started, follow these tutorials:

* [vLLM inference](/docs/examples/vllm_inference)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/websocket-launch
================================================================================

Introducing: WebSockets on Modal
================================

Here at Modal, we’re constantly cranking on complex infrastructure projects. We
want to start highlighting some of the heftier features we’ve released recently.
On the docket today: Modal now supports WebSocket connections.

WebSocket is a communication protocol for real-time, bidirectional transfer of
data between a client and server. Unlike HTTP, in which connections are opened
and closed per request/response, WebSocket establishes a persistent connection
between client and server. This is advantageous for applications that require
low latency and real-time updates.

How to set up a WebSocket server
--------------------------------

There’s nothing special you have to do to set up a WebSocket connection on a
Modal function; use your library of choice as you would normally. Here’s some
boilerplate of what that would look like using FastAPI, for instance:

```
import modal

app = modal.App("my-app")
app.image = modal.Image.debian_slim().pip_install("fastapi", "websockets")

@app.function()
@modal.asgi_app()
def endpoint():
    from fastapi import FastAPI, WebSocket

    app = FastAPI()

    @app.websocket("/ws")
    async def websocket_handler(websocket: WebSocket) -> None:
        await websocket.accept()
        while True:
            data = await websocket.receive_text()
            await websocket.send_text(f"Message text was: {data}")

    return app
```

Save the code above to a file called
`main.py`
, and deploy it with
`modal deploy main.py`
.

Modal treats each WebSocket connection as a single input, so you will want to
set your function to
[allow for concurrent inputs](/docs/guide/concurrent-inputs)

if it is not
CPU/GPU-bound. Otherwise, Modal will spin up a new container for each WebSocket
connection. Please see our
[WebSocket documentation](/docs/guide/webhooks#websockets)

for more info.

Why use WebSockets on Modal
---------------------------

One of Modal’s primary benefits is automatic scaling based on the volume of
inputs your functions are receiving. This applies to WebSocket handlers as well!
This makes it super easy for you to build applications that can handle variable
request volumes.

For example, let’s say you want to launch a real-time speech-to-text app that
will be able to handle many users at once. You can deploy both your WebSocket
server and transcription model as Modal Functions. Modal will auto-scale
containers for both Functions, without you having to write any of the scaling
logic.

![Diagram of clients connecting to a Modal WebSocket server](https://modal-cdn.com/cdnbot/websocket-launch-diagram.png)

Use cases
---------

At Modal, we’ve heard users ask for WebSocket support to facilitate a few
different use cases.

### 1. Real-time streaming responses

This use case is most common for those building features around audio streaming.
A speech-to-text application running on Whisper, for example, may need to stream
live, continuous transcription back to end users as audio input comes in. We’ve
also had users ask for this in the context of text-to-speech and text-to-image
features that require real-time responses to continuous inputs.

### 2. Status updates on long-running tasks

This use case is especially relevant for workloads that have a long processing
time—for example, prompting an LLM to pull insights from a very large body of
text. You may want to send progress indicators to the end user for these
long-running tasks. WebSockets come into play here because the server can send
intermediate updates to the client over the persisted connection.

### 3. Hosting open-source frameworks out-of-the-box

Several popular frameworks like ComfyUI, Streamlit, and Gradio require WebSocket
connections in order to be deployed. Many of our users are utilizing ComfyUI’s
GUI to build out stable diffusion pipelines; others are running Streamlit and
Gradio to prototype new ML features or build mini-apps. These frameworks rely on
WebSockets to power interactive visualizations in the client and surface
real-time updates when underlying data changes.

Check out our examples of how to run
[ComfyUI](/docs/examples/comfyapp)

and
[Streamlit](/docs/examples/serve_streamlit)

on Modal.

Got questions? Please reach out and join our
[community Slack](/slack)

. For
those curious, we’ll be publishing a technical deep dive on Modal’s web endpoint
system later this week.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/blog/what-is-ai-code-sandbox
================================================================================

What is an AI code sandbox?
===========================

The recent surge in LLM-generated code has revived a popular environment: code sandboxes.

Nowadays, a significant portion of code intended for production is written by non-deterministic systems, notably IDE-integrated AI agents. While this promises a massive boost in developer productivity, it also increases the risk of critical errors and security holes. While LLMs are proficient at coding, they are unpredictable and can make mistakes. Today’s AI-assisted engineering efforts need a safe environment that can isolate and execute generated code, containing the blast radius of a costly mistake.

This isolated environment is known as an
**AI code sandbox**
.

What are code sandboxes?
------------------------

Code sandboxes have a long, curious history. The earliest version of a code sandbox was found in Unix systems from the 1980s:
`chroot`
, a program that would alter the root directory of a running process, blocking it from accessing files outside of the designated file system. But the term
*sandbox*
only emerged when the Java VM made it possible to run
[untrusted applets in a bytecode verifier](https://phrack.org/issues/70/7)

. Since then, it’s existed as a feature in various runtimes. Today’s younger developers probably attribute the phrase
*code sandbox*
to
[JSFiddle](https://jsfiddle.net/)

, a super popular 2009 project that made it easy to test JavaScript, HTML, and CSS files.

Technically speaking, code sandboxes never disappeared. But from a branding standpoint, they were somewhat absorbed into the sprawling, dominant category of containerized architecture. In recent months, however, there’s been a resurgence of interest in sandbox solutions.

AI coding agents and the revival of sandboxes
---------------------------------------------

![Windsurf’s AI-powered code editor and agent](https://modal-cdn.com/blog/images/windsurftrimmed-optimized.gif)

Windsurf’s AI-powered code editor and agent

Soon after OpenAI released GPT-3,
[there was ample public amazement](https://x.com/sharifshameem/status/1282676454690451457)

at the language model’s ability to write detailed code. At the time, it was mainly Twitter users posting
[React snippets](https://x.com/wenquai/status/1378416315044614150)

manually generated with ChatGPT. There was also
[some skepticism](https://x.com/Madisonkanna/status/1649104808899665922)

about whether AI would actually make its way into the industry developer ecosystem.

Fast-forward two years, and
[most software businesses are using LLMs](https://techcrunch.com/2025/03/06/a-quarter-of-startups-in-ycs-current-cohort-have-codebases-that-are-almost-entirely-ai-generated/)

to generate code. They’re doing so via dedicated AI agents that directly integrate with popular software developer IDEs like
[VSCode](https://vscode.dev/)

. Popular examples of AI coding agents include
[Cursor](https://cursor.com/)

and
[Windsurf](https://windsurf.com/)

. The advantage of these tools is their straightforward UI and UX, where they’re more comparable to a programming partner than an independent SaaS product. These tools are
*a hit*
; while likely exaggerated, there are reports that
[97% of developers](https://github.blog/news-insights/research/survey-ai-wave-grows/)

are actively using AI coding tools.

This surge in AI coding agents gives rise to two contrasting forces. The first is that software development is radically accelerating, with features (or entire apps) being “vibe-coded” in the span of a day. The second is that the proliferation of code—especially code created by models
[that routinely hallucinate entire packages](https://www.darkreading.com/application-security/ai-code-tools-widely-hallucinate-packages)

—can lead to a proliferation of errors. The errors can cause a multi-tenant environment to crash, impacting other users. In worst case scenarios, a bad actor can prompt inject an LLM to create an intentional security vulnerability to exploit; for AI agents with long memory, this hack could happen even if the immediate user has good intentions but is oblivious to the agent’s poisoned context.

These risks are mitigated by isolating LLM-generated code’s execution, preventing it from crashing an environment or breaching other programs. AI code sandboxes are dynamic environments that can ephemerally containerize code at runtime.

How do AI code sandboxes actually work?
---------------------------------------

There are a couple different approaches to implementing AI code sandboxes under the hood. Either VMs or containers can be used as the underlying compute environment. On top of that, there needs to be an interface to quickly spin up these compute environments, sync over the code to be executed, and communicate with the internal processes.

It is critical to ensure that the underlying compute environment is airtight—that is the flagship purpose of sandboxes. This is where technologies like
[gVisor](https://gvisor.dev)

come into play. gVisor is a secure container runtime that is specifically designed for running untrusted code in production environments. Unlike standard container runtimes like
[runc](https://github.com/opencontainers/runc)

, gVisor intercepts syscalls from guest processes, providing a virtual kernel to the sandboxed container. This prevents these processes from having any ability to affect the host (see:
[Dirty Pipe vulnerability](https://securitylabs.datadoghq.com/articles/dirty-pipe-container-escape-poc/)

) outside of the permissions gVisor specifically gives them.

![Sandbox Diagram.png](https://modal-cdn.com/blog/images/sandbox_diagram.png)

Beyond the compute environment, there are a variety of additional features needed to make sandboxes usable. This includes:

* Filesystem APIs to sync data in and out of the sandbox
* Networking primitives to enable and control communications with other servers
* An orchestration layer to provision instances, scale resources, and manage the sandbox lifecycle
* Filesystem and/or memory snapshotting to save and restore sandbox state

Build vs buy
------------

Hypothetically, a company could build its own AI code sandbox infrastructure by leveraging Kubernetes and open-source isolation environments like gVisor. This, however, would induce a lot of operational cost to build and maintain. Container cold starts would also be unoptimized without additional effort, making it slow to both iterate on and serve end users. Making the system
*scalable*
to millions of concurrent sessions would be another massive lift.

For these reasons, most companies prefer a managed AI code sandbox
[service](/blog/sandbox-launch)

that enables fast developer velocity and reduces cost of ownership.

What are the use cases for AI code sandboxes?
---------------------------------------------

There are multiple real-world problems where AI code sandboxes are particularly applicable. Some of these problems are pertinent to the recent wave of AI-focused companies. Others are problems that companies might face when using AI-integrated engineering products. In either scenario, AI code sandboxes provide a critical layer of isolation to protect their infrastructure.

Let’s discuss some of these real-world problems in detail.

### Background coding agents

Today, many developers use an AI coding plugin like
[Cursor](https://cursor.com/)

or
[Windsurf](https://windsurf.com)

; others are using AI-native IDEs like
[Devin by Cognition](https://cognition.ai/blog/devin-2)

. These companies have all introduced background
[coding agent](/docs/examples/agent)

products, which need to clone a developer environment, modify code, and execute it to effectively test changes. This is similar to how developers use dedicated continuous deployment (CD) systems to build an application before deploying it to staging or production nodes. There’s an infinite variety of contexts for which coding agents are writing code, which means they benefit from sandboxes that support dynamically-defined environments.

### Code reviews

Sometimes, teams opt to run linters or other tests that evaluate if an application can be safely built. Code reviewing platforms require AI code sandboxes because the test code, whether AI-generated or human written, is produced externally and cannot be trusted. For code reviewing products, AI code sandboxes can be spun up to run the tests and produce the results before being discarded.

### LLM code interpreters

![poe gif](https://modal-cdn.com/blog/images/poe.gif)

Quora’s Python code interpreter chatbot that runs on Modal

Many AI chatbots, like ChatGPT, will write code and test it on AI-generated tests to validate if the code results in expected behaviors. For example, Quora’s new chatbot platform
[Poe](https://poe.com)

has chatbots that can write and evaluate code. In other cases, AI chatbots might write and execute code to answer a user prompt that requires analysis. In all of these scenarios, the LLM
[code interpreter](/docs/examples/simple_code_interpreter)

needs a safe, ephemeral environment for execution.

### Reinforcement learning

In reinforcement learning (RL), an agent receives information about its current state and takes a next action that maximizes expected award. During the learning process, the agent’s model weights are updated to favor actions that are likely to result in a higher reward.

In the context of training code generation agents, sandboxes are used to safely execute AI code and score the outputs via tests. The results are then fed back to the model as a reward signal. This requires bursting up to many sandboxes at once to run evals in parallel. Without this parallelization, the training process is bottlenecked on CPU-dependent steps, causing GPU utilization to suffer.

### AI-generated apps

There’s an emerging category of tools that generate entire applications with agents. Examples of this include
[Lovable](https://lovable.dev)

(
[a Modal customer](/blog/lovable-case-study)

!),
[Replit](https://replit.com)

, and even
[Webflow](http://webflow.com)

. However, because huge volumes of code are written by the underlying AI agent, executing this generated code is unsafe for the host application’s stack (e.g. Lovable’s servers) unless it’s done in a sandbox.

What features are important for AI code sandboxes?
--------------------------------------------------

There are several AI code sandbox vendors that exist, including Modal! When evaluating solutions, developers should check for several must-have features:

### Fast cold starts

For agentic or on-demand coding workflows, every second counts. Sandboxes that cold start in under a second enable near-real-time interactivity, which is important for applications like AI pair programming or dynamic app generation.

For internal use cases, the cold start metric may matter less than how many concurrent sandboxes you can scale to. That said, there’s still a big difference to internal productivity between a 5-second lead time and a minute. At the end of the day, cold start length can be a large contributor to overall latency.

### Strong isolation and security

Given that AI code sandboxes are often leveraged for security guarantees, a vendor’s security posture is of paramount importance. First and foremost, you should evaluate the
[kernel-level isolation system](/docs/guide/sandbox-networking#security-model)

. Some systems are more robust than others (for example, gVisor creates an application-specific kernel while runc just uses the host kernel). You should also look for built-in egress control features, lest you want to build those yourself. Finally, for production-grade applications, you’ll want a vendor that has a proven track record with compliance standards like SOC 2 and HIPAA.

### Elastic, high-concurrency scaling

Some products, such as Modal, can spin up thousands of containers in seconds. If you’re running evals across a matrix of inputs or exposing a product to millions of active users, fast scalability is key for a good user experience.

### Dynamic runtime environments

Often times AI-generated code needs a specific environment to run in. Most vendors will require you to bring and manage your own set of Docker images, but this doesn’t work well if you’re evaluating code that has dependencies that are not known until generation time. Modal specifically offers more flexibility with Python-defined images, which means you can
[dynamically define sandboxes](/docs/guide/sandbox#dynamically-defined-environments)

at runtime.

### Filesystem and memory snapshotting

There are certain use cases where saving and then restoring a sandbox’s state can be extremely useful. For example, you might want to back up your sandbox’s state for debugging, run large-scale fan-out experiments, or branch the current state to test different code changes independently. To reduce cold start latency, look for providers that optimize how they cache and restore snapshot images.

### Built-in networking primitives

Good sandboxes let you expose live endpoints via tunnels—for example, to power a browser-based REPL or webhook tester. This isn’t essential for all workloads, but it can save hours of plumbing if your use case relies on streamed data (e.g. virtual shell, video files, live market data) over UDP. It also keeps sandboxes closer to real-world deployments.

On the security front, you will also want an easy way to control outbound networking.

### Developer-first workflow

The best sandboxes feel like an extension of your local dev environment. Look for tools that require minimal boilerplate and have clean SDKs and infra-as-code patterns. If your team values iteration speed and hates dealing with
[Dockerfiles](https://docs.docker.com/reference/dockerfile/)

and YAML, this matters.

For production use cases, having granular observability features for debugging is important, too. You will want dashboards, metrics, and logs that are easy to parse down to the individual sandbox level.

![sandbox observability](https://modal-cdn.com/blog/images/sandbox-observability.webp)

Modal’s sandbox observability dashboard

A closing thought: we’re building in this space
-----------------------------------------------

If it wasn’t already obvious, we have a dedicated
[AI code sandbox product](/blog/sandbox-launch)

. It’s used by companies like
[Lovable](/blog/lovable-case-study)

and
[Quora](/blog/quora-case-study)

at massive scale.

Modal Sandboxes deliver sub-second cold starts and rapid scaling through our proprietary container stack. On top of that, we’ve built a rich layer of features for
[networking](/docs/guide/sandbox-networking)

,
[filesystem access](/docs/guide/sandbox-files)

, and
[snapshotting](/docs/guide/sandbox-snapshots)

to help you ship your AI coding features faster!

Interested in trying it out? Simply
[sign up](/signup)

and run the snippet below:

```
import modal
app = modal.App.lookup("sandbox-manager", create_if_missing=True)
sb = modal.Sandbox.create(app=app)

p = sb.exec("python", "-c", "print('hello')")
print(p.stdout.read())
sb.terminate()
```

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/company
================================================================================

magic
==========================================

[We're hiring!](/careers)

![Modal and Dal](/_app/immutable/assets/mo-and-dal.DfM9hPhr.svg)

We started Modal™ with the goal to make it easier to iterate and
ship applications for data, AI, and machine learning. In order to
deliver the developer experience we wanted, we went deep and built our
own infrastructure — including our own custom file system, container
runtime, scheduler, container image builder, and much more.

Customers use Modal for a wide range of use cases, including Generative
AI inference, LLM fine-tuning, computational biotech, and media
processing. We're focused on developer experience at our core, letting
companies ship value faster without having to think about
infrastructure. In a few lines of code, we let you scale from zero to
thousands of CPUs or GPUs. And since pricing is entirely usage-based,
you only pay for the time your code is running.

Our team is based out of New York, Stockholm, and San Francisco. It
includes creators of popular open-source projects (e.g., Seaborn and
Luigi), academic researchers, international olympiad medalists, and
experienced engineering and product leaders with decades of experience.

![Modal Cloud](/_app/immutable/assets/mo-cloud.CSomHsbv.svg)

Join Us
-------

[View positions](/careers)

![](https://modal-cdn.com/tmpg_7uakv__8b53116f.webp)

![](https://modal-cdn.com/tmp_jgozw97_4e0060f5.webp)

![](https://modal-cdn.com/tmpcazacnyg_6050e202.webp)

![](https://modal-cdn.com/tmpwpueji4x_ba1d4fce.webp)

![](https://modal-cdn.com/tmpgh7p7kxo_7c6c49aa.webp)

![](https://modal-cdn.com/tmp37n7ilj0_6f099598.webp)

![](https://modal-cdn.com/tmpgnjpxsyc_868bfeb4.webp)

![](https://modal-cdn.com/tmpm0zcwb6__de366f5a.webp)

![](https://modal-cdn.com/tmp5n1xv4ax_7f41a628.webp)

![](https://modal-cdn.com/tmpjpuugq7s_1fdbd81b.webp)

### Investors

We've raised over $23M from some of the best investors and product
leaders in the industry. Our investors include
[Redpoint Ventures](https://www.redpoint.com/)

,
[Amplify Partners](https://www.amplifypartners.com/)

,
[Lux Capital](https://www.luxcapital.com/)

,
[Definition Capital](https://www.definitioncap.com/)

,
[Essence](https://www.essencevc.fund/)

,
[Creandum](https://creandum.com/)

, and more.

Barry McCardel

Co-Founder & CEO, Hex

Elad Gil

Entrepreneur & Investor

Barr Moses

CEO, Monte Carlo Data

Tristan Handy

Founder & CEO, dbt

Neha Narkhede

Co-Founder & CTO, Confluent

Jordan Tigani

Founder & CEO, Motherduck

Arjun Narayan

Co-Founder, Materialize

Benn Stancil

Founder & CTO, Mode

### Visit Us

New York City

584 Broadway, Floor 10

New York, NY 10012

![Manhattan](https://modal-cdn.com/manhattan-map.webp)

Stockholm

Wallingatan 24, Floor 3

111 24 Stockholm, Sweden

![Stockholm](https://modal-cdn.com/stockholm-map.webp)

San Francisco

945 Market St, Floor 5

San Francisco, CA 94103

![San Francisco](https://modal-cdn.com/san-francisco-map.webp)

Email

support@modal.com

Media inquiries

press@modal.com

Brand Guidelines

[Visit](/brand)

### Legal Notice

© 2025 Modal Labs, Inc. All rights reserved.

All content on the Modal website—including text, graphics, logos,
images, and software—is the property of Modal Labs, Inc. or its
licensors and is protected by copyright, trademark, and other
intellectual property laws.

Modal and the Modal blocks logo are trademarks of Modal Labs, Inc.

Unauthorized use, reproduction, or distribution of any content is
prohibited without prior written permission.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/customers
================================================================================

every size
==============================

[![

](https://modal-cdn.com/tmpb8nlc923_111ab184.webp)](https://modal-cdn.com/customers_page_cubes.mp4)

Modal is built for the fastest-growing teams in the world. We help
companies from startups to enterprises bring cutting-edge AI applications
to the market.

[Get Started](/signup)

Book a Demo

[![

](https://modal-cdn.com/tmpb8nlc923_111ab184.webp)](https://modal-cdn.com/customers_page_cubes.mp4)

![customer logo](/_app/immutable/assets/LovableTestimonial.C1kHoavl.svg)

“We've previously managed to break services like GitHub because of our load, so when Modal was able to handle the massive scale of our AI weekend event so smoothly, that meant a lot.”

[Full Case Study](/blog/lovable-case-study)

![customer logo](data:image/svg+xml,%3csvg%20width='117'%20height='32'%20viewBox='0%200%20117%2032'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M14.0939%200C21.4296%200%2028.361%205.71841%2028.361%2014.0361C28.361%2018.7148%2026.1661%2022.5271%2023.0469%2025.0686C23.9711%2026.5704%2025.1264%2027.6101%2026.5126%2027.6101C28.13%2027.6101%2028.8231%2026.3393%2028.9386%2025.2996H31.018C31.1336%2026.6859%2030.4404%2032%2024.6643%2032C21.083%2032%2019.2347%2029.9206%2017.8484%2027.6101C16.6931%2027.8412%2015.4224%2028.0722%2014.1516%2028.0722C7.04693%2028.0722%200%2022.3538%200%2014.0361C0%205.71841%207.04693%200%2014.0939%200ZM65.9061%206.29603C71.7978%206.29603%2076.5343%2010.4549%2076.5921%2016.6354C76.5921%2023.1625%2071.7978%2027.4368%2065.9061%2027.4368C60.1877%2027.4368%2055.2202%2023.1047%2055.2202%2016.6354C55.2202%2010.3394%2060.13%206.29603%2065.9061%206.29603ZM104.78%206.29603C109.978%206.29603%20113.213%207.68231%20113.213%2012.8809V21.7762C113.213%2023.1625%20113.675%2023.8556%20114.83%2023.8556C115.408%2023.8556%20115.87%2023.6245%20116.101%2023.509L116.621%2024.5487C116.159%2025.3574%20114.657%2026.8592%20111.884%2026.8592C109.458%2026.8592%20107.957%2025.704%20107.726%2023.8556H107.61C106.455%2025.935%20104.375%2027.3213%20101.372%2027.3213C97.7906%2027.3213%2095.5957%2025.4729%2095.5957%2022.1227C95.5957%2015.5379%20104.78%2017.3863%20107.437%2012.9964V11.9567C107.437%208.83754%20106.166%208.1444%20104.78%208.1444C100.621%208.1444%20102.469%2012.9964%2098.7726%2012.9964C96.9242%2012.9964%2096.231%2011.9567%2096.231%2010.6859C96.231%208.25993%2099.2346%206.29603%20104.78%206.29603ZM39.0469%206.75812V20.2744C39.0469%2022.8159%2040.3177%2023.9711%2042.1661%2023.9711C43.6679%2023.9711%2045.2852%2023.278%2046.0939%2021.6606V10.4549C46.0939%209.29964%2045.7473%208.83755%2044.4765%208.83755H43.0902V6.75812H51.87V21.6029C51.87%2022.9892%2052.3321%2023.6823%2053.9495%2023.6823H54.1805V25.8773L46.3249%2027.148V24.2022H46.2094C44.7076%2026.1083%2042.5126%2027.2635%2039.6245%2027.2635C36.0433%2027.2635%2033.3863%2025.4152%2033.3863%2020.4477V10.4549C33.3863%209.29964%2032.9242%208.83755%2031.6534%208.83755H30.3827V6.75812H39.0469ZM91.2058%206.41155C93.0541%206.41155%2094.5559%207.45126%2094.5559%209.53069C94.5559%2011.0325%2093.8628%2012.5343%2091.7834%2012.5343C90.0505%2012.5343%2089.704%2010.917%2088.2022%2010.917C86.9314%2010.917%2085.8917%2012.1877%2085.8917%2014.0361V22.2383C85.8917%2024.0866%2086.3538%2024.6643%2088.4332%2024.6643H89.5884V26.8592H77.1119V24.722H77.9206C80%2024.722%2080.231%2024.1444%2080.231%2022.296V10.4549C80.231%209.29964%2079.6534%208.83755%2078.3827%208.83755H77.2274V6.75812H85.1985L85.5451%2010.917H85.7762C86.5848%207.91336%2089.0108%206.41155%2091.2058%206.41155ZM14.1516%202.25271C8.83754%202.25271%206.52708%206.23827%206.52708%2013.9783C6.52708%2021.7184%208.83754%2025.704%2014.1516%2025.704C15.1336%2025.704%2016%2025.4729%2016.6931%2025.2419C15.6534%2022.8159%2014.2671%2020.5054%2011.6101%2020.5054C11.148%2020.5054%2010.6859%2020.6209%2010.2238%2020.852L9.41516%2019.2347C10.5704%2018.2527%2012.13%2017.5018%2014.2671%2017.5018C17.6173%2017.5018%2019.3502%2019.1191%2020.7365%2021.1986C21.5451%2019.4657%2021.8917%2017.0397%2021.8917%2013.9783C21.8917%206.23827%2019.5812%202.25271%2014.1516%202.25271ZM65.9061%208.25993C63.1336%208.25993%2061.5162%2011.0325%2061.5162%2016.5776C61.5162%2022.2383%2063.1336%2025.1264%2065.9061%2025.1264C68.9097%2025.1264%2070.065%2022.2383%2070.1805%2016.5776C70.296%2011.0903%2068.9097%208.25993%2065.9061%208.25993ZM107.379%2015.4224C105.531%2017.444%20101.256%2017.7328%20101.256%2021.4296C101.256%2023.278%20102.412%2024.3177%20103.913%2024.3177C106.455%2024.3177%20107.379%2022.1227%20107.379%2019.6967V15.4224Z'%20fill='%23B92B27'/%3e%3c/svg%3e)

“There would be a lot of edge cases and unknowns if we built code sandboxes ourselves. We offloaded this to Modal and are actively saving 2 engineers' worth of ongoing engineering time.”

[Full Case Study](/blog/quora-case-study)

“Modal makes it easy to write code that runs on 100s of GPUs in parallel, transcribing podcasts in a fraction of the time.”

[Full Case Study](/blog/substack-case-study)

![customer logo](data:image/svg+xml,%3csvg%20width='90'%20height='32'%20viewBox='0%200%2090%2032'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M12.31%2021.2409V26.454C5.51112%2026.454%200%2022.927%200%2018.5933C0%2017.6789%200.245791%2016.8034%200.696406%2015.9874C2.38258%2019.044%206.9449%2021.2402%2012.31%2021.2402V21.2409ZM12.31%200.250244V5.54663C6.96406%205.54663%202.41496%207.71051%200.714907%2010.7432C0.252398%209.91403%200%209.02271%200%208.09703C0%203.76267%205.51112%200.250244%2012.31%200.250244ZM3.14572%2013.361C2.07732%2014.1182%201.23886%2015.0082%200.696406%2015.9887C0.245791%2015.1721%200%2014.2946%200%2013.3809C0%2012.4671%200.252397%2011.5665%200.713584%2010.7432C1.2567%2011.7158%202.08922%2012.6045%203.14572%2013.361ZM3.14572%2013.3617C5.40012%2011.7641%208.67072%2010.7611%2012.31%2010.7611V16.0013C8.66939%2016.0013%205.40012%2014.9778%203.14572%2013.361V13.3617Z'%20fill='%232226FD'/%3e%3cpath%20d='M12.3096%2010.759V5.5459C19.1084%205.5459%2024.6196%209.07286%2024.6196%2013.4066C24.6196%2014.321%2024.3738%2015.1965%2023.9232%2016.0125C22.237%2012.9559%2017.6747%2010.7597%2012.3096%2010.7597V10.759ZM12.3096%2031.7497V26.4533C17.6555%2026.4533%2022.2046%2024.2894%2023.9047%2021.2567C24.3672%2022.0859%2024.6196%2022.9772%2024.6196%2023.9029C24.6196%2028.2372%2019.1084%2031.7497%2012.3096%2031.7497ZM21.4738%2018.6389C22.5422%2017.8817%2023.3807%2016.9917%2023.9232%2016.0111C24.3738%2016.8278%2024.6196%2017.7053%2024.6196%2018.619C24.6196%2019.5328%2024.3672%2020.4334%2023.906%2021.2567C23.3629%2020.2841%2022.5304%2019.3954%2021.4738%2018.6389ZM21.4738%2018.6382C19.2195%2020.2358%2015.9489%2021.2388%2012.3096%2021.2388V15.9986C15.9502%2015.9986%2019.2195%2017.0221%2021.4738%2018.6389V18.6382Z'%20fill='%232226FD'/%3e%3cpath%20d='M43.5081%2024.322C39.9431%2024.322%2038.1721%2022.482%2038.1031%2020.09H40.3801C40.4721%2021.447%2041.3001%2022.505%2043.4851%2022.505C45.4631%2022.505%2045.9921%2021.631%2045.9921%2020.78C45.9921%2019.308%2044.4281%2019.147%2042.9101%2018.825C40.8631%2018.342%2038.5171%2017.744%2038.5171%2015.306C38.5171%2013.282%2040.1501%2011.925%2042.9791%2011.925C46.1991%2011.925%2047.7401%2013.65%2047.9011%2015.674H45.6241C45.4631%2014.777%2044.9801%2013.742%2043.0251%2013.742C41.5071%2013.742%2040.8631%2014.34%2040.8631%2015.214C40.8631%2016.433%2042.1741%2016.548%2043.8301%2016.916C45.9921%2017.422%2048.3381%2018.043%2048.3381%2020.665C48.3381%2022.942%2046.5901%2024.322%2043.5081%2024.322ZM51.3416%2028.117C50.8356%2028.117%2050.4906%2028.071%2049.8926%2027.956V26.116C50.2836%2026.162%2050.4906%2026.185%2050.8586%2026.185C51.7556%2026.185%2052.8136%2025.725%2053.2736%2023.885L48.5356%2012.27H50.9966L54.4236%2021.24H54.4696L57.6896%2012.27H60.0586L55.4126%2024.276C54.2856%2027.174%2053.1586%2028.117%2051.3416%2028.117ZM67.5999%2011.925C69.7619%2011.925%2071.5099%2013.167%2071.5099%2015.858V24H69.2559V16.479C69.2559%2014.915%2068.5659%2013.857%2066.8639%2013.857C64.9319%2013.857%2063.6899%2015.03%2063.6899%2016.801V24H61.4359V12.27H63.6899V13.742H63.7359C64.3569%2012.845%2065.5759%2011.925%2067.5999%2011.925ZM79.0986%2024.345C75.6026%2024.345%2073.3256%2021.815%2073.3256%2018.135C73.3256%2014.455%2075.6026%2011.925%2079.0756%2011.925C82.0886%2011.925%2084.0896%2013.88%2084.3656%2016.525H82.0426C81.9276%2015.306%2081.2376%2013.788%2079.0986%2013.788C76.5916%2013.788%2075.6716%2015.927%2075.6716%2018.135C75.6716%2020.343%2076.5916%2022.459%2079.0986%2022.459C81.2606%2022.459%2081.9276%2020.987%2082.0426%2019.653H84.3656C84.2046%2022.344%2082.1116%2024.345%2079.0986%2024.345ZM88.4424%2024H85.9354V21.493H88.4424V24Z'%20fill='white'/%3e%3c/svg%3e)

“Leveraging Modal allowed us to focus on what we do best - training state of the art video generation models - without worrying about MLops and infrastructure.”

[Full Case Study](/blog/sync-labs-case-study)

“Tasks that would have taken days to complete take minutes instead. We’ve also saved thousands of dollars deploying open-source LLMs on Modal.”

[Full Case Study](/blog/ramp-case-study)

“The beauty of Modal is that all you need to know is that you can scale your function calls in the cloud with a few lines of Python.”

[Full Case Study](/blog/suno-case-study)

Customers use Modal for

[Language Models](/use-cases/language-models)
[Image, Video, 3D](/use-cases/image-video-3d)
[Audio Processing](/use-cases/audio)
[Fine-Tuning](/use-cases/fine-tuning)
[Batch Processing](/use-cases/job-queues)
[Sandboxed Code](/use-cases/sandboxes)
[Computational Bio](/use-cases/comp-bio)

[Language Models](/use-cases/language-models)
[Image, Video, 3D](/use-cases/image-video-3d)
[Audio Processing](/use-cases/audio)
[Fine-Tuning](/use-cases/fine-tuning)
[Batch Processing](/use-cases/job-queues)
[Sandboxed Code](/use-cases/sandboxes)
[Computational Bio](/use-cases/comp-bio)

[![customer logo](data:image/svg+xml,%3csvg%20width='100'%20height='30'%20viewBox='0%200%20100%2030'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cg%20clip-path='url(%23clip0_56_103)'%3e%3cpath%20d='M16.8833%2021.1123L14.1079%2022.9956V23.1278H22.8965V22.9956L20.1211%2021.1123V2.01542L22.8965%200.132159V0H14.1079V0.132159L16.8833%202.01542V10.4075H6.01322V2.01542L8.78855%200.132159V0H0V0.132159L2.77533%202.01542V21.1123L0%2022.9956V23.1278H8.78855V22.9956L6.01322%2021.1123V11.8612H16.8833V21.1123ZM29.141%2023.4582C30.8921%2023.4582%2032.6432%2022.4339%2033.8656%2021.0132V23.1278H39.4163V22.9956L36.9053%2021.0463V11.5969C36.9053%208.39207%2034.6586%206.80617%2031.0573%206.80617C29.174%206.80617%2026.5969%207.43392%2024.9449%208.09471V12.1916H25.0771C27.0595%209.48238%2028.9427%208.12775%2030.826%208.12775C32.7093%208.12775%2033.8656%209.44934%2033.8656%2011.9273V13.5793L29.7687%2014.5374C26.0683%2015.3634%2024.4163%2016.8172%2024.4163%2019.0969C24.4163%2021.6079%2026.4648%2023.4582%2029.141%2023.4582ZM30.1982%2021.3436C28.7115%2021.3436%2027.6211%2020.2533%2027.6211%2018.7665C27.6211%2017.2797%2028.6123%2016.0903%2030.5617%2015.6608L33.8656%2014.9009V19.8568C32.478%2020.848%2031.2555%2021.3436%2030.1982%2021.3436ZM52.4339%2010.2423L59.0419%2023.5903H59.6366L66.1123%209.28414L68.0947%207.3348V7.20264H62.0485V7.30176L64.5595%209.11894L60.0991%2018.9317L55.2093%209.11894L57.6542%207.30176V7.20264H52.5C49.3612%207.20264%2047.2797%208.16079%2045.6278%2010.0441V6.97137L40.0771%208.25991V8.42511L42.5881%209.87885V21.0463L40.0771%2022.9956V23.1278H48.7996V22.9956L45.6278%2021.0463V11.5969C46.7841%2010.3084%2048.1718%209.71366%2049.6256%209.71366C50.5507%209.71366%2051.4097%209.84581%2052.4339%2010.2423ZM75.2643%2023.4582C78.304%2023.4582%2079.9559%2021.5749%2081.674%2019.7247L80.848%2018.9648C79.163%2020.5507%2077.8745%2021.2445%2076.1894%2021.2445C72.9515%2021.2445%2070.5396%2018.2709%2070.5396%2014.1079H81.5749V13.6123C81.5749%209.71366%2079.2621%206.80617%2075.4626%206.80617C71.2335%206.80617%2067.7643%2010.6057%2067.7643%2015.2974C67.7643%2020.022%2071.0683%2023.4582%2075.2643%2023.4582ZM78.2709%2012.6872H70.6388C71.0352%209.81278%2072.8524%208.16079%2074.9339%208.16079C77.0154%208.16079%2078.304%209.64758%2078.304%2012.0264C78.304%2012.3238%2078.304%2012.5551%2078.2709%2012.6872ZM83.9207%209.28414L90.7599%2022.9295L85.804%2030H89.5374L97.8634%209.28414L99.8458%207.3348V7.20264H93.7996V7.30176L96.3436%209.11894L92.2137%2019.13L87.1916%209.11894L89.6366%207.30176V7.20264H81.8062V7.3348L83.9207%209.28414Z'%20fill='white'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_56_103'%3e%3crect%20width='99.8458'%20height='30'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://www.harvey.ai/)

“With the old in-house systems, we'd have to tune number of workers, instance size, parallelization strategy, all this stuff, which was very time-consuming and not directly generating business value. Modal magically handled all that.”

Samarth Goel
,
ML Engineer

[![customer logo](/_app/immutable/assets/Mistral.BuBT5ERP.svg)](https://www.mistral.ai/en)

“Modal Sandboxes enable us to execute generated code securely and flexibly. With Modal's support, we expedited the development of our code interpreter feature and successfully integrated it into our chat platform, Le Chat, to better assist our users.”

Wendy Shang
,
AI Scientist

[![customer logo](/_app/immutable/assets/OpenPipe.-dEo7fts.svg)](https://www.openpipe.ai/)

“Modal is the easiest way to experiment as we develop new fine-tuning techniques. We've been able to validate new features faster and beat competitors because of how quickly we can try new ideas.”

Kyle Corbitt
,
CTO

[![customer logo](/_app/immutable/assets/CodegenTestimonial.Dn_1_LRo.svg)](https://www.codegen.com/)

“Using Modal, Codegen has been able to move at lightning speed with full-stack AI development. The product is designed with developer experience front and center, and my team is incredibly happy having it as part of our arsenal.”

Jay Hack
,
Founder & CEO

[![customer logo](/_app/immutable/assets/CognitionTestimonial.9v75ksMR.svg)](https://www.cognition.ai/)

“Modal has been great for iterating quickly on our data pipelines. It enables us to process a large batch of logs in minutes! The infrastructure is amazing for experimentation.”

Steven Hao
,
Co-founder

[![customer logo](data:image/webp;base64,UklGRgwFAABXRUJQVlA4TP8EAAAvRcEPEO8gFkzmL92dwfzPv0CAxBRAMElTbceAHsHxGNJt28raaiNJaNBogmnSIuv9HxTBjza9+XVHRP8nQP9vfGLxKMmzOLxdRuX+3TNq3r93RpX9O2dUrX/fjKr375pRS/17ZtRyvxNzWvd1HJ1zbplzzh1f1Kg1/T5OrPs6JiAuAwivadS6/pcyTWPtv8motf3v1AP8k4xa379TRm3pd+K163+VUdv6d8morf17ZNT2/k9hrf0Zxlqzq6Zp1mmaZrVRe/Q/4yOEcK3xIYROMiGE7yyEEM6SQgjBVZgQQmiyPoRwlszlEfmcM5dHAni0zc7O9wSQ7m3FRwjhWuNDCF3hdI3k8e7rTPdIAA/frGG0T/sjeiDWBOAhWaoHSQBthQVw2QMY5CaA50yXKMdhT26iMp4LPRBrAvCYMXdq46miS5TjsMJv+LudmZ0Zqb/tp2fhsJWZWOgLI/XXP9Y41dzIp3H8nDJue+mZ/R4/v2cYNroCpGvrzt0jS2bmSh5DuE8Z10WHnTTbdc1Cs0HeZypukz8vrTtKLUB0ytsJwNXYxTUt+c1Ikh0z3DYJ+Daa7QCGzANEp9xPAKcF/rGTNmy22P8e0Wl+AmKjeTsBsWLluSm7qNhnz02OAAcVA/CVTUBsNG8nINZ5dkN4UbHRfAvQqOwA3B5agFGVTwC3xSlrSi2AJA9wVPkEcKrx7IjwmloVP4Gnap9A2MNn1tQcs9sWx8yXbN/3vaQ78FDtAwgVnl0RXlFSOQGXqh5Ie0jAU9XfQNxCCUi+UE5AV9UBqeTZGWETrxV/hWfJkrnaAcAUol1csgBD3Q3AbNGTx3t3MhUW4HKq7QGaOc/uCK8nlFy24rGkxSWXtXVdZrcwX9lsvHszc8pWPMx4fgDhr+F24urazWSuJSAOW7jM8yMIf7J2Z5INUwlis43nhxBelc3OdrHZzmaXuiEzK9yrJB3P/WOa4SHZ7MMuNpL4Mfifc9/JcS2TtVp1I5Pd6j6BpBUeS2aPIeMkk3mt+jJSzbTJpaJdSxNw+wF6AqkuAc81UqFpmsYUJJ910hdwe1kdgCmdWC0Bt4rHaj2QTM1XjHHYQw/galqAtpAqPIUvIFQoAb3UA8nUfMUY+5fxkQ0FM633DSRT6FjNAtwqOgC3B5OA2JTslDWSWoBTwU6lAKQVLMC1wgO4l2Gy1MyYb6rarCncAG5znvU0AgwFl4CoPagHiM3cYQIYJMlmsZmx35Q6gK7kAT4kXQGGwikBUS9DD/LgT/6aINV8ZPE2DE6Sy4iX8/nyANJqZgKI7dHa85282YeZyMezc+2dPGr2QR4+Tv6aqDAJ4NZkpk9AlCQzAUR/aJqPO3nzQg4pK8a+xiTmB0kas/IwrSY7sdBrH7JTVhubudNMMV4L+sggxZiY9ZnsxEKvJV/NcknN8u5nyNfEpq1RV2e+awZtIDtVpbP2IjvWPY2KfU1s+pL6mcpO83aqSh9atOIg6ZBYf1+yj8LNqE7HZ43UT3PRaROpnQppMNqP1D5LT6fa0zSXeqMa2VCRHgdV+qmQeqPtBuWHtKeNre9D3xqtaa21Zk46D7fbxWmH7jKMt4sz2rs9D7fb0BotPXZ96FujxcZ11xCunTNaeOz6cO2cUeV6g+YP6bf4111tUPmQ3jiDag/pbTOo/pDeNIOWHtJbZtDyQ3rDDFrzkN4dYXmndQ9h+em98P/FAA==)](https://www.futurehouse.org/)

“Our org runs on Modal. We use it for AI agent environments, scalable deployment of AI agents, hosting of deep learning models, and visualization. It dramatically simplified our engineering infrastructure and completely changed the scope of projects we can do.”

Andrew White
,
Co-Founder & Head of Science

[![customer logo](data:image/webp;base64,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)](https://contextual.ai/)

“Within a few hours I had CI on GPUs running on Modal. I'm shocked at how easy it was for us to go from manually running tests on GPUs to automating these in parallel across our growing team.”

Stas Bekman
,
ML Engineer

[![customer logo](/_app/immutable/assets/Martian.CrCqf3xu.svg)](https://withmartian.com/)

“Modal is the only platform that supported our custom infrastructure needs and had a simple developer experience. We deploy hundreds of predictive models for our core model routing functionality, and Modal helps us scale our product cleanly.”

Shriyash Upadhyay
,
Founder

[![customer logo](/_app/immutable/assets/StructifyTestimonial.DThxYceb.svg)](https://www.structify.ai/)

“Switched to Modal for our LLM inference instead of Azure. 1/4 the price for GPUs and so much simpler to set up/scale. Big fan.”

Alex Reichenbach
,
CEO

[![customer logo](/_app/immutable/assets/Flora.DAkAvBAJ.svg)](https://www.florafauna.ai/)

“We are constantly shipping the most cutting-edge creative AI machine learning techniques so our customers have access to the best creative models. Modal's has helped us streamline the process from idea to deployed pipeline, allowing us to both deploy quickly & scale rapidly.”

Weber Wong
,
Founder

[![customer logo](/_app/immutable/assets/Succinct.qq-4sTpt.svg)](https://succinct.xyz/)

“Modal made it incredibly easy for us to deploy complex computational jobs that burst up to hundreds of machines. Being able to iterate quickly without having to waste cycles on managing infra was a huge unlock.”

Uma Roy
,
Co-Founder and CEO

[![customer logo](/_app/immutable/assets/ProfoundTestimonial.tllV14QA.svg)](https://www.tryprofound.com)

“Our platform leverages Modal's infrastructure for the heavy lifting—handling ridiculous scale and concurrency behind the scenes. This lets us focus on what we do best: gathering and analyzing unstructured textual data with precision.”

Charles Zhou
,
Founding Engineer

[![customer logo](/_app/immutable/assets/CanOfSoup.Crz1Gd3y.svg)](https://www.canofsoup.com/)

“We fine-tune image models on Modal because we can experiment with new ideas quickly. We chose to deploy on Modal too because it's far more stable than any alternative solutions we found.”

Eric Meier
,
Co-founder

[![customer logo](/_app/immutable/assets/ChaiDiscovery.BHpR4uIE.webp)](https://www.chaidiscovery.com/)

“We used Modal to build an inference server for our model, Chai-1, which allows people to predict molecular structures via a web app. Modal allowed us to build and launch the server in days: our engineers didn't have to worry about maintaining infrastructure, delivering the product in record time.”

Jack Dent
,
Co-Founder

[![customer logo](data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='114'%20height='24'%20viewBox='0%200%20114%2024'%20fill='none'%3e%3cg%20clip-path='url(%23clip0_2126_15312)'%3e%3cpath%20d='M104.239%2019.4985H103.058C97.0158%2019.4985%2093.8574%2017.7683%2093.8574%2012.6051V11.3143C93.8574%206.12375%2097.0158%204.39355%20103.058%204.39355H104.239C110.033%204.39355%20112.78%206.23359%20113.329%209.39189H109.402C108.55%207.99126%20107.095%207.46946%20103.827%207.46946C99.7621%207.46946%2097.6749%208.10111%2097.6749%2011.534V12.3579C97.6749%2015.7909%2099.7621%2016.4225%20103.827%2016.4225C107.095%2016.4225%20108.55%2015.9007%20109.402%2014.5001H113.329C112.78%2017.6584%20110.033%2019.4985%20104.239%2019.4985Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M90.9522%200V2.71888H87.1074V0H90.9522ZM90.9522%204.66878V19.2243H87.1074V4.66878H90.9522Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M83.9533%2019.2238H80.1358V10.9848C80.1358%207.93633%2078.1036%207.46946%2075.6044%207.46946C72.3363%207.46946%2070.2765%208.4856%2070.2765%2011.6714V19.2238H66.4316V4.66819H70.2765V7.3596H70.4413C71.2927%205.68433%2073.1327%204.39355%2076.8128%204.39355C80.8225%204.39355%2083.9533%205.51956%2083.9533%2010.3531V19.2238Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M54.286%2019.4985H52.5833C46.5964%2019.4985%2043.3281%2017.7683%2043.3281%2012.6051V11.3143C43.3281%206.12375%2046.5964%204.39355%2052.5833%204.39355H54.286C60.3005%204.39355%2063.5687%206.12375%2063.5687%2011.3143V12.6051C63.5687%2017.7683%2060.3005%2019.4985%2054.286%2019.4985ZM53.4622%2016.4225C57.5542%2016.4225%2059.7512%2015.7909%2059.7512%2012.3579V11.534C59.7512%208.10111%2057.5542%207.46946%2053.4622%207.46946C49.3151%207.46946%2047.1456%208.10111%2047.1456%2011.534V12.3579C47.1456%2015.7909%2049.3151%2016.4225%2053.4622%2016.4225Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M40.6408%2019.2243H36.8235V10.9854C36.8235%207.93692%2034.7911%207.47005%2032.292%207.47005C29.0238%207.47005%2026.964%208.48618%2026.964%2011.6719V19.2243H23.1191V0H26.964V7.36019H27.1288C27.9802%205.68492%2029.8202%204.39415%2033.5003%204.39415C37.51%204.39415%2040.6408%205.52014%2040.6408%2010.3537V19.2243Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M0%204.66819H3.8174V7.3596H3.92726C4.86101%205.32731%207.05809%204.39355%2011.0678%204.39355H11.8367C17.0823%204.39355%2020.2404%206.12375%2020.2404%2011.2869V12.5776C20.2404%2017.7683%2017.0823%2019.4985%2011.8367%2019.4985H11.0678C7.05809%2019.4985%204.86101%2018.5647%203.92726%2016.5324H3.8174V23.8925H0V4.66819ZM10.1065%207.46946C6.01448%207.46946%203.8174%208.10111%203.8174%2011.4517V12.3579C3.8174%2015.7909%206.01448%2016.4225%2010.1065%2016.4225C14.2535%2016.4225%2016.4231%2015.7909%2016.4231%2012.3579V11.534C16.4231%208.10111%2014.2535%207.46946%2010.1065%207.46946Z'%20fill='%23FFFBFB'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_2126_15312'%3e%3crect%20width='113.408'%20height='24'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://phonic.co/)

“At Phonic, we train our own proprietary models for audio generation. We moved all our large-scale audio processing batch jobs to Modal. Our engineers are ecstatic with the result – we can run at a much larger scale than before, no longer have to babysit our batch jobs, and we can ship much faster.”

Moin Nadeem
,
Co-Founder

[![customer logo](data:image/svg+xml,%3csvg%20xmlns:xlink='http://www.w3.org/1999/xlink'%20xmlns='http://www.w3.org/2000/svg'%20width='96'%20height='21'%20viewBox='0%200%2096%2021'%20fill='white'%3e%3cpath%20fill-rule='evenodd'%20clip-rule='evenodd'%20d='M10.2121%206.22133H12.5688V10.8873L9.42658%2013.3759V14.7701C9.42658%2015.2025%209.78038%2015.5533%2010.2178%2015.5533H18.8532V20.2193H14.1399L12.3061%2017.7986C12.1691%2017.6176%2011.9636%2017.4994%2011.7367%2017.4708C11.5098%2017.4421%2011.2811%2017.5062%2011.1026%2017.6474L7.85548%2020.2193H1.44101C1.05892%2020.2193%200.691908%2020.0688%200.42168%2019.8013C0.151451%2019.5337%200%2019.171%200%2018.7928V15.5533L7.06993%209.33199V3.11066H10.2121V6.22133ZM10.2121%203.11066V0H13.3543V3.11066H10.2121Z'%20fill='white'/%3e%3cpath%20d='M27.6919%206.01123H32.3103C36.1622%206.01123%2039.5229%208.39931%2039.5229%2012.9576C39.5229%2017.5159%2036.4179%2019.9237%2032.389%2019.9237H27.6919V6.01123ZM32.0552%2018.2461C35.2192%2018.2461%2037.4795%2016.5098%2037.4795%2013.0169C37.4795%209.52392%2035.1995%207.68887%2032.1142%207.68887H29.6184V18.2467H32.0552V18.2461Z'%20fill='white'/%3e%3cpath%20d='M40.6162%2015.0291C40.6162%2012.049%2042.5028%209.85908%2045.392%209.85908C47.2983%209.85908%2048.4386%2010.767%2048.9101%2011.438V10.1553H50.6983V19.9237H48.9101V18.5028C48.6937%2018.8578%2047.5737%2020.2194%2045.4117%2020.2194C42.5225%2020.2194%2040.6162%2018.0881%2040.6162%2015.0297V15.0291ZM45.7264%2018.68C47.6523%2018.68%2048.9888%2017.2591%2048.9888%2015.0291C48.9888%2012.799%2047.6523%2011.3979%2045.7264%2011.3979C43.8005%2011.3979%2042.5034%2012.8187%2042.5034%2015.0291C42.5034%2017.2394%2043.7611%2018.68%2045.7264%2018.68Z'%20fill='white'/%3e%3cpath%20d='M57.9823%2015.0291C57.9823%2012.049%2059.8689%209.85908%2062.7581%209.85908C64.6644%209.85908%2065.8047%2010.767%2066.2762%2011.438V10.1553H68.0644V19.9237H66.2762V18.5028C66.0598%2018.8578%2064.9398%2020.2194%2062.7778%2020.2194C59.8886%2020.2194%2057.9823%2018.0881%2057.9823%2015.0297V15.0291ZM63.0925%2018.68C65.0185%2018.68%2066.3549%2017.2591%2066.3549%2015.0291C66.3549%2012.799%2065.0185%2011.3979%2063.0925%2011.3979C61.1666%2011.3979%2059.8695%2012.8187%2059.8695%2015.0291C59.8695%2017.2394%2061.1272%2018.68%2063.0925%2018.68Z'%20fill='white'/%3e%3cpath%20d='M73.7232%2015.0291C73.7232%2012.049%2075.6098%209.85908%2078.499%209.85908C80.4052%209.85908%2081.5455%2010.767%2082.017%2011.438V10.1553H83.8053V19.9237H82.017V18.5028C81.8007%2018.8578%2080.6806%2020.2194%2078.5186%2020.2194C75.6294%2020.2194%2073.7232%2018.0881%2073.7232%2015.0297V15.0291ZM78.8328%2018.68C80.7587%2018.68%2082.0951%2017.2591%2082.0951%2015.0291C82.0951%2012.799%2080.7587%2011.3979%2078.8328%2011.3979C76.9068%2011.3979%2075.6098%2012.8187%2075.6098%2015.0291C75.6098%2017.2394%2076.8675%2018.68%2078.8328%2018.68Z'%20fill='white'/%3e%3cpath%20d='M87.7064%2019.9237H85.9181V6.01123H87.7654V11.3985C88.2369%2010.767%2089.2395%209.85909%2091.1654%209.85909C94.0743%209.85909%2096.0002%2012.0497%2096.0002%2015.0291C96.0002%2018.0085%2094.0743%2020.2188%2091.1851%2020.2188C88.9838%2020.2188%2087.9227%2018.8572%2087.7064%2018.5023V19.9237ZM90.89%2018.6806C92.8553%2018.6806%2094.113%2017.24%2094.113%2015.0297C94.113%2012.8193%2092.8553%2011.3985%2090.89%2011.3985C88.9248%2011.3985%2087.6277%2012.8391%2087.6277%2015.0297C87.6277%2017.2202%2088.9641%2018.6806%2090.89%2018.6806Z'%20fill='white'/%3e%3cpath%20d='M55.2542%2011.5756H57.5735V10.1547H55.2542V7.68826H53.4266V10.1547H51.658V11.5756H53.4266V19.9231H57.3434V18.3442H55.2542V11.5756Z'%20fill='white'/%3e%3cpath%20d='M71.9759%2018.4041V6.01123H70.1483V19.9237H73.2139V18.4041H71.9759Z'%20fill='white'/%3e%3c/svg%3e)](https://www.datalab.to)

“Using Modal for inference is like having an extra infra team - it’s reliable, scalable, and fast - meaning I can get back to training models””

Vik Paruchari
,
Founder

[![customer logo](/_app/immutable/assets/Basis.DNhnLXGd.svg)](https://www.getbasis.ai/)

“We use Modal to securely run LLM-augmented code on a large scale. Modal’s powerful primitives like sandboxes and file systems have allowed us to focus on our core competencies without having to waste time on our own infra.”

Matt Harpe
,
Co-Founder

[![customer logo](/_app/immutable/assets/AchiraTestimonial.Czp2Ec1u.svg)](https://www.achira.ai/)

“Processing external quantum mechanical datasets comes with unique challenges. Jobs can fail in numerous ways. Modal's retry mechanism and batching primitives have made our data pipeline much more robust.”

Liz Decolvenaere
,
Quantum Chemical Engineer

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/contributors
================================================================================

Contributors
============

This list is incomplete; you can help by

[expanding it](https://github.com/modal-labs/gpu-glossary)

.

### Authors

* [Charles Frye](https://twitter.com/charles_irl)

  wrote the majority of the
  material and takes full responsibility for any errors.

* [Matthew Nappo](https://www.linkedin.com/in/mattnappo/)

  wrote the initial
  internal "GPU Glossary" document from which this sprung.

* [You](mailto:glossary@modal.com?subject=Contributing%20to%20GPU%20glossary)

  can contribute to keep the glossary up-to-date and erratum-free!

### Design

* [Sona Dolasia](https://twitter.com/teenychairs)

  designed the glossary.

* [Anna Carey](https://twitter.com/anna_carey)

  implemented the design and UX.

### Review

* [Abhinav Upadhyay](https://twitter.com/abhi9u)

  of

  [Coding Confessions](https://blog.codingconfessions.com/)

  and

  [`@Pauleonix`](https://github.com/pauleonix)

  of the

  [GPU MODE Discord](https://discord.gg/gpumode)

  , from outside Modal, provided
  valuable external technical review of the glossary. We particularly thank
  Abhinav for his perspective on comparisons with CPUs and Pauleonix for his
  detailed insights on GPU hardware internals.

* [Akshat Bubna](https://twitter.com/akshat_b)

  ,

  [Nathan Wang](https://www.linkedin.com/in/nathan-r-wang/)

  , and

  [Colin Weld](https://www.linkedin.com/in/colin-weld/)

  gave technical feedback
  on early drafts of the glossary.

* [Eric Zhang](https://twitter.com/ekzhang1)

  and

  [Ro Arepally](https://twitter.com/rarepally)

  reviewed the design and
  implementation.

### Acknowledgements

* [Mark Saroufim](https://twitter.com/marksaroufim)

  and Andreas Kopf for
  bringing together the

  [GPU MODE Discord community](https://discord.gg/gpumode)

* [Fabien Sanglard](https://twitter.com/fabynou)

  for authoring an

  [excellent history of CUDA GPUs](https://fabiensanglard.net/cuda)

* Jen-Hsun Huang for leading an organization that makes some pretty decent chips

### Error Correction

We thank the following GPU enthusiasts who came in through the world wide web to
correct errors:

* [Alex Zhang](https://alexzhang13.github.io/blog/2024/efficient-dl/)

* [Erik Schultheis](https://www.linkedin.com/in/erik-schultheis-606a52119/)

* Ismail Zaidi

* [Michal Nawrot](https://github.com/michalnawrot)

* [Nicolas Blin](https://www.nicolas-blin.fr/)

[CUDA Binary Utilities](/gpu-glossary/host-software/cuda-binary-utilities)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware
================================================================================

Device Hardware
===============

These terms and technologies are physical components of the GPU — the "device"
in NVIDIA's lingo.

[CUDA (Device Architecture)](/gpu-glossary/device-hardware/cuda-device-architecture)

[Streaming Multiprocessor

SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

[Core](/gpu-glossary/device-hardware/core)

[Special Function Unit

SFU](/gpu-glossary/device-hardware/special-function-unit)

[Load/Store Unit

LSU](/gpu-glossary/device-hardware/load-store-unit)

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

[CUDA Core](/gpu-glossary/device-hardware/cuda-core)

[Tensor Core](/gpu-glossary/device-hardware/tensor-core)

[Streaming Multiprocessor Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

[Texture Processing Cluster

TPC](/gpu-glossary/device-hardware/texture-processing-cluster)

[Graphics/GPU Processing Cluster

GPC](/gpu-glossary/device-hardware/graphics-processing-cluster)

[Register File](/gpu-glossary/device-hardware/register-file)

[L1 Data Cache](/gpu-glossary/device-hardware/l1-data-cache)

[GPU RAM](/gpu-glossary/device-hardware/gpu-ram)

[README](/gpu-glossary/readme)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA (Device Architecture)](/gpu-glossary/device-hardware/cuda-device-architecture)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/core
================================================================================

What is a GPU Core?
===================

The cores are the primary compute units that make up the

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)

.

Examples of GPU core types include

[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

and

[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

.

Though GPU cores are comparable to CPU cores in that they are the component that
effects actual computations, this analogy can be quite misleading. Instead, it
is perhaps more helpful to take the viewpoint of the

[quantitative computer architect](https://archive.org/details/computerarchitectureaquantitativeapproach6thedition)

and think of them as "pipes" into which data goes in and out of which
transformed data is returned. These pipes are associated in turn with specific

[instructions](/gpu-glossary/device-software/streaming-assembler)

from the
hardware's perspective and with different fundamental affordances of throughput
from the programmers' (e.g. floating point matrix multiplication arithmetic
throughput in the case of the

[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

).

The

[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor)

are closer to
being the equivalent of CPU cores, in that they have

[register memory](/gpu-glossary/device-hardware/register-file)

to store
information, cores to transform it, and an

[instruction scheduler](/gpu-glossary/device-hardware/warp-scheduler)

to specify
and command transformations.

[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Special Function Unit](/gpu-glossary/device-hardware/special-function-unit)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/cuda-core
================================================================================

What is a CUDA Core?
====================

The CUDA Cores are GPU

[cores](/gpu-glossary/device-hardware/core)

that execute
scalar arithmetic instructions.

They are to be contrasted with the

[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

, which execute matrix
operations.

Unlike CPU cores, instructions issued to CUDA Cores are not generally
independently scheduled. Instead, groups of cores are issued the same
instruction simultaneously by the

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

but apply it to
different

[registers](/gpu-glossary/device-software/registers)

. Commonly, these
groups are of size 32, the size of a

[warp](/gpu-glossary/device-software/warp)

,
but for contemporary GPUs groups can contain as little as one thread, at a cost
to performance.

The term "CUDA Core" is slightly slippery: in different

[Streaming Multiprocessor architectures](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

CUDA Cores can consist of different units -- a different mixture of 32 bit
integer and 32 bit and 64 bit floating point units.

So, for example, the

[H100 whitepaper](https://resources.nvidia.com/en-us-tensor-core)

indicates that
an H100 GPU's

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)

each have 128 "FP32 CUDA Cores", which accurately counts the number of 32 bit
floating point units but is double the number of 32 bit integer or 64 bit
floating point units (as evidenced by the diagram above). For estimating
performance, it's best to look directly at the number of hardware units for a
given operation.

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Tensor Core](/gpu-glossary/device-hardware/tensor-core)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/cuda-device-architecture
================================================================================

What is a CUDA Device Architecture?
===================================

CUDA stands for

*Compute Unified Device Architecture*

. Depending on the context,
"CUDA" can refer to multiple distinct things: a high-level device architecture,
a

[parallel programming model](/gpu-glossary/device-software/cuda-programming-model)

for architectures with that design, or a

[software platform](/gpu-glossary/host-software/cuda-software-platform)

that
extends high-level languages like C to add that programming model.

The vision for CUDA is laid out in the

[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)

white paper. We highly recommend this paper, which is the original source for
many claims, diagrams, and even specific turns of phrase in NVIDIA's
documentation.

Here, we focus on the

*device architecture*

part of CUDA. The core feature of a
"compute unified device architecture" is simplicity, relative to preceding GPU
architectures.

Prior to the GeForce 8800 and the Tesla data center GPUs it spawned, NVIDIA GPUs
were designed with a complex pipeline shader architecture that mapped software
shader stages onto heterogeneous, specialized hardware units. This architecture
was challenging for the software and hardware sides alike: it required software
engineers to map programs onto a fixed pipeline and forced hardware engineers to
guess the load ratios between pipeline steps.

GPU devices with a unified architecture are much simpler: the hardware units are
entirely uniform, each capable of a wide array of computations. These units are
known as

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)

and their main subcomponents are the

[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

and (for recent GPUs)

[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

.

For an accessible introduction to the history and design of CUDA hardware
architectures, see

[this blog post](https://fabiensanglard.net/cuda/)

by Fabien
Sanglard. That blog post cites its (high-quality) sources, like NVIDIA's

[Fermi Compute Architecture white paper](https://fabiensanglard.net/cuda/Fermi_Compute_Architecture_Whitepaper.pdf)

.
The white paper by

[Lindholm et al. in 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)

introducing the Tesla architecture is both well-written and thorough. The

[NVIDIA whitepaper for the Tesla P100](https://images.nvidia.com/content/pdf/tesla/whitepaper/pascal-architecture-whitepaper.pdf)

is less scholarly but documents the introduction of a number of features that
are critical for today's large-scale neural network workloads, like NVLink and

[on-package high-bandwidth memory](/gpu-glossary/device-hardware/gpu-ram)

.

[Device Hardware](/gpu-glossary/device-hardware)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/gpu-ram
================================================================================

What is GPU RAM?
================

The global memory of the GPU is a large (many megabytes to gigabytes) memory
store that is addressable by all of the GPU's

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)

.

It is also known as GPU RAM (random access memory) or video RAM (VRAM). It uses
Dynamic RAM (DRAM) cells, which are slower but smaller than the Static RAM
(SRAM) used in registers and shared memory. For details on DRAM and SRAM, we
recommend Ulrich Drepper's 2007 article

["What Every Programmer Should Know About Memory"](https://people.freebsd.org/~lstewart/articles/cpumemory.pdf)

.

It is generally not on the same die as the

[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor)

, though in the
latest data center-grade GPUs like the H100, it is located on a shared

[interposer](https://en.wikipedia.org/wiki/Interposer)

for decreased latency and
increased bandwidth (aka
"

[high-bandwidth memory](https://en.wikipedia.org/wiki/High_Bandwidth_Memory)

").

RAM is used to implement the

[global memory](/gpu-glossary/device-software/global-memory)

of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

and to store

[register](/gpu-glossary/device-software/registers)

data that
spills from the

[register file](/gpu-glossary/device-hardware/register-file)

.

An H100 can store 80 GiB (687,194,767,360 bits) in its RAM.

[L1 Data Cache](/gpu-glossary/device-hardware/l1-data-cache)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Device Software](/gpu-glossary/device-software)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/graphics-processing-cluster
================================================================================

What is a Graphics/GPU Processing Cluster?
==========================================

GPC

A GPC is a collection of

[Texture Processing Clusters (TPCs)](/gpu-glossary/device-hardware/texture-processing-cluster)

(themselves groups of

[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)

or SMs) plus a raster engine. Apparently, some people use NVIDIA GPUs for
graphics, for which the raster engine is important. Relatedly, the name used to
stand for Graphics Processing Cluster, but is now, e.g. in the

[NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)

,
expanded as "GPU Processing Cluster".

For the latest

[compute capability](/gpu-glossary/device-software/compute-capability)

9.0 GPUs
like H100s, there is an additional layer of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

's
thread hierarchy, a "cluster" of

[thread blocks](/gpu-glossary/device-software/thread-block)

, that are scheduled
onto the same GPC, just as the threads of a

[thread block](/gpu-glossary/device-software/thread-block)

are scheduled onto
the same

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

, and have
their own level of the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

. Elsewhere,
we elide discussion of this feature.

[Texture Processing Cluster](/gpu-glossary/device-hardware/texture-processing-cluster)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Register File](/gpu-glossary/device-hardware/register-file)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/l1-data-cache
================================================================================

What is the L1 Data Cache?
==========================

The L1 data cache is the private memory of the

[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)

(SM).

Each SM partitions that memory among

[groups of threads](/gpu-glossary/device-software/thread-block)

scheduled onto
it.

The L1 data cache is co-located with and nearly as fast as components that
effect computations (e.g. the

[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

).

It is implemented with SRAM, the same basic semiconductor cell used in CPU
caches and registers and in the

[memory subsystem of Groq LPUs](https://groq.com/wp-content/uploads/2023/05/GroqISCAPaper2022_ASoftwareDefinedTensorStreamingMultiprocessorForLargeScaleMachineLearning-1.pdf)

.
The L1 data cache is accessed by the

[Load/Store Units](/gpu-glossary/device-hardware/load-store-unit)

of the

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

.

CPUs also maintain an L1 cache. In CPUs, that cache is fully hardware-managed.
In GPUs that cache is mostly programmer-managed, even in high-level languages
like

[CUDA C](/gpu-glossary/host-software/cuda-c)

.

Each L1 data cache in an each of an H100's SMs can store 256 KiB (2,097,152
bits). Across the 132 SMs in an H100 SXM 5, that's 33 MiB (242,221,056 bits) of
cache space.

[Register File](/gpu-glossary/device-hardware/register-file)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[GPU RAM](/gpu-glossary/device-hardware/gpu-ram)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/load-store-unit
================================================================================

What is a Load/Store Unit?
==========================

LSU

The Load/Store Units (LSUs) dispatch requests to load or store data to the
memory subsystems of the GPU.

Most importantly for

[CUDA programmers](/gpu-glossary/host-software/cuda-software-platform)

they
interact with the

[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)

's
on-chip SRAM

[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache)

and
the off-chip, on-device

[global RAM](/gpu-glossary/device-hardware/gpu-ram)

that
respectively implement the lowest and highest levels of the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

in the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

.

[Special Function Unit](/gpu-glossary/device-hardware/special-function-unit)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/register-file
================================================================================

What is a Register File?
========================

The register file of the

[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)

stores bits in between their manipulation by the

[cores](/gpu-glossary/device-hardware/core)

.

The register file is split into 32 bit registers that can be dynamically
reallocated between different data types, like 32 bit integers, 64 bit floating
point numbers, and (pairs of) 16 bit floating point numbers.

Allocation of registers in a

[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)

to

[threads](/gpu-glossary/device-software/thread)

is therefore generally
managed by a compiler like

[nvcc](/gpu-glossary/host-software/nvcc)

, which
optimizes register usage by

[thread blocks](/gpu-glossary/device-software/thread-block)

.

[Graphics/GPU Processing Cluster](/gpu-glossary/device-hardware/graphics-processing-cluster)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[L1 Data Cache](/gpu-glossary/device-hardware/l1-data-cache)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/special-function-unit
================================================================================

What is a Special Function Unit?
================================

SFU

The Special Function Units (SFUs) in

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)

accelerate certain arithmetic operations.

Notable for neural network workloads are transcendental mathematical operations,
like

`exp`

,

`sin`

, and

`cos`

.

[Core](/gpu-glossary/device-hardware/core)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Load/Store Unit](/gpu-glossary/device-hardware/load-store-unit)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor
================================================================================

What is a Streaming Multiprocessor?
===================================

SM

When we

[program GPUs](/gpu-glossary/host-software/cuda-software-platform)

, we
produce

[sequences of instructions](/gpu-glossary/device-software/streaming-assembler)

for its Streaming Multiprocessors to carry out.

Streaming Multiprocessors (SMs) of NVIDIA GPUs are roughly analogous to the
cores of CPUs. That is, SMs both execute computations and store state available
for computation in registers, with associated caches. Compared to CPU cores, GPU
SMs are simple, weak processors. Execution in SMs is pipelined within an
instruction (as in almost all CPUs since the 1990s) but there is no speculative
execution or instruction pointer prediction (unlike all contemporary
high-performance CPUs).

However, GPU SMs can execute more

[threads](/gpu-glossary/device-software/thread)

in parallel.

For comparison: an

[AMD EPYC 9965](https://www.techpowerup.com/cpu-specs/epyc-9965.c3904)

CPU draws
at most 500 W and has 192 cores, each of which can execute instructions for at
most two threads at a time, for a total of 384 threads in parallel, running at
about 1.25 W per thread.

An H100 SXM GPU draws at most 700 W and has 132 SMs, each of which has four

[Warp Schedulers](/gpu-glossary/device-hardware/warp-scheduler)

that can each
issue instructions to 32 threads (aka a

[warp](/gpu-glossary/device-software/warp)

) in parallel per clock cycle, for a
total of 128 × 132 > 16,000 parallel threads running at about 5 cW apiece. Note
that this is truly parallel: each of the 16,000 threads can make progress with
each clock cycle.

GPU SMs also support a large number of

*concurrent*

threads -- threads of
execution whose instructions are interleaved.

A single SM on an H100 can concurrently execute up to 2048 threads split across
64 thread groups of 32 threads each. With 132 SMs, that's a total of over
250,000 concurrent threads.

CPUs can also run many threads concurrently. But switches between

[warps](/gpu-glossary/device-software/warp)

happen at the speed of a single
clock cycle (over 1000x faster than context switches on a CPU), again powered by
the SM's

[Warp Schedulers](/gpu-glossary/device-hardware/warp-scheduler)

. The
volume of available

[warps](/gpu-glossary/device-software/warp)

and the speed of
warp switches help hide latency caused by memory reads, thread synchronization,
or other expensive instructions, ensuring that the compute resources (especially
the

[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

and

[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

) are well utilized.

This latency-hiding is the secret to GPUs' strengths. CPUs seek to hide latency
from end-users and programmers by maintaining large, hardware-managed caches and
sophisticated instruction prediction. This extra hardware limits the fraction of
their silicon area, power, and heat budgets that CPUs can allocate to
computation.

For programs or functions like neural network inference or sequential database
scans for which it is relatively straightforward for programmers to

[express](/gpu-glossary/device-software/cuda-programming-model)

the behavior of

[caches](/gpu-glossary/device-hardware/l1-data-cache)

— e.g. store a chunk of
each input matrix and keep it in cache for long enough to compute the related
outputs — the result is much higher throughput.

[CUDA (Device Architecture)](/gpu-glossary/device-hardware/cuda-device-architecture)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Core](/gpu-glossary/device-hardware/core)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/streaming-multiprocessor-architecture
================================================================================

What is a Streaming Multiprocessor Architecture?
================================================

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)

are versioned with a particular "architecture" that defines their compatibility
with

[Streaming Assembler (SASS)](/gpu-glossary/device-software/streaming-assembler)

code.

Most

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

versions have
two components: a major version and a minor version.

The major version is

*almost*

synonymous with GPU architecture family. For
example, all SM versions

`6.x`

are of the Pascal Architecture. Some NVIDIA
documentation even

[makes this claim directly](https://docs.nvidia.com/cuda/ptx-writers-guide-to-interoperability/index.html)

.
But, as an example, Ada GPUs have

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

architecture
version

`8.9`

, the same major version as Ampere GPUs.

Target

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

versions for

[SASS](/gpu-glossary/device-software/streaming-assembler)

compilation can be
specified when invoking

`nvcc`

, the

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

. Compatibility
across major versions is explicitly not guaranteed. For more on compatibility
across minor versions, see the

[documentation](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#gpu-feature-list)

for

[nvcc](/gpu-glossary/host-software/nvcc)

.

[Tensor Core](/gpu-glossary/device-hardware/tensor-core)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Texture Processing Cluster](/gpu-glossary/device-hardware/texture-processing-cluster)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/tensor-core
================================================================================

What is a Tensor Core?
======================

Tensor Cores are GPU

[cores](/gpu-glossary/device-hardware/core)

that operate on
entire matrices with each instruction.

Operating on more data for a single instruction fetch dramatically reduces power
requirements, which unlocks increased performance (see

[this talk](https://youtu.be/kLiwvnr4L80?t=868)

by Bill Dally, Chief Scientist
at NVIDIA). As of the Blackwell

[Streaming Multiprocessor (SM) Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

generation, they are the only way to achieve the highest arithmetic throughput
on NVIDIA GPUs.

As an example, the

`HMMA16.16816.F32`

[SASS](/gpu-glossary/device-software/streaming-assembler)

instruction calculates
D = AB + C for matrices A, B, C, and D (where C is often the same physical
matrix as D). The

`MMA`

stands for "Matrix Multiply and Accumulate".

`HMMA16`

indicates that the inputs are half-precision (

`16`

bits) and the

`F32`

indicates
that the outputs are accumulated into

`32`

bit (aka single-precision) floats.

`16816`

is not single number larger than 16,000. Instead, the string of numbers

`16`

,

`8`

,

`16`

denote the dimensions of the matrices. These dimensions are
generally named

`m`

,

`k`

, and

`n`

by NVIDIA, for example in

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

instructions. The
outer dimensions of A and B, aka

`m`

and

`n`

, come first and last, respectively,
and the shared inner dimension for the accumulation,

`k`

, is in the middle.
Multiplying these out, we see that the

`HMMA16.16816.32`

instruction performs 16
× 8 × 8 × 16 = 16,384 multiply-accumulate (MAC) operations.

Note that a single instruction in a single

[thread](/gpu-glossary/device-software/thread)

does not produce the entire
matrix multiplication. Instead, the 32 threads of a

[warp](/gpu-glossary/device-software/warp)

cooperatively produce the result by
executing the instruction together. Most of the per-instruction power overhead
is in decoding, which is shared across a

[warp](/gpu-glossary/device-software/warp)

thanks to the

[warp scheduler](/gpu-glossary/device-hardware/warp-scheduler)

. But even spread
across those 32 threads, that's 512 = 16,384 ÷ 32 MACs per instruction.

For this reason, it is helpful to think of Tensor Cores, and similar hardware
like the systolic arrays in Google Tensor Processing Units (TPUs), as a form of

[complex instruction set computer (CISC)](https://www.omgwiki.org/ddsf/doku.php?id=ddsf:public:guidebook:06_append:glossary:c:cisc)

hardware. For more on this perspective, applied to TPUs, see

[this talk by computer architect David Patterson](https://youtu.be/fhHAArxwzvQ?t=2072)

,
who also

[coined the terms CISC and RISC](https://www.semanticscholar.org/paper/4d3a941a5749dbf0dd39554f12597c449c3c07ff)

.

That assembler-level instruction might be produced by a compiler to implement

[PTX-level](/gpu-glossary/device-software/parallel-thread-execution)

matrix-multiply-and-accumlate instructions like

`wmma`

(documented

[here](https://docs.nvidia.com/cuda/archive/12.8.0/parallel-thread-execution/index.html#warp-level-matrix-instructions)

).
Those instructions also calculate D = AB + C for matrices A, B, C, and D, but
are generally compiled into many individual

[SASS](/gpu-glossary/device-software/streaming-assembler)

Tensor Core
instructions that operate on smaller matrices.

These instructions from the

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

instruction set
architecture are exposed in the high-level

[CUDA C++ programming language](/gpu-glossary/host-software/cuda-c)

as
intrinsics.

In reverse order, a line of

[CUDA C++](/gpu-glossary/host-software/cuda-c)

coding a matrix multiplication

`C = A @ B`

, of two 16 by 16 matrices, like

cpp

```
wmma::mma_sync(c, a, b, c);

```

where

`c`

is initialized to all zeros, and the first appearance indicates it is
also the output, might be compiled by

[`nvcc`](/gpu-glossary/host-software/nvcc)

to the

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

intermediate representation as

ptx

```
wmma.mma.sync.aligned.col.row.m16n16k16.f32.f32 {%f2, %f3, %f4, %f5, %f6, %f7, %f8, %f9}, {%r2, %r3, %r4, %r5, %r6, %r7, %r8, %r9}, {%r10, %r11, %r12, %r13, %r14, %r15, %r16, %r17}, {%f1, %f1, %f1, %f1, %f1, %f1, %f1, %f1};

```

and then finally compiled by

`ptxas`

to

[SASS](/gpu-glossary/device-software/streaming-assembler)

as

sass

```
HMMA.1688.F32 R20, R12, R11, RZ   // 1
HMMA.1688.F32 R24, R12, R17, RZ   // 2
HMMA.1688.F32 R20, R14, R16, R20  // 3
HMMA.1688.F32 R24, R14, R18, R24  // 4

```

The operands to each

`HMMA`

instruction can be read, in order, as

`D = A @ B + C`

. For example, instruction 3 uses

[register](/gpu-glossary/device-hardware/register-file)

20 for its output

`D`

,
registers 14 and 16 for its inputs

`A`

and

`B`

, respectively, and re-uses
register 20 for its input

`C`

, effecting the computation

`C += A @ B`

.

This program partitions the full 16 by 16 square matrix multiplication into four
separate instructions, each itself a matrix multiplication of a 16 by 8 matrix
with an 8 by 8 matrix. Similarly, programs running large-scale matrix
multiplications must break their work down into smaller matrix multiplications,
like the 16 by 16 square matrix multiplication performed by the

`mma_sync`

call
we are dissecting. We walk through this program below.

The first two instructions compute the matrix multiplication of the first eight
columns of the input

`a`

, from

`R12`

, with the first eight rows of the input

`b`

, from

`R11`

and

`R17`

, producing a 16 by 16 matrix, which is stored in

`R20`

and

`R24`

. This is a sort of "outer product": a tall and skinny matrix
mutliplied by a short and wide matrix. (

`RZ`

is a special-purpose "register"
that contains the value

`Z`

ero).

The second two instructions compute a similar "outer product" for the second
eight columns of

`a`

and second eight rows of

`b`

, accumulating with the output
of the first two instructions to produce the final value in

`c`

.

Put another way: within a block of eight rows out of eight columns in B and
within an entire column of A, a number of multiplications and additions occur
inside the Tensor Core concurrently, with respect to the instruction, to
implement a matrix multiplication. Each instruction handles all

`m`

rows of A
for the given block of rows and columns from B. Together, they handle the full
matrix multiplication.

Explore

[this compiler output on Godbolt](https://godbolt.org/z/e6cqn8491)

if
you want to dive deeper. Note that this is far from a

[utilization-maximizing](https://modal.com/blog/gpu-utilization-guide)

matrix
multiplication using Tensor Cores! For that, see

[this worklog by Pranjal Shandkar](https://cudaforfun.substack.com/p/outperforming-cublas-on-h100-a-worklog)

.

Programming Hopper and Blackwell Tensor Cores for maximum performance cannot be
done in pure CUDA C++, requiring instead

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

intrinsics for
both computation and memory. It is generally recommended to instead use existing
kernels from kernel libraries like

[cuBLAS (CUDA Basic Linear Algebra Subroutines)](https://docs.nvidia.com/cuda/cublas/)

or higher-level kernel programming interfaces like

[CUTLASS (CUDA Templates for Linear Algebra Subroutines)](https://github.com/NVIDIA/cutlass)

.
For an introduction to CUTLASS, see

[this blog post series by Colfax Research](https://research.colfax-intl.com/cutlass-tutorial-wgmma-hopper/)

.

Tensor Cores are much larger and less numerous than CUDA Cores. An H100 SXM5 has
only four Tensor Cores per

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

, i.e. one per

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

, compared to
hundreds of

[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

.

Tensor Cores were introduced in the V100 GPU, which represented a major
improvement in the suitability of NVIDIA GPUs for large neural network worloads.
For more, see

[the NVIDIA white paper introducing the V100](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf)

.

The internals of Tensor Cores are unknown, and likely differ from

[SM Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

to

[SM Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

.
They are commonly assumed to be systolic arrays, like TPUs, but there is no
consensus in the microbenchmarking literature.

[CUDA Core](/gpu-glossary/device-hardware/cuda-core)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Streaming Multiprocessor Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/texture-processing-cluster
================================================================================

What is a Texture Processing Cluster?
=====================================

TPC

Generally synonymous with "pair of

[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)

".
Rarely encountered in contemporary discussions of GPUs and not mapped onto a
level of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

's

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

or

[thread hierarchy](/gpu-glossary/device-software/thread-block)

, unlike

[Graphics/GPU Processing Clusters](/gpu-glossary/device-hardware/graphics-processing-cluster)

.

[Streaming Multiprocessor Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Graphics/GPU Processing Cluster](/gpu-glossary/device-hardware/graphics-processing-cluster)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-hardware/warp-scheduler
================================================================================

What is a Warp Scheduler?
=========================

The Warp Scheduler of the

[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor)

decides which group of

[threads](/gpu-glossary/device-software/thread)

to
execute.

These groups of threads, known as

[warps](/gpu-glossary/device-software/warp)

,
are switched out on a per clock cycle basis — roughly one nanosecond.

CPU thread context switches, on the other hand, take few hundred to a few
thousand clock cycles (more like a microsecond than a nanosecond) due to the
need to save the context of one thread and restore the context of another.
Additionally, context switches on CPUs lead to reduced locality, further
reducing performance by increasing cache miss rates (see

[Mogul and Borg, 1991](https://www.researchgate.net/publication/220938995_The_Effect_of_Context_Switches_on_Cache_Performance)

).

Because each

[thread](/gpu-glossary/device-software/thread)

has its own private

[registers](/gpu-glossary/device-software/registers)

allocated from the

[register file](/gpu-glossary/device-hardware/register-file)

of the

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

, context switches
on the GPU do not require any data movement to save or restore contexts.

Because the

[L1 caches](/gpu-glossary/device-hardware/l1-data-cache)

on GPUs can
be entirely programmer-managed and are

[shared](/gpu-glossary/device-software/shared-memory)

between the

[warps](/gpu-glossary/device-software/warp)

scheduled together onto an

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

(see

[cooperative thread array](/gpu-glossary/device-software/cooperative-thread-array)

),
context switches on the GPU have much less impact on cache hit rates. For
details on the interaction between programmer-managed caches and
hardware-managed caches in GPUs, see

[the "Maximize Memory Throughput" section of the CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#maximize-memory-throughput)

[Load/Store Unit](/gpu-glossary/device-hardware/load-store-unit)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA Core](/gpu-glossary/device-hardware/cuda-core)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software
================================================================================

Device Software
===============

These terms and technologies are used for software that runs on GPU — the
"device" in NVIDIA's lingo.

[CUDA (Programming Model)](/gpu-glossary/device-software/cuda-programming-model)

[Streaming ASSembler

SASS](/gpu-glossary/device-software/streaming-assembler)

[Parallel Thread eXecution

PTX](/gpu-glossary/device-software/parallel-thread-execution)

[Compute Capability](/gpu-glossary/device-software/compute-capability)

[Thread](/gpu-glossary/device-software/thread)

[Warp](/gpu-glossary/device-software/warp)

[Cooperative Thread Array](/gpu-glossary/device-software/cooperative-thread-array)

[Kernel](/gpu-glossary/device-software/kernel)

[Thread Block](/gpu-glossary/device-software/thread-block)

[Thread Block Grid](/gpu-glossary/device-software/thread-block-grid)

[Thread Hierarchy](/gpu-glossary/device-software/thread-hierarchy)

[Memory Hierarchy](/gpu-glossary/device-software/memory-hierarchy)

[Registers](/gpu-glossary/device-software/registers)

[Shared Memory](/gpu-glossary/device-software/shared-memory)

[Global Memory](/gpu-glossary/device-software/global-memory)

[GPU RAM](/gpu-glossary/device-hardware/gpu-ram)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA (Programming Model)](/gpu-glossary/device-software/cuda-programming-model)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/compute-capability
================================================================================

What is Compute Capability?
===========================

Instructions in the

[Parallel Thread Execution](/gpu-glossary/device-software/parallel-thread-execution)

instruction set are compatible with only certain physical GPUs. The versioning
system used to abstract away details of physical GPUs from the instruction set
and

[compiler](/gpu-glossary/host-software/nvcc)

is called "Compute Capability".

Most compute capability version numbers have two components: a major version and
a minor version. NVIDIA promises forward compatibility (old

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

code runs on new
GPUs) across both major and minor versions following the

[onion layer](https://docs.nvidia.com/cuda/parallel-thread-execution/#ptx-module-directives-target)

model.

With Hopper, NVIDIA has introduced an additional version suffix, the

`a`

in

`9.0a`

, which includes features that deviate from the onion model: their future
support is not guaranteed.

Target compute capabilities for

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

compilation can
be specified when invoking

`nvcc`

, the

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

. By default, the
compiler will also generate optimized

[SASS](/gpu-glossary/device-software/streaming-assembler)

for the matching

[Streaming Multiprocessor (SM) architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

.
The

[documentation](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.html#virtual-architectures)

for

[nvcc](/gpu-glossary/host-software/nvcc)

refers to compute capability as a
"virtual GPU architecture", in contrast to the "physical GPU architecture"
expressed by the

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

version.

The technical specifications for each compute capability version can be found in
the

[Compute Capability section of the NVIDIA CUDA C Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html?highlight=compute%2520capability#compute-capabilities)

.

[Parallel Thread eXecution](/gpu-glossary/device-software/parallel-thread-execution)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Thread](/gpu-glossary/device-software/thread)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/cooperative-thread-array
================================================================================

What is a Cooperative Thread Array?
===================================

A cooperative thread array (CTA) is a collection of threads scheduled onto the
same

[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor)

.
CTAs are the

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

/

[SASS](/gpu-glossary/device-software/streaming-assembler)

implementation of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

's

[thread blocks](/gpu-glossary/device-software/thread-block)

. CTAs are composed
of one or more

[warps](/gpu-glossary/device-software/warp)

.

Programmers can direct

[threads](/gpu-glossary/device-software/thread)

within a
CTA to coordinate with each other. The programmer-managed

[shared memory](/gpu-glossary/device-software/shared-memory)

, in the

[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache)

of the

[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor)

, makes this
coordination fast. Threads in different CTAs cannot coordinate with each other
via barriers, unlike threads within a CTA, and instead must coordinate via

[global memory](/gpu-glossary/device-software/global-memory)

, e.g. via atomic
update instructions. Due to driver control over the scheduling of CTAs at
runtime, CTA execution order is indeterminate and blocking a CTA on another CTA
can easily lead to deadlock.

The number of CTAs that can be scheduled onto a single

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

depends on a number
of factors. Fundamentally, the

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

has a limited set
of resources — lines in the

[register file](/gpu-glossary/device-hardware/register-file)

, "slots" for

[warps](/gpu-glossary/device-software/warp)

, bytes of

[shared memory](/gpu-glossary/device-software/shared-memory)

in the

[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache)

— and each CTA uses
a certain amount of those resources (as calculated at

[compile](/gpu-glossary/host-software/nvcc)

time) when scheduled onto an

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

.

[Warp](/gpu-glossary/device-software/warp)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Kernel](/gpu-glossary/device-software/kernel)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/cuda-programming-model
================================================================================

What is the CUDA Programming Model?
===================================

CUDA stands for

*Compute Unified Device Architecture*

. Depending on the context,
"CUDA" can refer to multiple distinct things: a

[high-level device architecture](/gpu-glossary/device-hardware/cuda-device-architecture)

,
a parallel programming model for architectures with that design, or a

[software platform](/gpu-glossary/host-software/cuda-software-platform)

that
extends high-level languages like C to add that programming model.

The vision for CUDA is laid out in the

[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)

white paper. We highly recommend this paper, which is the original source for
many claims, diagrams, and even specific turns of phrase in NVIDIA's
documentation.

Here, we focus on the CUDA

*programming model*

.

The Compute Unified Device Architecture (CUDA) programming model is a
programming model for programming massively parallel processors.

Per the

[NVIDIA CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#a-scalable-programming-model)

,
there are three key abstractions in the CUDA programming model:

* **Hierarchy of thread groups**

  . Programs are executed in threads but can make
  reference to groups of threads in a nested hierarchy, from

  [blocks](/gpu-glossary/device-software/thread-block)

  to

  [grids](/gpu-glossary/device-software/thread-block-grid)

  .

* **Hierarchy of memories**

  . Thread groups have access to a

  [memory resource](/gpu-glossary/device-software/memory-hierarchy)

  for
  communication between

  [threads](/gpu-glossary/device-software/thread)

  in the
  group. Accessing the

  [lowest layer](/gpu-glossary/device-software/shared-memory)

  of the memory
  hierarchy should be

  [nearly as fast as executing an instruction](/gpu-glossary/device-hardware/l1-data-cache)

  .

* **Barrier synchronization.**

  Thread groups can coordinate execution by means
  of barriers.

The hierarchies of execution and memory and their mapping onto

[device hardware](/gpu-glossary/device-hardware)

are summarized in the following
diagram.

Together, these three abstractions encourage the expression of programs in a way
that scales transparently as GPU devices scale in their parallel execution
resources.

Put provocatively: this programming model prevents programmers from writing
programs for NVIDIA's

[CUDA-architected](/gpu-glossary/device-hardware/cuda-device-architecture)

GPUs
that fail to get faster when the program's user buys a new NVIDIA GPU.

For example, each

[thread block](/gpu-glossary/device-software/thread-block)

in
a CUDA program can coordinate tightly, but coordination between blocks is
limited. This ensures blocks capture parallelizable components of the program
and can be scheduled in any order — in the terminology of NVIDIA documentation,
the programmer "exposes" this parallelism to the compiler and hardware. When the
program is executed on a new GPU that has more scheduling units (specifically,
more

[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)

),
more of these blocks can be executed in parallel.

The CUDA programming model abstractions are made available to programmers as
extensions to high-level CPU programming languages, like the

[CUDA C++ extension of C++](/gpu-glossary/host-software/cuda-c)

. The programming
model is implemented in software by an instruction set architecture

[(Parallel Thread eXecution, or PTX)](/gpu-glossary/device-software/parallel-thread-execution)

and low-level assembly language

[(Streaming Assembler, or SASS)](/gpu-glossary/device-software/streaming-assembler)

.
For example, the

[thread block](/gpu-glossary/device-software/thread-block)

level of the thread hierarchy is implemented via

[cooperative thread arrays](/gpu-glossary/device-software/cooperative-thread-array)

in these languages.

[Device Software](/gpu-glossary/device-software)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Streaming ASSembler](/gpu-glossary/device-software/streaming-assembler)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/global-memory
================================================================================

What is Global Memory?
======================

As part of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

,
each level of the thread group hierarchy has access to matching memory from the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

. This memory
can be used for coordination and communication and is managed by the programmer
(not the hardware or a runtime).

The highest level of that memory hierarchy is the global memory. Global memory
is global in its scope and its lifetime. That is, it is accessible by every

[thread](/gpu-glossary/device-software/thread)

in a

[thread block grid](/gpu-glossary/device-software/thread-block-grid)

and its
lifetime is as long as the execution of the program.

Access to data structures in the global memory can be synchronized across all
accessors using atomic instructions, as with CPU memory. Within a

[cooperative thread array](/gpu-glossary/device-software/cooperative-thread-array)

,
access can be more tightly synchronized, e.g. with barriers.

This level of the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

is typically
implemented in the

[GPU's RAM](/gpu-glossary/device-hardware/gpu-ram)

and
allocated from the host using a memory allocator provided by the

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

or the

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

.

[Shared Memory](/gpu-glossary/device-software/shared-memory)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Host Software](/gpu-glossary/host-software)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/kernel
================================================================================

What is a Kernel?
=================

A kernel is the unit of CUDA code that programmers typically write and compose,
akin to a procedure or function in languages targeting CPUs.

Unlike procedures, a kernel is called ("launched") once and returns once, but is
executed many times, once each by a number of

[threads](/gpu-glossary/device-software/thread)

. These executions are generally
concurrent (their execution order is non-deterministic) and parallel (they occur
simultaneously on different execution units).

The collection of all threads executing a kernel is organized as a kernel grid —
aka a

[thread block grid](/gpu-glossary/device-software/thread-block-grid)

, the
highest level of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

's
thread hierarchy. A kernel grid executes across multiple

[Streaming Multiprocessors (SMs)](/gpu-glossary/device-hardware/streaming-multiprocessor)

and so operates at the scale of the entire GPU. The matching level of the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

is the

[global memory](/gpu-glossary/device-software/global-memory)

.

In

[CUDA C++](/gpu-glossary/host-software/cuda-c)

, kernels are passed pointers
to

[global memory](/gpu-glossary/device-software/global-memory)

on the device
when they are invoked by the host and return nothing — they just mutate memory.

To give a flavor for CUDA kernel programming, let's walk through two
implementations of the "hello world" of CUDA kernels: matrix multiplication of
two square matrices,

`A`

and

`B`

. The two implementations will differ in how
they map the textbook matrix multiplication algorithm onto the thread hierarchy
and

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

.

In the simplest implementation, inspired by the first matmul kernel in

[Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311)

(4th edition, Figure 3.11), each

[thread](/gpu-glossary/device-software/thread)

does all of the work to compute one element of the output matrix -- loading in
turn each element of a particular

`row`

of

`A`

and a particular

`col`

umn of

`B`

into

[registers](/gpu-glossary/device-software/registers)

, multiplying the
paired elements, summing the results, and placing the sum back in

[global memory](/gpu-glossary/device-software/global-memory)

.

cpp

```
__global__ void mm(float* A, float* B, float* C, int N) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < N && col < N) {
        float sum = 0.0f;
        for (int k = 0; k < N; k++) {
            sum += A[row * N + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}

```

In this kernel, each

[thread](/gpu-glossary/device-software/thread)

does one
floating point operation (FLOP) per read from

[global memory](/gpu-glossary/device-software/global-memory)

: a multiply and an
add; a load from

`A`

and a load from

`B`

. You'll never

[use the whole GPU](https://modal.com/blog/gpu-utilization-guide)

that way,
since the bandwidth of the

[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

in FLOPs/s is much higher than the bandwidth between the

[GPU RAM](/gpu-glossary/device-hardware/gpu-ram)

and the

[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor)

.

We can increase the ratio of FLOPs to reads by more carefully mapping the work
in this algorithm onto the thread hierarchy and

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

. In the
"tiled" matmul kernel below, inspired by that in Figure 5.9 of the 4th edition
of

[Programming Massively Parallel Processors](https://www.amazon.com/dp/0323912311)

,
we map the loading of submatrices of

`A`

and

`B`

and the computation of
submatrices of

`C`

onto

[shared memory](/gpu-glossary/device-software/shared-memory)

and

[thread blocks](/gpu-glossary/device-software/thread-block)

respectively.

cpp

```
#define TILE_WIDTH 16

__global__ void mm(float* A, float* B, float* C, int N) {

    // declare variables in shared memory ("smem")
    __shared__ float As[TILE_WIDTH][TILE_WIDTH];
    __shared__ float Bs[TILE_WIDTH][TILE_WIDTH];

    int row = blockIdx.y * TILE_WIDTH + threadIdx.y;
    int col = blockIdx.x * TILE_WIDTH + threadIdx.x;

    float c_output = 0;
    for (int m = 0; m < N/TILE_WIDTH; ++m) {

        // each thread loads one element of A and one of B from global memory into smem
        As[threadIdx.y][threadIdx.x] = A[row * N + (m * TILE_WIDTH + threadIdx.x)];
        Bs[threadIdx.y][threadIdx.x] = B[(m * TILE_WIDTH + threadIdx.y) * N + col];

        // we wait until all threads in the 16x16 block are done loading into smem
        // so that it contains two 16x16 tiles
        __syncthreads();

        // then we loop over the inner dimension,
        // performing 16 multiplies and 16 adds per pair of loads from global memory
        for (int k = 0; k < TILE_WIDTH; ++k) {
            c_output += As[threadIdx.y][k] * Bs[k][threadIdx.x];
        }
        // wait for all threads to finish computing
        // before any start loading the next tile into smem
        __syncthreads();
    }
    C[row * N + col] = c_output;
}

```

For each iteration of the outer loop, which loads two elements, a thread runs 16
iterations of the inner loop, which does a multiply and an add, for 16 FLOPs per
global memory read.

This is still far from a fully optimized kernel for matrix multiplication.

[This worklog by Si Boehm of Anthropic](https://siboehm.com/articles/22/CUDA-MMM)

walks through optimizations that further increase the FLOP to memory read ratio
and map the algorithm even more tightly onto the hardware. Our kernels resemble
his Kernel 1 and Kernel 3; the worklog covers ten kernels.

That worklog and this article only consider writing kernels for execution on the

[CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

. The absolute fastest
matrix multiplication kernels run instead on

[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

.

[Cooperative Thread Array](/gpu-glossary/device-software/cooperative-thread-array)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Thread Block](/gpu-glossary/device-software/thread-block)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/memory-hierarchy
================================================================================

What is the Memory Hierarchy?
=============================

As part of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

,
each level of the thread group hierarchy has access to a distinct block of
memory shared by all threads in a group at that level: a "memory hierarchy" to
match the thread group hierarchy. This memory can be used for coordination and
communication and is managed by the programmer (not the hardware or a runtime).

For a

[thread block grid](/gpu-glossary/device-software/thread-block-grid)

, that
shared memory is in the

[GPU's RAM](/gpu-glossary/device-hardware/gpu-ram)

and
is known as the

[global memory](/gpu-glossary/device-software/global-memory)

.
Access to this memory can be coordinated with atomic operations and barriers,
but execution order across

[thread blocks](/gpu-glossary/device-software/thread-block)

is indeterminate.

For a single

[thread](/gpu-glossary/device-software/thread)

, the memory is a
chunk of the

[Streaming Multiprocessor's (SM's)](/gpu-glossary/device-hardware/streaming-multiprocessor)

[register file](/gpu-glossary/device-hardware/register-file)

. In keeping with
the memory semantics of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

,
this memory is private.

In between, the

[shared memory](/gpu-glossary/device-software/shared-memory)

for
the

[thread block](/gpu-glossary/device-software/thread-block)

level of the
thread hierarchy is stored in the

[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache)

of each

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

. Careful management
of this cache — e.g. loading data into it to support the maximum number of
arithmetic operations before new data is loaded — is key to the art of designing
high-performance CUDA

[kernels](/gpu-glossary/device-software/kernel)

.

[Thread Hierarchy](/gpu-glossary/device-software/thread-hierarchy)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Registers](/gpu-glossary/device-software/registers)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/parallel-thread-execution
================================================================================

What is Parallel Thread Execution?
==================================

PTX

Parallel Thread eXecution (PTX) is an intermediate representation (IR) for code
that will run on a parallel processor (almost always an NVIDIA GPU). It is one
of the formats output by

`nvcc`

, the

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

.

NVIDIA documentation refers to PTX as both a "virtual machine" and an
"instruction set architecture".

From the programmer's perspective, PTX is an instruction set for programming
against a virtual machine model. Programmers or compilers producing PTX can be
confident their program will run with the same semantics on many distinct
physical machines, including machines that do not yet exist. In this way, it is
also similar to CPU instruction set architectures like

[x86\_64](https://www.intel.com/content/www/us/en/developer/articles/technical/intel-sdm.html)

,

[aarch64](https://developer.arm.com/documentation/ddi0487/latest/)

, or

[SPARC](https://www.gaisler.com/doc/sparcv8.pdf)

.

Unlike those ISAs, PTX is very much an

[intermediate representation](https://en.wikipedia.org/wiki/Intermediate_representation)

,
like LLVM-IR. The PTX components of a

[CUDA binary](/gpu-glossary/host-software/cuda-binary-utilities)

will be
just-in-time (JIT) compiled by the host

[CUDA Drivers](/gpu-glossary/host-software/nvidia-gpu-drivers)

into
device-specific

[SASS](/gpu-glossary/device-software/streaming-assembler)

for
execution.

In the case of NVIDIA GPUs, PTX is forward-compatible: GPUs with a matching or
higher

[compute capability](/gpu-glossary/device-software/compute-capability)

version will be able to run the program, thanks to this mechanisn of JIT
compilation.

Some exemplary PTX:

ptx

```
.reg .f32 %f<7>;

```

* a compiler directive for the
  PTX-to-

  [SASS](/gpu-glossary/device-software/streaming-assembler)

  compiler
  indicating that this kernel consumes seven 32-bit floating point

  [registers](/gpu-glossary/device-software/registers)

  . Registers are
  dynamically allocated to groups of

  [threads](/gpu-glossary/device-software/thread)

  (

  [warps](/gpu-glossary/device-software/warp)

  ) from the

  [SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

  's

  [register file](/gpu-glossary/device-hardware/register-file)

  .

ptx

```
fma.rn.f32 %f5, %f4, %f3, 0f3FC00000;

```

* apply a fused multiply-add (

  `fma`

  ) operation to multiply the contents of
  registers

  `f3`

  and

  `f4`

  and add the constant

  `0f3FC00000`

  , storing the result
  in

  `f5`

  . All numbers are in 32 bit floating point representation. The

  `rn`

  suffix for the FMA operation sets the floating point rounding mode to

  [IEEE 754 "round even"](https://en.wikipedia.org/wiki/IEEE_754)

  (the default).

ptx

```
mov.u32 %r1, %ctaid.x;
mov.u32 %r2, %ntid.x;
mov.u32 %r3, %tid.x;

```

* `mov`

  e the

  `x`

  -axis values of the

  `c`

  ooperative

  `t`

  hread

  `a`

  rray

  `i`

  n

  `d`

  ex,
  the cooperative thread array dimension index (

  `ntid`

  ), and the

  `t`

  hread

  `i`

  n

  `d`

  ex into three

  `u32`

  registers

  `r1`

  -

  `r3`

  .

The PTX programming model exposes multiple levels of parallelism to the
programmer. These levels map directly onto the hardware through the PTX machine
model, diagrammed below.

Notably, in this machine model there is a single instruction unit for multiple
processors. While each processor runs one

[thread](/gpu-glossary/device-software/thread)

, those threads must execute the
same instructions — hence

*parallel*

thread execution, or PTX. They coordinate
with each other through

[shared memory](/gpu-glossary/device-software/shared-memory)

and effect
different results by means of private

[registers](/gpu-glossary/device-software/registers)

.

The documentation for the latest version of PTX is available from NVIDIA

[here](https://docs.nvidia.com/cuda/parallel-thread-execution/)

. The instruction
sets of PTX are versioned with a number called the
"

[compute capability](/gpu-glossary/device-software/compute-capability)

", which
is synonymous with "minimum supported

[Streaming Multiprocessor architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

version".

Writing in-line PTX by hand is uncommon but not unheard of, similar to writing
in-line

`x86_64`

assembly, as is done in high-performance vectorized query
operators in analytical databases and in performance-sensitive sections of
operating system kernels. At time of writing in October of 2024, in-line PTX is
the only way to take advantage of some Hopper-specific hardware features like
the

`wgmma`

and

`tma`

instructions, as in

[Flash Attention 3](https://arxiv.org/abs/2407.08608)

or in the

[Machete w4a16 kernels](https://youtu.be/-4ZkpQ7agXM)

. Viewing

[CUDA C/C++](/gpu-glossary/host-software/cuda-c)

,

[SASS](/gpu-glossary/device-software/streaming-assembler)

, and

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

together is
supported on

[Godbolt](https://godbolt.org/#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAZgQDbYB2AhgLbYgDkAjF%2BTXRMiAZVQtGIHgBYBQogFUAztgAKAD24AGfgCsp5eiyahUAV0wtyKxqiIEh1ZpgDC6embZMQAJnLOAGQImbAA5TwAjbFJfWQAHdCViByY3Dy9fcgSk%2ByEgkPC2KJifWRtsOxSRIhZSIjTPbz9yyqFq2qJ8sMjo2OsauoaM5oHO4O6i3tKASmt0M1JUTi4AUh8AZjjSFmA2FgBqISWVrQBBE/ON4NQPHH2V9ZdxJRU6h9wLtfXr2%2Bx7x9QSiIhHQ70%2BVyYNwsfweTyBmHoBAiYLOXx%2B0P%2BTzMESMSgA%2BgA3HwAOiQKMu30hv0x5kseNIZmEBA4pPJFzxeOA9HQEQkHP2BPQBEw%2ByUwGwbDYnO5vPoeI4UowEmwSiWEGCRH2AFlyPsNftQrr9QBpXU0bksTUSOJIKwXfYOx1O50u11ul0YJhA/bm9CW/YAKlOus93t9/oDACFyPb3XH4/Hw5qojUzRbNQGXNN7gB2SOx/WRgIAeRcxpEAEkAFofdYAEXWPge%2BbODtDmv1qAASugAO7/Ov7HHoVAAawrmHUxPUgf2RdL5eruHuPkj%2BwgRCQpGwLEwE6nM4A9HOS2XKzXps3Y%2B29cJ9qg0gOh9yx/viQBPWfzs9Lldrjdbjue6TtOK4AKwngu564Je6wtucrb7Iex4EDQoo1EQErMB2Sj7CESwvLUn4kPseyjn8m7BMAuG9mQo77IyOCkPs9iMPsACOZjGPYABelopAWaEQN2faYtqK4AGxrBJ96PrCBrZiseaxg6SYsWwcRPloxJaFeiGqWQ676gQWnNnqYnGmZaz5quBCKcp%2BmOkQGl/g8g7nGBkYif2Ab7Maf56isYGDr5%2BaeSZvmhAFD7uEFdZ6acTpKfFjkuEFXk9j5BrRWkcVPtatqzs5mnWUO2A1LOaWed5s5RaVMX0HFCUOslnw5nWXCzPQ3Bgfw3hcDo5DoNwLgKHWiVpaupVKPMiwwhsfDkEQ2idbMSBAb0ECzKOsTEgAnAdh1HUdEmGNw0h9StQ3cPwSggFoS0rbMcCwCgGAaQw0SUNQ71xJ9MRMASqCoDwPA5uQOAEgQSwAGoENgvbFnEzBXXQ9CYaQd0QBEV0RMERHcIt70cMIxZMPQ75XTgewmJIA38IQ26VASKpXdg6gVGYmGE/wGrYN19OGEi2ykO%2Bbg4FdRCkMyPOzOaOxKHDCNIyjvD8IIwhiMqUiyBrigqBoV36DwhjGKYFhWIiER3ZAszoHEuRetwAC0xbrPszt1qEda4JGCgAOIe874ohNsmHOxgOBubUqBkvWv1c9geIABynZH2BuelCeYSnp1KFKqd5c7zsO%2BoLBKM79uO0obnOwSHvwm5qDWdZYO3fzFSO04TCuO4jQGIE4yFMUBjZMkQhDN4Jtj47XTD70JstI77SDH3GSLx3rRMCvYwFD0MSL6Mk8GECHRz/vUizDNCxLL4XU9ZdgvDVwBowy4Lj7KDxI5jp674MQhkvg%2BGmPwZa9NphrQ2jELa5AdqlH2sdRBB1ToCwuuQfqg1n63Xuo9cB5AXrIDQFgPAhASAUCoLQT6rAOA8zkJrcQkgZB0P1moTQgt9B%2BCMCYNAFtrCby7hAZwx8/CDz3pMEo8REjj1SGvJoWQpGzyHhfUofDbDLyPrIzIS8qijHPuIvop9V7pDkYY3eEwR4zDmDfZYXwtg7D2IcSEGdUSoghFCO4sJnivCIGyNx1JPHwgcL4yk7iYQAnhFbYJ6IPEAmxLiQkJI44fFcSE/xsS6QMiZCyJJnwzgci5DyPkeIBRChFGKCUUoCmynlBKPEjIlSMFVNgTA6o7w6lvJqQ0HS/Jpj9FaegNo7SOQTCMkZN41JBhDEIMM6ZAzRhUqMxZcY1IpisD6WZmZ7LwQdIWU8i4axuUbM1e80yOx3hqm5Z8I5xwgRnKFPZ0EAoAW3LuN8M41gQW/PsmCxybydjkvWK5r4QKfnuVBX8pVnlATeUhSCP4LwJVjMhPUaFT6YRJjhPCFQVRKCIixdApEWDkRYkgKiNE6IMR7tEFin0OJcSZHxR2gl1wXMeOJaS0lZLuDEqELZCy1LFS0jpY5BlmKtI7KZOC5l5KWSlS3WyfLhnqRKlNQFHkMqiV8v5UqBA8qhXShFbK9VcrBRFbmFKiVHRVQ1VlOqq4uWNWCvlAZhVfKCtKqsyqWdMq1RyrFU1cFYytVRO1TqZ0uC9XQVdZ%2Bo1xr7EmjZNc185orkbKAp6kDdybW2rtJBSCUHnX4GwEA6wf5gSjU/G61gcFgJ0M9RAEA3roA%2BowchP1m1/VbSAQGwNQbg0htDbAStEbIwwerBgGMsY40FnjVgotaHE2wmTCmVMJRmzpoNRmncCAszuoLdmnNuZq0oMIfmV0rYizFsQyW0ti1qzlkYaiw6Vb9UWnrLWjDdbyGUKwo2mQuHm0sELa28A7YOxSHu127tPbe19gHIOIdoiWmwBHYh0dFhxzrNnJOqdyDp0ztVDtidc7WALlJJ1xdS7l0ruB6Ztd67O0bvHeVkY26qO3Y4QRPdj4m1EeYhe8icgpB44J6ReiR4bzUTojoIntFtF0Uo/Rh8ZOaOU3UcTC8r6zVvjwe%2BEbH6YO4K/d%2Bn8eDf1/hAf%2BZDU26fTeAzNTFqA5vgXmxBBauBoLHddLg2CHq1tWuGnwRaQBgW0snHge0JI5h8DwdYoNpA5jAuWrzWDcF1vwQ2htKAB1LG%2BsJDt/1QjsGWKEN%2BH8v4/0Gs0gB0tMAGHfQwzgTC9Y/sNuwrR/CUjd17sYgePcNMH1E47ETM8UgDZPp1%2BTKneuSY49vBTYiJP9Gm/3NTZj54H1mFLbA2BhT3T05GlL3A6zYChksfYw7qWlZMxVizVnAELV1G4Ft1KgEgLSwF9aWboE5ukBJYkElwtA54MDoH4a0HFp4FoB6R2fPVr809DL8BG0gBy9gPLv1CvFe4Nd8rZnKv8Gq2QvbfgGva2a9%2Bg2bDBrG3Y1vbrIm%2BMbdHgo4TqmhtjcU0tuT82Vvrzp%2Bos%2BXOBOmNkwt/jm2lrbl23VrQB2DP8Gfids7fxLvMVx6Z8zWg/6kIe42J7BXW02fe/5iB5AvuOZgTtMC6xiRxdByDx3ydOGFvIJD6HFbDNw7ugj%2BzgXgvSD2sSEHWhGw5lTsnZOYFSgSVOrDuzdac1Q4egLdYCvvMJ4CyzTGXXpBAA%3D%3D)

.
See the

[NVIDIA "Inline PTX Assembly in CUDA" guide](https://docs.nvidia.com/cuda/inline-ptx-assembly/)

for details.

[Streaming ASSembler](/gpu-glossary/device-software/streaming-assembler)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Compute Capability](/gpu-glossary/device-software/compute-capability)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/registers
================================================================================

What are Registers?
===================

At the lowest level of the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

are the
registers, which store information manipulated by a single

[thread](/gpu-glossary/device-software/thread)

.

The values in registers are generally stored in the

[register file](/gpu-glossary/device-hardware/register-file)

of the

[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor)

,
but they can also spill to the

[global memory](/gpu-glossary/device-software/global-memory)

in the

[GPU RAM](/gpu-glossary/device-hardware/gpu-ram)

at a substantial performance
penalty.

As when programming CPUs, these registers are not directly manipulated by
high-level languages like

[CUDA C](/gpu-glossary/host-software/cuda-c)

. They are
only visible to lower-level languages like

[Parallel Thread Execution (PTX)](/gpu-glossary/device-software/parallel-thread-execution)

or

[Streaming Assembler (SASS)](/gpu-glossary/device-software/streaming-assembler)

and so are typically managed by a compiler like

[nvcc](/gpu-glossary/host-software/nvcc)

. Among the compiler's goals is to limit
the register space used by each

[thread](/gpu-glossary/device-software/thread)

so that more

[thread blocks](/gpu-glossary/device-software/thread-block)

can be
simultaneously scheduled into a single

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

.

The registers used in the

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

instruction set
architecture are documented

[here](https://docs.nvidia.com/cuda/parallel-thread-execution/#register-state-space)

.
The registers used in

[SASS](/gpu-glossary/device-software/streaming-assembler)

are not, to our knowledge, documented.

[Memory Hierarchy](/gpu-glossary/device-software/memory-hierarchy)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Shared Memory](/gpu-glossary/device-software/shared-memory)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/shared-memory
================================================================================

What is Shared Memory?
======================

Shared memory is the level of the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

corresponding
to the

[thread block](/gpu-glossary/device-software/thread-block)

level of the
thread group hierarchy in the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

.
It is generally expected to be much smaller but much faster (in throughput and
latency) than the

[global memory](/gpu-glossary/device-software/global-memory)

.

A fairly typical

[kernel](/gpu-glossary/device-software/kernel)

therefore looks
something like this:

* load data from

  [global memory](/gpu-glossary/device-software/global-memory)

  into shared memory

* perform a number of arithmetic operations on that data via the

  [CUDA Cores](/gpu-glossary/device-hardware/cuda-core)

  and

  [Tensor Cores](/gpu-glossary/device-hardware/tensor-core)

* optionally, synchronize

  [threads](/gpu-glossary/device-software/thread)

  within
  a

  [thread block](/gpu-glossary/device-software/thread-block)

  by means of
  barriers while performing those operations

* write data back into

  [global memory](/gpu-glossary/device-software/global-memory)

  , optionally
  preventing races across

  [thread blocks](/gpu-glossary/device-software/thread-block)

  by means of
  atomics

Shared memory is stored in the

[L1 data cache](/gpu-glossary/device-hardware/l1-data-cache)

of the GPU's

[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor)

.

[Registers](/gpu-glossary/device-software/registers)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Global Memory](/gpu-glossary/device-software/global-memory)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/streaming-assembler
================================================================================

What is Streaming Assembler?
============================

SASS

[Streaming ASSembler](https://stackoverflow.com/questions/9798258/what-is-sass-short-for)

(SASS) is the assembly format for programs running on NVIDIA GPUs. This is the
lowest-level format in which human-readable code can be written. It is one of
the formats output by

`nvcc`

, the

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

, alongside

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

. It is converted
to device-specific binary microcodes during execution. Presumably, the
"Streaming" in "Streaming Assembler" refers to the

[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)

which the assembly language programs.

SASS is versioned and tied to a specific NVIDIA GPU

[SM architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

.
See also

[Compute Capability](/gpu-glossary/device-software/compute-capability)

.

Some exemplary instructions in SASS for the SM90a architecture of Hopper GPUs:

* `FFMA R0, R7, R0, 1.5 ;`

  - perform a

  `F`

  used

  `F`

  loating point

  `M`

  ultiply

  `A`

  dd
  that multiplies the contents of

  `R`

  egister 7 and

  `R`

  egister 0, adds

  `1.5`

  , and
  stores the result in

  `R`

  egister 0.

* `S2UR UR4, SR_CTAID.X ;`

  - copy the

  `X`

  value of the

  [Cooperative Thread Array](/gpu-glossary/device-software/cooperative-thread-array)

  's

  `I`

  n

  `D`

  ex from its

  `S`

  pecial

  `R`

  egister to

  `U`

  niform

  `R`

  egister 4.

As for CPUs, writing this "GPU assembler" by hand is very uncommon. Viewing
compiler-generated SASS while profiling and editing high-level

[CUDA C/C++](/gpu-glossary/host-software/cuda-c)

code or in-line

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

is

[more common](https://docs.nvidia.com/gameworks/content/developertools/desktop/ptx_sass_assembly_debugging.htm)

,
especially in the production of the highest-performance kernels. Viewing

[CUDA C/C++](/gpu-glossary/host-software/cuda-c)

, SASS, and PTX together is
supported on

[Godbolt](https://godbolt.org/#z:OYLghAFBqd5TKALEBjA9gEwKYFFMCWALugE4A0BIEAZgQDbYB2AhgLbYgDkAjF%2BTXRMiAZVQtGIHgBYBQogFUAztgAKAD24AGfgCsp5eiyahUAV0wtyKxqiIEh1ZpgDC6embZMQAJnLOAGQImbAA5TwAjbFJfWQAHdCViByY3Dy9fcgSk%2ByEgkPC2KJifWRtsOxSRIhZSIjTPbz9yyqFq2qJ8sMjo2OsauoaM5oHO4O6i3tKASmt0M1JUTi4AUh8AZjjSFmA2FgBqISWVrQBBE/ON4NQPHH2V9ZdxJRU6h9wLtfXr2%2Bx7x9QSiIhHQ70%2BVyYNwsfweTyBmHoBAiYLOXx%2B0P%2BTzMESMSgA%2BgA3HwAOiQKMu30hv0x5kseNIZmEBA4pPJFzxeOA9HQEQkHP2BPQBEw%2ByUwGwbDYnO5vPoeI4UowEmwSiWEGCRH2AFlyPsNftQrr9QBpXU0bksTUSOJIKwXfYOx1O50u11ul0YJhA/bm9CW/YAKlOus93t9/oDACFyPb3XH4/Hw5qojUzRbNQGXNN7gB2SOx/WRgIAeRcxpEAEkAFofdYAEXWPge%2BbODtDmv1qAASugAO7/Ov7HHoVAAawrmHUxPUgf2RdL5eruHuPkj%2BwgRCQpGwLEwE6nM4A9HOS2XKzXps3Y%2B29cJ9qg0gOh9yx/viQBPWfzs9Lldrjdbjue6TtOK4AKwngu564Je6wtucrb7Iex4EDQoo1EQErMB2Sj7CESwvLUn4kPseyjn8m7BMAuG9mQo77IyOCkPs9iMPsACOZjGPYABelopAWaEQN2faYtqK4AGxrBJ96PrCBrZiseaxg6SYsWwcRPloxJaFeiGqWQ676gQWnNnqYnGmZaz5quBCKcp%2BmOkQGl/g8g7nGBkYif2Ab7Maf56isYGDr5%2BaeSZvmhAFD7uEFdZ6acTpKfFjkuEFXk9j5BrRWkcVPtatqzs5mnWUO2A1LOaWed5s5RaVMX0HFCUOslnw5nWXCzPQ3Bgfw3hcDo5DoNwLgKHWiVpaupVKPMiwwhsfDkEQ2idbMSBAb0ECzKOsTEgAnAdh1HUdEmGNw0h9StQ3cPwSggFoS0rbMcCwCgGAaQw0SUNQ71xJ9MRMASqCoDwPA5uQOAEgQSwAGoENgvbFnEzBXXQ9CYaQd0QBEV0RMERHcIt70cMIxZMPQ75XTgewmJIA38IQ26VASKpXdg6gVGYmGE/wGrYN19OGEi2ykO%2Bbg4FdRCkMyPOzOaOxKHDCNIyjvD8IIwhiMqUiyBrigqBoV36DwhjGKYFhWIiER3ZAszoHEuRetwAC0xbrPszt1qEda4JGCgAOIe874ohNsmHOxgOBubUqBkvWv1c9geIABynZH2BuelCeYSnp1KFKqd5c7zsO%2BoLBKM79uO0obnOwSHvwm5qDWdZYO3fzFSO04TCuO4jQGIE4yFMUBjZMkQhDN4Jtj47XTD70JstI77SDH3GSLx3rRMCvYwFD0MSL6Mk8GECHRz/vUizDNCxLL4XU9ZdgvDVwBowy4Lj7KDxI5jp674MQhkvg%2BGmPwZa9NphrQ2jELa5AdqlH2sdRBB1ToCwuuQfqg1n63Xuo9cB5AXrIDQFgPAhASAUCoLQT6rAOA8zkJrcQkgZB0P1moTQgt9B%2BCMCYNAFtrCby7hAZwx8/CDz3pMEo8REjj1SGvJoWQpGzyHhfUofDbDLyPrIzIS8qijHPuIvop9V7pDkYY3eEwR4zDmDfZYXwtg7D2IcSEGdUSoghFCO4sJnivCIGyNx1JPHwgcL4yk7iYQAnhFbYJ6IPEAmxLiQkJI44fFcSE/xsS6QMiZCyJJnwzgci5DyPkeIBRChFGKCUUoCmynlBKPEjIlSMFVNgTA6o7w6lvJqQ0HS/Jpj9FaegNo7SOQTCMkZN41JBhDEIMM6ZAzRhUqMxZcY1IpisD6WZmZ7LwQdIWU8i4axuUbM1e80yOx3hqm5Z8I5xwgRnKFPZ0EAoAW3LuN8M41gQW/PsmCxybydjkvWK5r4QKfnuVBX8pVnlATeUhSCP4LwJVjMhPUaFT6YRJjhPCFQVRKCIixdApEWDkRYkgKiNE6IMR7tEFin0OJcSZHxR2gl1wXMeOJaS0lZLuDEqELZCy1LFS0jpY5BlmKtI7KZOC5l5KWSlS3WyfLhnqRKlNQFHkMqiV8v5UqBA8qhXShFbK9VcrBRFbmFKiVHRVQ1VlOqq4uWNWCvlAZhVfKCtKqsyqWdMq1RyrFU1cFYytVRO1TqZ0uC9XQVdZ%2Bo1xr7EmjZNc185orkbKAp6kDdybW2rtJBSCUHnX4GwEA6wf5gSjU/G61gcFgJ0M9RAEA3roA%2BowchP1m1/VbSAQGwNQbg0htDbAStEbIwwerBgGMsY40FnjVgotaHE2wmTCmVMJRmzpoNRmncCAszuoLdmnNuZq0oMIfmV0rYizFsQyW0ti1qzlkYaiw6Vb9UWnrLWjDdbyGUKwo2mQuHm0sELa28A7YOxSHu127tPbe19gHIOIdoiWmwBHYh0dFhxzrNnJOqdyDp0ztVDtidc7WALlJJ1xdS7l0ruB6Ztd67O0bvHeVkY26qO3Y4QRPdj4m1EeYhe8icgpB44J6ReiR4bzUTojoIntFtF0Uo/Rh8ZOaOU3UcTC8r6zVvjwe%2BEbH6YO4K/d%2Bn8eDf1/hAf%2BZDU26fTeAzNTFqA5vgXmxBBauBoLHddLg2CHq1tWuGnwRaQBgW0snHge0JI5h8DwdYoNpA5jAuWrzWDcF1vwQ2htKAB1LG%2BsJDt/1QjsGWKEN%2BH8v4/0Gs0gB0tMAGHfQwzgTC9Y/sNuwrR/CUjd17sYgePcNMH1E47ETM8UgDZPp1%2BTKneuSY49vBTYiJP9Gm/3NTZj54H1mFLbA2BhT3T05GlL3A6zYChksfYw7qWlZMxVizVnAELV1G4Ft1KgEgLSwF9aWboE5ukBJYkElwtA54MDoH4a0HFp4FoB6R2fPVr809DL8BG0gBy9gPLv1CvFe4Nd8rZnKv8Gq2QvbfgGva2a9%2Bg2bDBrG3Y1vbrIm%2BMbdHgo4TqmhtjcU0tuT82Vvrzp%2Bos%2BXOBOmNkwt/jm2lrbl23VrQB2DP8Gfids7fxLvMVx6Z8zWg/6kIe42J7BXW02fe/5iB5AvuOZgTtMC6xiRxdByDx3ydOGFvIJD6HFbDNw7ugj%2BzgXgvSD2sSEHWhGw5lTsnZOYFSgSVOrDuzdac1Q4egLdYCvvMJ4CyzTGXXpBAA%3D%3D)

.
For more detail on SASS with a focus on performance debugging workflows, see

[this talk](https://www.youtube.com/watch?v=we3i5VuoPWk)

from Arun Demeure.

SASS is

*very*

lightly documented — the instructions are listed in the

[documentation for NVIDIA's CUDA binary utilities](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#instruction-set-ref)

,
but their semantics are not defined. The mapping from ASCII assembler to binary
opcodes and operands is entirely undocumented, but it has been
reverse-engineered in certain cases
(

[Maxwell](https://github.com/NervanaSystems/maxas)

,

[Lovelace](https://kuterdinel.com/nv_isa_sm89/)

).

[CUDA (Programming Model)](/gpu-glossary/device-software/cuda-programming-model)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Parallel Thread eXecution](/gpu-glossary/device-software/parallel-thread-execution)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/thread
================================================================================

What is a Thread?
=================

A

*thread of execution*

(or "thread" for short) is the lowest unit of
programming for GPUs, the atom of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

's
thread group hierarchy. A thread has its own

[registers](/gpu-glossary/device-software/registers)

, but little else.

Both

[SASS](/gpu-glossary/device-software/streaming-assembler)

and

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

programs target
threads. Compare this to a typical C program in a POSIX environment, which
targets a process, itself a collection of one or more threads.

Like a thread on a CPU, a GPU thread can have a private instruction
pointer/program counter. However, for performance reasons, GPU programs are
generally written so that all the threads in a

[warp](/gpu-glossary/device-software/warp)

share the same instruction pointer,
executing instructions in lock-step (see also

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

).

Also like threads on CPUs, GPU threads have stacks in

[global memory](/gpu-glossary/device-hardware/gpu-ram)

for storing spilled
registers and a function call stack, but high-performance

[kernels](/gpu-glossary/device-software/kernel)

generally avoid using either.

A single

[CUDA Core](/gpu-glossary/device-hardware/cuda-core)

executes
instructions from a single thread.

[Compute Capability](/gpu-glossary/device-software/compute-capability)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Warp](/gpu-glossary/device-software/warp)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/thread-block
================================================================================

What is a Thread Block?
=======================

A thread block is a level of the CUDA programming model's thread hierarchy below
a grid but above a

[warp](/gpu-glossary/device-software/warp)

. It is the CUDA
programming model's abstract equivalent of the concrete

[cooperative thread arrays](/gpu-glossary/device-software/cooperative-thread-array)

in

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

/

[SASS](/gpu-glossary/device-software/streaming-assembler)

.

Blocks are the smallest unit of thread coordination exposed to programmers.
Blocks must execute independently, so that any execution order for blocks is
valid, from fully serial in any order to all interleavings.

A single CUDA

[kernel](/gpu-glossary/device-software/kernel)

launch produces one
or more thread blocks (in the form of a

[block grid](/gpu-glossary/device-software/thread-block-grid)

), each of which
contains one or more

[warps](/gpu-glossary/device-software/warp)

. Blocks can be
arbitrarily sized, but they are typically multiples of the

[warp](/gpu-glossary/device-software/warp)

size (32 on all current CUDA GPUs).

[Kernel](/gpu-glossary/device-software/kernel)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Thread Block Grid](/gpu-glossary/device-software/thread-block-grid)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/thread-block-grid
================================================================================

What is a Thread Block Grid?
============================

When a CUDA

[kernel](/gpu-glossary/device-software/kernel)

is launched, it
creates a collection of

[threads](/gpu-glossary/device-software/thread)

known as
a thread block grid. Grids can be one, two, or three dimensional. They are made
up of

[thread blocks](/gpu-glossary/device-software/thread-block)

.

The matching level of the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

is the

[global memory](/gpu-glossary/device-software/global-memory)

.

[Thread blocks](/gpu-glossary/device-software/thread-block)

are effectively
independent units of computation. They execute concurrently, that is, with
indeterminate order, ranging from fully sequentially in the case of a GPU with a
single

[Streaming Multiprocessor](/gpu-glossary/device-hardware/streaming-multiprocessor)

to fully in parallel when run on a GPU with sufficient resources to run them all
simultaneously.

[Thread Block](/gpu-glossary/device-software/thread-block)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Thread Hierarchy](/gpu-glossary/device-software/thread-hierarchy)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/thread-hierarchy
================================================================================

Thread Hierarchy
================

The thread hierarchy is a key abstraction of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

,
alongside the

[memory hierarchy](/gpu-glossary/device-software/memory-hierarchy)

. It organizes
the execution of parallel programs across multiple levels, from individual
threads up to entire GPU devices.

At the lowest level are individual

[threads](/gpu-glossary/device-software/thread)

. Like a thread of execution on a
CPU, each CUDA thread executes a stream of instructions. The hardware resources
that effect arithmetic and logic instructions are called

[Cores](/gpu-glossary/device-hardware/core)

. Threads are selected for execution
by the

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

.

The intermediate level consists of

[thread blocks](/gpu-glossary/device-software/thread-block)

, which are also
known as

[cooperative thread arrays](/gpu-glossary/device-software/cooperative-thread-array)

in

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

and

[SASS](/gpu-glossary/device-software/streaming-assembler)

. Each

[thread](/gpu-glossary/device-software/thread)

has a unique identifier within
its

[thread block](/gpu-glossary/device-software/thread-block)

. These thread
identifiers are index-based, to support assignment of work to threads based on
indices into input or output arrays. All threads within a block are scheduled
simultaneously onto the same

[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor)

.
They can coordinate through

[shared memory](/gpu-glossary/device-software/shared-memory)

and synchronize
with barriers.

At the highest level, multiple

[thread blocks](/gpu-glossary/device-software/thread-block)

are organized into a

[thread block grid](/gpu-glossary/device-software/thread-block-grid)

that spans
the entire GPU.

[Thread blocks](/gpu-glossary/device-software/thread-block)

are
strictly limited in their coordiation and communication. Blocks within a grid
execute concurrently with respect to each other, with no guaranteed execution
order. CUDA programs must be written so that any interleaving of blocks is
valid, from fully serial to fully parallel. That means

[thread blocks](/gpu-glossary/device-software/thread-block)

cannot, for
instance, synchronize with barriers. Like

[threads](/gpu-glossary/device-software/thread)

, each

[thread block](/gpu-glossary/device-software/thread-block)

has a unique,
index-based identifier to support assignment of work based on array index.

This hierarchy maps directly onto the

[GPU hardware](/gpu-glossary/device-hardware)

:

[threads](/gpu-glossary/device-software/thread)

execute on individual

[cores](/gpu-glossary/device-hardware/core)

,

[thread blocks](/gpu-glossary/device-software/thread-block)

are scheduled onto

[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor)

, and

[grids](/gpu-glossary/device-software/thread-block-grid)

utilize all available

[SMs](/gpu-glossary/device-hardware/streaming-multiprocessor)

on the device.

[Thread Block Grid](/gpu-glossary/device-software/thread-block-grid)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Memory Hierarchy](/gpu-glossary/device-software/memory-hierarchy)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/device-software/warp
================================================================================

What is a Warp?
===============

A warp is a group of

[threads](/gpu-glossary/device-software/thread)

that are
scheduled together and execute in parallel. All threads in a warp are scheduled
onto a single

[Streaming Multiprocessor (SM)](/gpu-glossary/device-hardware/streaming-multiprocessor)

.
A single

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

typically
executes multiple warps, at the very least all warps from the same

[Cooperative Thread Array](/gpu-glossary/device-software/cooperative-thread-array)

,
aka

[thread block](/gpu-glossary/device-software/thread-block)

.

Warps are the typical unit of execution on a GPU. In normal execution, all

[threads](/gpu-glossary/device-software/thread)

of a warp execute the same
instruction in parallel — the so-called "Single-Instruction, Multiple Thread" or
SIMT model. Warp size is technically a machine-dependent constant, but in
practice it is 32.

When a warp is issued an instruction, the results are generally not available
within a single clock cycle, and so dependent instructions cannot be issued.
While this is most obviously true for fetches from

[global memory](/gpu-glossary/device-software/global-memory)

, which generally

[go off-chip](/gpu-glossary/device-hardware/gpu-ram)

, it is also true for some
arithmetic instructions (see

[the CUDA C++ Programing Guide's "Performance Guidelines"](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#arithmetic-instructions-throughput-native-arithmetic-instructions)

for a table of results per clock cycle for specific instructions).

Instead of waiting for a warp to return results, when multiple warps are
scheduled onto a single

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

, the

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

will select
another warp to execute. This "latency-hiding" is how GPUs achieve high
throughput and ensure work is always available for all of their cores during
execution. For this reason, it is often beneficial to maximize the number of
warps scheduled onto each

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

, ensuring there is
always a warp ready for the

[SM](/gpu-glossary/device-hardware/streaming-multiprocessor)

to run.

Warps are not actually part of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

's
thread group hierarchy. Instead, they are an implementation detail of the
implementation of that model on NVIDIA GPUs. In that way, they are somewhat akin
to

[cache lines](https://www.nic.uoregon.edu/~khuck/ts/acumem-report/manual_html/ch03s02.html)

in CPUs: a feature of the hardware that you don't directly control and don't
need to consider for program correctness, but which is important for achieving
maximum performance.

Warps are named in reference to weaving, "the first parallel thread technology",
according to

[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)

.
The equivalent of warps in other GPU programming models include

[subgroups](https://github.com/gpuweb/gpuweb/pull/4368)

in WebGPU,

[waves](https://microsoft.github.io/DirectX-Specs/d3d/HLSL_SM_6_6_WaveSize.html)

in DirectX, and

[simdgroups](https://developer.apple.com/documentation/metal/compute_passes/creating_threads_and_threadgroups#2928931)

in Metal.

[Thread](/gpu-glossary/device-software/thread)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Cooperative Thread Array](/gpu-glossary/device-software/cooperative-thread-array)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software
================================================================================

Host Software
=============

These terms and technologies are used on the CPU (the "host" in NVIDIA's lingo)
when running GPU programs.

[CUDA (Software Platform)](/gpu-glossary/host-software/cuda-software-platform)

[CUDA C++ (programming language)](/gpu-glossary/host-software/cuda-c)

[NVIDIA GPU Drivers](/gpu-glossary/host-software/nvidia-gpu-drivers)

[nvidia.ko](/gpu-glossary/host-software/nvidia-ko)

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

[libcuda.so](/gpu-glossary/host-software/libcuda)

[NVIDIA Management Library

NVML](/gpu-glossary/host-software/nvml)

[libnvml.so](/gpu-glossary/host-software/libnvml)

[nvidia-smi](/gpu-glossary/host-software/nvidia-smi)

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

[libcudart.so](/gpu-glossary/host-software/libcudart)

[NVIDIA CUDA Compiler Driver

nvcc](/gpu-glossary/host-software/nvcc)

[NVIDIA Runtime Compiler](/gpu-glossary/host-software/nvrtc)

[NVIDIA CUDA Profiling Tools Interface

CUPTI](/gpu-glossary/host-software/cupti)

[NVIDIA Nsight Systems](/gpu-glossary/host-software/nsight-systems)

[CUDA Binary Utilities](/gpu-glossary/host-software/cuda-binary-utilities)

[Global Memory](/gpu-glossary/device-software/global-memory)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA (Software Platform)](/gpu-glossary/host-software/cuda-software-platform)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/cuda-binary-utilities
================================================================================

What are the CUDA Binary Utilities?
===================================

The CUDA Binary Utilities are a collection of tools for examining the contents
of binaries like those output by

`nvcc`

, the

[NVIDIA CUDA Compiler driver](/gpu-glossary/host-software/nvcc)

.

One tool,

`cuobjdump`

, can be used to examine and manipulate the contents of
entire host binaries or of the CUDA-specific

`cubin`

files that are normally
embedded within those binaries.

Another,

`nvidisasm`

, is intended for manipulating

`cubin`

files. It can extract

[SASS assembler](/gpu-glossary/device-software/streaming-assembler)

and
manipulate it, e.g. constructing control flow graphs and mapping assembly
instructions to lines in CUDA program files.

You can find their documentation

[here](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html)

.

[NVIDIA Nsight Systems](/gpu-glossary/host-software/nsight-systems)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Contributors](/gpu-glossary/contributors)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/cuda-c
================================================================================

What is the CUDA C++ programming language?
==========================================

CUDA C++ is an implementation of the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

as an extension of the C++ programming language.

CUDA C++ adds several features to C++ to implement the

[CUDA programming model](/gpu-glossary/device-software/cuda-programming-model)

,
including:

* **[Kernel](/gpu-glossary/device-software/kernel)

  definition**

  with

  **`global`**

  . CUDA

  [kernels](/gpu-glossary/device-software/kernel)

  are
  implemented as C functions that take in pointers and have return type

  `void`

  ,
  annotated with this keyword.

* **[Kernel](/gpu-glossary/device-software/kernel)

  launches**

  with

  **`<<<>>>`**

  .

  [Kernels](/gpu-glossary/device-software/kernel)

  are executed from the CPU host
  using a triple bracket syntax that sets the

  [thread block grid](/gpu-glossary/device-software/thread-block-grid)

  dimensions.

* **[Shared memory](/gpu-glossary/device-software/shared-memory)

  allocation**

  with the

  `shared`

  keyword,

  **barrier synchronization**

  with the

  `__syncthreads()`

  intrinsic function, and

  **[thread block](/gpu-glossary/device-software/thread-block)**

  and

  **[thread](/gpu-glossary/device-software/thread)

  indexing**

  with the

  `blockDim`

  and

  `threadIdx`

  built-in variables.

CUDA C++ programs are compiled by a combination of host C/C++ compiler drivers
like

`gcc`

and the

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

,

`nvcc`

.

For information on how to use CUDA C++ on Modal, see

[this guide](https://modal.com/docs/guide/cuda)

.

[CUDA (Software Platform)](/gpu-glossary/host-software/cuda-software-platform)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[NVIDIA GPU Drivers](/gpu-glossary/host-software/nvidia-gpu-drivers)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/cuda-driver-api
================================================================================

What is the CUDA Driver API?
============================

The

[CUDA Driver API](https://docs.nvidia.com/cuda/cuda-driver-api/index.html)

is the userspace component of the NVIDIA CUDA drivers. It provides utilities
familiar to users of the C standard library: a

`cuMalloc`

function for
allocating

[memory](/gpu-glossary/device-software/global-memory)

on GPU devices,
for example.

Very few CUDA programs are written to directly use the CUDA Driver API. They
instead use the

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

. See

[this section](https://docs.nvidia.com/cuda/cuda-driver-api/driver-vs-runtime-api.html#driver-vs-runtime-api)

of the CUDA Driver API docs.

The CUDA Driver API is generally not linked statically. Instead, it is linked
dynamically, typically under the name

[libcuda.so](/gpu-glossary/host-software/libcuda)

on Linux systems.

The CUDA Driver API is binary-compatible: an application compiled against old
versions of the CUDA Driver API can run on systems with newer versions of the
CUDA Driver API. That is, the operating system's binary loader may load a newer
version of the CUDA Driver API and the program will function the same.

For details on distributing CUDA C applications, see the

[CUDA C/C++ Best Practices Guide](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide)

from NVIDIA.

The CUDA Driver API is closed source. You can find its documentation

[here](https://docs.nvidia.com/cuda/cuda-driver-api/index.html)

.

Though they are not commonly used, there are projects that attempt to provide or
use open source alternatives to the CUDA Driver API, like

[LibreCuda](https://github.com/mikex86/LibreCuda)

and

[tinygrad](https://github.com/tinygrad)

. See

[their source code](https://github.com/tinygrad/tinygrad/blob/77f7ddf62a78218bee7b4f7b9ff925a0e581fcad/tinygrad/runtime/ops_nv.py)

for details.

[nvidia.ko](/gpu-glossary/host-software/nvidia-ko)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[libcuda.so](/gpu-glossary/host-software/libcuda)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/cuda-runtime-api
================================================================================

What is the CUDA Runtime API?
=============================

The CUDA Runtime API wraps the

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

and provides a
higher-level API for the same functions.

It is generally preferred over the

[Driver API](/gpu-glossary/host-software/cuda-driver-api)

for better ergonomics,
but there are some small caveats around control of kernel launches and context
management. See

[this section](https://docs.nvidia.com/cuda/cuda-runtime-api/driver-vs-runtime-api.html#driver-vs-runtime-api)

of the CUDA Runtime API docs for more.

While the Runtime API may be statically linked, per

[Attachment A of the NVIDIA CUDA Toolkit EULA](https://docs.nvidia.com/cuda/eula/index.html#attachment-a)

,
it does not have to be. The shared object file for dynamic linking is usually
named

[libcudart.so](/gpu-glossary/host-software/libcudart)

on Linux systems.

The CUDA Runtime API is closed source. You can find its documentation

[here](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html)

.

[nvidia-smi](/gpu-glossary/host-software/nvidia-smi)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[libcudart.so](/gpu-glossary/host-software/libcudart)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/cuda-software-platform
================================================================================

What is the CUDA Software Platform?
===================================

CUDA stands for

*Compute Unified Device Architecture*

. Depending on the context,
"CUDA" can refer to multiple distinct things: a high-level device architecture,
a parallel programming model for architectures with that design, or a software
platform that extends high-level languages like C to add that programming model.

The vision for CUDA is laid out in the

[Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf)

white paper. We highly recommend this paper, which is the original source for
many claims, diagrams, and even specific turns of phrase in NVIDIA's
documentation.

Here, we focus on the CUDA

*software platform*

.

The CUDA software platform is a collection of software for developing CUDA
programs. Though CUDA software platforms exist for other languages, like
FORTRAN, we will focus on the dominant

[CUDA C++](/gpu-glossary/host-software/cuda-c)

version.

This platform can be roughly divided into the components used to

*build*

applications, like the

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

toolchain, and
the components used

*within*

or

*from*

applications, like the

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

and the

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

, diagrammed
below.

[Host Software](/gpu-glossary/host-software)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA C++ (programming language)](/gpu-glossary/host-software/cuda-c)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/cupti
================================================================================

What is the NVIDIA CUDA Profiling Tools Interface?
==================================================

CUPTI

The NVIDIA CUDA Profiling Tools Interface (CUPTI) provides a set of APIs for
profiling execution of

[CUDA C++](/gpu-glossary/host-software/cuda-c)

,

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

, and

[SASS](/gpu-glossary/device-software/streaming-assembler)

code on GPUs.
Critically, it synchronizes timestamps across the CPU host and the GPU device.

CUPTI's interfaces are consumed by, for example, the NSight Profiler and the

[PyTorch Profiler](/docs/examples/torch_profiling)

.

You can find its documentation

[here](https://docs.nvidia.com/cupti/)

.

For details on using profiling tools for GPU applications running on Modal, see

[this example from our documentation](/docs/examples/torch_profiling)

.

[NVIDIA Runtime Compiler](/gpu-glossary/host-software/nvrtc)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[NVIDIA Nsight Systems](/gpu-glossary/host-software/nsight-systems)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/libcuda
================================================================================

What is libcuda.so?
===================

The typical name for the binary shared object file that implements the

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

on Linux systems.
It is dynamically linked by CUDA programs. If it is missing, the drivers are
generally improperly installed.

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[NVIDIA Management Library](/gpu-glossary/host-software/nvml)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/libcudart
================================================================================

What is libcudart.so?
=====================

The typical name for the binary shared object file that implements the

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

on Linux
systems. Deployed CUDA binaries often statically link this file, but libraries
and frameworks built on the CUDA Toolkit, like PyTorch, typically load it
dynamically.

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/libnvml
================================================================================

What is libnvml.so?
===================

The typical name for the binary shared object file that implements the features
of

[NVML](/gpu-glossary/host-software/nvml)

on Linux systems.

[NVIDIA Management Library](/gpu-glossary/host-software/nvml)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[nvidia-smi](/gpu-glossary/host-software/nvidia-smi)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/nsight-systems
================================================================================

What is NVIDIA Nsight Systems?
==============================

NVIDIA Nsight Systems is a performance debugging tool for

[CUDA C++](/gpu-glossary/host-software/cuda-c)

programs. It combines profiling,
tracing, and expert systems analysis in a GUI.

No one wakes up and says "today I want to write a program that runs on a hard to
use, expensive piece of hardware using a proprietary software stack". Instead,
GPUs are selected when normal computing hardware doesn't perform well enough to
solve a computing problem. So almost all GPU programs are performance-sensitive,
and the performance debugging workflows supported by Nsight Systems or other
tools built on top of the

[CUDA Profiling Tools Interface](/gpu-glossary/host-software/cupti)

are
mission-critical.

You can find its documentation

[here](https://docs.nvidia.com/nsight-systems/index.html)

, but

[watching someone use the tool](https://www.youtube.com/watch?v=dUDGO66IadU)

is
usually more helpful. For details on how to profile GPU applications on Modal,
see

[our documentation](https://modal.com/docs/examples/torch_profiling)

.

[NVIDIA CUDA Profiling Tools Interface](/gpu-glossary/host-software/cupti)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA Binary Utilities](/gpu-glossary/host-software/cuda-binary-utilities)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/nvcc
================================================================================

What is the NVIDIA CUDA Compiler Driver?
========================================

nvcc

The NVIDIA CUDA Compiler Driver is a toolchain for compiling

[CUDA C](/gpu-glossary/host-software/cuda-c)

/C++ programs. It outputs binary
executables that conform to the host ABI and include

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

and/or

[SASS](/gpu-glossary/device-software/streaming-assembler)

to be executed on the
GPU — a so-called "fat binary". These binaries are inspectable with the same
tools used for other binaries, like

`readelf`

on Linux, but can be additionally
manipulated with the specialized

[CUDA Binary Utilities](/gpu-glossary/host-software/cuda-binary-utilities)

.

The included

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

code
is versioned by

[Compute Capability](/gpu-glossary/device-software/compute-capability)

,
configured by passing

`compute_XYz`

values to the

`--gpu-architecture`

or

`--gpu-code`

options.

The included

[SASS](/gpu-glossary/device-software/streaming-assembler)

code is
versioned by

[SM architecture version](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

,
configured by passing

`sm_XYz`

values to the

`--gpu-architecture`

or

`--gpu-code`

options. Passing

`compute_XYz`

to

`--gpu-code`

will also trigger
the generation of

[SASS](/gpu-glossary/device-software/streaming-assembler)

code
with the same version as the

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

.

Compilation of host/CPU code is done using the host system's compiler driver,
e.g. the

`gcc`

compiler driver. Note that compiler drivers are not to be
confused with hardware drivers, like the

[NVIDIA GPU Drivers](/gpu-glossary/host-software/nvidia-gpu-drivers)

.

The documentation for

`nvcc`

can be found

[here](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/)

.

[libcudart.so](/gpu-glossary/host-software/libcudart)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[NVIDIA Runtime Compiler](/gpu-glossary/host-software/nvrtc)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/nvidia-gpu-drivers
================================================================================

What are the NVIDIA GPU Drivers?
================================

The NVIDIA GPU drivers mediate the interaction between host programs or the host
operating system and the GPU device. The primary interfaces to the GPU drivers
for applications are, in order, the

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

and the

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

.

NVIDIA has released the

[source](https://github.com/NVIDIA/open-gpu-kernel-modules)

for their Linux Open
GPU

[Kernel Module](/gpu-glossary/host-software/nvidia-ko)

.

[CUDA C++ (programming language)](/gpu-glossary/host-software/cuda-c)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[nvidia.ko](/gpu-glossary/host-software/nvidia-ko)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/nvidia-ko
================================================================================

What is nvidia.ko?
==================

`nvidia.ko`

is a binary

[kernel module](https://wiki.archlinux.org/title/Kernel_module)

file at the core
of the

[NVIDIA GPU drivers](/gpu-glossary/host-software/nvidia-gpu-drivers)

for
Linux.

Like other kernel modules, it executes in privileged mode and communicates
directly with hardware on behalf of the user -- in this case, the GPU.

The Linux Open GPU Kernel Module is

[open source](https://github.com/NVIDIA/open-gpu-kernel-modules)

.

[NVIDIA GPU Drivers](/gpu-glossary/host-software/nvidia-gpu-drivers)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/nvidia-smi
================================================================================

What is nvidia-smi?
===================

This command line utility is used to query and manage the state of the GPU
exposed by the

[NVML](/gpu-glossary/host-software/nvml)

management libraries.
Its outputs, a sample of which appears below, are familiar to users of NVIDIA
GPUs to the point of being a

[meme](https://x.com/boborado/status/1752724223934578760)

.

`nvidia-smi`

reports the following:

* GPU identity information like the card's model name, a UUID, and the PCI ID

* live utilization metrics for kernel execution time and memory allocation

* live power and thermal information

For details on these metrics, including how to interpret power and thermal
readings, see

[this page on the Modal docs](/docs/guide/gpu-metrics)

.

`nvidia-smi`

can also list processes currently using the GPU (

`-q`

,

`--query`

,

`pmon`

). Common management tasks include setting persistence mode (

`-pm`

),
compute mode (

`-c`

), power limits (

`-pl`

), application/locked clocks (

`-ac`

,

`-lgc`

,

`-lmc`

), and performing GPU resets (

`-r`

).

Output can be formatted as human-readable text or XML (

`-x`

). While

`nvidia-smi`

's text output format is not guaranteed to be stable, the underlying

[NVML C library](/gpu-glossary/host-software/nvml)

offers a stable API for tool
development.

The documentation for

`nvidia-smi`

can be found

[here](https://docs.nvidia.com/deploy/nvidia-smi/)

, and the official Python
bindings can be found

[here](http://pypi.python.org/pypi/nvidia-ml-py/)

.

```
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 80GB HBM3          On  |   00000000:53:00.0 Off |                    0 |
| N/A   25C    P0             92W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   1  NVIDIA H100 80GB HBM3          On  |   00000000:64:00.0 Off |                    0 |
| N/A   27C    P0             93W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   2  NVIDIA H100 80GB HBM3          On  |   00000000:75:00.0 Off |                    0 |
| N/A   26C    P0             96W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   3  NVIDIA H100 80GB HBM3          On  |   00000000:86:00.0 Off |                    0 |
| N/A   27C    P0             93W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   4  NVIDIA H100 80GB HBM3          On  |   00000000:97:00.0 Off |                    0 |
| N/A   27C    P0             95W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   5  NVIDIA H100 80GB HBM3          On  |   00000000:A8:00.0 Off |                    0 |
| N/A   25C    P0             91W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   6  NVIDIA H100 80GB HBM3          On  |   00000000:B9:00.0 Off |                    0 |
| N/A   26C    P0             91W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
|   7  NVIDIA H100 80GB HBM3          On  |   00000000:CA:00.0 Off |                    0 |
| N/A   24C    P0             91W /  700W |       1MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

```

[libnvml.so](/gpu-glossary/host-software/libnvml)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[CUDA Runtime API](/gpu-glossary/host-software/cuda-runtime-api)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/nvml
================================================================================

What is the NVIDIA Management Library?
======================================

NVML

The NVIDIA Management Library (NVML) is used for monitoring and managing the
state of NVIDIA GPUs. It exposes, for example, the power draw and temperature of
the GPU, the allocated memory, and the device's power limit and power limiting
state.

The function of NVML are frequently accessed via the

[nvidia-smi](/gpu-glossary/host-software/nvidia-smi)

command line utility, but
are also accessible to programs via wrappers, like

[pynvml in Python](https://pypi.org/project/pynvml/)

and

[nvml\_wrapper in Rust](https://docs.rs/nvml-wrapper/latest/nvml_wrapper/)

.

[libcuda.so](/gpu-glossary/host-software/libcuda)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[libnvml.so](/gpu-glossary/host-software/libnvml)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/host-software/nvrtc
================================================================================

What is the NVIDIA Runtime Compiler?
====================================

nvrtc

The NVIDIA Runtime Compiler (

`nvrtc`

) is a runtime compilation library for CUDA
C. It compiles

[CUDA C++](/gpu-glossary/host-software/cuda-c)

to

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

without requiring
a separate launch of the

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

(

`nvcc`

) in
another process. It is used by some libraries or frameworks to, for example, map
generated C/C++ code to

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

code that can run
on a GPU.

Note that this

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

is
then further JIT-compiled from the

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

IR to the

[SASS assembly](/gpu-glossary/device-software/streaming-assembler)

. This is done
by the

[NVIDIA GPU drivers](/gpu-glossary/host-software/nvidia-gpu-drivers)

and
is distinct from the compilation done by NVRTC. CUDA binaries that contain

[PTX](/gpu-glossary/device-software/parallel-thread-execution)

, as required for
forward compatibility, also pass through this compilation step.

NVRTC is closed source. You can find its documentation

[here](https://docs.nvidia.com/cuda/nvrtc/index.html)

.

[NVIDIA CUDA Compiler Driver](/gpu-glossary/host-software/nvcc)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[NVIDIA CUDA Profiling Tools Interface](/gpu-glossary/host-software/cupti)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/gpu-glossary/readme
================================================================================

README
======

```
 ██████╗ ██████╗ ██╗   ██╗
██╔════╝ ██╔══██╗██║   ██║
██║  ███╗██████╔╝██║   ██║
██║   ██║██╔═══╝ ██║   ██║
╚██████╔╝██║     ╚██████╔╝
 ╚═════╝ ╚═╝      ╚═════╝
 ██████╗ ██╗      ██████╗ ███████╗███████╗ █████╗ ██████╗ ██╗   ██╗
██╔════╝ ██║     ██╔═══██╗██╔════╝██╔════╝██╔══██╗██╔══██╗╚██╗ ██╔╝
██║  ███╗██║     ██║   ██║███████╗███████╗███████║██████╔╝ ╚████╔╝
██║   ██║██║     ██║   ██║╚════██║╚════██║██╔══██║██╔══██╗  ╚██╔╝
╚██████╔╝███████╗╚██████╔╝███████║███████║██║  ██║██║  ██║   ██║
 ╚═════╝ ╚══════╝ ╚═════╝ ╚══════╝╚══════╝╚═╝  ╚═╝╚═╝  ╚═╝   ╚═╝

```

We wrote this glossary to solve a problem we ran into working with GPUs here at

[Modal](/)

: the documentation is fragmented, making it difficult to connect
concepts at different levels of the stack, like

[Streaming Multiprocessor Architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)

,

[Compute Capability](/gpu-glossary/device-software/compute-capability)

, and

[nvcc compiler flags](/gpu-glossary/host-software)

.

So we've read the

[PDFs from NVIDIA](https://docs.nvidia.com/cuda/pdf/PTX_Writers_Guide_To_Interoperability.pdf)

,
lurked in the

[good Discords](https://discord.gg/gpumode)

, and even bought

[dead-tree textbooks](https://www.amazon.com/Professional-CUDA-Programming-John-Cheng/dp/1118739329)

to put together a glossary that spans the whole stack in one place.

This glossary, unlike a PDF or a Discord or a book, is a

*hypertext document*

--
all pages are inter-linked with one another, so you can jump down to read about
the

[Warp Scheduler](/gpu-glossary/device-hardware/warp-scheduler)

so you can
better understand the

[threads](/gpu-glossary/device-software/thread)

that you
came across in the article on the

[CUDA programming model](/gpu-glossary/host-software/cuda-c)

.

You can also read it linearly. To navigate between pages, use the arrow keys,
the arrows at the bottom of each page, or the table of contents (in the sidebar
on desktop or in the hamburger menu on mobile).

The source for the glossary is available

[on GitHub](https://github.com/modal-labs/gpu-glossary)

.

[Home](/gpu-glossary)

Something seem wrong?

Or want to contribute?

Click this
button to

let us know on GitHub.

[Device Hardware](/gpu-glossary/device-hardware)

[?](https://github.com/modal-labs/gpu-glossary/issues/new)


================================================================================
SOURCE URL: https://modal.com/legal/privacy-policy
================================================================================

Privacy Policy
==============

*Last updated May 17th, 2023.*

Privacy Policy of Modal Labs
----------------------------

Modal Labs operates the modal.com website, which provides a cloud
computing service.

This page is used to inform website visitors regarding our policies with
the collection, use, and disclosure of Personal Information if anyone
decided to use our Service, the modal.com website. If you choose to use
our Service, then you agree to the collection and use of information in
relation with this policy. The Personal Information that we collect are
used for providing and improving the Service. We will not use or share
your information with anyone except as described in this Privacy Policy.
The terms used in this Privacy Policy have the same meanings as in our
Terms and Conditions, which is accessible at Website URL, unless otherwise
defined in this Privacy Policy.

### Information Collection and Use

For a better experience while using our Service, we may require you to
provide us with certain personally identifiable information, including but
not limited to your name, phone number, and postal address. The
information that we collect will be used to contact or identify you.

### Log Files and Data

We want to inform you that whenever you visit our Service, we collect information
that your browser sends to us that is called Log Data. This Log Data may include
information such as your computer's Internet Protocol (“IP”) address, browser
version, pages of our Service that you visit, the time and date of your visit,
the time spent on those pages, and other statistics.

### Cookies

Cookies are files with small amount of data that is commonly used an
anonymous unique identifier. These are sent to your browser from the
website that you visit and are stored on your computer's hard drive. Our
website uses these “cookies” to collection information and to improve our
Service. You have the option to either accept or refuse these cookies, and
know when a cookie is being sent to your computer. If you choose to refuse
our cookies, you may not be able to use some portions of our Service.

### Service Providers

We may employ third-party companies and individuals due to the following
reasons:

* To facilitate our Service;
* To provide the Service on our behalf;
* To perform Service-related services; or
* To assist us in analyzing how our Service is used.

We want to inform our Service users that these third parties have access
to your Personal Information. The reason is to perform the tasks assigned
to them on our behalf. However, they are obligated not to disclose or use
the information for any other purpose.

### Security

We value your trust in providing us your Personal Information, thus we are
striving to use commercially acceptable means of protecting it. But
remember that no method of transmission over the internet, or method of
electronic storage is 100% secure and reliable, and we cannot guarantee
its absolute security.

### Links to Other Sites

Our Service may contain links to other sites. If you click on a
third-party link, you will be directed to that site. Note that these
external sites are not operated by us. Therefore, we strongly advise you
to review the Privacy Policy of these websites. We have no control over,
and assume no responsibility for the content, privacy policies, or
practices of any third-party sites or services.

### Children's Privacy

Our Services do not address anyone under the age of 13. We do not
knowingly collect personal identifiable information from children under
13. In the case we discover that a child under 13 has provided us with
personal information, we immediately delete this from our servers. If you
are a parent or guardian and you are aware that your child has provided us
with personal information, please contact us so that we will be able to do
necessary actions.

### Changes to This Privacy Policy

We may update our Privacy Policy from time to time. The date of last
update is displayed at the top of this policy document. Thus, we advise
you to review this page periodically for any changes. We will notify you
of any changes by posting the new Privacy Policy on this page. These
changes are effective immediately, after they are posted on this page.

### Contact Us

If you have any questions or suggestions about our Privacy Policy, do not
hesitate to contact us.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/legal/terms
================================================================================

MODAL TERMS OF SERVICE
======================

*Last updated March 26th, 2024.*

PLEASE READ THESE TERMS AND CONDITIONS CAREFULLY BEFORE USING THE SERVICE
OFFERED BY MODAL LABS, INC. (“
**MODAL**
”). BY MUTUALLY
EXECUTING ONE OR MORE ORDER FORMS WITH COMPANY WHICH REFERENCE THESE TERMS
(EACH, A “
**SERVICE**
**ORDER**
”) OR BY
ACCESSING OR USING THE SERVICES IN ANY MANNER, YOU (“
**YOU**
”
OR “
**CUSTOMER**
”) AGREE TO BE BOUND BY THESE TERMS (TOGETHER
WITH ALL SERVICE DESCRIPTIONS AND/OR ORDER FORMS, IF ANY, THE “
**AGREEMENT**
”) TO THE EXCLUSION OF ALL OTHER TERMS. YOU REPRESENT AND WARRANT THAT
YOU HAVE THE AUTHORITY TO ENTER INTO THIS AGREEMENT; IF YOU ARE ENTERING
INTO THIS AGREEMENT ON BEHALF OF AN ORGANIZATION OR ENTITY, REFERENCES TO
“CUSTOMER” AND “YOU” IN THIS AGREEMENT, EXCEPT THIS SENTENCE, REFER TO
THAT ORGANIZATION OR ENTITY. IF YOU DO NOT AGREE TO ALL OF THE FOLLOWING,
YOU MAY NOT USE OR ACCESS THE SERVICES IN ANY MANNER. IF THE TERMS OF THIS
AGREEMENT ARE CONSIDERED AN OFFER, ACCEPTANCE IS EXPRESSLY LIMITED TO SUCH
TERMS.

1. **SCOPE OF SERVICE AND RESTRICTIONS**
   1. **Access and Scope of Service**
      . Subject to Customer’s
      compliance with the terms and conditions of the Agreement, including
      use in accordance with the applicable Service Description and, if
      applicable, Modal’s receipt of the applicable Fees with respect to
      the service specified in the corresponding Service Order (the “
      **Service**
      ”), Modal will use commercially reasonable efforts to make the
      Service available to Customer as set forth in this Agreement and the
      Service Description and/or Service Order. Subject to Customer’s
      compliance with the terms and conditions of the Agreement, Customer
      may access and use the Service during the period specified in the
      Service Description or Service Order, if applicable. Any such use of
      the Service by Customer is solely for Customer’s internal business.
      Notwithstanding the foregoing, Customer may use the Service with
      respect to third party data, and such third-party data shall be
      considered Customer Data.
   2. **Service Description**
      . Customer’s use of the Services
      shall at all times be limited and subject to the use limitations and
      authorizations as specified within Modal’s published listing
      currently located at the following URL
      <https://modal.com/pricing>
      for the Service (the “
      **Service Description**
      ”).
   3. **Modifications**
      . In the absence of any conflicting
      terms defined within a committed Service Order, Customer
      acknowledges and agrees that Modal may discontinue or terminate the
      Service at any time and for any reason or modify the applicable
      terms by publishing a notice on the Modal website or by other means
      to the extent required by applicable law.
   4. **Trial Use**
      . If Customer is accessing or making use
      of the Service on a trial basis (the “
      **Trial Use**
      ”)
      as identified in the corresponding Service Order, or as presented to
      Customer in connection with such trial basis or per an applicable
      Service Description (the “
      **Trial Use Limitations**
      ”),
      Customer may use the Service consistent with the Trial Use
      Limitations provided such use does not to exceed the Service levels
      or related entitlements set forth in the Trial Use Limitations.
      Customer acknowledges and agrees that the Trial Use is provided on
      an “as-is” basis and the Trial Use is provided without any
      indemnification, support, warranties or representation of any kind.
      Further, Trial Use may be subject to certain additional
      restrictions, limitations and differing terms all as specified in
      the corresponding Trial Use Limitations.
   5. **Restrictions**
      . Customer will use the Service only in
      accordance with all applicable laws, including, but not limited to,
      laws related to data (whether applicable within the United States,
      the European Union, or otherwise). Customer agrees not to (and will
      not allow any third party to): (i) remove or otherwise alter any
      proprietary notices or labels from the Service or any portion
      thereof; (ii) reverse engineer, decompile, disassemble, or otherwise
      attempt to discover the underlying structure, ideas, or algorithms
      of the Service or any software used to provide or make the Service
      available; or (iii) rent, resell or otherwise allow any third-party
      direct access to or use of the Service.
   6. **Ownership**
      . Modal retains all right, title, and
      interest in and to the Service, and any software, products, works or
      other intellectual property created, used, provided or made
      available by Modal under or in connection with the Service. Customer
      may from time to time provide suggestions, comments or other
      feedback to Modal with respect to the Service (“
      **Feedback”**
      ). Customer hereby grants to Modal a nonexclusive, worldwide,
      perpetual, irrevocable, transferable, sublicensable, royalty-free,
      fully paid-up license to use and exploit any Feedback for any
      purpose. Nothing in this Agreement will impair Modal’s right to
      develop, acquire, license, market, promote or distribute products,
      software or technologies that perform the same or similar functions
      as, or otherwise compete with any products, software or technologies
      that Customer may develop, produce, market, or distribute.
   7. **Software**
      . Customer acknowledges and agrees that no
      software code with respect to the Service will be provided to
      Customer hereunder, and that certain software libraries and tools,
      and updates thereto, are necessary to access and use the Service
      (the “
      **Modal Tools**
      ”). The Modal Tools should be
      available at the following URL:
      <https://github.com/modal-labs/modal-client>
      under applicable open source licensing terms. Customer agrees that
      it is responsible for obtaining, installing and maintaining the Modal
      Tools, and that Modal makes no representations, warranties or is otherwise
      liable or obligated hereunder with respect to such Modal tools.
   8. **Customer Data**
      . Customer is solely responsible for
      Customer Data including, but not limited to: (a) compliance with all
      applicable laws and regulations; and (b) any claims that Customer
      Data infringes, misappropriates, or otherwise violates the rights of
      any third party. Customer is responsible for the use of the Service
      by any person to whom Customer has given access to the Service, even
      if Customer did not authorize such use. Customer agrees and
      acknowledges that Customer Data may be irretrievably deleted if
      Customer’s account is terminated.
   9. **Use of Customer Data**
      . Customer hereby grants to
      Modal a limited license to use Customer Data as necessary to provide
      the Service to Customer. For purposes of this Agreement, “
      **Customer Data**
      ” shall mean any data, information or other material provided,
      uploaded, or submitted by Customer to the Service in the course of
      using the Service. Customer shall retain all right, title and
      interest in and to the Customer Data, including all intellectual
      property rights therein. Notwithstanding anything to the contrary
      herein, in no event will Modal sell or share any personal data to
      the extent included within Customer Data (including but not limited
      to any data inputs or outputs resulting from an authorized user's
      use of the Service) to any third party for marketing purposes.
   10. **Aggregated De-Identified Data**
       . Notwithstanding
       anything to the contrary, Modal may freely use Aggregated
       De-identified Data for Modal’s business purposes (including without
       limitation, for purposes of improving, testing, operating, promoting
       and marketing Modal’s current and future products and services). “
       **Aggregated De-identified Data**
       ” means data collected by Modal in connection with Customer’s use
       of the Service, but only in aggregate, de-identified form which is
       not linked specifically to Customer or any individual, excluding
       Customer Data uploaded or submitted by Customer.
   11. **Personal Data.**
       Customer represents and warrants
       that it will not provide access, transfer or otherwise make
       available to Modal any personally identifiable information or
       personal data subject to applicable law or regulation (“
       **Subject Data**
       ”). Customer acknowledges and agrees that Subject Data is not
       necessary for Customer to use the Service.
   12. **Service Suspension**
       . Modal may suspend Customer’s
       access to or use of the Service as follows: (a) immediately if Modal
       reasonably believes Customer’s use of the Service may pose a
       security risk to or may adversely impact the Service; (b)
       immediately if Modal reasonably believes Customer is or has used the
       Service in connection with any of the following: cryptocurrency
       mining or related blockchain related activities, denial of service
       attacks, peer-to-peer file sharing, or as a general file-hosting or
       media-serving platform; (c) immediately if Customer become
       insolvent, has ceased to operate in the ordinary course, made an
       assignment for the benefit of creditors, or becomes the subject of
       any bankruptcy, reorganization, liquidation, dissolution or similar
       proceeding; (d) following thirty (30) days written notice if
       Customer is in breach of this Agreement or any Service Description
       or Service Order (and has not cured such breach, if curable, within
       the thirty (30) days of such notice); or (e) Customer has failed to
       pay Modal the Fees with respect to the Service. If any amount owing
       by Customer is thirty (30) or more days overdue (or 10 or more days
       overdue in the case of invoices to be paid by credit card), Modal
       may, without limiting any rights and remedies, accelerate Customer’s
       unpaid fee obligations to become immediately due and payable, and
       suspend the provision of the Service to Customer until the overdue
       amounts are paid in full. Modal will give Customer at least ten (10)
       days’ prior notice that its account is overdue before suspending
       services to Customer due to overdue amounts.
   13. **Data Transfer**
       . For clarity, Customer understands
       and agrees that by using the Services, Customer is explicitly
       consenting to the processing and transfer of Customer Data (which
       may constitute Personal Data) within and to the United States,
       Canada, the European Economic Area, Australia, and Asia.
2. **FEES AND TAXES**
   1. **Fees**
      . Customer shall pay to Modal the fees as set
      forth in each applicable Service Order(s) or Service Description
      (collectively, the “Fees”) and will provide accurate and updated
      billing contact information. Minimum commitments as set forth in
      Service Orders and Service Descriptions are (a) based on the Service
      purchased and not actual usage, unless otherwise defined within a
      Service Description; (b) non-cancelable; and (c) cannot be decreased
      during the specified term set forth in an applicable Service Order.
      Fees are not refundable. To the extent defined within a Service
      Description, Customer may be auto-billed by Modal following each
      billing period.
   2. **Invoicing Terms.**
      Modal will invoice Customer either
      monthly or according to the billing frequency stated in the applicable
      Service Description or Service Order. Invoices are due pursuant to the
      corresponding Service Description or Service Order. If any invoiced amount
      is not received by Modal by the due date, then without limiting Modal’s
      rights or remedies: (a) those charges may accrue late interest at the
      rate of 1.5% of the outstanding balance per month, or the maximum rate
      permitted by law, whichever is lower, and (b) Modal may condition future
      renewals and Service Orders on shorter payment terms. If Modal is required
      to initiate legal action due to nonpayment of fees, Customer shall bear
      all costs resulting from the collection of such fees.
   3. **Taxes**
      . Any and all payments made by Modal in
      accordance with this Agreement are exclusive of any taxes that might
      be assessed against Customer by any jurisdiction. Customer shall pay
      or reimburse Modal for all value-added, sales, use, property and
      similar taxes; all customs duties, import fees, stamp duties,
      license fees and similar charges; and all other mandatory payments
      to government agencies of whatever kind, except taxes imposed on the
      net or gross income of Modal. All amounts payable to Modal under
      this Agreement shall be without set-off and without deduction of any
      taxes, levies, imposts, charges, withholdings and/or duties of any
      nature which may be levied or imposed, including without limitation,
      value added tax, customs duty and withholding tax.
3. **TERM AND TERMINATION**
   1. **Term**
      . The term of this Agreement shall commence on
      the Effective and unless terminated earlier according to this
      Section 3, will end on the last day of the term specified in an
      applicable Service Description or last Service Order (the “
      **Term**
      ”). Each Service Description or Service Order will renew
      automatically at the end of the applicable term unless either party
      provides to the other advance written notice with respect to
      non-renewal at least thirty (30) days prior to the end of the then
      current term.
   2. **Termination**
      . This Agreement and the applicable
      Service Description or Service Orders hereunder may be terminated:
      (a) by either party if the other has materially breached this
      Agreement, within thirty (30) calendar days after written notice of
      such breach to the other party if the breach is remediable or
      immediately upon notice if the breach is not remediable; or (b) by
      Modal upon written notice to Customer if Customer (i) has made or
      attempted to make any assignment for the benefit of its creditors or
      any compositions with creditors, (ii) has any action or proceedings
      under any bankruptcy or insolvency laws taken by or against it which
      have not been dismissed within sixty (60) days, (iii) has effected a
      compulsory or voluntary liquidation or dissolution, or (iv) has
      undergone the occurrence of any event analogous to any of the
      foregoing under the law of any jurisdiction.
   3. **Effect of Termination**
      . Upon any expiration or
      termination of this Agreement, Customer shall (i) immediately cease
      use of the Service, and (ii) return all Modal Confidential
      Information and other materials and information provided by Modal.
      Any termination or expiration shall not relieve Customer of its
      obligation to pay all Fees accruing prior to termination. If the
      Agreement is terminated due to Section 3.2 (a), Customer shall pay
      to Modal all Fees set forth in the corresponding Service Order(s).
   4. **Survival.**
      The following provisions will survive termination
      of this Agreement: Sections 1.4 (Ownership), 3.3 (Effect of Termination),
      Section 3.4 (Survival), Section 4 (Confidentiality), Section 7 (Limitation
      of Liability), Section 8 (Miscellaneous).
4. **CONFIDENTIALITY**

   During the term of this Agreement, either party may provide the other
   party with confidential and/or proprietary materials and information (
   ***“*
   Confidential Information
   *”***
   ). All materials and information provided by the disclosing party and
   identified at the time of disclosure as “Confidential” or bearing a
   similar legend, and all other information that the receiving party
   reasonably should have known was the Confidential Information of the
   disclosing party, shall be considered Confidential Information. This
   Agreement is Confidential Information, and all pricing terms are Modal
   Confidential Information. The receiving party shall maintain the
   confidentiality of the Confidential Information and will not disclose
   such information to any third party without the prior written consent
   of the disclosing party. The receiving party will only use the
   Confidential Information internally for the purposes contemplated
   hereunder. The obligations in this Section shall not apply to any
   information that: (a) is made generally available to the public
   without breach of this Agreement, (b) is developed by the receiving
   party independently from and without reference to the Confidential
   Information, (c) is disclosed to the receiving party by a third party
   without restriction, or (d) was in the receiving party’s lawful
   possession prior to the disclosure and was not obtained by the
   receiving party either directly or indirectly from the disclosing
   party. The receiving party may disclose Confidential Information as
   required by law or court order; provided that, the receiving party
   provides the disclosing with prompt written notice thereof and uses
   the receiving party’s best efforts to limit disclosure. At any time,
   upon the disclosing party’s written request, the receiving party shall
   return to the disclosing party all disclosing party’s Confidential
   Information in its possession, including, without limitation, all
   copies and extracts thereof.
5. **INDEMNIFICATION**
   1. **Indemnification by Customer**
      . Customer will defend,
      indemnify, and hold Modal, its affiliates, suppliers and licensors
      harmless and each of their respective officers, directors, employees
      and representatives from and against any claims, damages, losses,
      liabilities, costs, and expenses (including reasonable attorneys’
      fees) arising out of or relating to any third party claim with
      respect to: (a) Customer Data; (b) breach of this Agreement or
      violation of applicable law by Customer; or (c) alleged infringement
      or misappropriation of third-party’s intellectual property rights
      resulting from Customer Data.
   2. **Indemnification by Modal**
      . Modal will defend,
      indemnify, and hold Customer harmless from and against any third
      party claims, damages, losses, liabilities, costs, and expenses
      (including reasonable attorneys’ fees) arising from claims by a
      thirty party that Customer’s use of the Service directly infringes
      or misappropriates a third party’s United States (or Berne
      Convention signatory country) intellectual property rights (an “
      **Infringement Claim**
      ”). Notwithstanding any other provision in this Agreement, Modal
      shall have no obligation to indemnify or reimburse Customer with
      respect to any Infringement Claim to the extent arising from: (a)
      the combination of any Customer Data with the Service; (b) the
      combination of any products or services, other than those provided
      by Modal to Customer under this Agreement, with the Service; or (c)
      non-discretionary designs or specifications provided to Modal by
      Customer that caused such Infringement Claim. Customer agrees to
      reimburse Modal for any and all damages, losses, costs and expenses
      incurred as a result of any of the foregoing actions.
   3. **Notice of Claim and Indemnity Procedure**
      . In the
      event of a claim for which a party seeks indemnity or reimbursement
      under this Section 5 (each an “
      **Indemnified Party**
      ”)
      and as conditions of the indemnity, the Indemnified Party shall: (a)
      notify the indemnifying party in writing as soon as practicable, but
      in no event later than thirty (30) days after receipt of such claim,
      together with such further information as is necessary for the
      indemnifying party to evaluate such claim; and (b) the Indemnified
      Party allows the indemnifying party to assume full control of the
      defense of the claim, including retaining counsel of its own
      choosing. Upon the assumption by the indemnifying party of the
      defense of a claim with counsel of its choosing, the indemnifying
      party will not be liable for the fees and expenses of additional
      counsel retained by any Indemnified Party. The Indemnified Party
      shall cooperate with the indemnifying party in the defense of any
      such claim. Notwithstanding the foregoing provisions, the
      indemnifying party shall have no obligation to indemnify or
      reimburse for any losses, damages, costs, disbursements, expenses,
      settlement liability of a claim or other sums paid by any
      Indemnified Party voluntarily, and without the indemnifying party’s
      prior written consent, to settle a claim. Subject to the maximum
      liability set forth in Section 7, the provisions of this Section 5
      constitute the entire understanding of the parties regarding each
      party’s respective liability under this Section 5, including but not
      limited to Infringement Claims (including related claims for breach
      of warranty) and each party’s sole obligation to indemnify and
      reimburse any Indemnified Party.
6. **WARRANTY**
   1. **Warranty.**
      The Service, when used by Customer in accordance
      with the provisions of this Agreement and in compliance with the applicable
      Documentation, will perform, in all material respects, the functions
      described in the Documentation during the Term.
   2. **Exclusive Remedies.**
      Customer shall report to Modal,
      pursuant to the notice provision of this Agreement, any breach of the
      warranties set forth in this Section 6. In the event of a breach of warranty
      by Modal under this Agreement, Customer’s sole and exclusive remedy,
      and Modal’s entire liability, shall be prompt correction of any material
      non-conformance in order to minimize any material adverse effect on Customer’s
      business.
   3. **Disclaimer of Warranty**
      . Modal does not represent or
      warrant that the operation of the Service (or any portion thereof)
      will be uninterrupted or error free, or that the Service (or any
      portion thereof) will operate in combination with other hardware,
      software, systems or data not provided by Modal, except as expressly
      specified in the applicable Documentation. CUSTOMER ACKNOWLEDGES
      THAT, EXCEPT AS EXPRESSLY SET FORTH IN THIS SECTION 6.1, MODAL MAKES
      NO EXPRESS OR IMPLIED REPRESENTATIONS OR WARRANTIES OF ANY KIND WITH
      RESPECT TO THE SERVICE OR SERVICES, OR THEIR CONDITION. MODAL IS
      FURNISHING THE WARRANTIES SET FORTH IN THIS SECTION 6.1 IN LIEU OF,
      AND MODAL HEREBY EXPRESSLY EXCLUDES, ANY AND ALL OTHER EXPRESS OR
      IMPLIED REPRESENTATIONS OR WARRANTIES, WHETHER UNDER COMMON LAW,
      STATUTE OR OTHERWISE, INCLUDING WITHOUT LIMITATION ANY AND ALL
      WARRANTIES AS TO MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE,
      SATISFACTORY QUALITY OR NON-INFRINGEMENT OF THIRD-PARTY RIGHTS.
7. **LIMITATIONS OF LIABILITY**

   IN NO EVENT SHALL MODAL BE LIABLE FOR ANY, LOST PROFITS, BUSINESS
   INTERRUPTION, REPLACEMENT SERVICE OR OTHER SPECIAL, INCIDENTAL,
   CONSEQUENTIAL, PUNITIVE OR INDIRECT DAMAGES, HOWEVER CAUSED AND
   REGARDLESS OF THEORY OF LIABILITY. MODAL’S LIABILITY FOR ALL CLAIMS
   ARISING UNDER THIS AGREEMENT, WHETHER IN CONTRACT, TORT OR OTHERWISE,
   SHALL NOT EXCEED THE AMOUNT OF FEES PAID OR PAYABLE BY CUSTOMER UNDER
   THE APPLICABLE SERVICE DESCRIPTION OR SERVICE ORDER DURING THE TWELVE
   (12) MONTH PERIOD PRECEDING THE CLAIM.
8. **MISCELLANEOUS**
   1. **Export Control**
      . Customer hereby certifies that
      Customer will comply with all current applicable export control
      laws. Customer agrees to defend, indemnify and hold Modal harmless
      from any liability for Customer’s violation of any applicable export
      control laws.
   2. **Compliance with Laws.**
      Customer shall comply with all
      applicable laws and regulations in its use of any Service, including
      without limitation the unlawful gathering or collecting, or assisting
      in the gathering or collecting of information in violation of any privacy
      laws or regulations. Customer shall, at its own expense, defend, indemnify
      and hold harmless Modal from and against any and all claims, losses,
      liabilities, damages, judgments, government or federal sanctions, costs
      and expenses (including attorneys’ fees) incurred by Modal arising from
      any claim or assertion by any third party of violation of privacy laws
      or regulations by Customer or any of its agents, officers, directors
      or employees.
   3. **Assignment**
      . Neither party may transfer and assign
      its rights and obligations under this Agreement without the prior
      written consent of the other party. Notwithstanding the foregoing,
      Modal may transfer and assign its rights under this Agreement
      without consent from the other party in connection with a change in
      control, acquisition or sale of all or substantially all of its
      assets.
   4. **Force Majeure**
      . Neither party shall be responsible
      for failure or delay in performance by events out of their
      reasonable control, including but not limited to, acts of God,
      Internet outage, terrorism, war, fires, earthquakes and other
      disasters (each a “
      **Force Majeure**
      ”). Notwithstanding
      the foregoing: (i) Customer shall be liable for payment obligations
      for Service rendered; and (ii) if a Force Majeure continues for more
      than thirty (30) days, either party may to terminate this agreement
      upon written notice to the other party.
   5. **Notice**
      . All notices between the parties shall be in
      writing and shall be deemed to have been given if personally
      delivered or sent by registered or certified mail (return receipt),
      or by recognized courier service.
   6. **No Agency**
      . Both parties agree that no agency,
      partnership, joint venture, or employment is created as a result of
      this Agreement. Customer does not have any authority of any kind to
      bind Modal.
   7. **Governing Law**
      . This Agreement shall be governed
      exclusively by, and construed exclusively in accordance with, the
      laws of the United States and the State of California, without
      regard to its conflict of laws provisions. The federal courts of the
      United States in the Northern District of California and the state
      courts of the State of California shall have exclusive jurisdiction
      to adjudicate any dispute arising out of or relating to this
      Agreement. Each party hereby consents to the jurisdiction of such
      courts and waives any right it may otherwise have to challenge the
      appropriateness of such forums, whether on the basis of the doctrine
      of forum non conveniens or otherwise. The United Nations Convention
      on Contracts for the International Sale of Goods shall not apply to
      this Agreement or any purchase order issued under this Agreement.
   8. **Publicity**
      . Customer hereby authorizes Modal to
      identify Customer as a Modal Customer, and use Customer’s name, mark
      and/or logo on Modal’s website and/or in Modal’s marketing materials
      with respect to the same. In addition, Customer agrees to
      participate in certain publicity activity, such as a case study,
      customer quote, and joint press release all as further described in
      the corresponding Service Order or Service Description.
   9. **Entire Agreement**
      . This Agreement is the complete
      and exclusive statement of the mutual understanding of the parties
      and supersedes and cancels all previous written and oral agreements,
      communications, and other understandings relating to the subject
      matter of this Agreement, and all waivers and modifications must be
      in a writing signed by both parties, except as otherwise provided
      herein. Any term or provision of this Agreement held to be illegal
      or unenforceable shall be, to the fullest extent possible,
      interpreted so as to be construed as valid, but in any event the
      validity or enforceability of the remainder hereof shall not be
      affected. In the event of a conflict between this Agreement and the
      applicable Service Description or Service Order document, the terms
      of this Agreement shall control.

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/library
================================================================================

Model Library
=============

Top open source models ready to deploy or customize in seconds.

Large language models
---------------------

[LLM

### Llama 3.1 8B Instruct

Latest Llama model optimized for instruction following, in a memory-efficient size

3.1

•

Instruct

•

H100](/blog/how-to-run-llama-3-1-8b-instruct-on-modal)
[LLM

### Llama 3.1 70B Instruct

Latest Llama model optimized for instruction following, larger size

3.1

•

Instruct

•

H100](/blog/how-to-run-llama-3-1-70b-instruct-on-modal)

Audio transcription models
--------------------------

[Audio transcription

### Whisper Large v3

Latest Whisper model for speech-to-text

V3

•

H100](/blog/how-to-run-whisper-large-v3-on-modal)
[Audio transcription

### WhisperX

Fork of Whisper, optimized for speed and with some additional features like word-level timestamps

H100](/blog/how-to-run-whisperx-on-modal)

Image generation models
-----------------------

[Image generation

### Stable Diffusion 3.5 Large

Latest Stable Diffusion model for high-quality image generation, large size

3.5

•

H100](/blog/how-to-run-stable-diffusion-3-5-large-on-modal)
[Image generation

### Stable Diffusion 3.5 Medium

Latest Stable Diffusion model for high-quality image generation, medium size

3.5

•

H100](/blog/how-to-run-stable-diffusion-3-5-medium-on-modal)
[Image generation

### Stable Diffusion XL

Previous generation of Stable Diffusion model for generating 1024x1024 images

SDXL

•

H100](/blog/how-to-run-stable-diffusion-xl-on-modal)
[Image generation

### Flux.1-dev

Top new text-to-image model from Black Forest Labs

H100](/blog/how-to-run-flux1-dev-on-modal)

Text embedding models
---------------------

[Text embedding

### Nomic Embed v1.5

Top performing embedding model for RAG, clustering, and more

1.5

•

H100](/blog/how-to-run-nomic-embed-v1-5-on-modal)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/llm-almanac
================================================================================

[LLM Engine Advisor
==================](#ui)

![Dice](/_app/immutable/assets/dice-icon.DaJt-V2Z.png)

I'm feeling curious

I want to serve

Qwen 2.5 7B

Qwen 3 235B-A22B

Qwen 3 235B-A22B fp8

DeepSeek-V3 671B-A37B int4

Llama 3.1 8B

Llama 3.1 8B fp8

Llama 3.1 8B int4

Llama 3.1 70B

Llama 3.1 70B fp8

Llama 3.3 70B int4

Gemma 3 4B bf16

Gemma 3 12B bf16

Gemma 3 27B bf16

Ministral 8B

Mistral Small 3.1 24B

with

any engine

SGLang

vLLM

TensorRT-LLM

I expect on average

128 tokens in / 1024 tokens out

256 tokens in / 2048 tokens out

512 tokens in / 512 tokens out

512 tokens in / 4096 tokens out

1024 tokens in / 128 tokens out

1024 tokens in / 1024 tokens out

2048 tokens in / 256 tokens out

2048 tokens in / 2048 tokens out

4096 tokens in / 512 tokens out

Clients should receive

the first token

the last token

each token

in under

10 ms

30 ms

100 ms

300 ms

1 second

3 seconds

10 seconds

30 seconds

1 minute

95% of the time

I want to see

the highest throughput

the lowest latency

every benchmarked

configuration

![Dice](/_app/immutable/assets/dice-icon.DaJt-V2Z.png)
I'm
feeling curious

Loading...

Metric:

Time To First Token

Inter Token Latency

Time To Last Token

Aggregate:

Median

p90

p95

Show Data

---

Frequently Asked Questions
--------------------------

### What is this? How do I use it?

This interactive chart indicates the per-replica throughput and
client-side latency you can expect when running open weights language
models on open source inference engines, in particular on
[Modal](/)
. Select a workload (model, tokens in and out), set a latency objective,
and indicate whether you want to see all configurations or just the one
that got the best throughput or best latency. Select a line in the chart
to see a working code snippet you can use to try out the LLM engine on
Modal.

Results were measured for "out-of-the-box" configurations of the LLM
engines, and so represent an upper, not a lower, bound for performance,
especially for TensorRT-LLM. For a deep dive on the methodology, see
[this page](/llm-almanac/how-to-benchmark)
. For a high-level overview of the results, see
[our executive summary](/llm-almanac/summary)
.

Click
[the dice](#ui)
to see a random result.

### Should I use vLLM, SGLang, or TensorRT-LLM?

Our results indicate that vLLM and SGLang achieve comparable performance
out-of-the-box, so the decision between those two frameworks needs to be
made on other grounds, like their time-to-market on the features you care
about. Our internal and other published results indicate that TensorRT-LLM
can be faster if tuned for very specific workloads, but the engineering
lift and churn should not be under-estimated.

See
[our executive summary](/llm-almanac/summary)
for details.

### What do I do if my LLM engine needs to serve hundreds of requests per second? I only see loads ranging from under one RPS to a few dozen.

Our results are measured for a single replica, one instance of the LLM
inference engine. A high-throughput service is constructed by "scaling
out" these replicas. If you're interested in running a high-throughput
service that can handle variable load, consider
[Modal](/)
, the
serverless platform used to measure these results. Services on Modal can
scale from zero to thousands of replicas in minutes.

In our results, a single replica runs on at most a single node, which may
have up to eight GPUs. Contemporary deployments of large models often run
single replicas that are sharded across many nodes ("distributed
inference"), like the ~40 node-per-replica deployment described by the
DeepSeek team. These configurations can achieve lower latencies for larger
models and/or higher throughputs, including throughput per dollar, but are
much more complex to deploy, maintain, and scale up and down. If and as
this style of deployment becomes more common in open source LLM engine
software, we plan to add it to our benchmarking (see
[NVIDIA Dynamo](https://github.com/ai-dynamo/dynamo)
for one implementation).

### Why is the minimum time-to-first-token (TTFT) around 200 ms, even for small models?

Our latencies are measured from a client, not the server, and include
network and service delays of around 150 milliseconds (p95). These
overheads include network transmission and from systems that enable
auto-scaling, retries, request logging, and other features common to
production deployments. They might be reduced to a few dozen milliseconds
by replication at the edge, at the cost of much increased engineering
complexity, which we leave to future work.

### I want to run language models on CPUs/TPUs/LPUs. Do you have results for that?

Currently, our benchmark only includes GPU-accelerated language model
inference for models with over one billion parameters, which is the most
common case we see on our platform. See the excellent
[CPU-centric benchmarking work from Spare Cores](https://sparecores.com/article/llm-inference-speed)
for results with the
`llama.cpp`
engine.

### What data did you use?

We used the default dataset in
[`guidellm`](https://github.com/neuralmagic/guidellm)
, random chunks of
*Pride and Prejudice*
of varying lengths. This results
in a low KV cache hit rate and so is closer to the performance of a system
handling independent requests, like a translation service, than to the performance
of a system handling correlated requests, like a conversational chatbot.

### I want to know all the details about how you ran these benchmarks so that I can poke holes in your results. Where can I find them?

Great to hear! We've used our benchmarking system enough to know that it
is useful but we haven't done enough to make it bulletproof (and nothing
is perfect). We released the code open source
[here](https://github.com/modal-labs/stopwatch)
. Let us know if you spot any issues.

We did the minimum configuration to get workloads to run, but the breadth
of our benchmarking, across three frameworks on ten context lengths for
over a dozen models, meant that we couldn't give any particular
configuration the attention that an engineer focused on building a single
service would. So we welcome contributions from the community, including
teams building LLM engines, to
[our open repository of configs](https://github.com/modal-labs/stopwatch/tree/main/configs)
for this benchmark. We intend to keep this benchmark up-to-date as long as
there are users who want to run LLM engines on our platform.

You can find a detailed walkthrough of our general approach to
benchmarking LLM engines
[here](/llm-almanac/how-to-benchmark)
.

But benchmarks measure a software and hardware system together, not
separately. So below are some key technical details about the system we
ran on, rented serverlessly on the Modal platform.

The benchmarking code ran on Oracle Cloud (OCI) machines in a variety of
data centers in the United States (mostly in the Midwest and
Mid-Atlantic). We did not observe meaningful differences across data
centers. Machines were all AMD x86-64 CPUs running Oracle Linux.

All LLM engine serving machines used NVIDIA GPUs. Modal’s entire GPU fleet
is actively and automatically monitored for GPU health issues, including
[heating issues](https://twitter.com/charles_irl/status/1909320428600148249)
. The H100 GPU cards used were all of the SXM form factor (data sheet
[here](https://www.nvidia.com/en-us/data-center/h100/)
).
Experiments were run with version 570.86.15 of the
[NVIDIA GPU Driver](/gpu-glossary/host-software/nvidia-gpu-drivers)
and
[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)
version
12.8.

Version information for the LLM engine software is included with each
result. We used the latest version of each framework. We used container
images made publicly available by vLLM, SGLang, and NVIDIA (details in
sample code). We retrieved model weights from the Hugging Face Hub.

The benchmarking clients and LLM engine servers ran inside of the
`gvisor`
hypervisor as part of the Modal container runtime. The guest OS was Debian
Linux. CPU and RAM allocations were lightly tuned to avoid bottlenecking while
maximizing bin-packing. LLM engine servers all provided an OpenAI-compatible
REST API. Clients communicated with them via HTTP/TCP/IP. These requests passed
through the Modal input plane in the eastern United States, which would handle
routing and auto-scaling in a production deployment. All together, this stack
adds 100ms of overhead onto ~50ms of network latency in the 95th percentiles,
which could be reduced an order of magnitude by peering clients and edge servers
more directly, at increased engineering complexity (see
[this sample code for WebRTC on Modal](https://modal.com/docs/examples/webrtc_yolo)
, which achieves <25ms peer-to-peer over RTP for users near the edge
deployment).

*We would like to thank
[Michael Goin](https://github.com/mgoin)
of
[RedHat AI](https://www.redhat.com/en/products/ai)
,
[Moin Nadeem](https://twitter.com/moinnadeem)
and
[Nikhil Murthy](https://www.linkedin.com/in/nikhil-murthy/)
of
[Phonic](https://phonic.co/)
,
[Ishan Dhanani](https://www.linkedin.com/in/ishandhanani/)
of
[NVIDIA Dynamo](https://github.com/ai-dynamo/dynamo)
, and
[Charles Pierse](https://www.linkedin.com/in/charles-pierse)
of
[Weaviate](https://weaviate.io)
for feedback on early drafts
of this interface.*

---

![](/_app/immutable/assets/footer-logo.B402mbMr.png)

---


================================================================================
SOURCE URL: https://modal.com/llm-almanac/advisor
================================================================================

[LLM Engine Advisor
==================](#ui)

![Dice](/_app/immutable/assets/dice-icon.DaJt-V2Z.png)

I'm feeling curious

I want to serve

Qwen 2.5 7B

Qwen 3 235B-A22B

Qwen 3 235B-A22B fp8

DeepSeek-V3 671B-A37B int4

Llama 3.1 8B

Llama 3.1 8B fp8

Llama 3.1 8B int4

Llama 3.1 70B

Llama 3.1 70B fp8

Llama 3.3 70B int4

Gemma 3 4B bf16

Gemma 3 12B bf16

Gemma 3 27B bf16

Ministral 8B

Mistral Small 3.1 24B

with

any engine

SGLang

vLLM

TensorRT-LLM

I expect on average

128 tokens in / 1024 tokens out

256 tokens in / 2048 tokens out

512 tokens in / 512 tokens out

512 tokens in / 4096 tokens out

1024 tokens in / 128 tokens out

1024 tokens in / 1024 tokens out

2048 tokens in / 256 tokens out

2048 tokens in / 2048 tokens out

4096 tokens in / 512 tokens out

Clients should receive

the first token

the last token

each token

in under

10 ms

30 ms

100 ms

300 ms

1 second

3 seconds

10 seconds

30 seconds

1 minute

95% of the time

I want to see

the highest throughput

the lowest latency

every benchmarked

configuration

![Dice](/_app/immutable/assets/dice-icon.DaJt-V2Z.png)
I'm
feeling curious

Loading...

Metric:

Time To First Token

Inter Token Latency

Time To Last Token

Aggregate:

Median

p90

p95

Show Data

---

Frequently Asked Questions
--------------------------

### What is this? How do I use it?

This interactive chart indicates the per-replica throughput and
client-side latency you can expect when running open weights language
models on open source inference engines, in particular on
[Modal](/)
. Select a workload (model, tokens in and out), set a latency objective,
and indicate whether you want to see all configurations or just the one
that got the best throughput or best latency. Select a line in the chart
to see a working code snippet you can use to try out the LLM engine on
Modal.

Results were measured for "out-of-the-box" configurations of the LLM
engines, and so represent an upper, not a lower, bound for performance,
especially for TensorRT-LLM. For a deep dive on the methodology, see
[this page](/llm-almanac/how-to-benchmark)
. For a high-level overview of the results, see
[our executive summary](/llm-almanac/summary)
.

Click
[the dice](#ui)
to see a random result.

### Should I use vLLM, SGLang, or TensorRT-LLM?

Our results indicate that vLLM and SGLang achieve comparable performance
out-of-the-box, so the decision between those two frameworks needs to be
made on other grounds, like their time-to-market on the features you care
about. Our internal and other published results indicate that TensorRT-LLM
can be faster if tuned for very specific workloads, but the engineering
lift and churn should not be under-estimated.

See
[our executive summary](/llm-almanac/summary)
for details.

### What do I do if my LLM engine needs to serve hundreds of requests per second? I only see loads ranging from under one RPS to a few dozen.

Our results are measured for a single replica, one instance of the LLM
inference engine. A high-throughput service is constructed by "scaling
out" these replicas. If you're interested in running a high-throughput
service that can handle variable load, consider
[Modal](/)
, the
serverless platform used to measure these results. Services on Modal can
scale from zero to thousands of replicas in minutes.

In our results, a single replica runs on at most a single node, which may
have up to eight GPUs. Contemporary deployments of large models often run
single replicas that are sharded across many nodes ("distributed
inference"), like the ~40 node-per-replica deployment described by the
DeepSeek team. These configurations can achieve lower latencies for larger
models and/or higher throughputs, including throughput per dollar, but are
much more complex to deploy, maintain, and scale up and down. If and as
this style of deployment becomes more common in open source LLM engine
software, we plan to add it to our benchmarking (see
[NVIDIA Dynamo](https://github.com/ai-dynamo/dynamo)
for one implementation).

### Why is the minimum time-to-first-token (TTFT) around 200 ms, even for small models?

Our latencies are measured from a client, not the server, and include
network and service delays of around 150 milliseconds (p95). These
overheads include network transmission and from systems that enable
auto-scaling, retries, request logging, and other features common to
production deployments. They might be reduced to a few dozen milliseconds
by replication at the edge, at the cost of much increased engineering
complexity, which we leave to future work.

### I want to run language models on CPUs/TPUs/LPUs. Do you have results for that?

Currently, our benchmark only includes GPU-accelerated language model
inference for models with over one billion parameters, which is the most
common case we see on our platform. See the excellent
[CPU-centric benchmarking work from Spare Cores](https://sparecores.com/article/llm-inference-speed)
for results with the
`llama.cpp`
engine.

### What data did you use?

We used the default dataset in
[`guidellm`](https://github.com/neuralmagic/guidellm)
, random chunks of
*Pride and Prejudice*
of varying lengths. This results
in a low KV cache hit rate and so is closer to the performance of a system
handling independent requests, like a translation service, than to the performance
of a system handling correlated requests, like a conversational chatbot.

### I want to know all the details about how you ran these benchmarks so that I can poke holes in your results. Where can I find them?

Great to hear! We've used our benchmarking system enough to know that it
is useful but we haven't done enough to make it bulletproof (and nothing
is perfect). We released the code open source
[here](https://github.com/modal-labs/stopwatch)
. Let us know if you spot any issues.

We did the minimum configuration to get workloads to run, but the breadth
of our benchmarking, across three frameworks on ten context lengths for
over a dozen models, meant that we couldn't give any particular
configuration the attention that an engineer focused on building a single
service would. So we welcome contributions from the community, including
teams building LLM engines, to
[our open repository of configs](https://github.com/modal-labs/stopwatch/tree/main/configs)
for this benchmark. We intend to keep this benchmark up-to-date as long as
there are users who want to run LLM engines on our platform.

You can find a detailed walkthrough of our general approach to
benchmarking LLM engines
[here](/llm-almanac/how-to-benchmark)
.

But benchmarks measure a software and hardware system together, not
separately. So below are some key technical details about the system we
ran on, rented serverlessly on the Modal platform.

The benchmarking code ran on Oracle Cloud (OCI) machines in a variety of
data centers in the United States (mostly in the Midwest and
Mid-Atlantic). We did not observe meaningful differences across data
centers. Machines were all AMD x86-64 CPUs running Oracle Linux.

All LLM engine serving machines used NVIDIA GPUs. Modal’s entire GPU fleet
is actively and automatically monitored for GPU health issues, including
[heating issues](https://twitter.com/charles_irl/status/1909320428600148249)
. The H100 GPU cards used were all of the SXM form factor (data sheet
[here](https://www.nvidia.com/en-us/data-center/h100/)
).
Experiments were run with version 570.86.15 of the
[NVIDIA GPU Driver](/gpu-glossary/host-software/nvidia-gpu-drivers)
and
[CUDA Driver API](/gpu-glossary/host-software/cuda-driver-api)
version
12.8.

Version information for the LLM engine software is included with each
result. We used the latest version of each framework. We used container
images made publicly available by vLLM, SGLang, and NVIDIA (details in
sample code). We retrieved model weights from the Hugging Face Hub.

The benchmarking clients and LLM engine servers ran inside of the
`gvisor`
hypervisor as part of the Modal container runtime. The guest OS was Debian
Linux. CPU and RAM allocations were lightly tuned to avoid bottlenecking while
maximizing bin-packing. LLM engine servers all provided an OpenAI-compatible
REST API. Clients communicated with them via HTTP/TCP/IP. These requests passed
through the Modal input plane in the eastern United States, which would handle
routing and auto-scaling in a production deployment. All together, this stack
adds 100ms of overhead onto ~50ms of network latency in the 95th percentiles,
which could be reduced an order of magnitude by peering clients and edge servers
more directly, at increased engineering complexity (see
[this sample code for WebRTC on Modal](https://modal.com/docs/examples/webrtc_yolo)
, which achieves <25ms peer-to-peer over RTP for users near the edge
deployment).

*We would like to thank
[Michael Goin](https://github.com/mgoin)
of
[RedHat AI](https://www.redhat.com/en/products/ai)
,
[Moin Nadeem](https://twitter.com/moinnadeem)
and
[Nikhil Murthy](https://www.linkedin.com/in/nikhil-murthy/)
of
[Phonic](https://phonic.co/)
,
[Ishan Dhanani](https://www.linkedin.com/in/ishandhanani/)
of
[NVIDIA Dynamo](https://github.com/ai-dynamo/dynamo)
, and
[Charles Pierse](https://www.linkedin.com/in/charles-pierse)
of
[Weaviate](https://weaviate.io)
for feedback on early drafts
of this interface.*

---

![](/_app/immutable/assets/footer-logo.B402mbMr.png)

---


================================================================================
SOURCE URL: https://modal.com/llm-almanac/how-to-benchmark
================================================================================

How do I benchmark an LLM engine?
=================================

This methodology depends substantially on the excellent work by Neural Magic
(acquired by Red Hat) on the
[guidellm](https://github.com/neuralmagic/guidellm)
tool for LLM engine benchmarking.

![](https://modal-cdn.com/llm-almanac/illustration-hand.png)

If you want to benchmark an LLM engine, you first need to know the
*workload*
(or loads) that you want to test.

That means you should have figured out a small set of options for:

* the model you want to run, including what levels of quantization are
  acceptable. These are concerns for the product team and AI/ML engineers.
  If that includes you, congratulations! You are a
  *full-stack AI engineer*
  .
* measuring or estimating typical counts of input tokens in requests and
  output tokens generated.
* the framework you want to run. See our
  [executive summary](/llm-almanac/summary)
  for some suggestions on how to choose a framework.

Additionally, you should already have some latency objectives in mind — on
the time-to-first-token if you're streaming responses or on the
time-to-last-token if you're sending complete responses (these terms are
defined below).

By the way, if you already have all of this information, you should check
out our
[LLM Engine Advisor](/llm-almanac/advisor)
, which
recommends an engine configuration based on these constraints.

In our open source LLM engine benchmarking framework,
`stopwatch`
, we first upper- and lower-bound the throughput a single replica of an LLM
engine can achieve for a given workload on a given hardware configuration.
Then we sweep through rates in between. In all cases, we collect up latency
metrics. In the rest of this article, we will walk through the
justification, the nuances, and the details of this benchmarking technique.

Our benchmark configuration format and Modal's serverless, auto-scaling
infrastructure together allow us to express and then run almost all of this
work in parallel — within configurations and across them. By Amdahl's Law,
we can, in principle, finish the several thousand experiments in our
benchmark suite in the time it takes to complete two experiments serially
(about ten minutes). We've achieved this in practice for up to dozens of
runs (at most 100 GPUs), which is all we needed to complete this project on
a satisfactory timeline — any more would've just been showing off.

If you're interested in scaling out your own CPU or GPU workloads onto
hundreds or thousands of containers,
[get in touch](https://form.fillout.com/t/onUBuQZ5vCus)
.

How do I benchmark the maximum throughput of an LLM engine?
-----------------------------------------------------------

The maximum throughput of an LLM engine is the maximum number of requests
per second (RPS) it can handle. If requests arrive at a rate higher than
this for a sustained period, then the system has no choice but to queue
them. It is a central result of queueing theory that this queue then grows
without bound, and so latencies grow without bound — until eventually the
system breaks, leaving a bunch of angry users.

It is measured from the perspective of all requests, not any request in
particular. The RPS is usually determined by clients of the service, in
aggregate, and the engineer has limited control of it. This makes it
something of an "independent variable", and so it appears along the x-axis
of many of our, and others', benchmarking charts. In developing an LLM
engine service or LLM application, you should work with your clients to
determine what their demand is or to estimate it.

To benchmark maximum throughput, we use
`guidellm`
from Red Hat
in its
`throughput`
mode. A batch of requests are sent from a client
to the server at the same time. If you're into calculus, you might think of this
as an "infinite request rate" that lasts for an "infinitely short period" (even
math-ier: a Dirac delta distribution, as in impulse response characterization
of linear time-invariant systems).

The client then waits for the entire batch to finish. We measure the total
duration and divide the number of requests by it to get a request rate.
Because all of the requests arrived at once, the maximum amount of
request-level parallelism is exposed to the engine, and so the engine has
the best chance to take advantage of that parallelism to increase aggregate
throughput, at the likely expense of latency.

We also measure the latency statistics per request. But dropping a requests
in huge batches and waiting for them to finish is not likely to be the way
anyone runs an LLM engine if they care about those latencies! For that
reason, we exclude these results from our
[LLM Engine Advisor](/llm-almanac/advisor)
display.

The figure above depicts the results of our maximum throughput benchmarking
experiments across a variety of workloads run by different LLM engines,
ranging from small models run at dozens of RPS to large models run at under
one RPS. To handle larger demand, multiple replicas of the engine must be
run.

Note that we don't measure tokens per second, but instead requests per
second. In our experience, this creates a false equivalence between
workloads - a false fungibility of tokens - that is more obfuscating than
clarifying.

How do I benchmark the minimum latency of an LLM engine?
--------------------------------------------------------

The minimum latency of an LLM engine is the fastest that it can service a
request — the minimum amount of time the work of the system is "latent", or
not visible, from the perspective of the client. This is generally a
service-level objective (SLO) determined by communication with clients of
the service and then made the engineer's responsibility. Note that this is
in part a matter of user perception and so can often be cleverly worked
around with user interface elements or interaction design tricks. See the
next section for more.

To benchmark minimum latency, we use
`guidellm`
in its
`synchronous`
mode. The client sends one single request and then waits
for a response before sending another. This ensures that the engine only needs
to service a single request at a time. The minimum amount of request-level parallelism
is exposed to the engine, but there is now no contention for resources between
requests, and so the engine has the best chance to service the request as quickly
as is possible.

Notice that the number of requests per second we observe now drops
precipitously. This is an incredibly expensive way to run an LLM engine!
Especially on GPUs, which are designed, from silicon to software, to
maximize throughput instead of minimize latency. It is at best used as a
temporary solution to hit a tight latency SLO you can't satisfy otherwise.

How do I measure latency for an LLM engine?
-------------------------------------------

The "latency" that matters for any system is use-case dependent. It is the
answer to the question: "what is the duration during which this system
appears idle to clients?". Like a bad manager, clients of a service
generally don't care about the work you are doing or how hard it is, they
only care about results — or did you stop to think about the
[submarine Internet cables](https://euripides.dk/setebos/frx/matrix/ai/books/stephenson_mother_earth_mother_board.pdf)
or the
[life of a pixel](https://youtu.be/K2QHdgAKP-s)
while you waited
for this page to load and render?

The critical latency metrics for an LLM engine, from the perspective of a
client, are how long it takes to return any tokens to that client (TTFT) and
how long it takes to return all tokens to that client (TTLT). The TTLT is
fraught with nuance, so LLM engineers often measure the time between tokens
(ITL) instead.

### What is time-to-first-token (TTFT)?

*Time-to-first-token*
(TTFT) is most important in systems where
response tokens can be streamed as soon as they are available. Human users
waiting for answers from chatbots, or to see the thinking tokens of a
"reasoning" model appear, care quite a bit about this number. From the
perspective of the LLM engineer and their engine, the TTFT is a reasonable
metric as well. It measures the time to complete the "prefill" or "prompt
processing" step of LLM inference, during which the Transformer can be run
in parallel across the sequence, as it was during training. It is named
analogously to
[time-to-first-byte](https://web.dev/articles/ttfb)
. TTFTs in our
dataset range from around two hundred ms to tens of seconds.

### What is time-to-last-token (TTLT)?

*Time-to-last-token*
(TTLT) is most important in systems where response
tokens cannot be returned to clients as they are available, but only once the
final token is available. If the client is another computer which needs to parse
the resulting tokens as, say, a JSON object, then TTLT represents the earliest
time at which that object can be available. TTLTs in our dataset range from a
large fraction of a second to many tens of seconds.

The TTLT is less commonly the figure of merit when the final client is a
human user, because humans can read partial results just fine. But designing
user interactions around streaming results can be challenging (e.g.
streaming multiline text in a code editor can be jarring).

The TTLT is a tricky metric. Perhaps surprisingly, it is under control of
the language model, not the engineer, the engine, or the even the user,
since in almost all applications the last token to be emitted is a "stop"
token. That is, with each token generated, the model calculates a
probability for the stop token, along with all other tokens its vocabulary,
and so estimates the chance that it is done with the request. Alien
intelligences that they are, this behavior of language models can be quite
unpredictable. During benchmarking, we continue generating after stop tokens
are emitted in order to get a cleaner number, but this is highly
unrealistic.

### What is inter-token latency (ITL)?

*Inter-token latency*
(ITL) is a "back-of-house" metric that is more useful
for understanding LLM engines on their own terms than it is for communicating
requirements with clients. It measures the time it takes to generate a single
token (typically averaged over a small number of tokens). The smallest inter-token
latencies are at around a millisecond, for latency-optimized small models running
on short contexts on large GPUs, and the largest are at a large fraction of a
second, for very large models.

The ITL, also known as the "time-per-output-token", or TPOT, is most useful
for estimating how the TTLT will vary as the output lengths vary. It
measures the time for each step in the "decode" (or "output generation" or
"autoregressive") phase of LLM inference, during which tokens are created
one (or a few) at a time. The average of the ITL can be used to estimate the
throughput of the decode phase.

### How can I estimate latencies without running in production?

High-performance computing is done relative to the "speed-of-light". That is
literal in the case of networks but figurative in most other cases, where
"speed-of-light" represents the maximum speed supported by the hardware.
This speed-of-light can be used to bound latency — for example, a model with
8B parameters, each one byte, will not be loaded up the memory hierarchy
from the
[GPU's RAM](/gpu-glossary/device-hardware/gpu-ram)
to the
[GPU's register files](/gpu-glossary/device-hardware/register-file)
any faster than 4 TB/s on an H100, so you will never beat half a millisecond
ITL in that setting (we've seen about 5ms when optimizing for latency). You can
find a now-classic walkthrough of the arithmetic by kipply
[here](https://kipp.ly/transformer-inference-arithmetic/)
and quick explainer of the most popular attainment metric, "Model FLOP/s Utilization",
in
[one of our blog posts](/blog/gpu-utilization-guide)
.

These give lower bounds, but LLM engineers are usually concerned with upper
bounds on percentiles, which don't admit such clean analysis. Where rational
methods fail, we turn to empirical methods and start measuring and
benchmarking.

That'll be more expensive than a pen and paper, so we want to be efficient
in our measurement. Given the data-dependence of TTLT described above, it
may seem that you need to run exactly the workload you expect to see in
production in order to determine performance.

Luckily the ITL can be used, along with the TTFT, to estimate the TTLT when
outputs have varying length: just add the ITL times the output length.
There's a bit of nuance here. The ITL should monotonically increase with
input length and with the number of previously generated tokens. Though it
doesn't admit a simple mathematical justification, we suggest a
hard-and-fast "three nineties" rule-of-thumb: use the 90th percentile (p90)
TTFT and multiply the p90 ITL by the number of output tokens to estimate the
p90 TTLT.

It is still a good idea to match benchmarking data with production data, at
least at a coarse level. Our benchmarks were run with natural language data
(
*Pride and Prejudice*
) which has different characteristics to
programming language data. For example, the latter admits higher success
rates for simple token speculation techniques, which improve performance.

How do latency and throughput for LLM engines depend on request load?
---------------------------------------------------------------------

Above are described the techniques we use to measure determine minimum
latency and maximum throughput of an LLM engine on a given workload (model,
quantization, and in/out sequence length). The minimum latency also gives us
an approximate minimum throughput — the lowest request rate at which the
engine always has work to do. You might call this the request rate at which
the engine achieves 100% "request utilization" (see
[this blog post](/blog/gpu-utilization-guide)
for discussion of utilization rates for GPUs).

We then use
`guidellm`
in its
`constant`
mode to probe
the latencies observed at rates in between the minimum sensible and maximum feasible
rate. Here, the client sends requests at the specified rate per second without
waiting for the server to respond and collects latency metrics.

This more closely resembles a realistic deployment of an LLM engine, where
the aggregate of all clients produce requests at a rate that typically
varies slowly (relative to the request rate).

These are the results shown above. You can explore them in our
[LLM Engine Advisor](/llm-almanac/advisor)
.

So, which LLM engine should I run?
----------------------------------

Head to the
[LLM Engine Advisor](/llm-almanac/advisor)
and put in
your workload to get a suggested configuration. If you want higher-level
advice about how to choose an engine or how to think about running your own
LLMs, see our
[executive summary](/llm-almanac/summary)
.

*The authors would like to thank
[Michael Goin](https://github.com/mgoin)
, vLLM committer,
[Ishan Dhanani](https://www.linkedin.com/in/ishandhanani/)
,
senior engineer on
[NVIDIA Dynamo](https://github.com/ai-dynamo/dynamo)
, and
[Yineng Zhang](https://github.com/zhyncs)
, inference lead for
SGLang, for feedback on this benchmarking approach and review of early
results.*

---

![](/_app/immutable/assets/footer-logo.B402mbMr.png)

---


================================================================================
SOURCE URL: https://modal.com/llm-almanac/summary
================================================================================

LLM Engines: An Executive Summary
=================================

![](https://modal-cdn.com/llm-almanac/illustration-engine.png)

Nearly every serious application of computer systems includes relational
database management software, from local SQLite on phones to planet-scale
Spanner in cloud data centers. These systems all trace their lineage back
through
[IBM System R](https://people.eecs.berkeley.edu/~brewer/cs262/SystemR.pdf)
to the
[Information Management System](https://www.ibm.com/docs/en/zos-basic-skills?topic=now-history-ims-beginnings-nasa)
built to send humans to the Moon — or at least track all the bills of material
for the rockets that carried them. In place of behemoth proprietary software
built for specific systems at great expense, there is now a legion of open
source software.

Language models are still closer in their lifecycle to the
[Peterlee Relational Test Vehicle](https://ieeexplore.ieee.org/document/5388096)
than to Postgres, but they hold the same promise: to be a part of nearly
every application of computer systems. When these applications need to store
and retrieve structured data, they currently deduce following the relational
algebra of Ted Codd; when they need to produce or process unstructured data,
they will infer with neural probabilistic models.

The weights for capable language models are now widely available under
permissive or open source licenses — the Llama, Qwen, DeepSeek, Mistral,
and Gemma model families, to name a few. Over the past year, these models
have rapidly improved, reaching the baseline of quality required for
inference to be useful. This is a key step enabling organizations to serve
their own language model applications. Below, we walk through the cases
we've seen where it makes sense to use these models in place of
proprietary systems.

But just as data needs a software system, the RDBMS, to manage storage and
execute queries, language models need a software system to drive their
inference, managing the storage of query caches and scheduling large
matrix multiplications on specialized hardware.

Like SQL database engines, there are open source "LLM engines" you can run
yourself. And like the models, this software stack has rapidly improved in
both usability and performance over the past year — reaching the baseline
of quality required for self-serve inference to be economical.

These engines are the primary subject of this report.

It answers the most common and most critical questions we hear asked by
technical leaders interested in running their own LLM engines. It is
informed by discussion with those leaders, with LLM engineers building
applications, and with the developers of LLM engines. It grounds its
claims in the same benchmarking work that supports our
[LLM Engine Advisor](/llm-almanac/advisor)
, which indicates baseline performance for engines on specific workloads
and provides starter code for running your own LLM engine on
[Modal's serverless cloud infrastructure](/)
.

When should I use open weights language models instead of proprietary
services like OpenAI or Anthropic?
--------------------------------------------------------------------------------------------------------

Many organizations are already building LLM applications based on
proprietary models that can't be self-hosted. So the first question to
consider is when and why they should switch away from those services.

### The standard arguments for building technology in-house instead of buying apply here.

The most common concern is data governance, which frequently mandates
tight control of the servers that process user data. Worries about model
providers secretly training on data are overblown, but legitimate
requirements remain, especially in regulated industries.

The other most commonly-cited motivation is cost, familiar to anyone who
has considered self-hosting anything. Competition (including from open
models) has prevented any provider from charging too high of a premium.
But when LLM applications mature and the requirements become clearer, the
capabilities of the systems provided by frontier labs focused on
artificial general intelligence or superintelligence become unnecessary.
Those capabilities come at a cost, relative to a smaller model tuned or
prompted carefully. Think of it like rewriting code from JavaScript or
Python to Go or Rust once the feature velocity decreases. For details, see
[this post](https://openpipe.ai/blog/a-founder-c-guide-to-ai-fine-tuning)
from one of our customers, OpenPipe, which provides post-training as a service.

This is just one instance of the kind of customization that's not possible
or not economical with proprietary models. Limits here are similar to
limits on extending proprietary software and we expect them to be
similarly durable. Models are, after all, valuable intellectual property,
and exposing them too much to tinkering and development risks leaking that
IP. We expect customization to only increase in importance over time, as
it has in domains like image generation, where open weights models are
more mature. Organizations that move now will be better prepared for this
future.

Finally, there is a less technical reason to consider this switch: the
movement of OpenAI and Anthropic into the application layer. Releases like
Claude Code and OpenAI Codex represent large steps away from language
modeling and towards applications of language models. And we've seen this
before: OpenAI's move to Chat Completions APIs (i.e. those for
instruction-tuned models designed for chat) represented a large step
towards applications relative to the original Completions APIs (i.e. those
for models trained just to predict text). OpenAI has further signaled that
they plan to continue this trajectory with their even more abstract
Responses API.

### Use open weights models when there's such a thing as "smart enough".

Collaborative, open source solutions tend to win out over competitive,
proprietary solutions when they
[pool together labor and resources](http://www.catb.org/esr/writings/magic-cauldron/magic-cauldron.html)
to build non-differentiating capabilities — think programming languages,
operating systems, and databases. These are needed by everyone, and almost
no one gets enough competitive edge from building their own to make it worth
the effort.

We see the same phenomenon with open weights\* language models. Basic
capabilities like code completion, assistant chat, and data extraction
have been commoditized by open weights models. In each case, there's a
maximum level of capability (or "intelligence") required to complete the
task satisfactorily. Each fixed level of capability has been reached by
the open weights models within a year, most recently by
[DeepSeek-R1-0528](https://huggingface.co/deepseek-ai/DeepSeek-R1-0528)
, which goes toe-to-toe with OpenAI's six-month-old o3 on a number of
benchmarks.

There are other cases where the demand for intelligence, like the demand
for RAM in computing systems, is effectively unbounded. These cases
include:

* zero-sum competitive settings (politics, markets, & other games)
  and
* settings with high tail risk (like human-off-the-loop control of
  computers, where
  `rm -rf ~/`
  is always only a few tokens away).

There, we see a continued role for proprietary language models, just as
there is still a role for proprietary database systems like Oracle,
Microsoft SQL Server, and IBM Db2 (all in the top ten on
[db-engines.com](http://db-engines.comhttps://db-engines.com/en/ranking)
).

*\*We use the term "open weights" here instead of "open source", since in
most cases the source code required to produce the weight binaries is
not provided under an OSI-approved license (or at all). We expect this
distinction to matter more, not less, in the future.*

How do I make the build vs buy decision for LLM inference?
----------------------------------------------------------

Open source databases like Postgres are often offered as managed services
and used by everyone from startups to the Fortune 500. Open weights
language models are no different. Startups like
[Together](https://www.together.ai/)
and hyperscalers like
[Amazon](https://aws.amazon.com/bedrock/)
are already offering inference
as a service. So why run it yourself?

### You can readily beat language model API providers on price if you're running batch workloads on shorter contexts.

Chatting and code completion are the most popular applications of large
language models, and they are both interactive. Engineering an
interactive, streaming, and latency-sensitive language model application
is challenging, just as it is for other computer systems (more on that
below).

But LLMs can also perform other tasks that are less latency-sensitive,
like extracting data from support chat logs or translating a large corpus
of documents. There, throughput is the most important factor — the name of
the game is queries per second. This setting is much easier to engineer
and optimize, as described below, and so it's easier to beat managed
services on price.

In one set of experiments, depicted below, we ran Meta's Llama 3.1 70B in
8bit floating point precision. The test data has more input tokens than
output tokens, as is common in retrieval-augmented generation (RAG) or
structured data extraction. In particular, the inputs have 1024 tokens
(about a page of text) and the outputs have 128 tokens (about a
paragraph).

Both vLLM and SGLang ran at ~17 QPS per 8xH100 replica without any tuning.
The chart below shows the median latency to first token as we varied the
request rate. Sacrificing interactivity (~200 ms end-to-end, leftmost
points) led to an 8x throughput increase (~4s end-to-end, rightmost
points). Details of our method are
[here](/how-to-benchmark)
.

Loading...

Metric:

Time To First Token

Inter Token Latency

Time To Last Token

Aggregate:

Median

p90

p95

Show Data

Running this configuration on
[Modal's starter plan](/pricing)
,
which has purely usage-based pricing, you can set up a batch system that
processes ~20k tok/s with Llama 3.1 70B fp8 at ~50¢ per million tokens.
Modal's paid plans allow this to scale up to hundreds of replicas. This
compares favorably with published rates from API providers. For
performance data for other configurations, see the
[LLM Engine Advisor](/llm-almanac/advisor)
released along with this
summary.

### Start by building your own batch "token factory" before you run a streaming token service.

Many of us were introduced to language models in an interactive system,
like OpenAI ChatGPT or Anthropic Claude, that streams the outputs. These
systems are harder to set up and to run economically than batch systems,
so start with batch.

This is typically the case in computing, where batch (say, Spotify
Discover Weekly) precedes streaming (say, TikTok feeds). Consider: early
computers started out as batch job machines, processing large bulks of
data like the U.S. Census or company payrolls. Users submitted very large
tasks, via punch cards, and then waited for them to finish. The
interactivity we are used to today, derived from time-sharing systems like
MULTICS/UNIX, was only added later.

It is in general better to start with the easier, batch case and then to
build the more difficult one afterwards, with the benefit of hard-won
experience. This is true both as a matter of your organization's internal
technical growth and the development of the broader field of open source
language model inference — i.e., outside of the handful of organizations
that have driven the frontier so far.

### But don't write your own LLM engine (unless you're betting the company).

If you're running the language model yourself, you need to think about the
software used to pass inputs through those weights to create outputs —
language model "inference" using an "LLM engine" or "LLM serving
framework".

LLM engines are less complex than database management systems, but they
are not simple software. Contemporary models are based on the Transformer
architecture, which is optimized for efficiency during training. This is
necessary to achieve the dizzying scale of frontier model training runs (
[estimated](https://epoch.ai/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year)
at 6 x 10
25
, or ~100 mol, FLOPs). But it means that running
this architecture efficiently when serving, say, chatbot requests is not
as simple as writing a few lines of PyTorch.

One reason for the complication is the primacy of performance. Running
large language models is expensive (often on the order of cents per
thousand user queries), which incentivizes close attention to performance.
Engineering for performance melts abstractions and reveals the thickets of
complexity hidden underneath.

But it is nowhere nearly the complexity and expense of training your own
model. If running LLMs is a key differentiating capability for your
organization, building your own engine is worth considering. A small,
talented team of engineers can start from published research and open
source code and develop just the features you need within a few months.
The primary technical risks are hum-drum: maintenance, churn, and tooling
compatibility in a rapidly-changing field.

But the same arguments about differentiation used above to argue the case
for using open weights also apply here. This is something many teams need
to do, and now a number of them are collaborating to build it together —
the topic of our next section. You can join them!

Which open source LLM engine should I choose?
---------------------------------------------

There are three main open source LLM engines:
[vLLM](https://docs.vllm.ai/en/latest/)
,
[SGLang](https://docs.sglang.ai/)
, and
[TensorRT-LLM](https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html)
.

vLLM and SGLang are open source, open governance projects in the same
basic mold as Postgres — to the point of both also coming out of the
University of California, Berkeley. Contributions to these projects come
from the usual suspects in infrastructure: large organizations like Red
Hat, late-stage startups like Anyscale, and leading teams serving
proprietary models like xAI. Both build on Meta's
[PyTorch framework](https://docs.pytorch.org/docs/stable/index.html)
.

[TensorRT-LLM](https://nvidia.github.io/TensorRT-LLM/index.html)
is an open source but closed governance project by NVIDIA. It builds on top
of NVIDIA's
[TensorRT](https://docs.nvidia.com/deeplearning/tensorrt/latest/index.html)
framework.

We'll cover the differences between these projects and how to pick which
one to use below.

### All the engines stand on the shoulders of giants. All will get better as those giants get taller.

Because of the high cost of large language model inference, performance is
the first factor used to evaluate LLM engines. There is less daylight here
than you might expect from the intensity of benchmark wars on social
media. This is due to the fact that all of them are building with the same
basic tools and constraints.

First, the limits of performance are set by the hardware. Hardware
engineers refer often to the "speed-of-light" — not literally, but as the
maximum speed at which the hardware can run, set by clock speeds and bus
widths. Almost all open source LLM inference is done on NVIDIA GPUs and so
has the same speed of light.

Unlike typical CPU workloads,
[LLM inference on GPUs frequently runs at a high fraction of the speed
of light](/blog/gpu-utilization-guide#what-is-model-flops-utilization-mfu)
, set by either the arithmetic bandwidth of the matrix multiplication
hardware (
[Tensor Cores](/gpu-glossary/device-hardware/tensor-core)
) or the memory bandwidth between
[GPU RAM](/gpu-glossary/device-hardware/gpu-ram)
and the
[registers](/gpu-glossary/device-hardware/register-file)
of the
[Streaming Multiprocessors](/gpu-glossary/device-hardware/streaming-multiprocessor)
. This limits the domain for speedups to ~2-3x at most, absent
algorithmic differences, which are generally small, due to rapid diffusion
of innovations.

More deeply, the same basic stack is used by each of the engines — the
[CUDA software platform](/gpu-glossary/host-software/cuda-software-platform)
, the
[CUDA Basic Linear Algebra Subroutine](https://docs.nvidia.com/cuda/cublas/index.html)
(cuBLAS) library, and the
[CUDA Templates for Linear Algebra Subroutines](https://docs.nvidia.com/cutlass/index.html)
(CUTLASS) kernel framework. In addition, vLLM and SGLang both use PyTorch.
All of the engines stand to see their performance improve as new hardware is
released and these bedrock libraries update to take advantage of it.

### vLLM and SGLang achieve comparable out-of-the-box performance. Other factors should drive your decision.

We ran both vLLM and SGLang, out of the box, on dozens of LLM inference
workloads. These workloads included models ranging in size from a few
billion to nearly a trillion parameters and on sequence lengths ranging
from one thousand to ten thousand tokens. The results were strikingly
similar across the two frameworks especially when looking at throughput
during batch processing -- see how closely the points hug the SGLang =
vLLM line in the plot below. Read more about our methodology
[here](/llm-almanac/how-to-benchmark)
.

That means you'll need to consider other factors in making your decision.
Because vLLM has been around for longer and has historically been faster
to market with new features, we have accumulated more experience with it.
But SGLang's recent rapid development is promising, and we're looking
closely at both. A little competition is good for everyone (else).

### In our experience, vLLM is fastest to the market with new features.

At time of completion of our experiments in late May 2025, TensorRT-LLM (
`0.20.0.rc3`
) did not support Gemma 3 or Qwen 3.

On SGLang (
`0.4.6-post5-cu124`
), we hit
[this issue](https://github.com/sgl-project/sglang/issues/6054)
(resolved but not released) running DeepSeek-V3 in INT4 quant and CUDA OOMs
when running Qwen 3 235B A22B we couldn't resolve in time for release.

We didn't find any workloads of interest that we couldn't run on vLLM (
`0.8.x`
and then
`0.9.0`
). We did, however, discover that we couldn't
independently toggle CUDA graph capture and Torch graph compilation (also
[now resolved](https://github.com/vllm-project/vllm/pull/17345)
but not released) as needed to match SGLang's behavior more closely. See the
next section.

Finally, at time of writing in early June 2025, SGLang does not have
accelerated kernels for Blackwell GPUs like the
[B200](/blog/introducing-b200-h200)
in a stable release, as they are still on PyTorch 2.6. Kernels
[compiled for max performance](/gpu-glossary/device-software/compute-capability)
on the Blackwell
[SM architecture](/gpu-glossary/device-hardware/streaming-multiprocessor-architecture)
were only added to PyTorch wheels in 2.7. Blackwell support, including partially-optimized
kernels, was released for vLLM just as we wrapped up our work.

### Startup times are slower with vLLM than with SGLang by default — mostly due to Torch compilation.

With the default settings, startup times for vLLM servers were much
longer, around five minutes for 8B models to SGLang's one minute. In both
cases, model weights were loaded from
[our distributed model cache](/docs/guide/model-weights)
at about the same rate, ~1 GB/s.

The primary difference is in the out-of-the-box configuration. vLLM turns
Torch graph compilation on by default. Compilation can improve
performance, in particular for models that don't have custom fused kernels
available, but it incurs a startup cost that is hard to manage with
cacheing (
[docs](https://docs.pytorch.org/tutorials/recipes/torch_compile_caching_tutorial.html)
).

Separately, both frameworks use CUDA graph capture, which also reduces
latency, in particular at low (~3-5ms) inter-token latencies. CUDA graph
capture is simpler and faster than Torch graph compilation — well under a
minute for typical configurations.

vLLM has recently
[bet heavily on Torch compilation](https://docs.vllm.ai/en/latest/design/v1/torch_compile.html)
, while SGLang seems to have
[done the opposite](https://github.com/sgl-project/sglang/issues/4748)
. Which choice turns out to be best will depend on the progress of that
project.

### TensorRT-LLM can provide big wins, especially at the lowest latencies — but don't underestimate the engineering cost.

While vLLM and SGLang are strikingly similar in many ways, TensorRT-LLM is
quite different. Its Python interface is a thinner wrapper over the
underlying
[CUDA C++](/gpu-glossary/host-software/cuda-c)
software
than theirs. TensorRT-LLM also requires the LLM engine be compiled ahead of
time per workload. These artifacts are stored on disk, which reduces startup
times, but this extra manual build step adds complexity. Our sample code for
running it on Modal is thus about three times longer, at 150 lines to 50 for
the other two frameworks.

And out of the box, we observe worse performance with TensorRT-LLM than
with vLLM or SGLang. This is to be expected, since there is no intention
by the developers that the default settings should be used in production
serving, but it makes an apples-to-apples benchmark challenging.

Instead, TensorRT-LLM's build step exposes a bewildering array of un- or
under-documented flags and parameters, like
`--reduce-fusion`
(presented as a strict improvement for end-to-end performance in the
[docstring](https://nvidia.github.io/TensorRT-LLM/commands/trtllm-build.html)
, caveats explained
[elsewhere](https://nvidia.github.io/TensorRT-LLM/performance/performance-tuning-guide/useful-build-time-flags.html#reduce-norm-fusion-plugin-for-llama-models)
). It is widely reported, and we have observed in a few cases, that the
proper setting of these flags can make a substantial difference in
performance, including going from much slower than the other engines to
much faster.

This is a tough challenge for engineers and engineering leaders — is it
worth it to pour a few weeks of very expensive engineering time into this
tuning? That depends on what speedup is possible. Reported numbers help,
but in our experience, the impact of configuration changes is very
sensitive to surprising features of workloads, including features that
might change during serving, so estimation is challenging and churn is
high.

Another engineering challenge arises from the TensorRT-LLM development
model. Until the most recent stable release,
`v0.19.0`
in mid-May 2025, TensorRT-LLM's
[source code on GitHub](https://github.com/NVIDIA/TensorRT-LLM)
was updated per release, many thousands of lines at a time. Seemingly, this
was done to mirror a in bulk large number of changes made to an internal GitLab
repo during actual development. This made it essentially impossible to connect
changes in code to changes in behavior (using tools like
`git bisect`
, for instance). These releases also offered no
backwards compatibility guarantee, so every update was a tedious, manual
process requiring careful review of documentation, examples, and code.
According to a
[recent announcement](https://github.com/NVIDIA/TensorRT-LLM/issues/3148)
, they have officially adopted a "GitHub-first" development flow and are
planning for a 1.0 release to improve stability and reduce churn. Very
welcome!

Our current practice when we approach a new workload is to try vLLM or
SGLang first, get benchmark numbers as quickly as possible, do some light
tuning, and compare results to the latency objectives. If those frameworks
meet the objective, we presume that the performance benefits of
TensorRT-LLM aren't worth the extra complexity, brittleness, and delays in
time-to-market until proven otherwise.

Meanwhile, we are slowly accumulating and sharing optimized TensorRT-LLM
configurations as we discover them. We welcome contributions
[here](https://github.com/modal-labs/stopwatch/blob/main/CONTRIBUTING.md)
. The biggest win we've seen so far came in a case where latency was at a
premium but cost efficiency was not — perhaps unsurprising for software
written by the hardware provider. That case is described in detail in our
docs
[here](/docs/examples/trtllm_latency)
.

What next?
----------

If you either know what workload you want to run or are curious to see our
results in more detail, check out the
[LLM Engine Advisor](/llm-almanac/advisor)
, which reports the latency numbers (time-to-first-token,
time-to-last-token, and inter-token latency) we observed across a variety
of request rate loads for popular open weights models run with vLLM,
SGLang, and TensorRT-LLM. Code snippets are included.

If you'd like to know more about how to think about and benchmark LLM
engine performance, check out our
[benchmarking guide](/llm-almanac/how-to-benchmark)
.

---

![](/_app/immutable/assets/footer-logo.B402mbMr.png)

---


================================================================================
SOURCE URL: https://modal.com/pricing
================================================================================

as our product
=================================

With Modal, you always pay for what you use and nothing more. You never
pay for idle resources — just actual compute time, by the CPU cycle.

[Get Started](/signup)

Book a Demo

### Compute costs

Per hour

Per second

Per hour

Per second

---

GPU Tasks

Nvidia B200

$0.001736
/ sec

Nvidia H200

$0.001261
/ sec

Nvidia H100

$0.001097
/ sec

Nvidia A100, 80 GB

$0.000694
/ sec

Nvidia A100, 40 GB

$0.000583
/ sec

Nvidia L40S

$0.000542
/ sec

Nvidia A10G

$0.000306
/ sec

Nvidia L4

$0.000222
/ sec

Nvidia T4

$0.000164
/ sec

---

CPU

Physical core

(2 vCPU
equivalent
)

$0.0000131
/ core / sec

\*minimum of 0.125 cores per container

---

Memory

$0.00000222
/ GiB / sec

Starter

$0

+

compute / month

Built for small teams and independent developers looking to level up.

[Get started with $30 / month free credit

Get Started](/signup)

- $30 / month free credits
- 3 workspace seats included
- 100 containers + 10 GPU concurrency
- Crons and web endpoints (limited)
- Real-time metrics and logs
- Region selection

Team

$250

+

compute / month

Built for startups and larger organizations looking to scale quickly.

[Sign in to upgrade

Sign in to upgrade](/settings/plans)

- $100 / month free credits
- Unlimited seats
- 1000 containers + 50 GPU concurrency
- Unlimited crons and web endpoints
- Custom domains
- Static IP proxy
- Deployment rollbacks

Enterprise

Custom

For organizations prioritizing security, support, and everlasting confidence.

[Get in touch

Get in touch](https://form.fillout.com/t/onUBuQZ5vCus)

- Volume-based pricing
- Unlimited seats
- Custom GPU concurrency
- Support via private Slack
- Personalized integration help
- Audit logs, Okta SSO, and HIPAA

[![

](https://modal-cdn.com/tmpm9makfxp_73c169ef.webp)](https://modal-cdn.com/landscape-vids/Modal_Bars-hevc-safari.mp4)

### Modal Sandbox Pricing

Only pay for what you use, by the CPU cycle. For GPU Sandboxes, refer to
our standard GPU prices.

### Compute costs

Per hour

Per second

Per hour

Per second

---

CPU

Physical core

(2 vCPU
equivalent
)

$0.00003942
/ core / sec

\*minimum of 0.125 cores per container

---

Memory

$0.00000672
/ GiB / sec

![](https://modal-cdn.com/use-case-sandboxes-header.webp)

### Why serverless?

#### Serverless pricing vs. traditional cloud pricing

Modal is serverless, which means that we instantly autoscale up and down for
you based on request volume. For spiky or unpredictable workloads, we are more
cost-effective than fixed on-demand/reserved compute.

[

](https://modal-cdn.com/pricing/Modal_Graph_Mobile-Left_250501_v01.hevc.mp4)

#### Traditional cloud: $5,400

75 GPUs \* 24 hrs \* $3 / GPU-hr

[

](https://modal-cdn.com/pricing/Modal_Graph_Mobile-Right_250501_v01.hevc.mp4)

#### Modal serverless cloud: $4,740

Avg 50 GPUs \* 24 hrs \* $3.95 / GPU-hr

### Get up to $50k in free credits

Startups and academic researchers can receive up to $50k in free
credits on Modal. The credits can be used on both GPU and CPU compute.

Apply Now

### Use committed spend on Modal

You can use committed AWS spend on Modal via AWS Marketplace. Coming
soon to Google Cloud Marketplace.

Get In Touch

Comparison Table

### Starter

$0

+

compute / month

### Team

$250

+

compute / month

### Enterprise

Custom Compute

Workspace

Number of seats

Up to 3

Unlimited

Unlimited

Compute and Variable Costs

Included compute

$30 / month

$100 / month

Custom

Features

Cron jobs

5 deployed crons

Unlimited

Unlimited

Webhooks

8 deployed endpoints

Unlimited

Unlimited

Log retention

1 day

30 days

Custom

Containers

100

1000

Custom

GPU concurrency

10

50

Custom

Deployed apps

200

1000

1000

Custom domains

Real-time metrics

Sharing and collaboration

Secrets

Unlimited

Unlimited

Unlimited

Custom images

Unlimited

Unlimited

Unlimited

Instant deploys

Unlimited

Unlimited

Unlimited

Deployment rollbacks

3 versions

Custom

Distributed queue

Unlimited

Unlimited

Unlimited

Distributed dict

Unlimited

Unlimited

Unlimited

[Region selection](/docs/guide/region-selection)

1.25 - 2.5x base prices

1.25 - 2.5x base prices

1.25 - 2.5x base prices

Support

Access to Modal Community Slack

Access to Modal Community Slack

+ Support via private Slack

+ Personalized integration help

Security

SOC 2 compliance

HIPAA compatibility

Audit logs

Static IP proxy

RBAC
Coming Soon

SSO

Frequently asked questions
--------------------------

---

How does serverless pricing differ from traditional on-demand pricing?

---

What counts as billable time?

---

How are CPU and memory usage metered?

---

What's an example of real-world CPU billing?

---

What's an example of real-world GPU billing?

---

Do you charge for storage?

---

Can I add more than three team members on the starter plan?

---

What kinds of applications can I deploy using Modal?

---

Can I use my AWS, GCP, or Azure credits on Modal?

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/use-cases/audio
================================================================================

Audio inference
===============

![](https://modal-cdn.com/tmp8ziv5kad_0b111555.webp)

Build flexible pipelines to transcribe audio, generate voice or synthesize
high-fidelity music.

[Get Started](/signup)

Book a Demo

![](https://modal-cdn.com/tmp8ziv5kad_0b111555.webp)

[![customer logo](data:image/svg+xml,%3csvg%20fill='none'%20height='24'%20viewBox='0%200%2091%2024'%20width='91'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20clip-rule='evenodd'%20d='m84.3719%202.09163c.1835.36702.3923.78447%201.0765%201.23464v-.00125c.7077.46606%201.3576.58864%201.9787.70575.7492.14132%201.4562.27466%202.1718.99381%201.0009%201.0049%201.0021%201.89608%201.0021%203.52146v6.98486c0%202.4375-.7661%204.3199-2.2959%205.6475-1.5323%201.3275-4.5071%201.9924-7.4807%201.9924h-.0025c-2.9724%200-5.9496-.6649-7.4783-1.9924-1.531-1.3276-2.2959-3.21-2.2959-5.6475v-6.98486c0-2.47454.7698-4.38297%202.3107-5.72778%201.5384-1.34605%204.5194-2.018455%207.4918-2.018455.7441%200%201.5163.022249%202.2124.224955.9078.26407%201.0907.62969%201.3093%201.06687zm.6189%2016.48117c.6752-.665%201.0134-1.8047%201.0134-3.4238l-.0012.0012s-.0652-4.8576.1267-6.15176c.4871-3.28528.0299-3.6243-.6796-4.15038l-.0154-.01135c-1.9835-1.46965-6.728-.97523-8.7313.67858-.7391.6106-1.0625%201.75765-1.0625%203.41394v6.21977c0%201.6191.3382%202.7588%201.0132%203.4238.6752.6649%202.5458.9962%204.1678.9962%201.6221%200%203.4939-.3313%204.1689-.9962zm-52.6062%204.627c3.2125%200%206.2538-.6667%207.6697-2.004h.0012c1.4158-1.3372%202.1244-3.4045%202.1244-6.208v-13.88732h-4.6026v13.50492c0%201.7469-.3067%202.9836-.9204%203.7124-.4948.5868-1.3784.7342-2.3804.9016h-.0003c-.2405.0401-.4877.0815-.7381.1302-.2473.0481-.4846.0963-.7135.1429-1.9491.3965-3.2776.6668-4.7443-.4421-1.1345-1.096-1.075-1.9028-.9507-3.5896.0194-.2623.0404-.546.0586-.8554.292-4.96765%200-13.50492%200-13.50492h-4.6025v13.88732c0%201.182.2082%203.0531.7713%203.8614.4426.6345%201.0497.8785%201.6415%201.1164.4412.1776.8741.3515%201.2236.6815.2488.2341.4037.4596.5563.6816.2827.4113.5572.8108%201.4054%201.2317.9588.4743%202.5903.6394%204.1958.6394zm27.8924-.4687-9.2863-16.84469v16.84469h-4.0245v-21.63062h6.3906l9.2875%2016.89662v-16.89662h3.9984v21.63062h-6.3633zm-57.80147-1.3735c2.12631%201.1047%204.40609%201.5285%206.39156%201.5415-.00012%200-.00022%200-.00033%200h.00078c-.00015%200-.0003%200-.00045%200%201.27261-.0114%202.15451-.0604%203.03061-.1989%201.1689-.1846%202.2945-.5021%203.3406-1.0693%201.3011-.7058%202.3229-1.681%202.8771-3.0862.4946-1.2532.5191-2.5536.2999-3.8618-.1827-1.0904-.6821-2.0124-1.5299-2.7426-.8985-.7737-1.893-1.056-3.0458-.7104-.3701.1109-.7436.2111-1.1171.3113-.1445.0387-.289.0775-.4332.1168-.8627.2353-1.6168.0538-2.2501-.5922-.22079-.2249-.43095-.4598-.61829-.7128-.45339-.61281-1.01542-1.04295-1.78643-1.18292-.30052-.05463-.59351-.15977-.88425-.26411-.04166-.01495-.08328-.02988-.12488-.04465-.42308-.15024-.81745-.35661-1.10475-.71955-.67065-.84684-.45457-2.30768.4412-2.98532.40341-.30521.86271-.49221%201.35192-.59856%201.37002-.2981%202.73568-.20597%204.08998.09805%201.6574.37202%203.1415%201.12636%204.5014%202.13531.1376.102.1731.06879.2357-.06564.4121-.8848.8276-1.76761%201.2532-2.64569.0641-.13206.0439-.20243-.05-.30009-.7722-.80097-1.6873-1.38769-2.6976-1.83364-2.0328-.89785-4.1668-1.228364-6.37394-1.129525-1.10791.049419-2.19929.209145-3.25131.579585-1.75888.61952-3.10529%201.72413-3.90464%203.43404-.613973%201.31219-.684029%202.69592-.424664%204.10653.269598%201.46798%201.077604%202.52478%202.387804%203.22648%201.5632.8372%203.80065%201.2256%205.77392%201.5682%201.19593.2076%202.29483.3984%203.08773.662.8033.2673%201.72.8868%201.9148%201.4252.3188.8809-.0512%202.3378-1.309%202.9727-.6494.3277-1.3472.4681-2.0612.5018-1.48651.0694-2.95887-.057-4.39934-.4516-1.62074-.444-3.05846-1.2433-4.35174-2.3104-.14642-.121-.19798-.1013-.27944.0605-.465993.9244-.939855%201.8443-1.4168657%202.7631-.0625782.1201-.050771.1925.0491967.2866.720633.6784%201.510139%201.2604%202.387819%201.7162z'%20fill='%23fff'%20fill-rule='evenodd'/%3e%3c/svg%3e)](https://www.suno.com)

“Suno has developed proprietary state-of-the-art models that generate music and speech using AI. Modal's superb developer experience enables our team to ship new models to production quickly, and with and confidence we'll scale to thousands of simultaneous users.”

Georg Kucsko
,
Co-Founder

[![customer logo](data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='114'%20height='24'%20viewBox='0%200%20114%2024'%20fill='none'%3e%3cg%20clip-path='url(%23clip0_2126_15312)'%3e%3cpath%20d='M104.239%2019.4985H103.058C97.0158%2019.4985%2093.8574%2017.7683%2093.8574%2012.6051V11.3143C93.8574%206.12375%2097.0158%204.39355%20103.058%204.39355H104.239C110.033%204.39355%20112.78%206.23359%20113.329%209.39189H109.402C108.55%207.99126%20107.095%207.46946%20103.827%207.46946C99.7621%207.46946%2097.6749%208.10111%2097.6749%2011.534V12.3579C97.6749%2015.7909%2099.7621%2016.4225%20103.827%2016.4225C107.095%2016.4225%20108.55%2015.9007%20109.402%2014.5001H113.329C112.78%2017.6584%20110.033%2019.4985%20104.239%2019.4985Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M90.9522%200V2.71888H87.1074V0H90.9522ZM90.9522%204.66878V19.2243H87.1074V4.66878H90.9522Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M83.9533%2019.2238H80.1358V10.9848C80.1358%207.93633%2078.1036%207.46946%2075.6044%207.46946C72.3363%207.46946%2070.2765%208.4856%2070.2765%2011.6714V19.2238H66.4316V4.66819H70.2765V7.3596H70.4413C71.2927%205.68433%2073.1327%204.39355%2076.8128%204.39355C80.8225%204.39355%2083.9533%205.51956%2083.9533%2010.3531V19.2238Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M54.286%2019.4985H52.5833C46.5964%2019.4985%2043.3281%2017.7683%2043.3281%2012.6051V11.3143C43.3281%206.12375%2046.5964%204.39355%2052.5833%204.39355H54.286C60.3005%204.39355%2063.5687%206.12375%2063.5687%2011.3143V12.6051C63.5687%2017.7683%2060.3005%2019.4985%2054.286%2019.4985ZM53.4622%2016.4225C57.5542%2016.4225%2059.7512%2015.7909%2059.7512%2012.3579V11.534C59.7512%208.10111%2057.5542%207.46946%2053.4622%207.46946C49.3151%207.46946%2047.1456%208.10111%2047.1456%2011.534V12.3579C47.1456%2015.7909%2049.3151%2016.4225%2053.4622%2016.4225Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M40.6408%2019.2243H36.8235V10.9854C36.8235%207.93692%2034.7911%207.47005%2032.292%207.47005C29.0238%207.47005%2026.964%208.48618%2026.964%2011.6719V19.2243H23.1191V0H26.964V7.36019H27.1288C27.9802%205.68492%2029.8202%204.39415%2033.5003%204.39415C37.51%204.39415%2040.6408%205.52014%2040.6408%2010.3537V19.2243Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M0%204.66819H3.8174V7.3596H3.92726C4.86101%205.32731%207.05809%204.39355%2011.0678%204.39355H11.8367C17.0823%204.39355%2020.2404%206.12375%2020.2404%2011.2869V12.5776C20.2404%2017.7683%2017.0823%2019.4985%2011.8367%2019.4985H11.0678C7.05809%2019.4985%204.86101%2018.5647%203.92726%2016.5324H3.8174V23.8925H0V4.66819ZM10.1065%207.46946C6.01448%207.46946%203.8174%208.10111%203.8174%2011.4517V12.3579C3.8174%2015.7909%206.01448%2016.4225%2010.1065%2016.4225C14.2535%2016.4225%2016.4231%2015.7909%2016.4231%2012.3579V11.534C16.4231%208.10111%2014.2535%207.46946%2010.1065%207.46946Z'%20fill='%23FFFBFB'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_2126_15312'%3e%3crect%20width='113.408'%20height='24'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://phonic.co/)

“At Phonic, we train our own proprietary models for audio generation. We moved all our large-scale audio processing batch jobs to Modal. Our engineers are ecstatic with the result – we can run at a much larger scale than before, no longer have to babysit our batch jobs, and we can ship much faster.”

Moin Nadeem
,
Co-Founder

[![customer logo](/_app/immutable/assets/Substack.B7qlgtDV.svg)](https://www.substack.com/)

“When Substack launched a feature for AI-powered audio transcriptions. The data team picked Modal because it makes it easy to write code that runs on 100s of GPUs in parallel, transcribing podcasts in a fraction of the time.”

Mike Cohen
,
Head of Data

### Cheap, efficient transcription

[View Examples](/docs/examples/whisper-transcriber)

---

Outperform managed APIs

Get faster speeds at lower costs compared to popular transcription APIs like AssemblyAI and Deepgram by leveraging open-source models on Modal.

---

Scale on demand

Distribute transcription tasks across hundreds of containers simultaneously.

![](https://modal-cdn.com/use-case-transcription-code.webp)

---

Outperform managed APIs

---

Scale on demand

[View Examples](/docs/examples/whisper-transcriber)

### Build your own AI voice

[View Examples](/docs/examples/llm-voice-chat)

---

Transform text into natural-sounding speech using the latest open-source models.

Deploy text-to-speech models like XTTS directly on Modal's platform.

---

Cutting-edge hardware access

Tap into Modal's fleet of A100 and H100 GPUs for memory-intensive voice models.

---

Lightning-fast cold starts

Generate speech on-demand without lengthy startup times with our optimized container file system and engine.

![](https://modal-cdn.com/use-case-voice-chat.webp)

---

Transform text into natural-sounding speech using the latest open-source models.

---

Cutting-edge hardware access

---

Lightning-fast cold starts

[View Examples](/docs/examples/llm-voice-chat)

Try it out
----------

[View all](/docs/examples)

[### Transcribe speech in batches with Whisper

Turn audio bytes into text at scale](/docs/examples/batched_whisper)

[### Voice chat with LLMs

Build an interactive voice chat app](/docs/examples/llm-voice-chat)

[### Transcribe speech with Kyutai STT

Stream transcripts at the speed of speech](/docs/examples/streaming_kyutai_stt)

[### Create music

Turn prompts into music with MusicGen](/docs/examples/musicgen)

[### Fast podcast transcriptions

Build an end-to-end podcast transcription app that leverages dozens of containers for super-fast processing](/docs/examples/whisper-transcriber)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/use-cases/comp-bio
================================================================================

Computational bio
=================

![](https://modal-cdn.com/cdnbot/use-case-comp-bio-header.webp)

Accelerate your compute-intensive R&D with Modal.

[Get Started](/signup)

Book a Demo

![](https://modal-cdn.com/cdnbot/use-case-comp-bio-header.webp)

[![customer logo](data:image/webp;base64,UklGRgwFAABXRUJQVlA4TP8EAAAvRcEPEO8gFkzmL92dwfzPv0CAxBRAMElTbceAHsHxGNJt28raaiNJaNBogmnSIuv9HxTBjza9+XVHRP8nQP9vfGLxKMmzOLxdRuX+3TNq3r93RpX9O2dUrX/fjKr375pRS/17ZtRyvxNzWvd1HJ1zbplzzh1f1Kg1/T5OrPs6JiAuAwivadS6/pcyTWPtv8motf3v1AP8k4xa379TRm3pd+K163+VUdv6d8morf17ZNT2/k9hrf0Zxlqzq6Zp1mmaZrVRe/Q/4yOEcK3xIYROMiGE7yyEEM6SQgjBVZgQQmiyPoRwlszlEfmcM5dHAni0zc7O9wSQ7m3FRwjhWuNDCF3hdI3k8e7rTPdIAA/frGG0T/sjeiDWBOAhWaoHSQBthQVw2QMY5CaA50yXKMdhT26iMp4LPRBrAvCYMXdq46miS5TjsMJv+LudmZ0Zqb/tp2fhsJWZWOgLI/XXP9Y41dzIp3H8nDJue+mZ/R4/v2cYNroCpGvrzt0jS2bmSh5DuE8Z10WHnTTbdc1Cs0HeZypukz8vrTtKLUB0ytsJwNXYxTUt+c1Ikh0z3DYJ+Daa7QCGzANEp9xPAKcF/rGTNmy22P8e0Wl+AmKjeTsBsWLluSm7qNhnz02OAAcVA/CVTUBsNG8nINZ5dkN4UbHRfAvQqOwA3B5agFGVTwC3xSlrSi2AJA9wVPkEcKrx7IjwmloVP4Gnap9A2MNn1tQcs9sWx8yXbN/3vaQ78FDtAwgVnl0RXlFSOQGXqh5Ie0jAU9XfQNxCCUi+UE5AV9UBqeTZGWETrxV/hWfJkrnaAcAUol1csgBD3Q3AbNGTx3t3MhUW4HKq7QGaOc/uCK8nlFy24rGkxSWXtXVdZrcwX9lsvHszc8pWPMx4fgDhr+F24urazWSuJSAOW7jM8yMIf7J2Z5INUwlis43nhxBelc3OdrHZzmaXuiEzK9yrJB3P/WOa4SHZ7MMuNpL4Mfifc9/JcS2TtVp1I5Pd6j6BpBUeS2aPIeMkk3mt+jJSzbTJpaJdSxNw+wF6AqkuAc81UqFpmsYUJJ910hdwe1kdgCmdWC0Bt4rHaj2QTM1XjHHYQw/galqAtpAqPIUvIFQoAb3UA8nUfMUY+5fxkQ0FM633DSRT6FjNAtwqOgC3B5OA2JTslDWSWoBTwU6lAKQVLMC1wgO4l2Gy1MyYb6rarCncAG5znvU0AgwFl4CoPagHiM3cYQIYJMlmsZmx35Q6gK7kAT4kXQGGwikBUS9DD/LgT/6aINV8ZPE2DE6Sy4iX8/nyANJqZgKI7dHa85282YeZyMezc+2dPGr2QR4+Tv6aqDAJ4NZkpk9AlCQzAUR/aJqPO3nzQg4pK8a+xiTmB0kas/IwrSY7sdBrH7JTVhubudNMMV4L+sggxZiY9ZnsxEKvJV/NcknN8u5nyNfEpq1RV2e+awZtIDtVpbP2IjvWPY2KfU1s+pL6mcpO83aqSh9atOIg6ZBYf1+yj8LNqE7HZ43UT3PRaROpnQppMNqP1D5LT6fa0zSXeqMa2VCRHgdV+qmQeqPtBuWHtKeNre9D3xqtaa21Zk46D7fbxWmH7jKMt4sz2rs9D7fb0BotPXZ96FujxcZ11xCunTNaeOz6cO2cUeV6g+YP6bf4111tUPmQ3jiDag/pbTOo/pDeNIOWHtJbZtDyQ3rDDFrzkN4dYXmndQ9h+em98P/FAA==)](https://www.futurehouse.org/)

“Our org runs on Modal. We use it for AI agent environments, scalable deployment of AI agents, hosting of deep learning models, and visualization. It dramatically simplified our engineering infrastructure and completely changed the scope of projects we can do.”

Andrew White
,
Co-Founder & Head of Science

[![customer logo](/_app/immutable/assets/ChaiDiscovery.BHpR4uIE.webp)](https://www.chaidiscovery.com/)

“We used Modal to build an inference server for our model, Chai-1, which allows people to predict molecular structures via a web app. Modal allowed us to build and launch the server in days: our engineers didn't have to worry about maintaining infrastructure, delivering the product in record time.”

Jack Dent
,
Co-Founder

[![customer logo](/_app/immutable/assets/AchiraTestimonial.Czp2Ec1u.svg)](https://www.achira.ai/)

“Processing external quantum mechanical datasets comes with unique challenges. Jobs can fail in numerous ways. Modal's retry mechanism and batching primitives have made our data pipeline much more robust.”

Liz Decolvenaere
,
Quantum Chemical Engineer

### Run large-scale batch processing on GPUs and CPUs

---

Powerful and easy fan-out support

Parallelize your Python functions over thousands of containers

---

Go from zero to 100 GPUs in seconds

Run structure prediction, molecular dynamics, and more

---

Scale up and scale out CPU-heavy jobs

Search terabyte-scale sequence databases in minutes, not hours

![](https://modal-cdn.com/cdnbot/use-case-sequence-alignment.webp)

---

Powerful and easy fan-out support

---

Go from zero to 100 GPUs in seconds

---

Scale up and scale out CPU-heavy jobs

### Run anything on Modal

---

A general-purpose compute layer

Modal's infrastructure runs your code in the cloud

---

Bring your custom container images to Modal

Run any packages, frameworks, or binaries you want

---

Build multi-step workflows that mix environments

Seamlessly exchange data between containers

![](https://modal-cdn.com/cdnbot/use-case-structure-prediction.webp)

---

A general-purpose compute layer

---

Bring your custom container images to Modal

---

Build multi-step workflows that mix environments

### You do the science, we do the infrastructure

---

Declare your infrastructure needs in a few lines of Python

No YAML needed. Get started in minutes, then scale for years.

---

Fully usage-based pricing

No need to make long-term reservations. Spin up and down in seconds.

---

Get access to a range of different GPU types

T4s, L4s, A10Gs, A100s, H100s. And CPUs too!

![](https://modal-cdn.com/use-case-active-resources.webp)

---

Declare your infrastructure needs in a few lines of Python

---

Fully usage-based pricing

---

Get access to a range of different GPU types

Try it out
----------

[View all](/docs/examples)

[### Fold proteins with Boltz-2

Predict molecular structures and binding affinities from sequences with SotA open source models](/docs/examples/boltz_predict)

[### Build a protein folding dashboard

Serve a web UI for a protein model with ESM3, Molstar, and Gradio](/docs/examples/esm3)

[### Fold proteins with Chai-1

Predict molecular structures from sequences with SotA open source models](/docs/examples/chai1)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/use-cases/fine-tuning
================================================================================

Fine-tuning
===========

![](https://modal-cdn.com/tmpk4zx_cyn_93cd21d9.webp)

Fine-tune open-source models on your own data with Modal's serverless
GPUs.

[Get Started](/signup)

Book a Demo

![](https://modal-cdn.com/tmpk4zx_cyn_93cd21d9.webp)

[![customer logo](/_app/immutable/assets/CanOfSoup.Crz1Gd3y.svg)](https://www.canofsoup.com/)

“We fine-tune image models on Modal because we can experiment with new ideas quickly. We chose to deploy on Modal too because it's far more stable than any alternative solutions we found.”

Eric Meier
,
Co-Founder

[![customer logo](/_app/immutable/assets/OpenPipe.-dEo7fts.svg)](https://www.openpipe.ai/)

“Modal is the easiest way to experiment as we develop new fine-tuning techniques. We’ve been able to validate new features faster and beat competitors because of how quickly we can try new ideas.”

Kyle Corbitt
,
CTO

[![customer logo](data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='113'%20height='30'%20viewBox='0%200%20113%2030'%20fill='none'%3e%3cpath%20d='M7.78157%2010.1212C5.1079%2010.1212%203.79102%2012.481%203.79102%2015.6408V23.5604H0V6.88134H3.71121V11.2011H3.79102C4.58913%208.52125%206.14545%206.40137%208.57968%206.40137C10.2956%206.40137%2011.0139%207.00133%2011.0139%207.00133L9.29798%2010.4411C9.29798%2010.4411%208.77921%2010.1212%207.78157%2010.1212ZM53.3936%2012.441V23.6004H49.7222V13.8009C49.7222%2011.0011%2048.8443%209.52119%2046.6096%209.52119C44.2951%209.52119%2043.1777%2011.4011%2043.1777%2015.0009V23.6004H39.5064V13.8009C39.5064%2011.1211%2038.6285%209.52119%2036.4337%209.52119C33.8798%209.52119%2032.922%2011.7611%2032.922%2015.0009V23.6004H29.2108V6.88134H32.922V10.6811H32.9619C33.5206%208.08127%2035.1168%206.44137%2037.7905%206.44137C40.4243%206.44137%2042.1402%207.88128%2042.8186%2010.4011C43.4571%207.96128%2045.0932%206.44137%2047.6472%206.44137C51.1189%206.44137%2053.3936%208.64124%2053.3936%2012.441ZM18.4363%206.40137C15.0045%206.40137%2012.8097%208.00128%2011.7721%2010.9211L14.9247%2012.081C15.4833%2010.3211%2016.6805%209.3212%2018.5162%209.3212C20.5513%209.3212%2021.7884%2010.2411%2021.7884%2011.6011C21.7884%2013.001%2020.8307%2013.321%2018.6758%2013.681C16.2814%2014.0809%2010.5351%2014.2009%2010.5351%2019.0406C10.5351%2021.8805%2012.8895%2024.0004%2016.4411%2024.0004C19.1147%2024.0004%2020.9504%2022.9204%2021.7884%2020.8805H21.8283V23.6004H25.4996V13.321C25.4597%208.80123%2023.225%206.40137%2018.4363%206.40137ZM21.8682%2015.7208C21.8682%2019.2406%2020.1523%2021.4805%2017.3589%2021.4805C15.4035%2021.4805%2014.2463%2020.4006%2014.2463%2018.8007C14.2463%2017.3207%2015.4434%2016.2808%2017.7579%2015.8408C20.1124%2015.4009%2021.3095%2014.8809%2021.8682%2013.561V15.7208ZM66.4826%206.44137C63.6892%206.44137%2061.8136%208.00128%2061.0155%2010.3211V6.88134H57.1048V30H60.9756V20.1606H61.0155C61.8934%2022.6804%2063.6892%2024.0804%2066.4826%2024.0804C70.952%2024.0804%2074.1444%2020.4006%2074.1444%2015.2009C74.1444%2010.0012%2070.952%206.44137%2066.4826%206.44137ZM65.5248%2020.9605C62.4521%2020.9605%2060.7362%2018.7207%2060.7362%2015.2409C60.7362%2011.7611%2062.6516%209.52119%2065.5248%209.52119C68.398%209.52119%2070.3135%2011.8811%2070.3135%2015.2409C70.3135%2018.6007%2068.398%2020.9605%2065.5248%2020.9605Z'%20fill='%23E7FB21'/%3e%3cpath%20d='M112.493%2023.4787V23.5987H97.3688V23.4787C99.5636%2022.2387%20101.04%2020.9988%20102.397%2019.6789H108.622L112.493%2023.4787ZM108.742%203.75979L104.911%200H104.791C104.791%200%20104.871%206.9996%2098.4063%2013.3992C92.1013%2019.6389%2084.7188%2019.6789%2084.7188%2019.6789V19.7989L88.6295%2023.6387C88.6295%2023.6387%2095.9322%2023.7187%20102.357%2017.359C108.782%2011.0394%20108.742%203.75979%20108.742%203.75979Z'%20fill='%23E7FB21'/%3e%3c/svg%3e)](https://www.ramp.com/)

“Modal's user-friendly interface and efficient tools have truly empowered our team to navigate data-intensive tasks with ease, enabling us to achieve our project goals more efficiently.”

Karim Atiyeh
,
Co-Founder & CTO

### Programmable, unlimited training infrastructure

---

Define and share reproducible environments

Define your requirements in code and let your team run the same without having to set up a local environment.

---

Fast hyperparameter sweeps

Scale up to hundreds of multi-GPU fine-tuning runs in just a few seconds with a single function call.

---

Flexible framework integration

Use your favorite ML fine-tuning frameworks, like Hugging Face, PyTorch, and Axolotl…or write your own training loop.

![](https://modal-cdn.com/use-case-tensor-rt.webp)

---

Define and share reproducible environments

---

Fast hyperparameter sweeps

---

Flexible framework integration

### Monitoring and MLOps

---

Integration with popular tools

Monitor experiment results with Weights and Biases and visualize training progress using TensorBoard.

---

Real-time resource metrics

Access detailed performance data from your Modal dashboard.

![](https://modal-cdn.com/use-case-active-resources.webp)

---

Integration with popular tools

---

Real-time resource metrics

### Unified infrastructure

---

Efficient data management

Store private datasets and model weights in Modal Volumes as easily as writing to local disk.

---

End-to-end model lifecycle

Deploy your functions for data processing, fine-tuning, and serving, all on Modal.

![](https://modal-cdn.com/use-case-function-calls.webp)

---

Efficient data management

---

End-to-end model lifecycle

Try it out
----------

[View all](/docs/examples)

[### Custom pet art from Flux with Hugging Face and Gradio

Fine-tune an image generation model on pictures of your pet](/docs/examples/dreambooth_app)

[### Star in custom music videos

Fine-tune a Wan2.1 video model on your face and run it in parallel](/docs/examples/music-video-gen)

[### Fine-tune FLAN-T5

Fine-tune a small language model for summarization](/docs/examples/flan_t5_finetune)

[### Fine-Tuning and Inference for Computer Vision with YOLO

Customize and deploy lightning-fast object detection models](/docs/examples/finetune_yolo)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/use-cases/image-video-3d
================================================================================

Image, video & 3D inference
===========================

![](https://modal-cdn.com/tmpe_0g7omu_0bfb7d2c.webp)

Run cutting-edge AI models for image and video generation on Modal's
serverless GPUs.

[Get Started](/signup)

Book a Demo

![](https://modal-cdn.com/tmpe_0g7omu_0bfb7d2c.webp)

[![customer logo](data:image/svg+xml,%3csvg%20width='90'%20height='32'%20viewBox='0%200%2090%2032'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M12.31%2021.2409V26.454C5.51112%2026.454%200%2022.927%200%2018.5933C0%2017.6789%200.245791%2016.8034%200.696406%2015.9874C2.38258%2019.044%206.9449%2021.2402%2012.31%2021.2402V21.2409ZM12.31%200.250244V5.54663C6.96406%205.54663%202.41496%207.71051%200.714907%2010.7432C0.252398%209.91403%200%209.02271%200%208.09703C0%203.76267%205.51112%200.250244%2012.31%200.250244ZM3.14572%2013.361C2.07732%2014.1182%201.23886%2015.0082%200.696406%2015.9887C0.245791%2015.1721%200%2014.2946%200%2013.3809C0%2012.4671%200.252397%2011.5665%200.713584%2010.7432C1.2567%2011.7158%202.08922%2012.6045%203.14572%2013.361ZM3.14572%2013.3617C5.40012%2011.7641%208.67072%2010.7611%2012.31%2010.7611V16.0013C8.66939%2016.0013%205.40012%2014.9778%203.14572%2013.361V13.3617Z'%20fill='%232226FD'/%3e%3cpath%20d='M12.3096%2010.759V5.5459C19.1084%205.5459%2024.6196%209.07286%2024.6196%2013.4066C24.6196%2014.321%2024.3738%2015.1965%2023.9232%2016.0125C22.237%2012.9559%2017.6747%2010.7597%2012.3096%2010.7597V10.759ZM12.3096%2031.7497V26.4533C17.6555%2026.4533%2022.2046%2024.2894%2023.9047%2021.2567C24.3672%2022.0859%2024.6196%2022.9772%2024.6196%2023.9029C24.6196%2028.2372%2019.1084%2031.7497%2012.3096%2031.7497ZM21.4738%2018.6389C22.5422%2017.8817%2023.3807%2016.9917%2023.9232%2016.0111C24.3738%2016.8278%2024.6196%2017.7053%2024.6196%2018.619C24.6196%2019.5328%2024.3672%2020.4334%2023.906%2021.2567C23.3629%2020.2841%2022.5304%2019.3954%2021.4738%2018.6389ZM21.4738%2018.6382C19.2195%2020.2358%2015.9489%2021.2388%2012.3096%2021.2388V15.9986C15.9502%2015.9986%2019.2195%2017.0221%2021.4738%2018.6389V18.6382Z'%20fill='%232226FD'/%3e%3cpath%20d='M43.5081%2024.322C39.9431%2024.322%2038.1721%2022.482%2038.1031%2020.09H40.3801C40.4721%2021.447%2041.3001%2022.505%2043.4851%2022.505C45.4631%2022.505%2045.9921%2021.631%2045.9921%2020.78C45.9921%2019.308%2044.4281%2019.147%2042.9101%2018.825C40.8631%2018.342%2038.5171%2017.744%2038.5171%2015.306C38.5171%2013.282%2040.1501%2011.925%2042.9791%2011.925C46.1991%2011.925%2047.7401%2013.65%2047.9011%2015.674H45.6241C45.4631%2014.777%2044.9801%2013.742%2043.0251%2013.742C41.5071%2013.742%2040.8631%2014.34%2040.8631%2015.214C40.8631%2016.433%2042.1741%2016.548%2043.8301%2016.916C45.9921%2017.422%2048.3381%2018.043%2048.3381%2020.665C48.3381%2022.942%2046.5901%2024.322%2043.5081%2024.322ZM51.3416%2028.117C50.8356%2028.117%2050.4906%2028.071%2049.8926%2027.956V26.116C50.2836%2026.162%2050.4906%2026.185%2050.8586%2026.185C51.7556%2026.185%2052.8136%2025.725%2053.2736%2023.885L48.5356%2012.27H50.9966L54.4236%2021.24H54.4696L57.6896%2012.27H60.0586L55.4126%2024.276C54.2856%2027.174%2053.1586%2028.117%2051.3416%2028.117ZM67.5999%2011.925C69.7619%2011.925%2071.5099%2013.167%2071.5099%2015.858V24H69.2559V16.479C69.2559%2014.915%2068.5659%2013.857%2066.8639%2013.857C64.9319%2013.857%2063.6899%2015.03%2063.6899%2016.801V24H61.4359V12.27H63.6899V13.742H63.7359C64.3569%2012.845%2065.5759%2011.925%2067.5999%2011.925ZM79.0986%2024.345C75.6026%2024.345%2073.3256%2021.815%2073.3256%2018.135C73.3256%2014.455%2075.6026%2011.925%2079.0756%2011.925C82.0886%2011.925%2084.0896%2013.88%2084.3656%2016.525H82.0426C81.9276%2015.306%2081.2376%2013.788%2079.0986%2013.788C76.5916%2013.788%2075.6716%2015.927%2075.6716%2018.135C75.6716%2020.343%2076.5916%2022.459%2079.0986%2022.459C81.2606%2022.459%2081.9276%2020.987%2082.0426%2019.653H84.3656C84.2046%2022.344%2082.1116%2024.345%2079.0986%2024.345ZM88.4424%2024H85.9354V21.493H88.4424V24Z'%20fill='white'/%3e%3c/svg%3e)](https://sync.so/)

“Modal's beautifully designed devex makes writing distributed code as easy as writing scripts. We can turn research code into something that's deployed to production in hours instead of days.”

Prady Modukuru
,
Co-Founder & CEO

[![customer logo](/_app/immutable/assets/OpenArt.Bai-MLd_.svg)](https://openart.ai/)

“As a startup, you need to iterate on things quickly. So it's really helpful when the developer experience and development speed is suddenly like 5x or 10x. It's a lot easier to deploy a ComfyUI workflow because Modal is serverless, so it auto-scales really well.”

Coco Mao
,
CEO & Cofounder

[![customer logo](/_app/immutable/assets/Flora.DAkAvBAJ.svg)](https://florafauna.ai/)

“We are constantly shipping the most cutting-edge creative AI machine learning techniques so our customers have access to the best creative models. Modal's has helped us streamline the process from idea to deployed pipeline, allowing us to both deploy quickly & scale rapidly.”

Weber Wong
,
Founder

### One-stop shop for media generation

[View Examples](/docs/examples/stable_diffusion_cli)

---

Deploy custom models

From Stable Diffusion to Flux to OpenSora, deploy any model, including custom ones, on Modal.

---

Advanced techniques

Fine-tuning with LoRA, personalization with Dreambooth, image manipulation with Controlnet - do it all on Modal!

![](https://modal-cdn.com/use-case-image-gen-code.webp)

---

Deploy custom models

---

Advanced techniques

[View Examples](/docs/examples/stable_diffusion_cli)

### Performant and efficient

---

Premium hardware

Leverage Modal's fleet of A100 and H100 GPUs to run resource-intensive models like Stable Diffusion XL or complex video processing tasks.

---

Lightning-fast cold starts

Achieve near-instant model initialization for responsive image generation.

---

Infinite scalability, zero idle costs

Automatically scale to hundreds of GPUs during traffic spikes, then scale back to zero when idle.

![](https://modal-cdn.com/use-case-function-calls.webp)

---

Premium hardware

---

Lightning-fast cold starts

---

Infinite scalability, zero idle costs

### Ecosystem integrations

[View Examples](/docs/examples/comfyapp)

---

Framework support

Run popular frameworks like ComfyUI, A1111 and Fooocus.

---

Share prototypes instantly

Publish a public HTTP endpoint or server with a single Python decorator.

![](https://modal-cdn.com/use-case-comfy-ui.webp)

---

Framework support

---

Share prototypes instantly

[View Examples](/docs/examples/comfyapp)

Try it out
----------

[View all](/docs/examples)

[### Custom pet art from Flux with Hugging Face and Gradio

Fine-tune an image generation model on pictures of your pet](/docs/examples/dreambooth_app)

[### Edit images with Flux Kontext

Transform images with SotA diffusion models](/docs/examples/image_to_image)

[### Serverless WebRTC

Stream YOLO detections on webcam footage in real time](/docs/examples/webrtc_yolo)

[### Serve diffusion models

Serve Flux on Modal with optimizations for blazingly fast inference](/docs/examples/flux)

[### Star in custom music videos

Fine-tune a Wan2.1 video model on your face and run it in parallel](/docs/examples/music-video-gen)

[### Bring images to life

Prompt a generative video model to animate an image](/docs/examples/image_to_video)

[### Render a video with Blender

Render an animated 3D scene using Blender's Python interface across many processors in parallel](/docs/examples/blender_video)

[### ControlNet playgrounds

Play with all 10 demo Gradio apps from the ControlNet project](/docs/examples/controlnet_gradio_demos)

[### Run vision-language models with SGLang

Ask questions about images and get back answers from a multimodal model](/docs/examples/sgl_vlm)

[### Stable Diffusion CLI, API, and Web UI

Generate images with diffusion models from the interface of your choosing](/docs/examples/stable_diffusion_cli)

[### Fine-Tuning and Inference for Computer Vision with YOLO

Customize and deploy lightning-fast object detection models](/docs/examples/finetune_yolo)

[### Document OCR job queue

Use Modal as an infinitely scalable job queue that can service async tasks from a web app](/docs/examples/doc_ocr_jobs)

[### LoRAs Galore

Create a LoRA Playground with Modal, Gradio, and S3](/docs/examples/cloud_bucket_mount_loras)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/use-cases/job-queues
================================================================================

Job queues & batch processing
=============================

![](https://modal-cdn.com/use-case-job-queues-header.webp)

An infinitely scalable job queue and a batch job system that can provision
thousands of containers on a schedule, all on Modal.

[Get Started](/signup)

Book a Demo

![](https://modal-cdn.com/use-case-job-queues-header.webp)

[![customer logo](/_app/immutable/assets/Succinct.qq-4sTpt.svg)](https://www.succinct.xyz/)

“Modal made it incredibly easy for us to deploy complex computational jobs that burst up to hundreds of machines. Being able to iterate quickly without having to waste cycles on managing infra was a huge unlock.”

Uma Roy
,
Co-Founder & CEO

[![customer logo](/_app/immutable/assets/ChaiDiscovery.BHpR4uIE.webp)](https://www.chaidiscovery.com/)

“We used Modal to build an inference server for our model, Chai-1, which allows people to predict molecular structures via a web app. Modal allowed us to build and launch the server in days: our engineers didn't have to worry about maintaining infrastructure, delivering the product in record time.”

Jack Dent
,
Co-Founder

[![customer logo](data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='114'%20height='24'%20viewBox='0%200%20114%2024'%20fill='none'%3e%3cg%20clip-path='url(%23clip0_2126_15312)'%3e%3cpath%20d='M104.239%2019.4985H103.058C97.0158%2019.4985%2093.8574%2017.7683%2093.8574%2012.6051V11.3143C93.8574%206.12375%2097.0158%204.39355%20103.058%204.39355H104.239C110.033%204.39355%20112.78%206.23359%20113.329%209.39189H109.402C108.55%207.99126%20107.095%207.46946%20103.827%207.46946C99.7621%207.46946%2097.6749%208.10111%2097.6749%2011.534V12.3579C97.6749%2015.7909%2099.7621%2016.4225%20103.827%2016.4225C107.095%2016.4225%20108.55%2015.9007%20109.402%2014.5001H113.329C112.78%2017.6584%20110.033%2019.4985%20104.239%2019.4985Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M90.9522%200V2.71888H87.1074V0H90.9522ZM90.9522%204.66878V19.2243H87.1074V4.66878H90.9522Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M83.9533%2019.2238H80.1358V10.9848C80.1358%207.93633%2078.1036%207.46946%2075.6044%207.46946C72.3363%207.46946%2070.2765%208.4856%2070.2765%2011.6714V19.2238H66.4316V4.66819H70.2765V7.3596H70.4413C71.2927%205.68433%2073.1327%204.39355%2076.8128%204.39355C80.8225%204.39355%2083.9533%205.51956%2083.9533%2010.3531V19.2238Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M54.286%2019.4985H52.5833C46.5964%2019.4985%2043.3281%2017.7683%2043.3281%2012.6051V11.3143C43.3281%206.12375%2046.5964%204.39355%2052.5833%204.39355H54.286C60.3005%204.39355%2063.5687%206.12375%2063.5687%2011.3143V12.6051C63.5687%2017.7683%2060.3005%2019.4985%2054.286%2019.4985ZM53.4622%2016.4225C57.5542%2016.4225%2059.7512%2015.7909%2059.7512%2012.3579V11.534C59.7512%208.10111%2057.5542%207.46946%2053.4622%207.46946C49.3151%207.46946%2047.1456%208.10111%2047.1456%2011.534V12.3579C47.1456%2015.7909%2049.3151%2016.4225%2053.4622%2016.4225Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M40.6408%2019.2243H36.8235V10.9854C36.8235%207.93692%2034.7911%207.47005%2032.292%207.47005C29.0238%207.47005%2026.964%208.48618%2026.964%2011.6719V19.2243H23.1191V0H26.964V7.36019H27.1288C27.9802%205.68492%2029.8202%204.39415%2033.5003%204.39415C37.51%204.39415%2040.6408%205.52014%2040.6408%2010.3537V19.2243Z'%20fill='%23FFFBFB'/%3e%3cpath%20d='M0%204.66819H3.8174V7.3596H3.92726C4.86101%205.32731%207.05809%204.39355%2011.0678%204.39355H11.8367C17.0823%204.39355%2020.2404%206.12375%2020.2404%2011.2869V12.5776C20.2404%2017.7683%2017.0823%2019.4985%2011.8367%2019.4985H11.0678C7.05809%2019.4985%204.86101%2018.5647%203.92726%2016.5324H3.8174V23.8925H0V4.66819ZM10.1065%207.46946C6.01448%207.46946%203.8174%208.10111%203.8174%2011.4517V12.3579C3.8174%2015.7909%206.01448%2016.4225%2010.1065%2016.4225C14.2535%2016.4225%2016.4231%2015.7909%2016.4231%2012.3579V11.534C16.4231%208.10111%2014.2535%207.46946%2010.1065%207.46946Z'%20fill='%23FFFBFB'/%3e%3c/g%3e%3cdefs%3e%3cclipPath%20id='clip0_2126_15312'%3e%3crect%20width='113.408'%20height='24'%20fill='white'/%3e%3c/clipPath%3e%3c/defs%3e%3c/svg%3e)](https://phonic.co/)

“At Phonic, we train our own proprietary models for audio generation. We moved all our large-scale audio processing batch jobs to Modal. Our engineers are ecstatic with the result – we can run at a much larger scale than before, no longer have to babysit our batch jobs, and we can ship much faster.”

Moin Nadeem
,
Co-Founder

### A flexible batch system

[View Examples](/docs/examples/doc_ocr_jobs)

---

Modal as a job queue

Spawn async jobs and poll for the results later; no need to set up a separate job queue.

---

Integrate with your existing workflow system

Run
[GPU workers from Airflow](/blog/modal-airflow)
or other orchestration tools.

---

Parallel computation

Distribute your work over thousands of GPUs with a single call to .map().

![](https://modal-cdn.com/use-case-function-calls.webp)

---

Modal as a job queue

---

Integrate with your existing workflow system

---

Parallel computation

[View Examples](/docs/examples/doc_ocr_jobs)

### Cost-effective data platform

[View Examples](/docs/examples/dbt_duckdb)

---

Cron jobs on demand

Run code on a schedule
[with a single line of code](/docs/guide/cron)
.

---

Batch processing for data-intensive workloads

Use Modal for all your data processing needs, from video processing to embedding large datasets.

---

Rich web interface for data observability

Monitor runs, send email / Slack notifications on failures, and view logs all in your Modal dashboard.

![](https://modal-cdn.com/use-case-logs.webp)

---

Cron jobs on demand

---

Batch processing for data-intensive workloads

---

Rich web interface for data observability

[View Examples](/docs/examples/dbt_duckdb)

Try it out
----------

[View all](/docs/examples)

[### Transcribe speech in batches with Whisper

Turn audio bytes into text at scale](/docs/examples/batched_whisper)

[### Document OCR job queue

Use Modal as an infinitely scalable job queue that can service async tasks from a web app](/docs/examples/doc_ocr_jobs)

[### Publish interactive datasets with Datasette

Use Datasette to serve dataset exploration UIs on Modal](/docs/examples/cron_datasette)

[### Build your own data warehouse

Use DuckDB and dbt from Modal to build a simple OLAP system](/docs/examples/dbt_duckdb)

[### Parallel processing of Parquet files on S3

Analyze data from the Taxi and Limousine Commission of NYC in parallel](/docs/examples/s3_bucket_mount)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/use-cases/language-models
================================================================================

Language model inference
========================

![](https://modal-cdn.com/use-case-language-models-header.webp)

Run the latest open-source LLM and embedding models with Modal's
serverless GPUs.

[Get Started](/signup)

Book a Demo

![](https://modal-cdn.com/use-case-language-models-header.webp)

[![customer logo](data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='113'%20height='30'%20viewBox='0%200%20113%2030'%20fill='none'%3e%3cpath%20d='M7.78157%2010.1212C5.1079%2010.1212%203.79102%2012.481%203.79102%2015.6408V23.5604H0V6.88134H3.71121V11.2011H3.79102C4.58913%208.52125%206.14545%206.40137%208.57968%206.40137C10.2956%206.40137%2011.0139%207.00133%2011.0139%207.00133L9.29798%2010.4411C9.29798%2010.4411%208.77921%2010.1212%207.78157%2010.1212ZM53.3936%2012.441V23.6004H49.7222V13.8009C49.7222%2011.0011%2048.8443%209.52119%2046.6096%209.52119C44.2951%209.52119%2043.1777%2011.4011%2043.1777%2015.0009V23.6004H39.5064V13.8009C39.5064%2011.1211%2038.6285%209.52119%2036.4337%209.52119C33.8798%209.52119%2032.922%2011.7611%2032.922%2015.0009V23.6004H29.2108V6.88134H32.922V10.6811H32.9619C33.5206%208.08127%2035.1168%206.44137%2037.7905%206.44137C40.4243%206.44137%2042.1402%207.88128%2042.8186%2010.4011C43.4571%207.96128%2045.0932%206.44137%2047.6472%206.44137C51.1189%206.44137%2053.3936%208.64124%2053.3936%2012.441ZM18.4363%206.40137C15.0045%206.40137%2012.8097%208.00128%2011.7721%2010.9211L14.9247%2012.081C15.4833%2010.3211%2016.6805%209.3212%2018.5162%209.3212C20.5513%209.3212%2021.7884%2010.2411%2021.7884%2011.6011C21.7884%2013.001%2020.8307%2013.321%2018.6758%2013.681C16.2814%2014.0809%2010.5351%2014.2009%2010.5351%2019.0406C10.5351%2021.8805%2012.8895%2024.0004%2016.4411%2024.0004C19.1147%2024.0004%2020.9504%2022.9204%2021.7884%2020.8805H21.8283V23.6004H25.4996V13.321C25.4597%208.80123%2023.225%206.40137%2018.4363%206.40137ZM21.8682%2015.7208C21.8682%2019.2406%2020.1523%2021.4805%2017.3589%2021.4805C15.4035%2021.4805%2014.2463%2020.4006%2014.2463%2018.8007C14.2463%2017.3207%2015.4434%2016.2808%2017.7579%2015.8408C20.1124%2015.4009%2021.3095%2014.8809%2021.8682%2013.561V15.7208ZM66.4826%206.44137C63.6892%206.44137%2061.8136%208.00128%2061.0155%2010.3211V6.88134H57.1048V30H60.9756V20.1606H61.0155C61.8934%2022.6804%2063.6892%2024.0804%2066.4826%2024.0804C70.952%2024.0804%2074.1444%2020.4006%2074.1444%2015.2009C74.1444%2010.0012%2070.952%206.44137%2066.4826%206.44137ZM65.5248%2020.9605C62.4521%2020.9605%2060.7362%2018.7207%2060.7362%2015.2409C60.7362%2011.7611%2062.6516%209.52119%2065.5248%209.52119C68.398%209.52119%2070.3135%2011.8811%2070.3135%2015.2409C70.3135%2018.6007%2068.398%2020.9605%2065.5248%2020.9605Z'%20fill='%23E7FB21'/%3e%3cpath%20d='M112.493%2023.4787V23.5987H97.3688V23.4787C99.5636%2022.2387%20101.04%2020.9988%20102.397%2019.6789H108.622L112.493%2023.4787ZM108.742%203.75979L104.911%200H104.791C104.791%200%20104.871%206.9996%2098.4063%2013.3992C92.1013%2019.6389%2084.7188%2019.6789%2084.7188%2019.6789V19.7989L88.6295%2023.6387C88.6295%2023.6387%2095.9322%2023.7187%20102.357%2017.359C108.782%2011.0394%20108.742%203.75979%20108.742%203.75979Z'%20fill='%23E7FB21'/%3e%3c/svg%3e)](https://www.ramp.com/)

“Modal's user-friendly interface and efficient tools have truly empowered our team to navigate data-intensive tasks with ease, enabling us to achieve our project goals more efficiently.”

Karim Atiyeh
,
Co-Founder & CTO

[![customer logo](/_app/immutable/assets/Structify.BLcIxYcE.webp)](https://www.structify.ai/)

“Switched to Modal for our LLM inference instead of Azure. 1/4 the price for GPUs and so much simpler to set up/scale. Big fan.”

Alex Reichenbach
,
CEO

[![customer logo](data:image/webp;base64,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)](https://www.datalab.to)

“Using Modal for inference is like having an extra infra team - it’s reliable, scalable, and fast - meaning I can get back to training models”

Vik Paruchari
,
Founder

### GPUs on demand

[View Examples](/docs/guide/gpu)

---

Top of the line hardware

Access A100s and H100s to run the latest and largest models, like Llama3-405B.

---

Cheaper than running your own cluster

No more paying for idle GPUs.

---

Seamless autoscaling

When your app gets an influx of traffic, Modal scales with you.

![](https://modal-cdn.com/use-case-gpu-code.webp)

---

Top of the line hardware

---

Cheaper than running your own cluster

---

Seamless autoscaling

[View Examples](/docs/guide/gpu)

### Blazing-fast performance

[View Examples](/docs/examples/vllm_inference)

---

Fast cold starts

Load gigabytes of weights in seconds with our optimized container file system and engine.

---

Support for inference engines

Easily run any framework or model on Modal (e.g. TensorRT and vLLM).

---

Dynamic batching

Use Modal's batching feature to process requests in dynamically-sized batches.

![](https://modal-cdn.com/use-case-logs.webp)

---

Fast cold starts

---

Support for inference engines

---

Dynamic batching

[View Examples](/docs/examples/vllm_inference)

### Best-in-class developer experience

---

Metrics and observability

Visualize and debug failures.

---

Monitor resource utilization

Track your usage and spending in real-time.

---

Ready for production

Support for webhooks, batching, and token streaming.

![](https://modal-cdn.com/use-case-active-resources.webp)

---

Metrics and observability

---

Monitor resource utilization

---

Ready for production

Try it out
----------

[View all](/docs/examples)

[### Deploy an OpenAI-compatible LLM service

Run large language models with a drop-in replacement for the OpenAI API](/docs/examples/vllm_inference)

[### Run llama.cpp

Run DeepSeek-R1 and Phi-4 on llama.cpp](/docs/examples/llama_cpp)

[### Serverless TensorRT-LLM (LLaMA 3 8B)

Run interactive language model applications](/docs/examples/trtllm_latency)

[### RAG Chat with PDFs

Use ColBERT-style, multimodal embeddings with a Vision-Language Model to answer questions about documents](/docs/examples/chat_with_pdf_vision)

[### Fine-tune FLAN-T5

Fine-tune a small language model for summarization](/docs/examples/flan_t5_finetune)

[### Run vision-language models with SGLang

Ask questions about images and get back answers from a multimodal model](/docs/examples/sgl_vlm)

[### Structured data extraction using Instructor

Extract structured data from unstructured text using LLMs](/docs/examples/instructor_generate)

[### Enforcing JSON outputs on LLMs

Guarantee that language model outputs match a JSON schema](/docs/examples/outlines_generate)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025


================================================================================
SOURCE URL: https://modal.com/use-cases/sandboxes
================================================================================

Modal Sandboxes
===============

![](https://modal-cdn.com/use-case-sandboxes-header.webp)

Run AI-generated code in secure, dynamically defined sandboxes. Scale to
10,000+ simultaneous sessions with ease.

[Get Started](/signup)

Book a Demo

![](https://modal-cdn.com/use-case-sandboxes-header.webp)

[![customer logo](/_app/immutable/assets/Codegen.ibfW0-BL.svg)](https://www.codegen.com/)

“Using Modal, Codegen has been able to move at lightning speed with full-stack AI development. The product is designed with developer experience front and center, and my team is incredibly happy having it as part of our arsenal.”

Jay Hack
,
Founder & CEO

[![customer logo](/_app/immutable/assets/Basis.DNhnLXGd.svg)](https://www.getbasis.ai/)

“We use Modal to securely run LLM-augmented code on a large scale. Modal’s powerful primitives like sandboxes and file systems have allowed us to focus on our core competencies without having to waste time on our own infra.”

Matt Harpe
,
Co-Founder

[![customer logo](/_app/immutable/assets/Mistral.BuBT5ERP.svg)](https://mistral.ai/en)

“Modal Sandboxes enable us to execute generated code securely and flexibly. With Modal's support, we expedited the development of our code interpreter feature and successfully integrated it into our chat platform, Le Chat, to better assist our users.”

Wendy Shang
,
AI Scientist

### Modal Sandbox Pricing

Only pay for what you use, by the CPU cycle. For GPU Sandboxes, refer to
our standard GPU prices.

[View full pricing](/pricing)

### Compute costs

Per hour

Per second

Per hour

Per second

---

CPU

Physical core

(2 vCPU
equivalent
)

$0.00003942
/ core / sec

\*minimum of 0.125 cores per container

---

Memory

$0.00000672
/ GiB / sec

### Safe code execution

[View Examples](/docs/examples/simple_code_interpreter)

---

Execution of AI-generated code

Run code snippets created by language models or submitted by users in a secure environment.

---

LLM evaluations

Test the latest LLM models for safety and quality.

---

gVisor-based runtime

Rely on strict security and isolation guarantees from our battle-tested containerization technology.

![](https://modal-cdn.com/use-case-sandbox-code.webp)

---

Execution of AI-generated code

---

LLM evaluations

---

gVisor-based runtime

[View Examples](/docs/examples/simple_code_interpreter)

### Fully programmable container environment

[View Examples](/docs/guide/sandbox)

---

Universal code execution

Orchestrate and run any kind of code or container image, even if it's not Python-based.

---

Out of the box interactivity

Execute processes in an ongoing sandbox session, and get input/output streaming with each.

![](https://modal-cdn.com/use-case-tensor-rt.webp)

---

Universal code execution

---

Out of the box interactivity

[View Examples](/docs/guide/sandbox)

### Advanced sandbox features

[View Examples](/docs/guide/tunnels)

---

Persistent data management

Maintain stateful data across sessions with dynamically-provisioned network file systems.

---

Network accessibility

Create tunnels to expose ports inside the sandbox.

---

Network security

Control over outbound networking.

![](https://modal-cdn.com/use-case-tunneling.webp)

---

Persistent data management

---

Network accessibility

---

Network security

[View Examples](/docs/guide/tunnels)

Try it out
----------

[View all](/docs/examples)

[### Sandbox a LangGraph agent's code

Run an LLM coding agent that runs its own language models](/docs/examples/agent)

[### Build a stateful, sandboxed code interpreter

Execute untrusted Python code in a sandboxed environment](/docs/examples/simple_code_interpreter)

[### Safely run untrusted Node.js, Ruby, PHP, and more

Execute arbitrary code in your language of choice inside a Sandbox](/docs/examples/safe_code_execution)

[### Run a sandboxed Jupyter notebook

Set up a sandboxed Jupyter server and issue commands via an API](/docs/examples/jupyter_sandbox)

[![

](https://modal-cdn.com/tmpk5brmwoa_0b5eb343.webp)](https://modal-cdn.com/landscape-vids/Modal_CTA.mp4)

Ship your first app in
minutes.
-------------------------------

[Get Started](/signup)

$30 / month free compute

[![Modal logo](data:image/svg+xml,%3csvg%20width='368'%20height='192'%20viewBox='0%200%20368%20192'%20fill='none'%20xmlns='http://www.w3.org/2000/svg'%3e%3cpath%20d='M148.873%204L183.513%2064L111.922%20188C110.492%20190.47%20107.853%20192%20104.993%20192H40.3325C38.9025%20192%2037.5325%20191.62%2036.3325%20190.93C35.1325%20190.24%2034.1226%20189.24%2033.4026%20188L1.0725%20132C-0.3575%20129.53%20-0.3575%20126.48%201.0725%20124L70.3625%204C71.0725%202.76%2072.0925%201.76001%2073.2925%201.07001C74.4925%200.380007%2075.8625%200%2077.2925%200H141.952C144.812%200%20147.453%201.53%20148.883%204H148.873ZM365.963%20124L296.672%204C295.962%202.76%20294.943%201.76001%20293.743%201.07001C292.543%200.380007%20291.173%200%20289.743%200H225.083C222.223%200%20219.583%201.53%20218.153%204L183.513%2064L255.103%20188C256.533%20190.47%20259.173%20192%20262.033%20192H326.693C328.122%20192%20329.492%20191.62%20330.693%20190.93C331.893%20190.24%20332.902%20189.24%20333.622%20188L365.953%20132C367.383%20129.53%20367.383%20126.48%20365.953%20124H365.963Z'%20fill='%2362DE61'/%3e%3cpath%20d='M109.623%2064H183.523L148.883%204C147.453%201.53%20144.813%200%20141.953%200H77.2925C75.8625%200%2074.4925%200.380007%2073.2925%201.07001L109.623%2064Z'%20fill='url(%23paint0_linear_342_139)'/%3e%3cpath%20d='M109.623%2064L73.2925%201.07001C72.0925%201.76001%2071.0825%202.76%2070.3625%204L1.0725%20124C-0.3575%20126.48%20-0.3575%20129.52%201.0725%20132L33.4026%20188C34.1126%20189.24%2035.1325%20190.24%2036.3325%20190.93L109.613%2064H109.623Z'%20fill='url(%23paint1_linear_342_139)'/%3e%3cpath%20d='M183.513%2064H109.613L36.3325%20190.93C37.5325%20191.62%2038.9025%20192%2040.3325%20192H104.993C107.853%20192%20110.492%20190.47%20111.922%20188L183.513%2064Z'%20fill='%2309AF58'/%3e%3cpath%20d='M365.963%20132C366.673%20130.76%20367.033%20129.38%20367.033%20128H294.372L258.042%20190.93C259.242%20191.62%20260.612%20192%20262.042%20192H326.703C329.563%20192%20332.202%20190.47%20333.632%20188L365.963%20132Z'%20fill='%2309AF58'/%3e%3cpath%20d='M225.083%200C223.653%200%20222.283%200.380007%20221.083%201.07001L294.362%20128H367.023C367.023%20126.62%20366.663%20125.24%20365.953%20124L296.672%204C295.242%201.53%20292.603%200%20289.743%200H225.073H225.083Z'%20fill='url(%23paint2_linear_342_139)'/%3e%3cpath%20d='M258.033%20190.93L294.362%20128L221.083%201.07001C219.883%201.76001%20218.873%202.76%20218.153%204L183.513%2064L255.103%20188C255.813%20189.24%20256.833%20190.24%20258.033%20190.93Z'%20fill='url(%23paint3_linear_342_139)'/%3e%3cdefs%3e%3clinearGradient%20id='paint0_linear_342_139'%20x1='155.803'%20y1='80'%20x2='101.003'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint1_linear_342_139'%20x1='8.62251'%20y1='174.93'%20x2='100.072'%20y2='16.54'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint2_linear_342_139'%20x1='340.243'%20y1='143.46'%20x2='248.793'%20y2='-14.93'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%23BFF9B4'/%3e%3cstop%20offset='1'%20stop-color='%2380EE64'/%3e%3c/linearGradient%3e%3clinearGradient%20id='paint3_linear_342_139'%20x1='284.822'%20y1='175.47'%20x2='193.372'%20y2='17.0701'%20gradientUnits='userSpaceOnUse'%3e%3cstop%20stop-color='%2380EE64'/%3e%3cstop%20offset='0.18'%20stop-color='%237BEB63'/%3e%3cstop%20offset='0.36'%20stop-color='%236FE562'/%3e%3cstop%20offset='0.55'%20stop-color='%235ADA60'/%3e%3cstop%20offset='0.74'%20stop-color='%233DCA5D'/%3e%3cstop%20offset='0.93'%20stop-color='%2318B759'/%3e%3cstop%20offset='1'%20stop-color='%2309AF58'/%3e%3c/linearGradient%3e%3c/defs%3e%3c/svg%3e)](/)

© Modal 2025

Use Cases

[Language Model Inference](/use-cases/language-models)

[Image, Video & 3D](/use-cases/image-video-3d)

[Audio Processing](/use-cases/audio)

[Fine-Tuning](/use-cases/fine-tuning)

[Job Queues & Batch Processing](/use-cases/job-queues)

[Sandboxing Code](/use-cases/sandboxes)

[Computational Biology](/use-cases/comp-bio)

Resources

[Documentation](/docs/guide)

[Pricing](/pricing)

[Slack Community](/slack)

[Articles](/articles)

[GPU Glossary](/gpu-glossary)

[LLM Engine Advisor](/llm-almanac)

[Model Library](/library)

Popular Examples

[Serve LLM APIs with vLLM](/docs/examples/vllm_inference)

[Create custom art of your pet](/docs/examples/dreambooth_app)

[Analyze Parquet files from S3 with DuckDB](/docs/examples/s3_bucket_mount)

[Run hundreds of LoRAs from one app](/docs/examples/cloud_bucket_mount_loras)

[Finetune an LLM to replace your CEO](/docs/examples/llm-finetuning)

Company

[About](/company)

[Blog](/blog)

[Careers](/careers)

[Privacy Policy](/legal/privacy-policy)

[Security & Privacy](/docs/guide/security)

[Terms](/legal/terms)

© Modal 2025
